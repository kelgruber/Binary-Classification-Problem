<!DOCTYPE html>

<html lang="en">
<head><meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Kel_Gruber_Final_Phase_2</title><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
<style type="text/css">
    pre { line-height: 125%; }
td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
.highlight .hll { background-color: var(--jp-cell-editor-active-background) }
.highlight { background: var(--jp-cell-editor-background); color: var(--jp-mirror-editor-variable-color) }
.highlight .c { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment */
.highlight .err { color: var(--jp-mirror-editor-error-color) } /* Error */
.highlight .k { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword */
.highlight .o { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator */
.highlight .p { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation */
.highlight .ch { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Multiline */
.highlight .cp { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Preproc */
.highlight .cpf { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Single */
.highlight .cs { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Special */
.highlight .kc { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Pseudo */
.highlight .kr { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Type */
.highlight .m { color: var(--jp-mirror-editor-number-color) } /* Literal.Number */
.highlight .s { color: var(--jp-mirror-editor-string-color) } /* Literal.String */
.highlight .ow { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator.Word */
.highlight .pm { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation.Marker */
.highlight .w { color: var(--jp-mirror-editor-variable-color) } /* Text.Whitespace */
.highlight .mb { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Bin */
.highlight .mf { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Float */
.highlight .mh { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Hex */
.highlight .mi { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer */
.highlight .mo { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Oct */
.highlight .sa { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Affix */
.highlight .sb { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Backtick */
.highlight .sc { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Char */
.highlight .dl { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Delimiter */
.highlight .sd { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Doc */
.highlight .s2 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Double */
.highlight .se { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Escape */
.highlight .sh { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Heredoc */
.highlight .si { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Interpol */
.highlight .sx { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Other */
.highlight .sr { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Regex */
.highlight .s1 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Single */
.highlight .ss { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Symbol */
.highlight .il { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer.Long */
  </style>
<style type="text/css">
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*
 * Mozilla scrollbar styling
 */

/* use standard opaque scrollbars for most nodes */
[data-jp-theme-scrollbars='true'] {
  scrollbar-color: rgb(var(--jp-scrollbar-thumb-color))
    var(--jp-scrollbar-background-color);
}

/* for code nodes, use a transparent style of scrollbar. These selectors
 * will match lower in the tree, and so will override the above */
[data-jp-theme-scrollbars='true'] .CodeMirror-hscrollbar,
[data-jp-theme-scrollbars='true'] .CodeMirror-vscrollbar {
  scrollbar-color: rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent;
}

/* tiny scrollbar */

.jp-scrollbar-tiny {
  scrollbar-color: rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent;
  scrollbar-width: thin;
}

/* tiny scrollbar */

.jp-scrollbar-tiny::-webkit-scrollbar,
.jp-scrollbar-tiny::-webkit-scrollbar-corner {
  background-color: transparent;
  height: 4px;
  width: 4px;
}

.jp-scrollbar-tiny::-webkit-scrollbar-thumb {
  background: rgba(var(--jp-scrollbar-thumb-color), 0.5);
}

.jp-scrollbar-tiny::-webkit-scrollbar-track:horizontal {
  border-left: 0 solid transparent;
  border-right: 0 solid transparent;
}

.jp-scrollbar-tiny::-webkit-scrollbar-track:vertical {
  border-top: 0 solid transparent;
  border-bottom: 0 solid transparent;
}

/*
 * Lumino
 */

.lm-ScrollBar[data-orientation='horizontal'] {
  min-height: 16px;
  max-height: 16px;
  min-width: 45px;
  border-top: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='vertical'] {
  min-width: 16px;
  max-width: 16px;
  min-height: 45px;
  border-left: 1px solid #a0a0a0;
}

.lm-ScrollBar-button {
  background-color: #f0f0f0;
  background-position: center center;
  min-height: 15px;
  max-height: 15px;
  min-width: 15px;
  max-width: 15px;
}

.lm-ScrollBar-button:hover {
  background-color: #dadada;
}

.lm-ScrollBar-button.lm-mod-active {
  background-color: #cdcdcd;
}

.lm-ScrollBar-track {
  background: #f0f0f0;
}

.lm-ScrollBar-thumb {
  background: #cdcdcd;
}

.lm-ScrollBar-thumb:hover {
  background: #bababa;
}

.lm-ScrollBar-thumb.lm-mod-active {
  background: #a0a0a0;
}

.lm-ScrollBar[data-orientation='horizontal'] .lm-ScrollBar-thumb {
  height: 100%;
  min-width: 15px;
  border-left: 1px solid #a0a0a0;
  border-right: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='vertical'] .lm-ScrollBar-thumb {
  width: 100%;
  min-height: 15px;
  border-top: 1px solid #a0a0a0;
  border-bottom: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='horizontal']
  .lm-ScrollBar-button[data-action='decrement'] {
  background-image: var(--jp-icon-caret-left);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='horizontal']
  .lm-ScrollBar-button[data-action='increment'] {
  background-image: var(--jp-icon-caret-right);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='vertical']
  .lm-ScrollBar-button[data-action='decrement'] {
  background-image: var(--jp-icon-caret-up);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='vertical']
  .lm-ScrollBar-button[data-action='increment'] {
  background-image: var(--jp-icon-caret-down);
  background-size: 17px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-Widget {
  box-sizing: border-box;
  position: relative;
  overflow: hidden;
}

.lm-Widget.lm-mod-hidden {
  display: none !important;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.lm-AccordionPanel[data-orientation='horizontal'] > .lm-AccordionPanel-title {
  /* Title is rotated for horizontal accordion panel using CSS */
  display: block;
  transform-origin: top left;
  transform: rotate(-90deg) translate(-100%);
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-CommandPalette {
  display: flex;
  flex-direction: column;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-CommandPalette-search {
  flex: 0 0 auto;
}

.lm-CommandPalette-content {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  min-height: 0;
  overflow: auto;
  list-style-type: none;
}

.lm-CommandPalette-header {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.lm-CommandPalette-item {
  display: flex;
  flex-direction: row;
}

.lm-CommandPalette-itemIcon {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemContent {
  flex: 1 1 auto;
  overflow: hidden;
}

.lm-CommandPalette-itemShortcut {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemLabel {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.lm-close-icon {
  border: 1px solid transparent;
  background-color: transparent;
  position: absolute;
  z-index: 1;
  right: 3%;
  top: 0;
  bottom: 0;
  margin: auto;
  padding: 7px 0;
  display: none;
  vertical-align: middle;
  outline: 0;
  cursor: pointer;
}
.lm-close-icon:after {
  content: 'X';
  display: block;
  width: 15px;
  height: 15px;
  text-align: center;
  color: #000;
  font-weight: normal;
  font-size: 12px;
  cursor: pointer;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-DockPanel {
  z-index: 0;
}

.lm-DockPanel-widget {
  z-index: 0;
}

.lm-DockPanel-tabBar {
  z-index: 1;
}

.lm-DockPanel-handle {
  z-index: 2;
}

.lm-DockPanel-handle.lm-mod-hidden {
  display: none !important;
}

.lm-DockPanel-handle:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  content: '';
}

.lm-DockPanel-handle[data-orientation='horizontal'] {
  cursor: ew-resize;
}

.lm-DockPanel-handle[data-orientation='vertical'] {
  cursor: ns-resize;
}

.lm-DockPanel-handle[data-orientation='horizontal']:after {
  left: 50%;
  min-width: 8px;
  transform: translateX(-50%);
}

.lm-DockPanel-handle[data-orientation='vertical']:after {
  top: 50%;
  min-height: 8px;
  transform: translateY(-50%);
}

.lm-DockPanel-overlay {
  z-index: 3;
  box-sizing: border-box;
  pointer-events: none;
}

.lm-DockPanel-overlay.lm-mod-hidden {
  display: none !important;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-Menu {
  z-index: 10000;
  position: absolute;
  white-space: nowrap;
  overflow-x: hidden;
  overflow-y: auto;
  outline: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-Menu-content {
  margin: 0;
  padding: 0;
  display: table;
  list-style-type: none;
}

.lm-Menu-item {
  display: table-row;
}

.lm-Menu-item.lm-mod-hidden,
.lm-Menu-item.lm-mod-collapsed {
  display: none !important;
}

.lm-Menu-itemIcon,
.lm-Menu-itemSubmenuIcon {
  display: table-cell;
  text-align: center;
}

.lm-Menu-itemLabel {
  display: table-cell;
  text-align: left;
}

.lm-Menu-itemShortcut {
  display: table-cell;
  text-align: right;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-MenuBar {
  outline: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-MenuBar-content {
  margin: 0;
  padding: 0;
  display: flex;
  flex-direction: row;
  list-style-type: none;
}

.lm-MenuBar-item {
  box-sizing: border-box;
}

.lm-MenuBar-itemIcon,
.lm-MenuBar-itemLabel {
  display: inline-block;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-ScrollBar {
  display: flex;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-ScrollBar[data-orientation='horizontal'] {
  flex-direction: row;
}

.lm-ScrollBar[data-orientation='vertical'] {
  flex-direction: column;
}

.lm-ScrollBar-button {
  box-sizing: border-box;
  flex: 0 0 auto;
}

.lm-ScrollBar-track {
  box-sizing: border-box;
  position: relative;
  overflow: hidden;
  flex: 1 1 auto;
}

.lm-ScrollBar-thumb {
  box-sizing: border-box;
  position: absolute;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-SplitPanel-child {
  z-index: 0;
}

.lm-SplitPanel-handle {
  z-index: 1;
}

.lm-SplitPanel-handle.lm-mod-hidden {
  display: none !important;
}

.lm-SplitPanel-handle:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  content: '';
}

.lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle {
  cursor: ew-resize;
}

.lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle {
  cursor: ns-resize;
}

.lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle:after {
  left: 50%;
  min-width: 8px;
  transform: translateX(-50%);
}

.lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle:after {
  top: 50%;
  min-height: 8px;
  transform: translateY(-50%);
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-TabBar {
  display: flex;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-TabBar[data-orientation='horizontal'] {
  flex-direction: row;
  align-items: flex-end;
}

.lm-TabBar[data-orientation='vertical'] {
  flex-direction: column;
  align-items: flex-end;
}

.lm-TabBar-content {
  margin: 0;
  padding: 0;
  display: flex;
  flex: 1 1 auto;
  list-style-type: none;
}

.lm-TabBar[data-orientation='horizontal'] > .lm-TabBar-content {
  flex-direction: row;
}

.lm-TabBar[data-orientation='vertical'] > .lm-TabBar-content {
  flex-direction: column;
}

.lm-TabBar-tab {
  display: flex;
  flex-direction: row;
  box-sizing: border-box;
  overflow: hidden;
  touch-action: none; /* Disable native Drag/Drop */
}

.lm-TabBar-tabIcon,
.lm-TabBar-tabCloseIcon {
  flex: 0 0 auto;
}

.lm-TabBar-tabLabel {
  flex: 1 1 auto;
  overflow: hidden;
  white-space: nowrap;
}

.lm-TabBar-tabInput {
  user-select: all;
  width: 100%;
  box-sizing: border-box;
}

.lm-TabBar-tab.lm-mod-hidden {
  display: none !important;
}

.lm-TabBar-addButton.lm-mod-hidden {
  display: none !important;
}

.lm-TabBar.lm-mod-dragging .lm-TabBar-tab {
  position: relative;
}

.lm-TabBar.lm-mod-dragging[data-orientation='horizontal'] .lm-TabBar-tab {
  left: 0;
  transition: left 150ms ease;
}

.lm-TabBar.lm-mod-dragging[data-orientation='vertical'] .lm-TabBar-tab {
  top: 0;
  transition: top 150ms ease;
}

.lm-TabBar.lm-mod-dragging .lm-TabBar-tab.lm-mod-dragging {
  transition: none;
}

.lm-TabBar-tabLabel .lm-TabBar-tabInput {
  user-select: all;
  width: 100%;
  box-sizing: border-box;
  background: inherit;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-TabPanel-tabBar {
  z-index: 1;
}

.lm-TabPanel-stackedPanel {
  z-index: 0;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Collapse {
  display: flex;
  flex-direction: column;
  align-items: stretch;
}

.jp-Collapse-header {
  padding: 1px 12px;
  background-color: var(--jp-layout-color1);
  border-bottom: solid var(--jp-border-width) var(--jp-border-color2);
  color: var(--jp-ui-font-color1);
  cursor: pointer;
  display: flex;
  align-items: center;
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  text-transform: uppercase;
  user-select: none;
}

.jp-Collapser-icon {
  height: 16px;
}

.jp-Collapse-header-collapsed .jp-Collapser-icon {
  transform: rotate(-90deg);
  margin: auto 0;
}

.jp-Collapser-title {
  line-height: 25px;
}

.jp-Collapse-contents {
  padding: 0 12px;
  background-color: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  overflow: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* This file was auto-generated by ensureUiComponents() in @jupyterlab/buildutils */

/**
 * (DEPRECATED) Support for consuming icons as CSS background images
 */

/* Icons urls */

:root {
  --jp-icon-add-above: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgY2xpcC1wYXRoPSJ1cmwoI2NsaXAwXzEzN18xOTQ5MikiPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGQ9Ik00Ljc1IDQuOTMwNjZINi42MjVWNi44MDU2NkM2LjYyNSA3LjAxMTkxIDYuNzkzNzUgNy4xODA2NiA3IDcuMTgwNjZDNy4yMDYyNSA3LjE4MDY2IDcuMzc1IDcuMDExOTEgNy4zNzUgNi44MDU2NlY0LjkzMDY2SDkuMjVDOS40NTYyNSA0LjkzMDY2IDkuNjI1IDQuNzYxOTEgOS42MjUgNC41NTU2NkM5LjYyNSA0LjM0OTQxIDkuNDU2MjUgNC4xODA2NiA5LjI1IDQuMTgwNjZINy4zNzVWMi4zMDU2NkM3LjM3NSAyLjA5OTQxIDcuMjA2MjUgMS45MzA2NiA3IDEuOTMwNjZDNi43OTM3NSAxLjkzMDY2IDYuNjI1IDIuMDk5NDEgNi42MjUgMi4zMDU2NlY0LjE4MDY2SDQuNzVDNC41NDM3NSA0LjE4MDY2IDQuMzc1IDQuMzQ5NDEgNC4zNzUgNC41NTU2NkM0LjM3NSA0Ljc2MTkxIDQuNTQzNzUgNC45MzA2NiA0Ljc1IDQuOTMwNjZaIiBmaWxsPSIjNjE2MTYxIiBzdHJva2U9IiM2MTYxNjEiIHN0cm9rZS13aWR0aD0iMC43Ii8+CjwvZz4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTExLjUgOS41VjExLjVMMi41IDExLjVWOS41TDExLjUgOS41Wk0xMiA4QzEyLjU1MjMgOCAxMyA4LjQ0NzcyIDEzIDlWMTJDMTMgMTIuNTUyMyAxMi41NTIzIDEzIDEyIDEzTDIgMTNDMS40NDc3MiAxMyAxIDEyLjU1MjMgMSAxMlY5QzEgOC40NDc3MiAxLjQ0NzcxIDggMiA4TDEyIDhaIiBmaWxsPSIjNjE2MTYxIi8+CjxkZWZzPgo8Y2xpcFBhdGggaWQ9ImNsaXAwXzEzN18xOTQ5MiI+CjxyZWN0IGNsYXNzPSJqcC1pY29uMyIgd2lkdGg9IjYiIGhlaWdodD0iNiIgZmlsbD0id2hpdGUiIHRyYW5zZm9ybT0ibWF0cml4KC0xIDAgMCAxIDEwIDEuNTU1NjYpIi8+CjwvY2xpcFBhdGg+CjwvZGVmcz4KPC9zdmc+Cg==);
  --jp-icon-add-below: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgY2xpcC1wYXRoPSJ1cmwoI2NsaXAwXzEzN18xOTQ5OCkiPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGQ9Ik05LjI1IDEwLjA2OTNMNy4zNzUgMTAuMDY5M0w3LjM3NSA4LjE5NDM0QzcuMzc1IDcuOTg4MDkgNy4yMDYyNSA3LjgxOTM0IDcgNy44MTkzNEM2Ljc5Mzc1IDcuODE5MzQgNi42MjUgNy45ODgwOSA2LjYyNSA4LjE5NDM0TDYuNjI1IDEwLjA2OTNMNC43NSAxMC4wNjkzQzQuNTQzNzUgMTAuMDY5MyA0LjM3NSAxMC4yMzgxIDQuMzc1IDEwLjQ0NDNDNC4zNzUgMTAuNjUwNiA0LjU0Mzc1IDEwLjgxOTMgNC43NSAxMC44MTkzTDYuNjI1IDEwLjgxOTNMNi42MjUgMTIuNjk0M0M2LjYyNSAxMi45MDA2IDYuNzkzNzUgMTMuMDY5MyA3IDEzLjA2OTNDNy4yMDYyNSAxMy4wNjkzIDcuMzc1IDEyLjkwMDYgNy4zNzUgMTIuNjk0M0w3LjM3NSAxMC44MTkzTDkuMjUgMTAuODE5M0M5LjQ1NjI1IDEwLjgxOTMgOS42MjUgMTAuNjUwNiA5LjYyNSAxMC40NDQzQzkuNjI1IDEwLjIzODEgOS40NTYyNSAxMC4wNjkzIDkuMjUgMTAuMDY5M1oiIGZpbGw9IiM2MTYxNjEiIHN0cm9rZT0iIzYxNjE2MSIgc3Ryb2tlLXdpZHRoPSIwLjciLz4KPC9nPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMi41IDUuNUwyLjUgMy41TDExLjUgMy41TDExLjUgNS41TDIuNSA1LjVaTTIgN0MxLjQ0NzcyIDcgMSA2LjU1MjI4IDEgNkwxIDNDMSAyLjQ0NzcyIDEuNDQ3NzIgMiAyIDJMMTIgMkMxMi41NTIzIDIgMTMgMi40NDc3MiAxMyAzTDEzIDZDMTMgNi41NTIyOSAxMi41NTIzIDcgMTIgN0wyIDdaIiBmaWxsPSIjNjE2MTYxIi8+CjxkZWZzPgo8Y2xpcFBhdGggaWQ9ImNsaXAwXzEzN18xOTQ5OCI+CjxyZWN0IGNsYXNzPSJqcC1pY29uMyIgd2lkdGg9IjYiIGhlaWdodD0iNiIgZmlsbD0id2hpdGUiIHRyYW5zZm9ybT0ibWF0cml4KDEgMS43NDg0NmUtMDcgMS43NDg0NmUtMDcgLTEgNCAxMy40NDQzKSIvPgo8L2NsaXBQYXRoPgo8L2RlZnM+Cjwvc3ZnPgo=);
  --jp-icon-add: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDEzaC02djZoLTJ2LTZINXYtMmg2VjVoMnY2aDZ2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-bell: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE2IDE2IiB2ZXJzaW9uPSIxLjEiPgogICA8cGF0aCBjbGFzcz0ianAtaWNvbjIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMzMzMzMzIgogICAgICBkPSJtOCAwLjI5Yy0xLjQgMC0yLjcgMC43My0zLjYgMS44LTEuMiAxLjUtMS40IDMuNC0xLjUgNS4yLTAuMTggMi4yLTAuNDQgNC0yLjMgNS4zbDAuMjggMS4zaDVjMC4wMjYgMC42NiAwLjMyIDEuMSAwLjcxIDEuNSAwLjg0IDAuNjEgMiAwLjYxIDIuOCAwIDAuNTItMC40IDAuNi0xIDAuNzEtMS41aDVsMC4yOC0xLjNjLTEuOS0wLjk3LTIuMi0zLjMtMi4zLTUuMy0wLjEzLTEuOC0wLjI2LTMuNy0xLjUtNS4yLTAuODUtMS0yLjItMS44LTMuNi0xLjh6bTAgMS40YzAuODggMCAxLjkgMC41NSAyLjUgMS4zIDAuODggMS4xIDEuMSAyLjcgMS4yIDQuNCAwLjEzIDEuNyAwLjIzIDMuNiAxLjMgNS4yaC0xMGMxLjEtMS42IDEuMi0zLjQgMS4zLTUuMiAwLjEzLTEuNyAwLjMtMy4zIDEuMi00LjQgMC41OS0wLjcyIDEuNi0xLjMgMi41LTEuM3ptLTAuNzQgMTJoMS41Yy0wLjAwMTUgMC4yOCAwLjAxNSAwLjc5LTAuNzQgMC43OS0wLjczIDAuMDAxNi0wLjcyLTAuNTMtMC43NC0wLjc5eiIgLz4KPC9zdmc+Cg==);
  --jp-icon-bug-dot: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiPgogICAgICAgIDxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMTcuMTkgOEgyMFYxMEgxNy45MUMxNy45NiAxMC4zMyAxOCAxMC42NiAxOCAxMVYxMkgyMFYxNEgxOC41SDE4VjE0LjAyNzVDMTUuNzUgMTQuMjc2MiAxNCAxNi4xODM3IDE0IDE4LjVDMTQgMTkuMjA4IDE0LjE2MzUgMTkuODc3OSAxNC40NTQ5IDIwLjQ3MzlDMTMuNzA2MyAyMC44MTE3IDEyLjg3NTcgMjEgMTIgMjFDOS43OCAyMSA3Ljg1IDE5Ljc5IDYuODEgMThINFYxNkg2LjA5QzYuMDQgMTUuNjcgNiAxNS4zNCA2IDE1VjE0SDRWMTJINlYxMUM2IDEwLjY2IDYuMDQgMTAuMzMgNi4wOSAxMEg0VjhINi44MUM3LjI2IDcuMjIgNy44OCA2LjU1IDguNjIgNi4wNEw3IDQuNDFMOC40MSAzTDEwLjU5IDUuMTdDMTEuMDQgNS4wNiAxMS41MSA1IDEyIDVDMTIuNDkgNSAxMi45NiA1LjA2IDEzLjQyIDUuMTdMMTUuNTkgM0wxNyA0LjQxTDE1LjM3IDYuMDRDMTYuMTIgNi41NSAxNi43NCA3LjIyIDE3LjE5IDhaTTEwIDE2SDE0VjE0SDEwVjE2Wk0xMCAxMkgxNFYxMEgxMFYxMloiIGZpbGw9IiM2MTYxNjEiLz4KICAgICAgICA8cGF0aCBkPSJNMjIgMTguNUMyMiAyMC40MzMgMjAuNDMzIDIyIDE4LjUgMjJDMTYuNTY3IDIyIDE1IDIwLjQzMyAxNSAxOC41QzE1IDE2LjU2NyAxNi41NjcgMTUgMTguNSAxNUMyMC40MzMgMTUgMjIgMTYuNTY3IDIyIDE4LjVaIiBmaWxsPSIjNjE2MTYxIi8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-bug: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0yMCA4aC0yLjgxYy0uNDUtLjc4LTEuMDctMS40NS0xLjgyLTEuOTZMMTcgNC40MSAxNS41OSAzbC0yLjE3IDIuMTdDMTIuOTYgNS4wNiAxMi40OSA1IDEyIDVjLS40OSAwLS45Ni4wNi0xLjQxLjE3TDguNDEgMyA3IDQuNDFsMS42MiAxLjYzQzcuODggNi41NSA3LjI2IDcuMjIgNi44MSA4SDR2MmgyLjA5Yy0uMDUuMzMtLjA5LjY2LS4wOSAxdjFINHYyaDJ2MWMwIC4zNC4wNC42Ny4wOSAxSDR2MmgyLjgxYzEuMDQgMS43OSAyLjk3IDMgNS4xOSAzczQuMTUtMS4yMSA1LjE5LTNIMjB2LTJoLTIuMDljLjA1LS4zMy4wOS0uNjYuMDktMXYtMWgydi0yaC0ydi0xYzAtLjM0LS4wNC0uNjctLjA5LTFIMjBWOHptLTYgOGgtNHYtMmg0djJ6bTAtNGgtNHYtMmg0djJ6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-build: url(data:image/svg+xml;base64,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);
  --jp-icon-caret-down-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iOS45LDEzLjYgMy42LDcuNCA0LjQsNi42IDkuOSwxMi4yIDE1LjQsNi43IDE2LjEsNy40ICIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNS45TDksOS43bDMuOC0zLjhsMS4yLDEuMmwtNC45LDVsLTQuOS01TDUuMiw1Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNy41TDksMTEuMmwzLjgtMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-caret-left: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik0xMC44LDEyLjhMNy4xLDlsMy44LTMuOGwwLDcuNkgxMC44eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-caret-right: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik03LjIsNS4yTDEwLjksOWwtMy44LDMuOFY1LjJINy4yeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-caret-up-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iMTUuNCwxMy4zIDkuOSw3LjcgNC40LDEzLjIgMy42LDEyLjUgOS45LDYuMyAxNi4xLDEyLjYgIi8+Cgk8L2c+Cjwvc3ZnPgo=);
  --jp-icon-caret-up: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik01LjIsMTAuNUw5LDYuOGwzLjgsMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-case-sensitive: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWFjY2VudDIiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTcuNiw4aDAuOWwzLjUsOGgtMS4xTDEwLDE0SDZsLTAuOSwySDRMNy42LDh6IE04LDkuMUw2LjQsMTNoMy4yTDgsOS4xeiIvPgogICAgPHBhdGggZD0iTTE2LjYsOS44Yy0wLjIsMC4xLTAuNCwwLjEtMC43LDAuMWMtMC4yLDAtMC40LTAuMS0wLjYtMC4yYy0wLjEtMC4xLTAuMi0wLjQtMC4yLTAuNyBjLTAuMywwLjMtMC42LDAuNS0wLjksMC43Yy0wLjMsMC4xLTAuNywwLjItMS4xLDAuMmMtMC4zLDAtMC41LDAtMC43LTAuMWMtMC4yLTAuMS0wLjQtMC4yLTAuNi0wLjNjLTAuMi0wLjEtMC4zLTAuMy0wLjQtMC41IGMtMC4xLTAuMi0wLjEtMC40LTAuMS0wLjdjMC0wLjMsMC4xLTAuNiwwLjItMC44YzAuMS0wLjIsMC4zLTAuNCwwLjQtMC41QzEyLDcsMTIuMiw2LjksMTIuNSw2LjhjMC4yLTAuMSwwLjUtMC4xLDAuNy0wLjIgYzAuMy0wLjEsMC41LTAuMSwwLjctMC4xYzAuMiwwLDAuNC0wLjEsMC42LTAuMWMwLjIsMCwwLjMtMC4xLDAuNC0wLjJjMC4xLTAuMSwwLjItMC4yLDAuMi0wLjRjMC0xLTEuMS0xLTEuMy0xIGMtMC40LDAtMS40LDAtMS40LDEuMmgtMC45YzAtMC40LDAuMS0wLjcsMC4yLTFjMC4xLTAuMiwwLjMtMC40LDAuNS0wLjZjMC4yLTAuMiwwLjUtMC4zLDAuOC0wLjNDMTMuMyw0LDEzLjYsNCwxMy45LDQgYzAuMywwLDAuNSwwLDAuOCwwLjFjMC4zLDAsMC41LDAuMSwwLjcsMC4yYzAuMiwwLjEsMC40LDAuMywwLjUsMC41QzE2LDUsMTYsNS4yLDE2LDUuNnYyLjljMCwwLjIsMCwwLjQsMCwwLjUgYzAsMC4xLDAuMSwwLjIsMC4zLDAuMmMwLjEsMCwwLjIsMCwwLjMsMFY5Ljh6IE0xNS4yLDYuOWMtMS4yLDAuNi0zLjEsMC4yLTMuMSwxLjRjMCwxLjQsMy4xLDEsMy4xLTAuNVY2Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik05IDE2LjE3TDQuODMgMTJsLTEuNDIgMS40MUw5IDE5IDIxIDdsLTEuNDEtMS40MXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-circle-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDJDNi40NyAyIDIgNi40NyAyIDEyczQuNDcgMTAgMTAgMTAgMTAtNC40NyAxMC0xMFMxNy41MyAyIDEyIDJ6bTAgMThjLTQuNDEgMC04LTMuNTktOC04czMuNTktOCA4LTggOCAzLjU5IDggOC0zLjU5IDgtOCA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-circle: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iOSIgY3k9IjkiIHI9IjgiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-clear: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8bWFzayBpZD0iZG9udXRIb2xlIj4KICAgIDxyZWN0IHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgZmlsbD0id2hpdGUiIC8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSI4IiBmaWxsPSJibGFjayIvPgogIDwvbWFzaz4KCiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxyZWN0IGhlaWdodD0iMTgiIHdpZHRoPSIyIiB4PSIxMSIgeT0iMyIgdHJhbnNmb3JtPSJyb3RhdGUoMzE1LCAxMiwgMTIpIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIxMCIgbWFzaz0idXJsKCNkb251dEhvbGUpIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-close: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1ub25lIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIGpwLWljb24zLWhvdmVyIiBmaWxsPSJub25lIj4KICAgIDxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjExIi8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIGpwLWljb24tYWNjZW50Mi1ob3ZlciIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMTkgNi40MUwxNy41OSA1IDEyIDEwLjU5IDYuNDEgNSA1IDYuNDEgMTAuNTkgMTIgNSAxNy41OSA2LjQxIDE5IDEyIDEzLjQxIDE3LjU5IDE5IDE5IDE3LjU5IDEzLjQxIDEyeiIvPgogIDwvZz4KCiAgPGcgY2xhc3M9ImpwLWljb24tbm9uZSBqcC1pY29uLWJ1c3kiIGZpbGw9Im5vbmUiPgogICAgPGNpcmNsZSBjeD0iMTIiIGN5PSIxMiIgcj0iNyIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-code-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBzaGFwZS1yZW5kZXJpbmc9Imdlb21ldHJpY1ByZWNpc2lvbiI+CiAgICA8cGF0aCBkPSJNNi41OSwzLjQxTDIsOEw2LjU5LDEyLjZMOCwxMS4xOEw0LjgyLDhMOCw0LjgyTDYuNTksMy40MU0xMi40MSwzLjQxTDExLDQuODJMMTQuMTgsOEwxMSwxMS4xOEwxMi40MSwxMi42TDE3LDhMMTIuNDEsMy40MU0yMS41OSwxMS41OUwxMy41LDE5LjY4TDkuODMsMTZMOC40MiwxNy40MUwxMy41LDIyLjVMMjMsMTNMMjEuNTksMTEuNTlaIiAvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-code: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIiIGhlaWdodD0iMjIiIHZpZXdCb3g9IjAgMCAyOCAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTExLjQgMTguNkw2LjggMTRMMTEuNCA5LjRMMTAgOEw0IDE0TDEwIDIwTDExLjQgMTguNlpNMTYuNiAxOC42TDIxLjIgMTRMMTYuNiA5LjRMMTggOEwyNCAxNEwxOCAyMEwxNi42IDE4LjZWMTguNloiLz4KCTwvZz4KPC9zdmc+Cg==);
  --jp-icon-collapse-all: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTggMmMxIDAgMTEgMCAxMiAwczIgMSAyIDJjMCAxIDAgMTEgMCAxMnMwIDItMiAyQzIwIDE0IDIwIDQgMjAgNFMxMCA0IDYgNGMwLTIgMS0yIDItMnoiIC8+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTE4IDhjMC0xLTEtMi0yLTJTNSA2IDQgNnMtMiAxLTIgMmMwIDEgMCAxMSAwIDEyczEgMiAyIDJjMSAwIDExIDAgMTIgMHMyLTEgMi0yYzAtMSAwLTExIDAtMTJ6bS0yIDB2MTJINFY4eiIgLz4KICAgICAgICA8cGF0aCBkPSJNNiAxM3YyaDh2LTJ6IiAvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-console: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwMCAyMDAiPgogIDxnIGNsYXNzPSJqcC1jb25zb2xlLWljb24tYmFja2dyb3VuZC1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMjg4RDEiPgogICAgPHBhdGggZD0iTTIwIDE5LjhoMTYwdjE1OS45SDIweiIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtY29uc29sZS1pY29uLWNvbG9yIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIj4KICAgIDxwYXRoIGQ9Ik0xMDUgMTI3LjNoNDB2MTIuOGgtNDB6TTUxLjEgNzdMNzQgOTkuOWwtMjMuMyAyMy4zIDEwLjUgMTAuNSAyMy4zLTIzLjNMOTUgOTkuOSA4NC41IDg5LjQgNjEuNiA2Ni41eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-copy: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTExLjksMUgzLjJDMi40LDEsMS43LDEuNywxLjcsMi41djEwLjJoMS41VjIuNWg4LjdWMXogTTE0LjEsMy45aC04Yy0wLjgsMC0xLjUsMC43LTEuNSwxLjV2MTAuMmMwLDAuOCwwLjcsMS41LDEuNSwxLjVoOCBjMC44LDAsMS41LTAuNywxLjUtMS41VjUuNEMxNS41LDQuNiwxNC45LDMuOSwxNC4xLDMuOXogTTE0LjEsMTUuNWgtOFY1LjRoOFYxNS41eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-copyright: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGVuYWJsZS1iYWNrZ3JvdW5kPSJuZXcgMCAwIDI0IDI0IiBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCI+CiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0xMS44OCw5LjE0YzEuMjgsMC4wNiwxLjYxLDEuMTUsMS42MywxLjY2aDEuNzljLTAuMDgtMS45OC0xLjQ5LTMuMTktMy40NS0zLjE5QzkuNjQsNy42MSw4LDksOCwxMi4xNCBjMCwxLjk0LDAuOTMsNC4yNCwzLjg0LDQuMjRjMi4yMiwwLDMuNDEtMS42NSwzLjQ0LTIuOTVoLTEuNzljLTAuMDMsMC41OS0wLjQ1LDEuMzgtMS42MywxLjQ0QzEwLjU1LDE0LjgzLDEwLDEzLjgxLDEwLDEyLjE0IEMxMCw5LjI1LDExLjI4LDkuMTYsMTEuODgsOS4xNHogTTEyLDJDNi40OCwyLDIsNi40OCwyLDEyczQuNDgsMTAsMTAsMTBzMTAtNC40OCwxMC0xMFMxNy41MiwyLDEyLDJ6IE0xMiwyMGMtNC40MSwwLTgtMy41OS04LTggczMuNTktOCw4LThzOCwzLjU5LDgsOFMxNi40MSwyMCwxMiwyMHoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-cut: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkuNjQgNy42NGMuMjMtLjUuMzYtMS4wNS4zNi0xLjY0IDAtMi4yMS0xLjc5LTQtNC00UzIgMy43OSAyIDZzMS43OSA0IDQgNGMuNTkgMCAxLjE0LS4xMyAxLjY0LS4zNkwxMCAxMmwtMi4zNiAyLjM2QzcuMTQgMTQuMTMgNi41OSAxNCA2IDE0Yy0yLjIxIDAtNCAxLjc5LTQgNHMxLjc5IDQgNCA0IDQtMS43OSA0LTRjMC0uNTktLjEzLTEuMTQtLjM2LTEuNjRMMTIgMTRsNyA3aDN2LTFMOS42NCA3LjY0ek02IDhjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTAgMTJjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTYtNy41Yy0uMjggMC0uNS0uMjItLjUtLjVzLjIyLS41LjUtLjUuNS4yMi41LjUtLjIyLjUtLjUuNXpNMTkgM2wtNiA2IDIgMiA3LTdWM3oiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-delete: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2cHgiIGhlaWdodD0iMTZweCI+CiAgICA8cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIiAvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjI2MjYyIiBkPSJNNiAxOWMwIDEuMS45IDIgMiAyaDhjMS4xIDAgMi0uOSAyLTJWN0g2djEyek0xOSA0aC0zLjVsLTEtMWgtNWwtMSAxSDV2MmgxNFY0eiIgLz4KPC9zdmc+Cg==);
  --jp-icon-download: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDloLTRWM0g5djZINWw3IDcgNy03ek01IDE4djJoMTR2LTJINXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-duplicate: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTIuNzk5OTggMC44NzVIOC44OTU4MkM5LjIwMDYxIDAuODc1IDkuNDQ5OTggMS4xMzkxNCA5LjQ0OTk4IDEuNDYxOThDOS40NDk5OCAxLjc4NDgyIDkuMjAwNjEgMi4wNDg5NiA4Ljg5NTgyIDIuMDQ4OTZIMy4zNTQxNUMzLjA0OTM2IDIuMDQ4OTYgMi43OTk5OCAyLjMxMzEgMi43OTk5OCAyLjYzNTk0VjkuNjc5NjlDMi43OTk5OCAxMC4wMDI1IDIuNTUwNjEgMTAuMjY2NyAyLjI0NTgyIDEwLjI2NjdDMS45NDEwMyAxMC4yNjY3IDEuNjkxNjUgMTAuMDAyNSAxLjY5MTY1IDkuNjc5NjlWMi4wNDg5NkMxLjY5MTY1IDEuNDAzMjggMi4xOTA0IDAuODc1IDIuNzk5OTggMC44NzVaTTUuMzY2NjUgMTEuOVY0LjU1SDExLjA4MzNWMTEuOUg1LjM2NjY1Wk00LjE0MTY1IDQuMTQxNjdDNC4xNDE2NSAzLjY5MDYzIDQuNTA3MjggMy4zMjUgNC45NTgzMiAzLjMyNUgxMS40OTE3QzExLjk0MjcgMy4zMjUgMTIuMzA4MyAzLjY5MDYzIDEyLjMwODMgNC4xNDE2N1YxMi4zMDgzQzEyLjMwODMgMTIuNzU5NCAxMS45NDI3IDEzLjEyNSAxMS40OTE3IDEzLjEyNUg0Ljk1ODMyQzQuNTA3MjggMTMuMTI1IDQuMTQxNjUgMTIuNzU5NCA0LjE0MTY1IDEyLjMwODNWNC4xNDE2N1oiIGZpbGw9IiM2MTYxNjEiLz4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNOS40MzU3NCA4LjI2NTA3SDguMzY0MzFWOS4zMzY1QzguMzY0MzEgOS40NTQzNSA4LjI2Nzg4IDkuNTUwNzggOC4xNTAwMiA5LjU1MDc4QzguMDMyMTcgOS41NTA3OCA3LjkzNTc0IDkuNDU0MzUgNy45MzU3NCA5LjMzNjVWOC4yNjUwN0g2Ljg2NDMxQzYuNzQ2NDUgOC4yNjUwNyA2LjY1MDAyIDguMTY4NjQgNi42NTAwMiA4LjA1MDc4QzYuNjUwMDIgNy45MzI5MiA2Ljc0NjQ1IDcuODM2NSA2Ljg2NDMxIDcuODM2NUg3LjkzNTc0VjYuNzY1MDdDNy45MzU3NCA2LjY0NzIxIDguMDMyMTcgNi41NTA3OCA4LjE1MDAyIDYuNTUwNzhDOC4yNjc4OCA2LjU1MDc4IDguMzY0MzEgNi42NDcyMSA4LjM2NDMxIDYuNzY1MDdWNy44MzY1SDkuNDM1NzRDOS41NTM2IDcuODM2NSA5LjY1MDAyIDcuOTMyOTIgOS42NTAwMiA4LjA1MDc4QzkuNjUwMDIgOC4xNjg2NCA5LjU1MzYgOC4yNjUwNyA5LjQzNTc0IDguMjY1MDdaIiBmaWxsPSIjNjE2MTYxIiBzdHJva2U9IiM2MTYxNjEiIHN0cm9rZS13aWR0aD0iMC41Ii8+Cjwvc3ZnPgo=);
  --jp-icon-edit: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMgMTcuMjVWMjFoMy43NUwxNy44MSA5Ljk0bC0zLjc1LTMuNzVMMyAxNy4yNXpNMjAuNzEgNy4wNGMuMzktLjM5LjM5LTEuMDIgMC0xLjQxbC0yLjM0LTIuMzRjLS4zOS0uMzktMS4wMi0uMzktMS40MSAwbC0xLjgzIDEuODMgMy43NSAzLjc1IDEuODMtMS44M3oiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-ellipses: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iNSIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxOSIgY3k9IjEyIiByPSIyIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-error: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj48Y2lyY2xlIGN4PSIxMiIgY3k9IjE5IiByPSIyIi8+PHBhdGggZD0iTTEwIDNoNHYxMmgtNHoiLz48L2c+CjxwYXRoIGZpbGw9Im5vbmUiIGQ9Ik0wIDBoMjR2MjRIMHoiLz4KPC9zdmc+Cg==);
  --jp-icon-expand-all: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTggMmMxIDAgMTEgMCAxMiAwczIgMSAyIDJjMCAxIDAgMTEgMCAxMnMwIDItMiAyQzIwIDE0IDIwIDQgMjAgNFMxMCA0IDYgNGMwLTIgMS0yIDItMnoiIC8+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTE4IDhjMC0xLTEtMi0yLTJTNSA2IDQgNnMtMiAxLTIgMmMwIDEgMCAxMSAwIDEyczEgMiAyIDJjMSAwIDExIDAgMTIgMHMyLTEgMi0yYzAtMSAwLTExIDAtMTJ6bS0yIDB2MTJINFY4eiIgLz4KICAgICAgICA8cGF0aCBkPSJNMTEgMTBIOXYzSDZ2MmgzdjNoMnYtM2gzdi0yaC0zeiIgLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-extension: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwLjUgMTFIMTlWN2MwLTEuMS0uOS0yLTItMmgtNFYzLjVDMTMgMi4xMiAxMS44OCAxIDEwLjUgMVM4IDIuMTIgOCAzLjVWNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAydjMuOEgzLjVjMS40OSAwIDIuNyAxLjIxIDIuNyAyLjdzLTEuMjEgMi43LTIuNyAyLjdIMlYyMGMwIDEuMS45IDIgMiAyaDMuOHYtMS41YzAtMS40OSAxLjIxLTIuNyAyLjctMi43IDEuNDkgMCAyLjcgMS4yMSAyLjcgMi43VjIySDE3YzEuMSAwIDItLjkgMi0ydi00aDEuNWMxLjM4IDAgMi41LTEuMTIgMi41LTIuNVMyMS44OCAxMSAyMC41IDExeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-fast-forward: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTQgMThsOC41LTZMNCA2djEyem05LTEydjEybDguNS02TDEzIDZ6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-file-upload: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkgMTZoNnYtNmg0bC03LTctNyA3aDR6bS00IDJoMTR2Mkg1eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-file: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuMyA4LjJsLTUuNS01LjVjLS4zLS4zLS43LS41LTEuMi0uNUgzLjljLS44LjEtMS42LjktMS42IDEuOHYxNC4xYzAgLjkuNyAxLjYgMS42IDEuNmgxNC4yYy45IDAgMS42LS43IDEuNi0xLjZWOS40Yy4xLS41LS4xLS45LS40LTEuMnptLTUuOC0zLjNsMy40IDMuNmgtMy40VjQuOXptMy45IDEyLjdINC43Yy0uMSAwLS4yIDAtLjItLjJWNC43YzAtLjIuMS0uMy4yLS4zaDcuMnY0LjRzMCAuOC4zIDEuMWMuMy4zIDEuMS4zIDEuMS4zaDQuM3Y3LjJzLS4xLjItLjIuMnoiLz4KPC9zdmc+Cg==);
  --jp-icon-filter-dot: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTE0LDEyVjE5Ljg4QzE0LjA0LDIwLjE4IDEzLjk0LDIwLjUgMTMuNzEsMjAuNzFDMTMuMzIsMjEuMSAxMi42OSwyMS4xIDEyLjMsMjAuNzFMMTAuMjksMTguN0MxMC4wNiwxOC40NyA5Ljk2LDE4LjE2IDEwLDE3Ljg3VjEySDkuOTdMNC4yMSw0LjYyQzMuODcsNC4xOSAzLjk1LDMuNTYgNC4zOCwzLjIyQzQuNTcsMy4wOCA0Ljc4LDMgNSwzVjNIMTlWM0MxOS4yMiwzIDE5LjQzLDMuMDggMTkuNjIsMy4yMkMyMC4wNSwzLjU2IDIwLjEzLDQuMTkgMTkuNzksNC42MkwxNC4wMywxMkgxNFoiIC8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWRvdCIgZmlsbD0iI0ZGRiI+CiAgICA8Y2lyY2xlIGN4PSIxOCIgY3k9IjE3IiByPSIzIj48L2NpcmNsZT4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-filter-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEwIDE4aDR2LTJoLTR2MnpNMyA2djJoMThWNkgzem0zIDdoMTJ2LTJINnYyeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-filter: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTE0LDEyVjE5Ljg4QzE0LjA0LDIwLjE4IDEzLjk0LDIwLjUgMTMuNzEsMjAuNzFDMTMuMzIsMjEuMSAxMi42OSwyMS4xIDEyLjMsMjAuNzFMMTAuMjksMTguN0MxMC4wNiwxOC40NyA5Ljk2LDE4LjE2IDEwLDE3Ljg3VjEySDkuOTdMNC4yMSw0LjYyQzMuODcsNC4xOSAzLjk1LDMuNTYgNC4zOCwzLjIyQzQuNTcsMy4wOCA0Ljc4LDMgNSwzVjNIMTlWM0MxOS4yMiwzIDE5LjQzLDMuMDggMTkuNjIsMy4yMkMyMC4wNSwzLjU2IDIwLjEzLDQuMTkgMTkuNzksNC42MkwxNC4wMywxMkgxNFoiIC8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-folder-favorite: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjRweCIgdmlld0JveD0iMCAwIDI0IDI0IiB3aWR0aD0iMjRweCIgZmlsbD0iIzAwMDAwMCI+CiAgPHBhdGggZD0iTTAgMGgyNHYyNEgwVjB6IiBmaWxsPSJub25lIi8+PHBhdGggY2xhc3M9ImpwLWljb24zIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzYxNjE2MSIgZD0iTTIwIDZoLThsLTItMkg0Yy0xLjEgMC0yIC45LTIgMnYxMmMwIDEuMS45IDIgMiAyaDE2YzEuMSAwIDItLjkgMi0yVjhjMC0xLjEtLjktMi0yLTJ6bS0yLjA2IDExTDE1IDE1LjI4IDEyLjA2IDE3bC43OC0zLjMzLTIuNTktMi4yNCAzLjQxLS4yOUwxNSA4bDEuMzQgMy4xNCAzLjQxLjI5LTIuNTkgMi4yNC43OCAzLjMzeiIvPgo8L3N2Zz4K);
  --jp-icon-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY4YzAtMS4xLS45LTItMi0yaC04bC0yLTJ6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-home: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjRweCIgdmlld0JveD0iMCAwIDI0IDI0IiB3aWR0aD0iMjRweCIgZmlsbD0iIzAwMDAwMCI+CiAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPjxwYXRoIGNsYXNzPSJqcC1pY29uMyBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xMCAyMHYtNmg0djZoNXYtOGgzTDEyIDMgMiAxMmgzdjh6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-html5: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uMCBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMDAiIGQ9Ik0xMDguNCAwaDIzdjIyLjhoMjEuMlYwaDIzdjY5aC0yM1Y0NmgtMjF2MjNoLTIzLjJNMjA2IDIzaC0yMC4zVjBoNjMuN3YyM0gyMjl2NDZoLTIzbTUzLjUtNjloMjQuMWwxNC44IDI0LjNMMzEzLjIgMGgyNC4xdjY5aC0yM1YzNC44bC0xNi4xIDI0LjgtMTYuMS0yNC44VjY5aC0yMi42bTg5LjItNjloMjN2NDYuMmgzMi42VjY5aC01NS42Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI2U0NGQyNiIgZD0iTTEwNy42IDQ3MWwtMzMtMzcwLjRoMzYyLjhsLTMzIDM3MC4yTDI1NS43IDUxMiIvPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNmMTY1MjkiIGQ9Ik0yNTYgNDgwLjVWMTMxaDE0OC4zTDM3NiA0NDciLz4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNlYmViZWIiIGQ9Ik0xNDIgMTc2LjNoMTE0djQ1LjRoLTY0LjJsNC4yIDQ2LjVoNjB2NDUuM0gxNTQuNG0yIDIyLjhIMjAybDMuMiAzNi4zIDUwLjggMTMuNnY0Ny40bC05My4yLTI2Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIiBkPSJNMzY5LjYgMTc2LjNIMjU1Ljh2NDUuNGgxMDkuNm0tNC4xIDQ2LjVIMjU1Ljh2NDUuNGg1NmwtNS4zIDU5LTUwLjcgMTMuNnY0Ny4ybDkzLTI1LjgiLz4KPC9zdmc+Cg==);
  --jp-icon-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1icmFuZDQganAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNGRkYiIGQ9Ik0yLjIgMi4yaDE3LjV2MTcuNUgyLjJ6Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzNGNTFCNSIgZD0iTTIuMiAyLjJ2MTcuNWgxNy41bC4xLTE3LjVIMi4yem0xMi4xIDIuMmMxLjIgMCAyLjIgMSAyLjIgMi4ycy0xIDIuMi0yLjIgMi4yLTIuMi0xLTIuMi0yLjIgMS0yLjIgMi4yLTIuMnpNNC40IDE3LjZsMy4zLTguOCAzLjMgNi42IDIuMi0zLjIgNC40IDUuNEg0LjR6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-info: url(data:image/svg+xml;base64,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);
  --jp-icon-inspector: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaW5zcGVjdG9yLWljb24tY29sb3IganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY2YzAtMS4xLS45LTItMi0yem0tNSAxNEg0di00aDExdjR6bTAtNUg0VjloMTF2NHptNSA1aC00VjloNHY5eiIvPgo8L3N2Zz4K);
  --jp-icon-json: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtanNvbi1pY29uLWNvbG9yIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI0Y5QTgyNSI+CiAgICA8cGF0aCBkPSJNMjAuMiAxMS44Yy0xLjYgMC0xLjcuNS0xLjcgMSAwIC40LjEuOS4xIDEuMy4xLjUuMS45LjEgMS4zIDAgMS43LTEuNCAyLjMtMy41IDIuM2gtLjl2LTEuOWguNWMxLjEgMCAxLjQgMCAxLjQtLjggMC0uMyAwLS42LS4xLTEgMC0uNC0uMS0uOC0uMS0xLjIgMC0xLjMgMC0xLjggMS4zLTItMS4zLS4yLTEuMy0uNy0xLjMtMiAwLS40LjEtLjguMS0xLjIuMS0uNC4xLS43LjEtMSAwLS44LS40LS43LTEuNC0uOGgtLjVWNC4xaC45YzIuMiAwIDMuNS43IDMuNSAyLjMgMCAuNC0uMS45LS4xIDEuMy0uMS41LS4xLjktLjEgMS4zIDAgLjUuMiAxIDEuNyAxdjEuOHpNMS44IDEwLjFjMS42IDAgMS43LS41IDEuNy0xIDAtLjQtLjEtLjktLjEtMS4zLS4xLS41LS4xLS45LS4xLTEuMyAwLTEuNiAxLjQtMi4zIDMuNS0yLjNoLjl2MS45aC0uNWMtMSAwLTEuNCAwLTEuNC44IDAgLjMgMCAuNi4xIDEgMCAuMi4xLjYuMSAxIDAgMS4zIDAgMS44LTEuMyAyQzYgMTEuMiA2IDExLjcgNiAxM2MwIC40LS4xLjgtLjEgMS4yLS4xLjMtLjEuNy0uMSAxIDAgLjguMy44IDEuNC44aC41djEuOWgtLjljLTIuMSAwLTMuNS0uNi0zLjUtMi4zIDAtLjQuMS0uOS4xLTEuMy4xLS41LjEtLjkuMS0xLjMgMC0uNS0uMi0xLTEuNy0xdi0xLjl6Ii8+CiAgICA8Y2lyY2xlIGN4PSIxMSIgY3k9IjEzLjgiIHI9IjIuMSIvPgogICAgPGNpcmNsZSBjeD0iMTEiIGN5PSI4LjIiIHI9IjIuMSIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-julia: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDMyNSAzMDAiPgogIDxnIGNsYXNzPSJqcC1icmFuZDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjY2IzYzMzIj4KICAgIDxwYXRoIGQ9Ik0gMTUwLjg5ODQzOCAyMjUgQyAxNTAuODk4NDM4IDI2Ni40MjE4NzUgMTE3LjMyMDMxMiAzMDAgNzUuODk4NDM4IDMwMCBDIDM0LjQ3NjU2MiAzMDAgMC44OTg0MzggMjY2LjQyMTg3NSAwLjg5ODQzOCAyMjUgQyAwLjg5ODQzOCAxODMuNTc4MTI1IDM0LjQ3NjU2MiAxNTAgNzUuODk4NDM4IDE1MCBDIDExNy4zMjAzMTIgMTUwIDE1MC44OTg0MzggMTgzLjU3ODEyNSAxNTAuODk4NDM4IDIyNSIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzM4OTgyNiI+CiAgICA8cGF0aCBkPSJNIDIzNy41IDc1IEMgMjM3LjUgMTE2LjQyMTg3NSAyMDMuOTIxODc1IDE1MCAxNjIuNSAxNTAgQyAxMjEuMDc4MTI1IDE1MCA4Ny41IDExNi40MjE4NzUgODcuNSA3NSBDIDg3LjUgMzMuNTc4MTI1IDEyMS4wNzgxMjUgMCAxNjIuNSAwIEMgMjAzLjkyMTg3NSAwIDIzNy41IDMzLjU3ODEyNSAyMzcuNSA3NSIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzk1NThiMiI+CiAgICA8cGF0aCBkPSJNIDMyNC4xMDE1NjIgMjI1IEMgMzI0LjEwMTU2MiAyNjYuNDIxODc1IDI5MC41MjM0MzggMzAwIDI0OS4xMDE1NjIgMzAwIEMgMjA3LjY3OTY4OCAzMDAgMTc0LjEwMTU2MiAyNjYuNDIxODc1IDE3NC4xMDE1NjIgMjI1IEMgMTc0LjEwMTU2MiAxODMuNTc4MTI1IDIwNy42Nzk2ODggMTUwIDI0OS4xMDE1NjIgMTUwIEMgMjkwLjUyMzQzOCAxNTAgMzI0LjEwMTU2MiAxODMuNTc4MTI1IDMyNC4xMDE1NjIgMjI1Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-jupyter-favicon: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTUyIiBoZWlnaHQ9IjE2NSIgdmlld0JveD0iMCAwIDE1MiAxNjUiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgPGcgY2xhc3M9ImpwLWp1cHl0ZXItaWNvbi1jb2xvciIgZmlsbD0iI0YzNzcyNiI+CiAgICA8cGF0aCB0cmFuc2Zvcm09InRyYW5zbGF0ZSgwLjA3ODk0NywgMTEwLjU4MjkyNykiIGQ9Ik03NS45NDIyODQyLDI5LjU4MDQ1NjEgQzQzLjMwMjM5NDcsMjkuNTgwNDU2MSAxNC43OTY3ODMyLDE3LjY1MzQ2MzQgMCwwIEM1LjUxMDgzMjExLDE1Ljg0MDY4MjkgMTUuNzgxNTM4OSwyOS41NjY3NzMyIDI5LjM5MDQ5NDcsMzkuMjc4NDE3MSBDNDIuOTk5Nyw0OC45ODk4NTM3IDU5LjI3MzcsNTQuMjA2NzgwNSA3NS45NjA1Nzg5LDU0LjIwNjc4MDUgQzkyLjY0NzQ1NzksNTQuMjA2NzgwNSAxMDguOTIxNDU4LDQ4Ljk4OTg1MzcgMTIyLjUzMDY2MywzOS4yNzg0MTcxIEMxMzYuMTM5NDUzLDI5LjU2Njc3MzIgMTQ2LjQxMDI4NCwxNS44NDA2ODI5IDE1MS45MjExNTgsMCBDMTM3LjA4Nzg2OCwxNy42NTM0NjM0IDEwOC41ODI1ODksMjkuNTgwNDU2MSA3NS45NDIyODQyLDI5LjU4MDQ1NjEgTDc1Ljk0MjI4NDIsMjkuNTgwNDU2MSBaIiAvPgogICAgPHBhdGggdHJhbnNmb3JtPSJ0cmFuc2xhdGUoMC4wMzczNjgsIDAuNzA0ODc4KSIgZD0iTTc1Ljk3ODQ1NzksMjQuNjI2NDA3MyBDMTA4LjYxODc2MywyNC42MjY0MDczIDEzNy4xMjQ0NTgsMzYuNTUzNDQxNSAxNTEuOTIxMTU4LDU0LjIwNjc4MDUgQzE0Ni40MTAyODQsMzguMzY2MjIyIDEzNi4xMzk0NTMsMjQuNjQwMTMxNyAxMjIuNTMwNjYzLDE0LjkyODQ4NzggQzEwOC45MjE0NTgsNS4yMTY4NDM5IDkyLjY0NzQ1NzksMCA3NS45NjA1Nzg5LDAgQzU5LjI3MzcsMCA0Mi45OTk3LDUuMjE2ODQzOSAyOS4zOTA0OTQ3LDE0LjkyODQ4NzggQzE1Ljc4MTUzODksMjQuNjQwMTMxNyA1LjUxMDgzMjExLDM4LjM2NjIyMiAwLDU0LjIwNjc4MDUgQzE0LjgzMzA4MTYsMzYuNTg5OTI5MyA0My4zMzg1Njg0LDI0LjYyNjQwNzMgNzUuOTc4NDU3OSwyNC42MjY0MDczIEw3NS45Nzg0NTc5LDI0LjYyNjQwNzMgWiIgLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-jupyter: url(data:image/svg+xml;base64,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);
  --jp-icon-jupyterlab-wordmark: url(data:image/svg+xml;base64,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);
  --jp-icon-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgZmlsbD0iIzYxNjE2MSIgZD0iTTE1IDlIOXY2aDZWOXptLTIgNGgtMnYtMmgydjJ6bTgtMlY5aC0yVjdjMC0xLjEtLjktMi0yLTJoLTJWM2gtMnYyaC0yVjNIOXYySDdjLTEuMSAwLTIgLjktMiAydjJIM3YyaDJ2MkgzdjJoMnYyYzAgMS4xLjkgMiAyIDJoMnYyaDJ2LTJoMnYyaDJ2LTJoMmMxLjEgMCAyLS45IDItMnYtMmgydi0yaC0ydi0yaDJ6bS00IDZIN1Y3aDEwdjEweiIvPgo8L3N2Zz4K);
  --jp-icon-keyboard: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMTdjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY3YzAtMS4xLS45LTItMi0yem0tOSAzaDJ2MmgtMlY4em0wIDNoMnYyaC0ydi0yek04IDhoMnYySDhWOHptMCAzaDJ2Mkg4di0yem0tMSAySDV2LTJoMnYyem0wLTNINVY4aDJ2MnptOSA3SDh2LTJoOHYyem0wLTRoLTJ2LTJoMnYyem0wLTNoLTJWOGgydjJ6bTMgM2gtMnYtMmgydjJ6bTAtM2gtMlY4aDJ2MnoiLz4KPC9zdmc+Cg==);
  --jp-icon-launch: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMzIgMzIiIHdpZHRoPSIzMiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0yNiwyOEg2YTIuMDAyNywyLjAwMjcsMCwwLDEtMi0yVjZBMi4wMDI3LDIuMDAyNywwLDAsMSw2LDRIMTZWNkg2VjI2SDI2VjE2aDJWMjZBMi4wMDI3LDIuMDAyNywwLDAsMSwyNiwyOFoiLz4KICAgIDxwb2x5Z29uIHBvaW50cz0iMjAgMiAyMCA0IDI2LjU4NiA0IDE4IDEyLjU4NiAxOS40MTQgMTQgMjggNS40MTQgMjggMTIgMzAgMTIgMzAgMiAyMCAyIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-launcher: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkgMTlINVY1aDdWM0g1YTIgMiAwIDAwLTIgMnYxNGEyIDIgMCAwMDIgMmgxNGMxLjEgMCAyLS45IDItMnYtN2gtMnY3ek0xNCAzdjJoMy41OWwtOS44MyA5LjgzIDEuNDEgMS40MUwxOSA2LjQxVjEwaDJWM2gtN3oiLz4KPC9zdmc+Cg==);
  --jp-icon-line-form: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGZpbGw9IndoaXRlIiBkPSJNNS44OCA0LjEyTDEzLjc2IDEybC03Ljg4IDcuODhMOCAyMmwxMC0xMEw4IDJ6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-link: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMuOSAxMmMwLTEuNzEgMS4zOS0zLjEgMy4xLTMuMWg0VjdIN2MtMi43NiAwLTUgMi4yNC01IDVzMi4yNCA1IDUgNWg0di0xLjlIN2MtMS43MSAwLTMuMS0xLjM5LTMuMS0zLjF6TTggMTNoOHYtMkg4djJ6bTktNmgtNHYxLjloNGMxLjcxIDAgMy4xIDEuMzkgMy4xIDMuMXMtMS4zOSAzLjEtMy4xIDMuMWgtNFYxN2g0YzIuNzYgMCA1LTIuMjQgNS01cy0yLjI0LTUtNS01eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xOSA1djE0SDVWNWgxNG0xLjEtMkgzLjljLS41IDAtLjkuNC0uOS45djE2LjJjMCAuNC40LjkuOS45aDE2LjJjLjQgMCAuOS0uNS45LS45VjMuOWMwLS41LS41LS45LS45LS45ek0xMSA3aDZ2MmgtNlY3em0wIDRoNnYyaC02di0yem0wIDRoNnYyaC02ek03IDdoMnYySDd6bTAgNGgydjJIN3ptMCA0aDJ2Mkg3eiIvPgo8L3N2Zz4K);
  --jp-icon-markdown: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjN0IxRkEyIiBkPSJNNSAxNC45aDEybC02LjEgNnptOS40LTYuOGMwLTEuMy0uMS0yLjktLjEtNC41LS40IDEuNC0uOSAyLjktMS4zIDQuM2wtMS4zIDQuM2gtMkw4LjUgNy45Yy0uNC0xLjMtLjctMi45LTEtNC4zLS4xIDEuNi0uMSAzLjItLjIgNC42TDcgMTIuNEg0LjhsLjctMTFoMy4zTDEwIDVjLjQgMS4yLjcgMi43IDEgMy45LjMtMS4yLjctMi42IDEtMy45bDEuMi0zLjdoMy4zbC42IDExaC0yLjRsLS4zLTQuMnoiLz4KPC9zdmc+Cg==);
  --jp-icon-move-down: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNMTIuNDcxIDcuNTI4OTlDMTIuNzYzMiA3LjIzNjg0IDEyLjc2MzIgNi43NjMxNiAxMi40NzEgNi40NzEwMVY2LjQ3MTAxQzEyLjE3OSA2LjE3OTA1IDExLjcwNTcgNi4xNzg4NCAxMS40MTM1IDYuNDcwNTRMNy43NSAxMC4xMjc1VjEuNzVDNy43NSAxLjMzNTc5IDcuNDE0MjEgMSA3IDFWMUM2LjU4NTc5IDEgNi4yNSAxLjMzNTc5IDYuMjUgMS43NVYxMC4xMjc1TDIuNTk3MjYgNi40NjgyMkMyLjMwMzM4IDYuMTczODEgMS44MjY0MSA2LjE3MzU5IDEuNTMyMjYgNi40Njc3NFY2LjQ2Nzc0QzEuMjM4MyA2Ljc2MTcgMS4yMzgzIDcuMjM4MyAxLjUzMjI2IDcuNTMyMjZMNi4yOTI4OSAxMi4yOTI5QzYuNjgzNDIgMTIuNjgzNCA3LjMxNjU4IDEyLjY4MzQgNy43MDcxMSAxMi4yOTI5TDEyLjQ3MSA3LjUyODk5WiIgZmlsbD0iIzYxNjE2MSIvPgo8L3N2Zz4K);
  --jp-icon-move-up: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNMS41Mjg5OSA2LjQ3MTAxQzEuMjM2ODQgNi43NjMxNiAxLjIzNjg0IDcuMjM2ODQgMS41Mjg5OSA3LjUyODk5VjcuNTI4OTlDMS44MjA5NSA3LjgyMDk1IDIuMjk0MjYgNy44MjExNiAyLjU4NjQ5IDcuNTI5NDZMNi4yNSAzLjg3MjVWMTIuMjVDNi4yNSAxMi42NjQyIDYuNTg1NzkgMTMgNyAxM1YxM0M3LjQxNDIxIDEzIDcuNzUgMTIuNjY0MiA3Ljc1IDEyLjI1VjMuODcyNUwxMS40MDI3IDcuNTMxNzhDMTEuNjk2NiA3LjgyNjE5IDEyLjE3MzYgNy44MjY0MSAxMi40Njc3IDcuNTMyMjZWNy41MzIyNkMxMi43NjE3IDcuMjM4MyAxMi43NjE3IDYuNzYxNyAxMi40Njc3IDYuNDY3NzRMNy43MDcxMSAxLjcwNzExQzcuMzE2NTggMS4zMTY1OCA2LjY4MzQyIDEuMzE2NTggNi4yOTI4OSAxLjcwNzExTDEuNTI4OTkgNi40NzEwMVoiIGZpbGw9IiM2MTYxNjEiLz4KPC9zdmc+Cg==);
  --jp-icon-new-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwIDZoLThsLTItMkg0Yy0xLjExIDAtMS45OS44OS0xLjk5IDJMMiAxOGMwIDEuMTEuODkgMiAyIDJoMTZjMS4xMSAwIDItLjg5IDItMlY4YzAtMS4xMS0uODktMi0yLTJ6bS0xIDhoLTN2M2gtMnYtM2gtM3YtMmgzVjloMnYzaDN2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-not-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI1IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDMgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMTkgMTcuMTg0NCAyLjk2OTY4IDE0LjMwMzIgMS44NjA5NCAxMS40NDA5WiIvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24yIiBzdHJva2U9IiMzMzMzMzMiIHN0cm9rZS13aWR0aD0iMiIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOS4zMTU5MiA5LjMyMDMxKSIgZD0iTTcuMzY4NDIgMEwwIDcuMzY0NzkiLz4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDkuMzE1OTIgMTYuNjgzNikgc2NhbGUoMSAtMSkiIGQ9Ik03LjM2ODQyIDBMMCA3LjM2NDc5Ii8+Cjwvc3ZnPgo=);
  --jp-icon-notebook: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtbm90ZWJvb2staWNvbi1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNFRjZDMDAiPgogICAgPHBhdGggZD0iTTE4LjcgMy4zdjE1LjRIMy4zVjMuM2gxNS40bTEuNS0xLjVIMS44djE4LjNoMTguM2wuMS0xOC4zeiIvPgogICAgPHBhdGggZD0iTTE2LjUgMTYuNWwtNS40LTQuMy01LjYgNC4zdi0xMWgxMXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-numbering: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIiIGhlaWdodD0iMjIiIHZpZXdCb3g9IjAgMCAyOCAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTQgMTlINlYxOS41SDVWMjAuNUg2VjIxSDRWMjJIN1YxOEg0VjE5Wk01IDEwSDZWNkg0VjdINVYxMFpNNCAxM0g1LjhMNCAxNS4xVjE2SDdWMTVINS4yTDcgMTIuOVYxMkg0VjEzWk05IDdWOUgyM1Y3SDlaTTkgMjFIMjNWMTlIOVYyMVpNOSAxNUgyM1YxM0g5VjE1WiIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-offline-bolt: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDIuMDJjLTUuNTEgMC05Ljk4IDQuNDctOS45OCA5Ljk4czQuNDcgOS45OCA5Ljk4IDkuOTggOS45OC00LjQ3IDkuOTgtOS45OFMxNy41MSAyLjAyIDEyIDIuMDJ6TTExLjQ4IDIwdi02LjI2SDhMMTMgNHY2LjI2aDMuMzVMMTEuNDggMjB6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-palette: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE4IDEzVjIwSDRWNkg5LjAyQzkuMDcgNS4yOSA5LjI0IDQuNjIgOS41IDRINEMyLjkgNCAyIDQuOSAyIDZWMjBDMiAyMS4xIDIuOSAyMiA0IDIySDE4QzE5LjEgMjIgMjAgMjEuMSAyMCAyMFYxNUwxOCAxM1pNMTkuMyA4Ljg5QzE5Ljc0IDguMTkgMjAgNy4zOCAyMCA2LjVDMjAgNC4wMSAxNy45OSAyIDE1LjUgMkMxMy4wMSAyIDExIDQuMDEgMTEgNi41QzExIDguOTkgMTMuMDEgMTEgMTUuNDkgMTFDMTYuMzcgMTEgMTcuMTkgMTAuNzQgMTcuODggMTAuM0wyMSAxMy40MkwyMi40MiAxMkwxOS4zIDguODlaTTE1LjUgOUMxNC4xMiA5IDEzIDcuODggMTMgNi41QzEzIDUuMTIgMTQuMTIgNCAxNS41IDRDMTYuODggNCAxOCA1LjEyIDE4IDYuNUMxOCA3Ljg4IDE2Ljg4IDkgMTUuNSA5WiIvPgogICAgPHBhdGggZmlsbC1ydWxlPSJldmVub2RkIiBjbGlwLXJ1bGU9ImV2ZW5vZGQiIGQ9Ik00IDZIOS4wMTg5NEM5LjAwNjM5IDYuMTY1MDIgOSA2LjMzMTc2IDkgNi41QzkgOC44MTU3NyAxMC4yMTEgMTAuODQ4NyAxMi4wMzQzIDEySDlWMTRIMTZWMTIuOTgxMUMxNi41NzAzIDEyLjkzNzcgMTcuMTIgMTIuODIwNyAxNy42Mzk2IDEyLjYzOTZMMTggMTNWMjBINFY2Wk04IDhINlYxMEg4VjhaTTYgMTJIOFYxNEg2VjEyWk04IDE2SDZWMThIOFYxNlpNOSAxNkgxNlYxOEg5VjE2WiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-paste: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE5IDJoLTQuMThDMTQuNC44NCAxMy4zIDAgMTIgMGMtMS4zIDAtMi40Ljg0LTIuODIgMkg1Yy0xLjEgMC0yIC45LTIgMnYxNmMwIDEuMS45IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjRjMC0xLjEtLjktMi0yLTJ6bS03IDBjLjU1IDAgMSAuNDUgMSAxcy0uNDUgMS0xIDEtMS0uNDUtMS0xIC40NS0xIDEtMXptNyAxOEg1VjRoMnYzaDEwVjRoMnYxNnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-pdf: url(data:image/svg+xml;base64,PHN2ZwogICB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyMiAyMiIgd2lkdGg9IjE2Ij4KICAgIDxwYXRoIHRyYW5zZm9ybT0icm90YXRlKDQ1KSIgY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI0ZGMkEyQSIKICAgICAgIGQ9Im0gMjIuMzQ0MzY5LC0zLjAxNjM2NDIgaCA1LjYzODYwNCB2IDEuNTc5MjQzMyBoIC0zLjU0OTIyNyB2IDEuNTA4NjkyOTkgaCAzLjMzNzU3NiBWIDEuNjUwODE1NCBoIC0zLjMzNzU3NiB2IDMuNDM1MjYxMyBoIC0yLjA4OTM3NyB6IG0gLTcuMTM2NDQ0LDEuNTc5MjQzMyB2IDQuOTQzOTU0MyBoIDAuNzQ4OTIgcSAxLjI4MDc2MSwwIDEuOTUzNzAzLC0wLjYzNDk1MzUgMC42NzgzNjksLTAuNjM0OTUzNSAwLjY3ODM2OSwtMS44NDUxNjQxIDAsLTEuMjA0NzgzNTUgLTAuNjcyOTQyLC0xLjgzNDMxMDExIC0wLjY3Mjk0MiwtMC42Mjk1MjY1OSAtMS45NTkxMywtMC42Mjk1MjY1OSB6IG0gLTIuMDg5Mzc3LC0xLjU3OTI0MzMgaCAyLjIwMzM0MyBxIDEuODQ1MTY0LDAgMi43NDYwMzksMC4yNjU5MjA3IDAuOTA2MzAxLDAuMjYwNDkzNyAxLjU1MjEwOCwwLjg5MDAyMDMgMC41Njk4MywwLjU0ODEyMjMgMC44NDY2MDUsMS4yNjQ0ODAwNiAwLjI3Njc3NCwwLjcxNjM1NzgxIDAuMjc2Nzc0LDEuNjIyNjU4OTQgMCwwLjkxNzE1NTEgLTAuMjc2Nzc0LDEuNjM4OTM5OSAtMC4yNzY3NzUsMC43MTYzNTc4IC0wLjg0NjYwNSwxLjI2NDQ4IC0wLjY1MTIzNCwwLjYyOTUyNjYgLTEuNTYyOTYyLDAuODk1NDQ3MyAtMC45MTE3MjgsMC4yNjA0OTM3IC0yLjczNTE4NSwwLjI2MDQ5MzcgaCAtMi4yMDMzNDMgeiBtIC04LjE0NTg1NjUsMCBoIDMuNDY3ODIzIHEgMS41NDY2ODE2LDAgMi4zNzE1Nzg1LDAuNjg5MjIzIDAuODMwMzI0LDAuNjgzNzk2MSAwLjgzMDMyNCwxLjk1MzcwMzE0IDAsMS4yNzUzMzM5NyAtMC44MzAzMjQsMS45NjQ1NTcwNiBRIDkuOTg3MTk2MSwyLjI3NDkxNSA4LjQ0MDUxNDUsMi4yNzQ5MTUgSCA3LjA2MjA2ODQgViA1LjA4NjA3NjcgSCA0Ljk3MjY5MTUgWiBtIDIuMDg5Mzc2OSwxLjUxNDExOTkgdiAyLjI2MzAzOTQzIGggMS4xNTU5NDEgcSAwLjYwNzgxODgsMCAwLjkzODg2MjksLTAuMjkzMDU1NDcgMC4zMzEwNDQxLC0wLjI5ODQ4MjQxIDAuMzMxMDQ0MSwtMC44NDExNzc3MiAwLC0wLjU0MjY5NTMxIC0wLjMzMTA0NDEsLTAuODM1NzUwNzQgLTAuMzMxMDQ0MSwtMC4yOTMwNTU1IC0wLjkzODg2MjksLTAuMjkzMDU1NSB6IgovPgo8L3N2Zz4K);
  --jp-icon-python: url(data:image/svg+xml;base64,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);
  --jp-icon-r-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjE5NkYzIiBkPSJNNC40IDIuNWMxLjItLjEgMi45LS4zIDQuOS0uMyAyLjUgMCA0LjEuNCA1LjIgMS4zIDEgLjcgMS41IDEuOSAxLjUgMy41IDAgMi0xLjQgMy41LTIuOSA0LjEgMS4yLjQgMS43IDEuNiAyLjIgMyAuNiAxLjkgMSAzLjkgMS4zIDQuNmgtMy44Yy0uMy0uNC0uOC0xLjctMS4yLTMuN3MtMS4yLTIuNi0yLjYtMi42aC0uOXY2LjRINC40VjIuNXptMy43IDYuOWgxLjRjMS45IDAgMi45LS45IDIuOS0yLjNzLTEtMi4zLTIuOC0yLjNjLS43IDAtMS4zIDAtMS42LjJ2NC41aC4xdi0uMXoiLz4KPC9zdmc+Cg==);
  --jp-icon-react: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMTUwIDE1MCA1NDEuOSAyOTUuMyI+CiAgPGcgY2xhc3M9ImpwLWljb24tYnJhbmQyIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzYxREFGQiI+CiAgICA8cGF0aCBkPSJNNjY2LjMgMjk2LjVjMC0zMi41LTQwLjctNjMuMy0xMDMuMS04Mi40IDE0LjQtNjMuNiA4LTExNC4yLTIwLjItMTMwLjQtNi41LTMuOC0xNC4xLTUuNi0yMi40LTUuNnYyMi4zYzQuNiAwIDguMy45IDExLjQgMi42IDEzLjYgNy44IDE5LjUgMzcuNSAxNC45IDc1LjctMS4xIDkuNC0yLjkgMTkuMy01LjEgMjkuNC0xOS42LTQuOC00MS04LjUtNjMuNS0xMC45LTEzLjUtMTguNS0yNy41LTM1LjMtNDEuNi01MCAzMi42LTMwLjMgNjMuMi00Ni45IDg0LTQ2LjlWNzhjLTI3LjUgMC02My41IDE5LjYtOTkuOSA1My42LTM2LjQtMzMuOC03Mi40LTUzLjItOTkuOS01My4ydjIyLjNjMjAuNyAwIDUxLjQgMTYuNSA4NCA0Ni42LTE0IDE0LjctMjggMzEuNC00MS4zIDQ5LjktMjIuNiAyLjQtNDQgNi4xLTYzLjYgMTEtMi4zLTEwLTQtMTkuNy01LjItMjktNC43LTM4LjIgMS4xLTY3LjkgMTQuNi03NS44IDMtMS44IDYuOS0yLjYgMTEuNS0yLjZWNzguNWMtOC40IDAtMTYgMS44LTIyLjYgNS42LTI4LjEgMTYuMi0zNC40IDY2LjctMTkuOSAxMzAuMS02Mi4yIDE5LjItMTAyLjcgNDkuOS0xMDIuNyA4Mi4zIDAgMzIuNSA0MC43IDYzLjMgMTAzLjEgODIuNC0xNC40IDYzLjYtOCAxMTQuMiAyMC4yIDEzMC40IDYuNSAzLjggMTQuMSA1LjYgMjIuNSA1LjYgMjcuNSAwIDYzLjUtMTkuNiA5OS45LTUzLjYgMzYuNCAzMy44IDcyLjQgNTMuMiA5OS45IDUzLjIgOC40IDAgMTYtMS44IDIyLjYtNS42IDI4LjEtMTYuMiAzNC40LTY2LjcgMTkuOS0xMzAuMSA2Mi0xOS4xIDEwMi41LTQ5LjkgMTAyLjUtODIuM3ptLTEzMC4yLTY2LjdjLTMuNyAxMi45LTguMyAyNi4yLTEzLjUgMzkuNS00LjEtOC04LjQtMTYtMTMuMS0yNC00LjYtOC05LjUtMTUuOC0xNC40LTIzLjQgMTQuMiAyLjEgMjcuOSA0LjcgNDEgNy45em0tNDUuOCAxMDYuNWMtNy44IDEzLjUtMTUuOCAyNi4zLTI0LjEgMzguMi0xNC45IDEuMy0zMCAyLTQ1LjIgMi0xNS4xIDAtMzAuMi0uNy00NS0xLjktOC4zLTExLjktMTYuNC0yNC42LTI0LjItMzgtNy42LTEzLjEtMTQuNS0yNi40LTIwLjgtMzkuOCA2LjItMTMuNCAxMy4yLTI2LjggMjAuNy0zOS45IDcuOC0xMy41IDE1LjgtMjYuMyAyNC4xLTM4LjIgMTQuOS0xLjMgMzAtMiA0NS4yLTIgMTUuMSAwIDMwLjIuNyA0NSAxLjkgOC4zIDExLjkgMTYuNCAyNC42IDI0LjIgMzggNy42IDEzLjEgMTQuNSAyNi40IDIwLjggMzkuOC02LjMgMTMuNC0xMy4yIDI2LjgtMjAuNyAzOS45em0zMi4zLTEzYzUuNCAxMy40IDEwIDI2LjggMTMuOCAzOS44LTEzLjEgMy4yLTI2LjkgNS45LTQxLjIgOCA0LjktNy43IDkuOC0xNS42IDE0LjQtMjMuNyA0LjYtOCA4LjktMTYuMSAxMy0yNC4xek00MjEuMiA0MzBjLTkuMy05LjYtMTguNi0yMC4zLTI3LjgtMzIgOSAuNCAxOC4yLjcgMjcuNS43IDkuNCAwIDE4LjctLjIgMjcuOC0uNy05IDExLjctMTguMyAyMi40LTI3LjUgMzJ6bS03NC40LTU4LjljLTE0LjItMi4xLTI3LjktNC43LTQxLTcuOSAzLjctMTIuOSA4LjMtMjYuMiAxMy41LTM5LjUgNC4xIDggOC40IDE2IDEzLjEgMjQgNC43IDggOS41IDE1LjggMTQuNCAyMy40ek00MjAuNyAxNjNjOS4zIDkuNiAxOC42IDIwLjMgMjcuOCAzMi05LS40LTE4LjItLjctMjcuNS0uNy05LjQgMC0xOC43LjItMjcuOC43IDktMTEuNyAxOC4zLTIyLjQgMjcuNS0zMnptLTc0IDU4LjljLTQuOSA3LjctOS44IDE1LjYtMTQuNCAyMy43LTQuNiA4LTguOSAxNi0xMyAyNC01LjQtMTMuNC0xMC0yNi44LTEzLjgtMzkuOCAxMy4xLTMuMSAyNi45LTUuOCA0MS4yLTcuOXptLTkwLjUgMTI1LjJjLTM1LjQtMTUuMS01OC4zLTM0LjktNTguMy01MC42IDAtMTUuNyAyMi45LTM1LjYgNTguMy01MC42IDguNi0zLjcgMTgtNyAyNy43LTEwLjEgNS43IDE5LjYgMTMuMiA0MCAyMi41IDYwLjktOS4yIDIwLjgtMTYuNiA0MS4xLTIyLjIgNjAuNi05LjktMy4xLTE5LjMtNi41LTI4LTEwLjJ6TTMxMCA0OTBjLTEzLjYtNy44LTE5LjUtMzcuNS0xNC45LTc1LjcgMS4xLTkuNCAyLjktMTkuMyA1LjEtMjkuNCAxOS42IDQuOCA0MSA4LjUgNjMuNSAxMC45IDEzLjUgMTguNSAyNy41IDM1LjMgNDEuNiA1MC0zMi42IDMwLjMtNjMuMiA0Ni45LTg0IDQ2LjktNC41LS4xLTguMy0xLTExLjMtMi43em0yMzcuMi03Ni4yYzQuNyAzOC4yLTEuMSA2Ny45LTE0LjYgNzUuOC0zIDEuOC02LjkgMi42LTExLjUgMi42LTIwLjcgMC01MS40LTE2LjUtODQtNDYuNiAxNC0xNC43IDI4LTMxLjQgNDEuMy00OS45IDIyLjYtMi40IDQ0LTYuMSA2My42LTExIDIuMyAxMC4xIDQuMSAxOS44IDUuMiAyOS4xem0zOC41LTY2LjdjLTguNiAzLjctMTggNy0yNy43IDEwLjEtNS43LTE5LjYtMTMuMi00MC0yMi41LTYwLjkgOS4yLTIwLjggMTYuNi00MS4xIDIyLjItNjAuNiA5LjkgMy4xIDE5LjMgNi41IDI4LjEgMTAuMiAzNS40IDE1LjEgNTguMyAzNC45IDU4LjMgNTAuNi0uMSAxNS43LTIzIDM1LjYtNTguNCA1MC42ek0zMjAuOCA3OC40eiIvPgogICAgPGNpcmNsZSBjeD0iNDIwLjkiIGN5PSIyOTYuNSIgcj0iNDUuNyIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-redo: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgICA8cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZD0iTTE4LjQgMTAuNkMxNi41NSA4Ljk5IDE0LjE1IDggMTEuNSA4Yy00LjY1IDAtOC41OCAzLjAzLTkuOTYgNy4yMkwzLjkgMTZjMS4wNS0zLjE5IDQuMDUtNS41IDcuNi01LjUgMS45NSAwIDMuNzMuNzIgNS4xMiAxLjg4TDEzIDE2aDlWN2wtMy42IDMuNnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-refresh: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTkgMTMuNWMtMi40OSAwLTQuNS0yLjAxLTQuNS00LjVTNi41MSA0LjUgOSA0LjVjMS4yNCAwIDIuMzYuNTIgMy4xNyAxLjMzTDEwIDhoNVYzbC0xLjc2IDEuNzZDMTIuMTUgMy42OCAxMC42NiAzIDkgMyA1LjY5IDMgMy4wMSA1LjY5IDMuMDEgOVM1LjY5IDE1IDkgMTVjMi45NyAwIDUuNDMtMi4xNiA1LjktNWgtMS41MmMtLjQ2IDItMi4yNCAzLjUtNC4zOCAzLjV6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-regex: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbi1hY2NlbnQyIiBmaWxsPSIjRkZGIj4KICAgIDxjaXJjbGUgY2xhc3M9InN0MiIgY3g9IjUuNSIgY3k9IjE0LjUiIHI9IjEuNSIvPgogICAgPHJlY3QgeD0iMTIiIHk9IjQiIGNsYXNzPSJzdDIiIHdpZHRoPSIxIiBoZWlnaHQ9IjgiLz4KICAgIDxyZWN0IHg9IjguNSIgeT0iNy41IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjg2NiAtMC41IDAuNSAwLjg2NiAtMi4zMjU1IDcuMzIxOSkiIGNsYXNzPSJzdDIiIHdpZHRoPSI4IiBoZWlnaHQ9IjEiLz4KICAgIDxyZWN0IHg9IjEyIiB5PSI0IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjUgLTAuODY2IDAuODY2IDAuNSAtMC42Nzc5IDE0LjgyNTIpIiBjbGFzcz0ic3QyIiB3aWR0aD0iMSIgaGVpZ2h0PSI4Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-run: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTggNXYxNGwxMS03eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-running: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMjU2IDhDMTE5IDggOCAxMTkgOCAyNTZzMTExIDI0OCAyNDggMjQ4IDI0OC0xMTEgMjQ4LTI0OFMzOTMgOCAyNTYgOHptOTYgMzI4YzAgOC44LTcuMiAxNi0xNiAxNkgxNzZjLTguOCAwLTE2LTcuMi0xNi0xNlYxNzZjMC04LjggNy4yLTE2IDE2LTE2aDE2MGM4LjggMCAxNiA3LjIgMTYgMTZ2MTYweiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-save: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE3IDNINWMtMS4xMSAwLTIgLjktMiAydjE0YzAgMS4xLjg5IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjdsLTQtNHptLTUgMTZjLTEuNjYgMC0zLTEuMzQtMy0zczEuMzQtMyAzLTMgMyAxLjM0IDMgMy0xLjM0IDMtMyAzem0zLTEwSDVWNWgxMHY0eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-search: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjEsMTAuOWgtMC43bC0wLjItMC4yYzAuOC0wLjksMS4zLTIuMiwxLjMtMy41YzAtMy0yLjQtNS40LTUuNC01LjRTMS44LDQuMiwxLjgsNy4xczIuNCw1LjQsNS40LDUuNCBjMS4zLDAsMi41LTAuNSwzLjUtMS4zbDAuMiwwLjJ2MC43bDQuMSw0LjFsMS4yLTEuMkwxMi4xLDEwLjl6IE03LjEsMTAuOWMtMi4xLDAtMy43LTEuNy0zLjctMy43czEuNy0zLjcsMy43LTMuN3MzLjcsMS43LDMuNywzLjcgUzkuMiwxMC45LDcuMSwxMC45eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-settings: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuNDMgMTIuOThjLjA0LS4zMi4wNy0uNjQuMDctLjk4cy0uMDMtLjY2LS4wNy0uOThsMi4xMS0xLjY1Yy4xOS0uMTUuMjQtLjQyLjEyLS42NGwtMi0zLjQ2Yy0uMTItLjIyLS4zOS0uMy0uNjEtLjIybC0yLjQ5IDFjLS41Mi0uNC0xLjA4LS43My0xLjY5LS45OGwtLjM4LTIuNjVBLjQ4OC40ODggMCAwMDE0IDJoLTRjLS4yNSAwLS40Ni4xOC0uNDkuNDJsLS4zOCAyLjY1Yy0uNjEuMjUtMS4xNy41OS0xLjY5Ljk4bC0yLjQ5LTFjLS4yMy0uMDktLjQ5IDAtLjYxLjIybC0yIDMuNDZjLS4xMy4yMi0uMDcuNDkuMTIuNjRsMi4xMSAxLjY1Yy0uMDQuMzItLjA3LjY1LS4wNy45OHMuMDMuNjYuMDcuOThsLTIuMTEgMS42NWMtLjE5LjE1LS4yNC40Mi0uMTIuNjRsMiAzLjQ2Yy4xMi4yMi4zOS4zLjYxLjIybDIuNDktMWMuNTIuNCAxLjA4LjczIDEuNjkuOThsLjM4IDIuNjVjLjAzLjI0LjI0LjQyLjQ5LjQyaDRjLjI1IDAgLjQ2LS4xOC40OS0uNDJsLjM4LTIuNjVjLjYxLS4yNSAxLjE3LS41OSAxLjY5LS45OGwyLjQ5IDFjLjIzLjA5LjQ5IDAgLjYxLS4yMmwyLTMuNDZjLjEyLS4yMi4wNy0uNDktLjEyLS42NGwtMi4xMS0xLjY1ek0xMiAxNS41Yy0xLjkzIDAtMy41LTEuNTctMy41LTMuNXMxLjU3LTMuNSAzLjUtMy41IDMuNSAxLjU3IDMuNSAzLjUtMS41NyAzLjUtMy41IDMuNXoiLz4KPC9zdmc+Cg==);
  --jp-icon-share: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIHZpZXdCb3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTSAxOCAyIEMgMTYuMzU0OTkgMiAxNSAzLjM1NDk5MDQgMTUgNSBDIDE1IDUuMTkwOTUyOSAxNS4wMjE3OTEgNS4zNzcxMjI0IDE1LjA1NjY0MSA1LjU1ODU5MzggTCA3LjkyMTg3NSA5LjcyMDcwMzEgQyA3LjM5ODUzOTkgOS4yNzc4NTM5IDYuNzMyMDc3MSA5IDYgOSBDIDQuMzU0OTkwNCA5IDMgMTAuMzU0OTkgMyAxMiBDIDMgMTMuNjQ1MDEgNC4zNTQ5OTA0IDE1IDYgMTUgQyA2LjczMjA3NzEgMTUgNy4zOTg1Mzk5IDE0LjcyMjE0NiA3LjkyMTg3NSAxNC4yNzkyOTcgTCAxNS4wNTY2NDEgMTguNDM5NDUzIEMgMTUuMDIxNTU1IDE4LjYyMTUxNCAxNSAxOC44MDgzODYgMTUgMTkgQyAxNSAyMC42NDUwMSAxNi4zNTQ5OSAyMiAxOCAyMiBDIDE5LjY0NTAxIDIyIDIxIDIwLjY0NTAxIDIxIDE5IEMgMjEgMTcuMzU0OTkgMTkuNjQ1MDEgMTYgMTggMTYgQyAxNy4yNjc0OCAxNiAxNi42MDE1OTMgMTYuMjc5MzI4IDE2LjA3ODEyNSAxNi43MjI2NTYgTCA4Ljk0MzM1OTQgMTIuNTU4NTk0IEMgOC45NzgyMDk1IDEyLjM3NzEyMiA5IDEyLjE5MDk1MyA5IDEyIEMgOSAxMS44MDkwNDcgOC45NzgyMDk1IDExLjYyMjg3OCA4Ljk0MzM1OTQgMTEuNDQxNDA2IEwgMTYuMDc4MTI1IDcuMjc5Mjk2OSBDIDE2LjYwMTQ2IDcuNzIyMTQ2MSAxNy4yNjc5MjMgOCAxOCA4IEMgMTkuNjQ1MDEgOCAyMSA2LjY0NTAwOTYgMjEgNSBDIDIxIDMuMzU0OTkwNCAxOS42NDUwMSAyIDE4IDIgeiBNIDE4IDQgQyAxOC41NjQxMjkgNCAxOSA0LjQzNTg3MDYgMTkgNSBDIDE5IDUuNTY0MTI5NCAxOC41NjQxMjkgNiAxOCA2IEMgMTcuNDM1ODcxIDYgMTcgNS41NjQxMjk0IDE3IDUgQyAxNyA0LjQzNTg3MDYgMTcuNDM1ODcxIDQgMTggNCB6IE0gNiAxMSBDIDYuNTY0MTI5NCAxMSA3IDExLjQzNTg3MSA3IDEyIEMgNyAxMi41NjQxMjkgNi41NjQxMjk0IDEzIDYgMTMgQyA1LjQzNTg3MDYgMTMgNSAxMi41NjQxMjkgNSAxMiBDIDUgMTEuNDM1ODcxIDUuNDM1ODcwNiAxMSA2IDExIHogTSAxOCAxOCBDIDE4LjU2NDEyOSAxOCAxOSAxOC40MzU4NzEgMTkgMTkgQyAxOSAxOS41NjQxMjkgMTguNTY0MTI5IDIwIDE4IDIwIEMgMTcuNDM1ODcxIDIwIDE3IDE5LjU2NDEyOSAxNyAxOSBDIDE3IDE4LjQzNTg3MSAxNy40MzU4NzEgMTggMTggMTggeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-spreadsheet: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNENBRjUwIiBkPSJNMi4yIDIuMnYxNy42aDE3LjZWMi4ySDIuMnptMTUuNCA3LjdoLTUuNVY0LjRoNS41djUuNXpNOS45IDQuNHY1LjVINC40VjQuNGg1LjV6bS01LjUgNy43aDUuNXY1LjVINC40di01LjV6bTcuNyA1LjV2LTUuNWg1LjV2NS41aC01LjV6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-stop: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik02IDZoMTJ2MTJINnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-tab: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIxIDNIM2MtMS4xIDAtMiAuOS0yIDJ2MTRjMCAxLjEuOSAyIDIgMmgxOGMxLjEgMCAyLS45IDItMlY1YzAtMS4xLS45LTItMi0yem0wIDE2SDNWNWgxMHY0aDh2MTB6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-table-rows: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik0yMSw4SDNWNGgxOFY4eiBNMjEsMTBIM3Y0aDE4VjEweiBNMjEsMTZIM3Y0aDE4VjE2eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-tag: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjgiIGhlaWdodD0iMjgiIHZpZXdCb3g9IjAgMCA0MyAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTI4LjgzMzIgMTIuMzM0TDMyLjk5OTggMTYuNTAwN0wzNy4xNjY1IDEyLjMzNEgyOC44MzMyWiIvPgoJCTxwYXRoIGQ9Ik0xNi4yMDk1IDIxLjYxMDRDMTUuNjg3MyAyMi4xMjk5IDE0Ljg0NDMgMjIuMTI5OSAxNC4zMjQ4IDIxLjYxMDRMNi45ODI5IDE0LjcyNDVDNi41NzI0IDE0LjMzOTQgNi4wODMxMyAxMy42MDk4IDYuMDQ3ODYgMTMuMDQ4MkM1Ljk1MzQ3IDExLjUyODggNi4wMjAwMiA4LjYxOTQ0IDYuMDY2MjEgNy4wNzY5NUM2LjA4MjgxIDYuNTE0NzcgNi41NTU0OCA2LjA0MzQ3IDcuMTE4MDQgNi4wMzA1NUM5LjA4ODYzIDUuOTg0NzMgMTMuMjYzOCA1LjkzNTc5IDEzLjY1MTggNi4zMjQyNUwyMS43MzY5IDEzLjYzOUMyMi4yNTYgMTQuMTU4NSAyMS43ODUxIDE1LjQ3MjQgMjEuMjYyIDE1Ljk5NDZMMTYuMjA5NSAyMS42MTA0Wk05Ljc3NTg1IDguMjY1QzkuMzM1NTEgNy44MjU2NiA4LjYyMzUxIDcuODI1NjYgOC4xODI4IDguMjY1QzcuNzQzNDYgOC43MDU3MSA3Ljc0MzQ2IDkuNDE3MzMgOC4xODI4IDkuODU2NjdDOC42MjM4MiAxMC4yOTY0IDkuMzM1ODIgMTAuMjk2NCA5Ljc3NTg1IDkuODU2NjdDMTAuMjE1NiA5LjQxNzMzIDEwLjIxNTYgOC43MDUzMyA5Ljc3NTg1IDguMjY1WiIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-terminal: url(data:image/svg+xml;base64,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);
  --jp-icon-text-editor: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtdGV4dC1lZGl0b3ItaWNvbi1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xNSAxNUgzdjJoMTJ2LTJ6bTAtOEgzdjJoMTJWN3pNMyAxM2gxOHYtMkgzdjJ6bTAgOGgxOHYtMkgzdjJ6TTMgM3YyaDE4VjNIM3oiLz4KPC9zdmc+Cg==);
  --jp-icon-toc: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik03LDVIMjFWN0g3VjVNNywxM1YxMUgyMVYxM0g3TTQsNC41QTEuNSwxLjUgMCAwLDEgNS41LDZBMS41LDEuNSAwIDAsMSA0LDcuNUExLjUsMS41IDAgMCwxIDIuNSw2QTEuNSwxLjUgMCAwLDEgNCw0LjVNNCwxMC41QTEuNSwxLjUgMCAwLDEgNS41LDEyQTEuNSwxLjUgMCAwLDEgNCwxMy41QTEuNSwxLjUgMCAwLDEgMi41LDEyQTEuNSwxLjUgMCAwLDEgNCwxMC41TTcsMTlWMTdIMjFWMTlIN000LDE2LjVBMS41LDEuNSAwIDAsMSA1LjUsMThBMS41LDEuNSAwIDAsMSA0LDE5LjVBMS41LDEuNSAwIDAsMSAyLjUsMThBMS41LDEuNSAwIDAsMSA0LDE2LjVaIiAvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-tree-view: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik0yMiAxMVYzaC03djNIOVYzSDJ2OGg3VjhoMnYxMGg0djNoN3YtOGgtN3YzaC0yVjhoMnYzeiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDIgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMiAxNy4xODQ0IDIuOTY5NjggMTQuMzAzMiAxLjg2MDk0IDExLjQ0MDlaIi8+CiAgICA8cGF0aCBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiMzMzMzMzMiIHN0cm9rZT0iIzMzMzMzMyIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOCA5Ljg2NzE5KSIgZD0iTTIuODYwMTUgNC44NjUzNUwwLjcyNjU0OSAyLjk5OTU5TDAgMy42MzA0NUwyLjg2MDE1IDYuMTMxNTdMOCAwLjYzMDg3Mkw3LjI3ODU3IDBMMi44NjAxNSA0Ljg2NTM1WiIvPgo8L3N2Zz4K);
  --jp-icon-undo: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjUgOGMtMi42NSAwLTUuMDUuOTktNi45IDIuNkwyIDd2OWg5bC0zLjYyLTMuNjJjMS4zOS0xLjE2IDMuMTYtMS44OCA1LjEyLTEuODggMy41NCAwIDYuNTUgMi4zMSA3LjYgNS41bDIuMzctLjc4QzIxLjA4IDExLjAzIDE3LjE1IDggMTIuNSA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-user: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIHZpZXdCb3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE2IDdhNCA0IDAgMTEtOCAwIDQgNCAwIDAxOCAwek0xMiAxNGE3IDcgMCAwMC03IDdoMTRhNyA3IDAgMDAtNy03eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-users: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZlcnNpb249IjEuMSIgdmlld0JveD0iMCAwIDM2IDI0IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogPGcgY2xhc3M9ImpwLWljb24zIiB0cmFuc2Zvcm09Im1hdHJpeCgxLjczMjcgMCAwIDEuNzMyNyAtMy42MjgyIC4wOTk1NzcpIiBmaWxsPSIjNjE2MTYxIj4KICA8cGF0aCB0cmFuc2Zvcm09Im1hdHJpeCgxLjUsMCwwLDEuNSwwLC02KSIgZD0ibTEyLjE4NiA3LjUwOThjLTEuMDUzNSAwLTEuOTc1NyAwLjU2NjUtMi40Nzg1IDEuNDEwMiAwLjc1MDYxIDAuMzEyNzcgMS4zOTc0IDAuODI2NDggMS44NzMgMS40NzI3aDMuNDg2M2MwLTEuNTkyLTEuMjg4OS0yLjg4MjgtMi44ODA5LTIuODgyOHoiLz4KICA8cGF0aCBkPSJtMjAuNDY1IDIuMzg5NWEyLjE4ODUgMi4xODg1IDAgMCAxLTIuMTg4NCAyLjE4ODUgMi4xODg1IDIuMTg4NSAwIDAgMS0yLjE4ODUtMi4xODg1IDIuMTg4NSAyLjE4ODUgMCAwIDEgMi4xODg1LTIuMTg4NSAyLjE4ODUgMi4xODg1IDAgMCAxIDIuMTg4NCAyLjE4ODV6Ii8+CiAgPHBhdGggdHJhbnNmb3JtPSJtYXRyaXgoMS41LDAsMCwxLjUsMCwtNikiIGQ9Im0zLjU4OTggOC40MjE5Yy0xLjExMjYgMC0yLjAxMzcgMC45MDExMS0yLjAxMzcgMi4wMTM3aDIuODE0NWMwLjI2Nzk3LTAuMzczMDkgMC41OTA3LTAuNzA0MzUgMC45NTg5OC0wLjk3ODUyLTAuMzQ0MzMtMC42MTY4OC0xLjAwMzEtMS4wMzUyLTEuNzU5OC0xLjAzNTJ6Ii8+CiAgPHBhdGggZD0ibTYuOTE1NCA0LjYyM2ExLjUyOTQgMS41Mjk0IDAgMCAxLTEuNTI5NCAxLjUyOTQgMS41Mjk0IDEuNTI5NCAwIDAgMS0xLjUyOTQtMS41Mjk0IDEuNTI5NCAxLjUyOTQgMCAwIDEgMS41Mjk0LTEuNTI5NCAxLjUyOTQgMS41Mjk0IDAgMCAxIDEuNTI5NCAxLjUyOTR6Ii8+CiAgPHBhdGggZD0ibTYuMTM1IDEzLjUzNWMwLTMuMjM5MiAyLjYyNTktNS44NjUgNS44NjUtNS44NjUgMy4yMzkyIDAgNS44NjUgMi42MjU5IDUuODY1IDUuODY1eiIvPgogIDxjaXJjbGUgY3g9IjEyIiBjeT0iMy43Njg1IiByPSIyLjk2ODUiLz4KIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-vega: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbjEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjEyMTIxIj4KICAgIDxwYXRoIGQ9Ik0xMC42IDUuNGwyLjItMy4ySDIuMnY3LjNsNC02LjZ6Ii8+CiAgICA8cGF0aCBkPSJNMTUuOCAyLjJsLTQuNCA2LjZMNyA2LjNsLTQuOCA4djUuNWgxNy42VjIuMmgtNHptLTcgMTUuNEg1LjV2LTQuNGgzLjN2NC40em00LjQgMEg5LjhWOS44aDMuNHY3Ljh6bTQuNCAwaC0zLjRWNi41aDMuNHYxMS4xeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-word: url(data:image/svg+xml;base64,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);
  --jp-icon-yaml: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1jb250cmFzdDIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjRDgxQjYwIj4KICAgIDxwYXRoIGQ9Ik03LjIgMTguNnYtNS40TDMgNS42aDMuM2wxLjQgMy4xYy4zLjkuNiAxLjYgMSAyLjUuMy0uOC42LTEuNiAxLTIuNWwxLjQtMy4xaDMuNGwtNC40IDcuNnY1LjVsLTIuOS0uMXoiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxNi41IiByPSIyLjEiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxMSIgcj0iMi4xIi8+CiAgPC9nPgo8L3N2Zz4K);
}

/* Icon CSS class declarations */

.jp-AddAboveIcon {
  background-image: var(--jp-icon-add-above);
}

.jp-AddBelowIcon {
  background-image: var(--jp-icon-add-below);
}

.jp-AddIcon {
  background-image: var(--jp-icon-add);
}

.jp-BellIcon {
  background-image: var(--jp-icon-bell);
}

.jp-BugDotIcon {
  background-image: var(--jp-icon-bug-dot);
}

.jp-BugIcon {
  background-image: var(--jp-icon-bug);
}

.jp-BuildIcon {
  background-image: var(--jp-icon-build);
}

.jp-CaretDownEmptyIcon {
  background-image: var(--jp-icon-caret-down-empty);
}

.jp-CaretDownEmptyThinIcon {
  background-image: var(--jp-icon-caret-down-empty-thin);
}

.jp-CaretDownIcon {
  background-image: var(--jp-icon-caret-down);
}

.jp-CaretLeftIcon {
  background-image: var(--jp-icon-caret-left);
}

.jp-CaretRightIcon {
  background-image: var(--jp-icon-caret-right);
}

.jp-CaretUpEmptyThinIcon {
  background-image: var(--jp-icon-caret-up-empty-thin);
}

.jp-CaretUpIcon {
  background-image: var(--jp-icon-caret-up);
}

.jp-CaseSensitiveIcon {
  background-image: var(--jp-icon-case-sensitive);
}

.jp-CheckIcon {
  background-image: var(--jp-icon-check);
}

.jp-CircleEmptyIcon {
  background-image: var(--jp-icon-circle-empty);
}

.jp-CircleIcon {
  background-image: var(--jp-icon-circle);
}

.jp-ClearIcon {
  background-image: var(--jp-icon-clear);
}

.jp-CloseIcon {
  background-image: var(--jp-icon-close);
}

.jp-CodeCheckIcon {
  background-image: var(--jp-icon-code-check);
}

.jp-CodeIcon {
  background-image: var(--jp-icon-code);
}

.jp-CollapseAllIcon {
  background-image: var(--jp-icon-collapse-all);
}

.jp-ConsoleIcon {
  background-image: var(--jp-icon-console);
}

.jp-CopyIcon {
  background-image: var(--jp-icon-copy);
}

.jp-CopyrightIcon {
  background-image: var(--jp-icon-copyright);
}

.jp-CutIcon {
  background-image: var(--jp-icon-cut);
}

.jp-DeleteIcon {
  background-image: var(--jp-icon-delete);
}

.jp-DownloadIcon {
  background-image: var(--jp-icon-download);
}

.jp-DuplicateIcon {
  background-image: var(--jp-icon-duplicate);
}

.jp-EditIcon {
  background-image: var(--jp-icon-edit);
}

.jp-EllipsesIcon {
  background-image: var(--jp-icon-ellipses);
}

.jp-ErrorIcon {
  background-image: var(--jp-icon-error);
}

.jp-ExpandAllIcon {
  background-image: var(--jp-icon-expand-all);
}

.jp-ExtensionIcon {
  background-image: var(--jp-icon-extension);
}

.jp-FastForwardIcon {
  background-image: var(--jp-icon-fast-forward);
}

.jp-FileIcon {
  background-image: var(--jp-icon-file);
}

.jp-FileUploadIcon {
  background-image: var(--jp-icon-file-upload);
}

.jp-FilterDotIcon {
  background-image: var(--jp-icon-filter-dot);
}

.jp-FilterIcon {
  background-image: var(--jp-icon-filter);
}

.jp-FilterListIcon {
  background-image: var(--jp-icon-filter-list);
}

.jp-FolderFavoriteIcon {
  background-image: var(--jp-icon-folder-favorite);
}

.jp-FolderIcon {
  background-image: var(--jp-icon-folder);
}

.jp-HomeIcon {
  background-image: var(--jp-icon-home);
}

.jp-Html5Icon {
  background-image: var(--jp-icon-html5);
}

.jp-ImageIcon {
  background-image: var(--jp-icon-image);
}

.jp-InfoIcon {
  background-image: var(--jp-icon-info);
}

.jp-InspectorIcon {
  background-image: var(--jp-icon-inspector);
}

.jp-JsonIcon {
  background-image: var(--jp-icon-json);
}

.jp-JuliaIcon {
  background-image: var(--jp-icon-julia);
}

.jp-JupyterFaviconIcon {
  background-image: var(--jp-icon-jupyter-favicon);
}

.jp-JupyterIcon {
  background-image: var(--jp-icon-jupyter);
}

.jp-JupyterlabWordmarkIcon {
  background-image: var(--jp-icon-jupyterlab-wordmark);
}

.jp-KernelIcon {
  background-image: var(--jp-icon-kernel);
}

.jp-KeyboardIcon {
  background-image: var(--jp-icon-keyboard);
}

.jp-LaunchIcon {
  background-image: var(--jp-icon-launch);
}

.jp-LauncherIcon {
  background-image: var(--jp-icon-launcher);
}

.jp-LineFormIcon {
  background-image: var(--jp-icon-line-form);
}

.jp-LinkIcon {
  background-image: var(--jp-icon-link);
}

.jp-ListIcon {
  background-image: var(--jp-icon-list);
}

.jp-MarkdownIcon {
  background-image: var(--jp-icon-markdown);
}

.jp-MoveDownIcon {
  background-image: var(--jp-icon-move-down);
}

.jp-MoveUpIcon {
  background-image: var(--jp-icon-move-up);
}

.jp-NewFolderIcon {
  background-image: var(--jp-icon-new-folder);
}

.jp-NotTrustedIcon {
  background-image: var(--jp-icon-not-trusted);
}

.jp-NotebookIcon {
  background-image: var(--jp-icon-notebook);
}

.jp-NumberingIcon {
  background-image: var(--jp-icon-numbering);
}

.jp-OfflineBoltIcon {
  background-image: var(--jp-icon-offline-bolt);
}

.jp-PaletteIcon {
  background-image: var(--jp-icon-palette);
}

.jp-PasteIcon {
  background-image: var(--jp-icon-paste);
}

.jp-PdfIcon {
  background-image: var(--jp-icon-pdf);
}

.jp-PythonIcon {
  background-image: var(--jp-icon-python);
}

.jp-RKernelIcon {
  background-image: var(--jp-icon-r-kernel);
}

.jp-ReactIcon {
  background-image: var(--jp-icon-react);
}

.jp-RedoIcon {
  background-image: var(--jp-icon-redo);
}

.jp-RefreshIcon {
  background-image: var(--jp-icon-refresh);
}

.jp-RegexIcon {
  background-image: var(--jp-icon-regex);
}

.jp-RunIcon {
  background-image: var(--jp-icon-run);
}

.jp-RunningIcon {
  background-image: var(--jp-icon-running);
}

.jp-SaveIcon {
  background-image: var(--jp-icon-save);
}

.jp-SearchIcon {
  background-image: var(--jp-icon-search);
}

.jp-SettingsIcon {
  background-image: var(--jp-icon-settings);
}

.jp-ShareIcon {
  background-image: var(--jp-icon-share);
}

.jp-SpreadsheetIcon {
  background-image: var(--jp-icon-spreadsheet);
}

.jp-StopIcon {
  background-image: var(--jp-icon-stop);
}

.jp-TabIcon {
  background-image: var(--jp-icon-tab);
}

.jp-TableRowsIcon {
  background-image: var(--jp-icon-table-rows);
}

.jp-TagIcon {
  background-image: var(--jp-icon-tag);
}

.jp-TerminalIcon {
  background-image: var(--jp-icon-terminal);
}

.jp-TextEditorIcon {
  background-image: var(--jp-icon-text-editor);
}

.jp-TocIcon {
  background-image: var(--jp-icon-toc);
}

.jp-TreeViewIcon {
  background-image: var(--jp-icon-tree-view);
}

.jp-TrustedIcon {
  background-image: var(--jp-icon-trusted);
}

.jp-UndoIcon {
  background-image: var(--jp-icon-undo);
}

.jp-UserIcon {
  background-image: var(--jp-icon-user);
}

.jp-UsersIcon {
  background-image: var(--jp-icon-users);
}

.jp-VegaIcon {
  background-image: var(--jp-icon-vega);
}

.jp-WordIcon {
  background-image: var(--jp-icon-word);
}

.jp-YamlIcon {
  background-image: var(--jp-icon-yaml);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * (DEPRECATED) Support for consuming icons as CSS background images
 */

.jp-Icon,
.jp-MaterialIcon {
  background-position: center;
  background-repeat: no-repeat;
  background-size: 16px;
  min-width: 16px;
  min-height: 16px;
}

.jp-Icon-cover {
  background-position: center;
  background-repeat: no-repeat;
  background-size: cover;
}

/**
 * (DEPRECATED) Support for specific CSS icon sizes
 */

.jp-Icon-16 {
  background-size: 16px;
  min-width: 16px;
  min-height: 16px;
}

.jp-Icon-18 {
  background-size: 18px;
  min-width: 18px;
  min-height: 18px;
}

.jp-Icon-20 {
  background-size: 20px;
  min-width: 20px;
  min-height: 20px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.lm-TabBar .lm-TabBar-addButton {
  align-items: center;
  display: flex;
  padding: 4px;
  padding-bottom: 5px;
  margin-right: 1px;
  background-color: var(--jp-layout-color2);
}

.lm-TabBar .lm-TabBar-addButton:hover {
  background-color: var(--jp-layout-color1);
}

.lm-DockPanel-tabBar .lm-TabBar-tab {
  width: var(--jp-private-horizontal-tab-width);
}

.lm-DockPanel-tabBar .lm-TabBar-content {
  flex: unset;
}

.lm-DockPanel-tabBar[data-orientation='horizontal'] {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * Support for icons as inline SVG HTMLElements
 */

/* recolor the primary elements of an icon */
.jp-icon0[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon1[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon2[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon3[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon4[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon0[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon1[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon2[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon3[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon4[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/* recolor the accent elements of an icon */
.jp-icon-accent0[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-accent1[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-accent2[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-accent3[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-accent4[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-accent0[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-accent1[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-accent2[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-accent3[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-accent4[stroke] {
  stroke: var(--jp-layout-color4);
}

/* set the color of an icon to transparent */
.jp-icon-none[fill] {
  fill: none;
}

.jp-icon-none[stroke] {
  stroke: none;
}

/* brand icon colors. Same for light and dark */
.jp-icon-brand0[fill] {
  fill: var(--jp-brand-color0);
}

.jp-icon-brand1[fill] {
  fill: var(--jp-brand-color1);
}

.jp-icon-brand2[fill] {
  fill: var(--jp-brand-color2);
}

.jp-icon-brand3[fill] {
  fill: var(--jp-brand-color3);
}

.jp-icon-brand4[fill] {
  fill: var(--jp-brand-color4);
}

.jp-icon-brand0[stroke] {
  stroke: var(--jp-brand-color0);
}

.jp-icon-brand1[stroke] {
  stroke: var(--jp-brand-color1);
}

.jp-icon-brand2[stroke] {
  stroke: var(--jp-brand-color2);
}

.jp-icon-brand3[stroke] {
  stroke: var(--jp-brand-color3);
}

.jp-icon-brand4[stroke] {
  stroke: var(--jp-brand-color4);
}

/* warn icon colors. Same for light and dark */
.jp-icon-warn0[fill] {
  fill: var(--jp-warn-color0);
}

.jp-icon-warn1[fill] {
  fill: var(--jp-warn-color1);
}

.jp-icon-warn2[fill] {
  fill: var(--jp-warn-color2);
}

.jp-icon-warn3[fill] {
  fill: var(--jp-warn-color3);
}

.jp-icon-warn0[stroke] {
  stroke: var(--jp-warn-color0);
}

.jp-icon-warn1[stroke] {
  stroke: var(--jp-warn-color1);
}

.jp-icon-warn2[stroke] {
  stroke: var(--jp-warn-color2);
}

.jp-icon-warn3[stroke] {
  stroke: var(--jp-warn-color3);
}

/* icon colors that contrast well with each other and most backgrounds */
.jp-icon-contrast0[fill] {
  fill: var(--jp-icon-contrast-color0);
}

.jp-icon-contrast1[fill] {
  fill: var(--jp-icon-contrast-color1);
}

.jp-icon-contrast2[fill] {
  fill: var(--jp-icon-contrast-color2);
}

.jp-icon-contrast3[fill] {
  fill: var(--jp-icon-contrast-color3);
}

.jp-icon-contrast0[stroke] {
  stroke: var(--jp-icon-contrast-color0);
}

.jp-icon-contrast1[stroke] {
  stroke: var(--jp-icon-contrast-color1);
}

.jp-icon-contrast2[stroke] {
  stroke: var(--jp-icon-contrast-color2);
}

.jp-icon-contrast3[stroke] {
  stroke: var(--jp-icon-contrast-color3);
}

.jp-icon-dot[fill] {
  fill: var(--jp-warn-color0);
}

.jp-jupyter-icon-color[fill] {
  fill: var(--jp-jupyter-icon-color, var(--jp-warn-color0));
}

.jp-notebook-icon-color[fill] {
  fill: var(--jp-notebook-icon-color, var(--jp-warn-color0));
}

.jp-json-icon-color[fill] {
  fill: var(--jp-json-icon-color, var(--jp-warn-color1));
}

.jp-console-icon-color[fill] {
  fill: var(--jp-console-icon-color, white);
}

.jp-console-icon-background-color[fill] {
  fill: var(--jp-console-icon-background-color, var(--jp-brand-color1));
}

.jp-terminal-icon-color[fill] {
  fill: var(--jp-terminal-icon-color, var(--jp-layout-color2));
}

.jp-terminal-icon-background-color[fill] {
  fill: var(
    --jp-terminal-icon-background-color,
    var(--jp-inverse-layout-color2)
  );
}

.jp-text-editor-icon-color[fill] {
  fill: var(--jp-text-editor-icon-color, var(--jp-inverse-layout-color3));
}

.jp-inspector-icon-color[fill] {
  fill: var(--jp-inspector-icon-color, var(--jp-inverse-layout-color3));
}

/* CSS for icons in selected filebrowser listing items */
.jp-DirListing-item.jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}

.jp-DirListing-item.jp-mod-selected .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}

/* stylelint-disable selector-max-class, selector-max-compound-selectors */

/**
* TODO: come up with non css-hack solution for showing the busy icon on top
*  of the close icon
* CSS for complex behavior of close icon of tabs in the main area tabbar
*/
.lm-DockPanel-tabBar
  .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon3[fill] {
  fill: none;
}

.lm-DockPanel-tabBar
  .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon-busy[fill] {
  fill: var(--jp-inverse-layout-color3);
}

/* stylelint-enable selector-max-class, selector-max-compound-selectors */

/* CSS for icons in status bar */
#jp-main-statusbar .jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}

#jp-main-statusbar .jp-mod-selected .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}

/* special handling for splash icon CSS. While the theme CSS reloads during
   splash, the splash icon can loose theming. To prevent that, we set a
   default for its color variable */
:root {
  --jp-warn-color0: var(--md-orange-700);
}

/* not sure what to do with this one, used in filebrowser listing */
.jp-DragIcon {
  margin-right: 4px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * Support for alt colors for icons as inline SVG HTMLElements
 */

/* alt recolor the primary elements of an icon */
.jp-icon-alt .jp-icon0[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-alt .jp-icon1[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-alt .jp-icon2[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-alt .jp-icon3[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-alt .jp-icon4[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-alt .jp-icon0[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-alt .jp-icon1[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-alt .jp-icon2[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-alt .jp-icon3[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-alt .jp-icon4[stroke] {
  stroke: var(--jp-layout-color4);
}

/* alt recolor the accent elements of an icon */
.jp-icon-alt .jp-icon-accent0[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-alt .jp-icon-accent1[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-alt .jp-icon-accent2[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-alt .jp-icon-accent3[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-alt .jp-icon-accent4[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-alt .jp-icon-accent0[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-alt .jp-icon-accent1[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-alt .jp-icon-accent2[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-alt .jp-icon-accent3[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-alt .jp-icon-accent4[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-icon-hoverShow:not(:hover) .jp-icon-hoverShow-content {
  display: none !important;
}

/**
 * Support for hover colors for icons as inline SVG HTMLElements
 */

/**
 * regular colors
 */

/* recolor the primary elements of an icon */
.jp-icon-hover :hover .jp-icon0-hover[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-hover :hover .jp-icon1-hover[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-hover :hover .jp-icon2-hover[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-hover :hover .jp-icon3-hover[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-hover :hover .jp-icon4-hover[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-hover :hover .jp-icon0-hover[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-hover :hover .jp-icon1-hover[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-hover :hover .jp-icon2-hover[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-hover :hover .jp-icon3-hover[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-hover :hover .jp-icon4-hover[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/* recolor the accent elements of an icon */
.jp-icon-hover :hover .jp-icon-accent0-hover[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-hover :hover .jp-icon-accent1-hover[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-hover :hover .jp-icon-accent2-hover[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-hover :hover .jp-icon-accent3-hover[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-hover :hover .jp-icon-accent4-hover[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-hover :hover .jp-icon-accent0-hover[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-hover :hover .jp-icon-accent1-hover[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-hover :hover .jp-icon-accent2-hover[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-hover :hover .jp-icon-accent3-hover[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-hover :hover .jp-icon-accent4-hover[stroke] {
  stroke: var(--jp-layout-color4);
}

/* set the color of an icon to transparent */
.jp-icon-hover :hover .jp-icon-none-hover[fill] {
  fill: none;
}

.jp-icon-hover :hover .jp-icon-none-hover[stroke] {
  stroke: none;
}

/**
 * inverse colors
 */

/* inverse recolor the primary elements of an icon */
.jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[stroke] {
  stroke: var(--jp-layout-color4);
}

/* inverse recolor the accent elements of an icon */
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-IFrame {
  width: 100%;
  height: 100%;
}

.jp-IFrame > iframe {
  border: none;
}

/*
When drag events occur, `lm-mod-override-cursor` is added to the body.
Because iframes steal all cursor events, the following two rules are necessary
to suppress pointer events while resize drags are occurring. There may be a
better solution to this problem.
*/
body.lm-mod-override-cursor .jp-IFrame {
  position: relative;
}

body.lm-mod-override-cursor .jp-IFrame::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-HoverBox {
  position: fixed;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-FormGroup-content fieldset {
  border: none;
  padding: 0;
  min-width: 0;
  width: 100%;
}

/* stylelint-disable selector-max-type */

.jp-FormGroup-content fieldset .jp-inputFieldWrapper input,
.jp-FormGroup-content fieldset .jp-inputFieldWrapper select,
.jp-FormGroup-content fieldset .jp-inputFieldWrapper textarea {
  font-size: var(--jp-content-font-size2);
  border-color: var(--jp-input-border-color);
  border-style: solid;
  border-radius: var(--jp-border-radius);
  border-width: 1px;
  padding: 6px 8px;
  background: none;
  color: var(--jp-ui-font-color0);
  height: inherit;
}

.jp-FormGroup-content fieldset input[type='checkbox'] {
  position: relative;
  top: 2px;
  margin-left: 0;
}

.jp-FormGroup-content button.jp-mod-styled {
  cursor: pointer;
}

.jp-FormGroup-content .checkbox label {
  cursor: pointer;
  font-size: var(--jp-content-font-size1);
}

.jp-FormGroup-content .jp-root > fieldset > legend {
  display: none;
}

.jp-FormGroup-content .jp-root > fieldset > p {
  display: none;
}

/** copy of `input.jp-mod-styled:focus` style */
.jp-FormGroup-content fieldset input:focus,
.jp-FormGroup-content fieldset select:focus {
  -moz-outline-radius: unset;
  outline: var(--jp-border-width) solid var(--md-blue-500);
  outline-offset: -1px;
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-FormGroup-content fieldset input:hover:not(:focus),
.jp-FormGroup-content fieldset select:hover:not(:focus) {
  background-color: var(--jp-border-color2);
}

/* stylelint-enable selector-max-type */

.jp-FormGroup-content .checkbox .field-description {
  /* Disable default description field for checkbox:
   because other widgets do not have description fields,
   we add descriptions to each widget on the field level.
  */
  display: none;
}

.jp-FormGroup-content #root__description {
  display: none;
}

.jp-FormGroup-content .jp-modifiedIndicator {
  width: 5px;
  background-color: var(--jp-brand-color2);
  margin-top: 0;
  margin-left: calc(var(--jp-private-settingeditor-modifier-indent) * -1);
  flex-shrink: 0;
}

.jp-FormGroup-content .jp-modifiedIndicator.jp-errorIndicator {
  background-color: var(--jp-error-color0);
  margin-right: 0.5em;
}

/* RJSF ARRAY style */

.jp-arrayFieldWrapper legend {
  font-size: var(--jp-content-font-size2);
  color: var(--jp-ui-font-color0);
  flex-basis: 100%;
  padding: 4px 0;
  font-weight: var(--jp-content-heading-font-weight);
  border-bottom: 1px solid var(--jp-border-color2);
}

.jp-arrayFieldWrapper .field-description {
  padding: 4px 0;
  white-space: pre-wrap;
}

.jp-arrayFieldWrapper .array-item {
  width: 100%;
  border: 1px solid var(--jp-border-color2);
  border-radius: 4px;
  margin: 4px;
}

.jp-ArrayOperations {
  display: flex;
  margin-left: 8px;
}

.jp-ArrayOperationsButton {
  margin: 2px;
}

.jp-ArrayOperationsButton .jp-icon3[fill] {
  fill: var(--jp-ui-font-color0);
}

button.jp-ArrayOperationsButton.jp-mod-styled:disabled {
  cursor: not-allowed;
  opacity: 0.5;
}

/* RJSF form validation error */

.jp-FormGroup-content .validationErrors {
  color: var(--jp-error-color0);
}

/* Hide panel level error as duplicated the field level error */
.jp-FormGroup-content .panel.errors {
  display: none;
}

/* RJSF normal content (settings-editor) */

.jp-FormGroup-contentNormal {
  display: flex;
  align-items: center;
  flex-wrap: wrap;
}

.jp-FormGroup-contentNormal .jp-FormGroup-contentItem {
  margin-left: 7px;
  color: var(--jp-ui-font-color0);
}

.jp-FormGroup-contentNormal .jp-FormGroup-description {
  flex-basis: 100%;
  padding: 4px 7px;
}

.jp-FormGroup-contentNormal .jp-FormGroup-default {
  flex-basis: 100%;
  padding: 4px 7px;
}

.jp-FormGroup-contentNormal .jp-FormGroup-fieldLabel {
  font-size: var(--jp-content-font-size1);
  font-weight: normal;
  min-width: 120px;
}

.jp-FormGroup-contentNormal fieldset:not(:first-child) {
  margin-left: 7px;
}

.jp-FormGroup-contentNormal .field-array-of-string .array-item {
  /* Display `jp-ArrayOperations` buttons side-by-side with content except
    for small screens where flex-wrap will place them one below the other.
  */
  display: flex;
  align-items: center;
  flex-wrap: wrap;
}

.jp-FormGroup-contentNormal .jp-objectFieldWrapper .form-group {
  padding: 2px 8px 2px var(--jp-private-settingeditor-modifier-indent);
  margin-top: 2px;
}

/* RJSF compact content (metadata-form) */

.jp-FormGroup-content.jp-FormGroup-contentCompact {
  width: 100%;
}

.jp-FormGroup-contentCompact .form-group {
  display: flex;
  padding: 0.5em 0.2em 0.5em 0;
}

.jp-FormGroup-contentCompact
  .jp-FormGroup-compactTitle
  .jp-FormGroup-description {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color2);
}

.jp-FormGroup-contentCompact .jp-FormGroup-fieldLabel {
  padding-bottom: 0.3em;
}

.jp-FormGroup-contentCompact .jp-inputFieldWrapper .form-control {
  width: 100%;
  box-sizing: border-box;
}

.jp-FormGroup-contentCompact .jp-arrayFieldWrapper .jp-FormGroup-compactTitle {
  padding-bottom: 7px;
}

.jp-FormGroup-contentCompact
  .jp-objectFieldWrapper
  .jp-objectFieldWrapper
  .form-group {
  padding: 2px 8px 2px var(--jp-private-settingeditor-modifier-indent);
  margin-top: 2px;
}

.jp-FormGroup-contentCompact ul.error-detail {
  margin-block-start: 0.5em;
  margin-block-end: 0.5em;
  padding-inline-start: 1em;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-SidePanel {
  display: flex;
  flex-direction: column;
  min-width: var(--jp-sidebar-min-width);
  overflow-y: auto;
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);
  font-size: var(--jp-ui-font-size1);
}

.jp-SidePanel-header {
  flex: 0 0 auto;
  display: flex;
  border-bottom: var(--jp-border-width) solid var(--jp-border-color2);
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  letter-spacing: 1px;
  margin: 0;
  padding: 2px;
  text-transform: uppercase;
}

.jp-SidePanel-toolbar {
  flex: 0 0 auto;
}

.jp-SidePanel-content {
  flex: 1 1 auto;
}

.jp-SidePanel-toolbar,
.jp-AccordionPanel-toolbar {
  height: var(--jp-private-toolbar-height);
}

.jp-SidePanel-toolbar.jp-Toolbar-micro {
  display: none;
}

.lm-AccordionPanel .jp-AccordionPanel-title {
  box-sizing: border-box;
  line-height: 25px;
  margin: 0;
  display: flex;
  align-items: center;
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  font-size: var(--jp-ui-font-size0);
}

.jp-AccordionPanel-title {
  cursor: pointer;
  user-select: none;
  -moz-user-select: none;
  -webkit-user-select: none;
  text-transform: uppercase;
}

.lm-AccordionPanel[data-orientation='horizontal'] > .jp-AccordionPanel-title {
  /* Title is rotated for horizontal accordion panel using CSS */
  display: block;
  transform-origin: top left;
  transform: rotate(-90deg) translate(-100%);
}

.jp-AccordionPanel-title .lm-AccordionPanel-titleLabel {
  user-select: none;
  text-overflow: ellipsis;
  white-space: nowrap;
  overflow: hidden;
}

.jp-AccordionPanel-title .lm-AccordionPanel-titleCollapser {
  transform: rotate(-90deg);
  margin: auto 0;
  height: 16px;
}

.jp-AccordionPanel-title.lm-mod-expanded .lm-AccordionPanel-titleCollapser {
  transform: rotate(0deg);
}

.lm-AccordionPanel .jp-AccordionPanel-toolbar {
  background: none;
  box-shadow: none;
  border: none;
  margin-left: auto;
}

.lm-AccordionPanel .lm-SplitPanel-handle:hover {
  background: var(--jp-layout-color3);
}

.jp-text-truncated {
  overflow: hidden;
  text-overflow: ellipsis;
  white-space: nowrap;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Spinner {
  position: absolute;
  display: flex;
  justify-content: center;
  align-items: center;
  z-index: 10;
  left: 0;
  top: 0;
  width: 100%;
  height: 100%;
  background: var(--jp-layout-color0);
  outline: none;
}

.jp-SpinnerContent {
  font-size: 10px;
  margin: 50px auto;
  text-indent: -9999em;
  width: 3em;
  height: 3em;
  border-radius: 50%;
  background: var(--jp-brand-color3);
  background: linear-gradient(
    to right,
    #f37626 10%,
    rgba(255, 255, 255, 0) 42%
  );
  position: relative;
  animation: load3 1s infinite linear, fadeIn 1s;
}

.jp-SpinnerContent::before {
  width: 50%;
  height: 50%;
  background: #f37626;
  border-radius: 100% 0 0;
  position: absolute;
  top: 0;
  left: 0;
  content: '';
}

.jp-SpinnerContent::after {
  background: var(--jp-layout-color0);
  width: 75%;
  height: 75%;
  border-radius: 50%;
  content: '';
  margin: auto;
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  right: 0;
}

@keyframes fadeIn {
  0% {
    opacity: 0;
  }

  100% {
    opacity: 1;
  }
}

@keyframes load3 {
  0% {
    transform: rotate(0deg);
  }

  100% {
    transform: rotate(360deg);
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

button.jp-mod-styled {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  border: none;
  box-sizing: border-box;
  text-align: center;
  line-height: 32px;
  height: 32px;
  padding: 0 12px;
  letter-spacing: 0.8px;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

input.jp-mod-styled {
  background: var(--jp-input-background);
  height: 28px;
  box-sizing: border-box;
  border: var(--jp-border-width) solid var(--jp-border-color1);
  padding-left: 7px;
  padding-right: 7px;
  font-size: var(--jp-ui-font-size2);
  color: var(--jp-ui-font-color0);
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

input[type='checkbox'].jp-mod-styled {
  appearance: checkbox;
  -webkit-appearance: checkbox;
  -moz-appearance: checkbox;
  height: auto;
}

input.jp-mod-styled:focus {
  border: var(--jp-border-width) solid var(--md-blue-500);
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-select-wrapper {
  display: flex;
  position: relative;
  flex-direction: column;
  padding: 1px;
  background-color: var(--jp-layout-color1);
  box-sizing: border-box;
  margin-bottom: 12px;
}

.jp-select-wrapper:not(.multiple) {
  height: 28px;
}

.jp-select-wrapper.jp-mod-focused select.jp-mod-styled {
  border: var(--jp-border-width) solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
  background-color: var(--jp-input-active-background);
}

select.jp-mod-styled:hover {
  cursor: pointer;
  color: var(--jp-ui-font-color0);
  background-color: var(--jp-input-hover-background);
  box-shadow: inset 0 0 1px rgba(0, 0, 0, 0.5);
}

select.jp-mod-styled {
  flex: 1 1 auto;
  width: 100%;
  font-size: var(--jp-ui-font-size2);
  background: var(--jp-input-background);
  color: var(--jp-ui-font-color0);
  padding: 0 25px 0 8px;
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  border-radius: 0;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

select.jp-mod-styled:not([multiple]) {
  height: 32px;
}

select.jp-mod-styled[multiple] {
  max-height: 200px;
  overflow-y: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-switch {
  display: flex;
  align-items: center;
  padding-left: 4px;
  padding-right: 4px;
  font-size: var(--jp-ui-font-size1);
  background-color: transparent;
  color: var(--jp-ui-font-color1);
  border: none;
  height: 20px;
}

.jp-switch:hover {
  background-color: var(--jp-layout-color2);
}

.jp-switch-label {
  margin-right: 5px;
  font-family: var(--jp-ui-font-family);
}

.jp-switch-track {
  cursor: pointer;
  background-color: var(--jp-switch-color, var(--jp-border-color1));
  -webkit-transition: 0.4s;
  transition: 0.4s;
  border-radius: 34px;
  height: 16px;
  width: 35px;
  position: relative;
}

.jp-switch-track::before {
  content: '';
  position: absolute;
  height: 10px;
  width: 10px;
  margin: 3px;
  left: 0;
  background-color: var(--jp-ui-inverse-font-color1);
  -webkit-transition: 0.4s;
  transition: 0.4s;
  border-radius: 50%;
}

.jp-switch[aria-checked='true'] .jp-switch-track {
  background-color: var(--jp-switch-true-position-color, var(--jp-warn-color0));
}

.jp-switch[aria-checked='true'] .jp-switch-track::before {
  /* track width (35) - margins (3 + 3) - thumb width (10) */
  left: 19px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

:root {
  --jp-private-toolbar-height: calc(
    28px + var(--jp-border-width)
  ); /* leave 28px for content */
}

.jp-Toolbar {
  color: var(--jp-ui-font-color1);
  flex: 0 0 auto;
  display: flex;
  flex-direction: row;
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  background: var(--jp-toolbar-background);
  min-height: var(--jp-toolbar-micro-height);
  padding: 2px;
  z-index: 8;
  overflow-x: hidden;
}

/* Toolbar items */

.jp-Toolbar > .jp-Toolbar-item.jp-Toolbar-spacer {
  flex-grow: 1;
  flex-shrink: 1;
}

.jp-Toolbar-item.jp-Toolbar-kernelStatus {
  display: inline-block;
  width: 32px;
  background-repeat: no-repeat;
  background-position: center;
  background-size: 16px;
}

.jp-Toolbar > .jp-Toolbar-item {
  flex: 0 0 auto;
  display: flex;
  padding-left: 1px;
  padding-right: 1px;
  font-size: var(--jp-ui-font-size1);
  line-height: var(--jp-private-toolbar-height);
  height: 100%;
}

/* Toolbar buttons */

/* This is the div we use to wrap the react component into a Widget */
div.jp-ToolbarButton {
  color: transparent;
  border: none;
  box-sizing: border-box;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0;
  margin: 0;
}

button.jp-ToolbarButtonComponent {
  background: var(--jp-layout-color1);
  border: none;
  box-sizing: border-box;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0 6px;
  margin: 0;
  height: 24px;
  border-radius: var(--jp-border-radius);
  display: flex;
  align-items: center;
  text-align: center;
  font-size: 14px;
  min-width: unset;
  min-height: unset;
}

button.jp-ToolbarButtonComponent:disabled {
  opacity: 0.4;
}

button.jp-ToolbarButtonComponent > span {
  padding: 0;
  flex: 0 0 auto;
}

button.jp-ToolbarButtonComponent .jp-ToolbarButtonComponent-label {
  font-size: var(--jp-ui-font-size1);
  line-height: 100%;
  padding-left: 2px;
  color: var(--jp-ui-font-color1);
  font-family: var(--jp-ui-font-family);
}

#jp-main-dock-panel[data-mode='single-document']
  .jp-MainAreaWidget
  > .jp-Toolbar.jp-Toolbar-micro {
  padding: 0;
  min-height: 0;
}

#jp-main-dock-panel[data-mode='single-document']
  .jp-MainAreaWidget
  > .jp-Toolbar {
  border: none;
  box-shadow: none;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-WindowedPanel-outer {
  position: relative;
  overflow-y: auto;
}

.jp-WindowedPanel-inner {
  position: relative;
}

.jp-WindowedPanel-window {
  position: absolute;
  left: 0;
  right: 0;
  overflow: visible;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* Sibling imports */

body {
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
}

/* Disable native link decoration styles everywhere outside of dialog boxes */
a {
  text-decoration: unset;
  color: unset;
}

a:hover {
  text-decoration: unset;
  color: unset;
}

/* Accessibility for links inside dialog box text */
.jp-Dialog-content a {
  text-decoration: revert;
  color: var(--jp-content-link-color);
}

.jp-Dialog-content a:hover {
  text-decoration: revert;
}

/* Styles for ui-components */
.jp-Button {
  color: var(--jp-ui-font-color2);
  border-radius: var(--jp-border-radius);
  padding: 0 12px;
  font-size: var(--jp-ui-font-size1);

  /* Copy from blueprint 3 */
  display: inline-flex;
  flex-direction: row;
  border: none;
  cursor: pointer;
  align-items: center;
  justify-content: center;
  text-align: left;
  vertical-align: middle;
  min-height: 30px;
  min-width: 30px;
}

.jp-Button:disabled {
  cursor: not-allowed;
}

.jp-Button:empty {
  padding: 0 !important;
}

.jp-Button.jp-mod-small {
  min-height: 24px;
  min-width: 24px;
  font-size: 12px;
  padding: 0 7px;
}

/* Use our own theme for hover styles */
.jp-Button.jp-mod-minimal:hover {
  background-color: var(--jp-layout-color2);
}

.jp-Button.jp-mod-minimal {
  background: none;
}

.jp-InputGroup {
  display: block;
  position: relative;
}

.jp-InputGroup input {
  box-sizing: border-box;
  border: none;
  border-radius: 0;
  background-color: transparent;
  color: var(--jp-ui-font-color0);
  box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color);
  padding-bottom: 0;
  padding-top: 0;
  padding-left: 10px;
  padding-right: 28px;
  position: relative;
  width: 100%;
  -webkit-appearance: none;
  -moz-appearance: none;
  appearance: none;
  font-size: 14px;
  font-weight: 400;
  height: 30px;
  line-height: 30px;
  outline: none;
  vertical-align: middle;
}

.jp-InputGroup input:focus {
  box-shadow: inset 0 0 0 var(--jp-border-width)
      var(--jp-input-active-box-shadow-color),
    inset 0 0 0 3px var(--jp-input-active-box-shadow-color);
}

.jp-InputGroup input:disabled {
  cursor: not-allowed;
  resize: block;
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color2);
}

.jp-InputGroup input:disabled ~ span {
  cursor: not-allowed;
  color: var(--jp-ui-font-color2);
}

.jp-InputGroup input::placeholder,
input::placeholder {
  color: var(--jp-ui-font-color2);
}

.jp-InputGroupAction {
  position: absolute;
  bottom: 1px;
  right: 0;
  padding: 6px;
}

.jp-HTMLSelect.jp-DefaultStyle select {
  background-color: initial;
  border: none;
  border-radius: 0;
  box-shadow: none;
  color: var(--jp-ui-font-color0);
  display: block;
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  height: 24px;
  line-height: 14px;
  padding: 0 25px 0 10px;
  text-align: left;
  -moz-appearance: none;
  -webkit-appearance: none;
}

.jp-HTMLSelect.jp-DefaultStyle select:disabled {
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color2);
  cursor: not-allowed;
  resize: block;
}

.jp-HTMLSelect.jp-DefaultStyle select:disabled ~ span {
  cursor: not-allowed;
}

/* Use our own theme for hover and option styles */
/* stylelint-disable-next-line selector-max-type */
.jp-HTMLSelect.jp-DefaultStyle select:hover,
.jp-HTMLSelect.jp-DefaultStyle select > option {
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color0);
}

select {
  box-sizing: border-box;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-StatusBar-Widget {
  display: flex;
  align-items: center;
  background: var(--jp-layout-color2);
  min-height: var(--jp-statusbar-height);
  justify-content: space-between;
  padding: 0 10px;
}

.jp-StatusBar-Left {
  display: flex;
  align-items: center;
  flex-direction: row;
}

.jp-StatusBar-Middle {
  display: flex;
  align-items: center;
}

.jp-StatusBar-Right {
  display: flex;
  align-items: center;
  flex-direction: row-reverse;
}

.jp-StatusBar-Item {
  max-height: var(--jp-statusbar-height);
  margin: 0 2px;
  height: var(--jp-statusbar-height);
  white-space: nowrap;
  text-overflow: ellipsis;
  color: var(--jp-ui-font-color1);
  padding: 0 6px;
}

.jp-mod-highlighted:hover {
  background-color: var(--jp-layout-color3);
}

.jp-mod-clicked {
  background-color: var(--jp-brand-color1);
}

.jp-mod-clicked:hover {
  background-color: var(--jp-brand-color0);
}

.jp-mod-clicked .jp-StatusBar-TextItem {
  color: var(--jp-ui-inverse-font-color1);
}

.jp-StatusBar-HoverItem {
  box-shadow: '0px 4px 4px rgba(0, 0, 0, 0.25)';
}

.jp-StatusBar-TextItem {
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  line-height: 24px;
  color: var(--jp-ui-font-color1);
}

.jp-StatusBar-GroupItem {
  display: flex;
  align-items: center;
  flex-direction: row;
}

.jp-Statusbar-ProgressCircle svg {
  display: block;
  margin: 0 auto;
  width: 16px;
  height: 24px;
  align-self: normal;
}

.jp-Statusbar-ProgressCircle path {
  fill: var(--jp-inverse-layout-color3);
}

.jp-Statusbar-ProgressBar-progress-bar {
  height: 10px;
  width: 100px;
  border: solid 0.25px var(--jp-brand-color2);
  border-radius: 3px;
  overflow: hidden;
  align-self: center;
}

.jp-Statusbar-ProgressBar-progress-bar > div {
  background-color: var(--jp-brand-color2);
  background-image: linear-gradient(
    -45deg,
    rgba(255, 255, 255, 0.2) 25%,
    transparent 25%,
    transparent 50%,
    rgba(255, 255, 255, 0.2) 50%,
    rgba(255, 255, 255, 0.2) 75%,
    transparent 75%,
    transparent
  );
  background-size: 40px 40px;
  float: left;
  width: 0%;
  height: 100%;
  font-size: 12px;
  line-height: 14px;
  color: #fff;
  text-align: center;
  animation: jp-Statusbar-ExecutionTime-progress-bar 2s linear infinite;
}

.jp-Statusbar-ProgressBar-progress-bar p {
  color: var(--jp-ui-font-color1);
  font-family: var(--jp-ui-font-family);
  font-size: var(--jp-ui-font-size1);
  line-height: 10px;
  width: 100px;
}

@keyframes jp-Statusbar-ExecutionTime-progress-bar {
  0% {
    background-position: 0 0;
  }

  100% {
    background-position: 40px 40px;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-commandpalette-search-height: 28px;
}

/*-----------------------------------------------------------------------------
| Overall styles
|----------------------------------------------------------------------------*/

.lm-CommandPalette {
  padding-bottom: 0;
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);

  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
}

/*-----------------------------------------------------------------------------
| Modal variant
|----------------------------------------------------------------------------*/

.jp-ModalCommandPalette {
  position: absolute;
  z-index: 10000;
  top: 38px;
  left: 30%;
  margin: 0;
  padding: 4px;
  width: 40%;
  box-shadow: var(--jp-elevation-z4);
  border-radius: 4px;
  background: var(--jp-layout-color0);
}

.jp-ModalCommandPalette .lm-CommandPalette {
  max-height: 40vh;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-close-icon::after {
  display: none;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-CommandPalette-header {
  display: none;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-CommandPalette-item {
  margin-left: 4px;
  margin-right: 4px;
}

.jp-ModalCommandPalette
  .lm-CommandPalette
  .lm-CommandPalette-item.lm-mod-disabled {
  display: none;
}

/*-----------------------------------------------------------------------------
| Search
|----------------------------------------------------------------------------*/

.lm-CommandPalette-search {
  padding: 4px;
  background-color: var(--jp-layout-color1);
  z-index: 2;
}

.lm-CommandPalette-wrapper {
  overflow: overlay;
  padding: 0 9px;
  background-color: var(--jp-input-active-background);
  height: 30px;
  box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color);
}

.lm-CommandPalette.lm-mod-focused .lm-CommandPalette-wrapper {
  box-shadow: inset 0 0 0 1px var(--jp-input-active-box-shadow-color),
    inset 0 0 0 3px var(--jp-input-active-box-shadow-color);
}

.jp-SearchIconGroup {
  color: white;
  background-color: var(--jp-brand-color1);
  position: absolute;
  top: 4px;
  right: 4px;
  padding: 5px 5px 1px;
}

.jp-SearchIconGroup svg {
  height: 20px;
  width: 20px;
}

.jp-SearchIconGroup .jp-icon3[fill] {
  fill: var(--jp-layout-color0);
}

.lm-CommandPalette-input {
  background: transparent;
  width: calc(100% - 18px);
  float: left;
  border: none;
  outline: none;
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  line-height: var(--jp-private-commandpalette-search-height);
}

.lm-CommandPalette-input::-webkit-input-placeholder,
.lm-CommandPalette-input::-moz-placeholder,
.lm-CommandPalette-input:-ms-input-placeholder {
  color: var(--jp-ui-font-color2);
  font-size: var(--jp-ui-font-size1);
}

/*-----------------------------------------------------------------------------
| Results
|----------------------------------------------------------------------------*/

.lm-CommandPalette-header:first-child {
  margin-top: 0;
}

.lm-CommandPalette-header {
  border-bottom: solid var(--jp-border-width) var(--jp-border-color2);
  color: var(--jp-ui-font-color1);
  cursor: pointer;
  display: flex;
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  letter-spacing: 1px;
  margin-top: 8px;
  padding: 8px 0 8px 12px;
  text-transform: uppercase;
}

.lm-CommandPalette-header.lm-mod-active {
  background: var(--jp-layout-color2);
}

.lm-CommandPalette-header > mark {
  background-color: transparent;
  font-weight: bold;
  color: var(--jp-ui-font-color1);
}

.lm-CommandPalette-item {
  padding: 4px 12px 4px 4px;
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  font-weight: 400;
  display: flex;
}

.lm-CommandPalette-item.lm-mod-disabled {
  color: var(--jp-ui-font-color2);
}

.lm-CommandPalette-item.lm-mod-active {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.lm-CommandPalette-item.lm-mod-active .lm-CommandPalette-itemLabel > mark {
  color: var(--jp-ui-inverse-font-color0);
}

.lm-CommandPalette-item.lm-mod-active .jp-icon-selectable[fill] {
  fill: var(--jp-layout-color0);
}

.lm-CommandPalette-item.lm-mod-active:hover:not(.lm-mod-disabled) {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.lm-CommandPalette-item:hover:not(.lm-mod-active):not(.lm-mod-disabled) {
  background: var(--jp-layout-color2);
}

.lm-CommandPalette-itemContent {
  overflow: hidden;
}

.lm-CommandPalette-itemLabel > mark {
  color: var(--jp-ui-font-color0);
  background-color: transparent;
  font-weight: bold;
}

.lm-CommandPalette-item.lm-mod-disabled mark {
  color: var(--jp-ui-font-color2);
}

.lm-CommandPalette-item .lm-CommandPalette-itemIcon {
  margin: 0 4px 0 0;
  position: relative;
  width: 16px;
  top: 2px;
  flex: 0 0 auto;
}

.lm-CommandPalette-item.lm-mod-disabled .lm-CommandPalette-itemIcon {
  opacity: 0.6;
}

.lm-CommandPalette-item .lm-CommandPalette-itemShortcut {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemCaption {
  display: none;
}

.lm-CommandPalette-content {
  background-color: var(--jp-layout-color1);
}

.lm-CommandPalette-content:empty::after {
  content: 'No results';
  margin: auto;
  margin-top: 20px;
  width: 100px;
  display: block;
  font-size: var(--jp-ui-font-size2);
  font-family: var(--jp-ui-font-family);
  font-weight: lighter;
}

.lm-CommandPalette-emptyMessage {
  text-align: center;
  margin-top: 24px;
  line-height: 1.32;
  padding: 0 8px;
  color: var(--jp-content-font-color3);
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Dialog {
  position: absolute;
  z-index: 10000;
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  top: 0;
  left: 0;
  margin: 0;
  padding: 0;
  width: 100%;
  height: 100%;
  background: var(--jp-dialog-background);
}

.jp-Dialog-content {
  display: flex;
  flex-direction: column;
  margin-left: auto;
  margin-right: auto;
  background: var(--jp-layout-color1);
  padding: 24px 24px 12px;
  min-width: 300px;
  min-height: 150px;
  max-width: 1000px;
  max-height: 500px;
  box-sizing: border-box;
  box-shadow: var(--jp-elevation-z20);
  word-wrap: break-word;
  border-radius: var(--jp-border-radius);

  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color1);
  resize: both;
}

.jp-Dialog-content.jp-Dialog-content-small {
  max-width: 500px;
}

.jp-Dialog-button {
  overflow: visible;
}

button.jp-Dialog-button:focus {
  outline: 1px solid var(--jp-brand-color1);
  outline-offset: 4px;
  -moz-outline-radius: 0;
}

button.jp-Dialog-button:focus::-moz-focus-inner {
  border: 0;
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-accept:focus,
button.jp-Dialog-button.jp-mod-styled.jp-mod-warn:focus,
button.jp-Dialog-button.jp-mod-styled.jp-mod-reject:focus {
  outline-offset: 4px;
  -moz-outline-radius: 0;
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-accept:focus {
  outline: 1px solid var(--jp-accept-color-normal, var(--jp-brand-color1));
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-warn:focus {
  outline: 1px solid var(--jp-warn-color-normal, var(--jp-error-color1));
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-reject:focus {
  outline: 1px solid var(--jp-reject-color-normal, var(--md-grey-600));
}

button.jp-Dialog-close-button {
  padding: 0;
  height: 100%;
  min-width: unset;
  min-height: unset;
}

.jp-Dialog-header {
  display: flex;
  justify-content: space-between;
  flex: 0 0 auto;
  padding-bottom: 12px;
  font-size: var(--jp-ui-font-size3);
  font-weight: 400;
  color: var(--jp-ui-font-color1);
}

.jp-Dialog-body {
  display: flex;
  flex-direction: column;
  flex: 1 1 auto;
  font-size: var(--jp-ui-font-size1);
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  overflow: auto;
}

.jp-Dialog-footer {
  display: flex;
  flex-direction: row;
  justify-content: flex-end;
  align-items: center;
  flex: 0 0 auto;
  margin-left: -12px;
  margin-right: -12px;
  padding: 12px;
}

.jp-Dialog-checkbox {
  padding-right: 5px;
}

.jp-Dialog-checkbox > input:focus-visible {
  outline: 1px solid var(--jp-input-active-border-color);
  outline-offset: 1px;
}

.jp-Dialog-spacer {
  flex: 1 1 auto;
}

.jp-Dialog-title {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.jp-Dialog-body > .jp-select-wrapper {
  width: 100%;
}

.jp-Dialog-body > button {
  padding: 0 16px;
}

.jp-Dialog-body > label {
  line-height: 1.4;
  color: var(--jp-ui-font-color0);
}

.jp-Dialog-button.jp-mod-styled:not(:last-child) {
  margin-right: 12px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-Input-Boolean-Dialog {
  flex-direction: row-reverse;
  align-items: end;
  width: 100%;
}

.jp-Input-Boolean-Dialog > label {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-MainAreaWidget > :focus {
  outline: none;
}

.jp-MainAreaWidget .jp-MainAreaWidget-error {
  padding: 6px;
}

.jp-MainAreaWidget .jp-MainAreaWidget-error > pre {
  width: auto;
  padding: 10px;
  background: var(--jp-error-color3);
  border: var(--jp-border-width) solid var(--jp-error-color1);
  border-radius: var(--jp-border-radius);
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  white-space: pre-wrap;
  word-wrap: break-word;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/**
 * google-material-color v1.2.6
 * https://github.com/danlevan/google-material-color
 */
:root {
  --md-red-50: #ffebee;
  --md-red-100: #ffcdd2;
  --md-red-200: #ef9a9a;
  --md-red-300: #e57373;
  --md-red-400: #ef5350;
  --md-red-500: #f44336;
  --md-red-600: #e53935;
  --md-red-700: #d32f2f;
  --md-red-800: #c62828;
  --md-red-900: #b71c1c;
  --md-red-A100: #ff8a80;
  --md-red-A200: #ff5252;
  --md-red-A400: #ff1744;
  --md-red-A700: #d50000;
  --md-pink-50: #fce4ec;
  --md-pink-100: #f8bbd0;
  --md-pink-200: #f48fb1;
  --md-pink-300: #f06292;
  --md-pink-400: #ec407a;
  --md-pink-500: #e91e63;
  --md-pink-600: #d81b60;
  --md-pink-700: #c2185b;
  --md-pink-800: #ad1457;
  --md-pink-900: #880e4f;
  --md-pink-A100: #ff80ab;
  --md-pink-A200: #ff4081;
  --md-pink-A400: #f50057;
  --md-pink-A700: #c51162;
  --md-purple-50: #f3e5f5;
  --md-purple-100: #e1bee7;
  --md-purple-200: #ce93d8;
  --md-purple-300: #ba68c8;
  --md-purple-400: #ab47bc;
  --md-purple-500: #9c27b0;
  --md-purple-600: #8e24aa;
  --md-purple-700: #7b1fa2;
  --md-purple-800: #6a1b9a;
  --md-purple-900: #4a148c;
  --md-purple-A100: #ea80fc;
  --md-purple-A200: #e040fb;
  --md-purple-A400: #d500f9;
  --md-purple-A700: #a0f;
  --md-deep-purple-50: #ede7f6;
  --md-deep-purple-100: #d1c4e9;
  --md-deep-purple-200: #b39ddb;
  --md-deep-purple-300: #9575cd;
  --md-deep-purple-400: #7e57c2;
  --md-deep-purple-500: #673ab7;
  --md-deep-purple-600: #5e35b1;
  --md-deep-purple-700: #512da8;
  --md-deep-purple-800: #4527a0;
  --md-deep-purple-900: #311b92;
  --md-deep-purple-A100: #b388ff;
  --md-deep-purple-A200: #7c4dff;
  --md-deep-purple-A400: #651fff;
  --md-deep-purple-A700: #6200ea;
  --md-indigo-50: #e8eaf6;
  --md-indigo-100: #c5cae9;
  --md-indigo-200: #9fa8da;
  --md-indigo-300: #7986cb;
  --md-indigo-400: #5c6bc0;
  --md-indigo-500: #3f51b5;
  --md-indigo-600: #3949ab;
  --md-indigo-700: #303f9f;
  --md-indigo-800: #283593;
  --md-indigo-900: #1a237e;
  --md-indigo-A100: #8c9eff;
  --md-indigo-A200: #536dfe;
  --md-indigo-A400: #3d5afe;
  --md-indigo-A700: #304ffe;
  --md-blue-50: #e3f2fd;
  --md-blue-100: #bbdefb;
  --md-blue-200: #90caf9;
  --md-blue-300: #64b5f6;
  --md-blue-400: #42a5f5;
  --md-blue-500: #2196f3;
  --md-blue-600: #1e88e5;
  --md-blue-700: #1976d2;
  --md-blue-800: #1565c0;
  --md-blue-900: #0d47a1;
  --md-blue-A100: #82b1ff;
  --md-blue-A200: #448aff;
  --md-blue-A400: #2979ff;
  --md-blue-A700: #2962ff;
  --md-light-blue-50: #e1f5fe;
  --md-light-blue-100: #b3e5fc;
  --md-light-blue-200: #81d4fa;
  --md-light-blue-300: #4fc3f7;
  --md-light-blue-400: #29b6f6;
  --md-light-blue-500: #03a9f4;
  --md-light-blue-600: #039be5;
  --md-light-blue-700: #0288d1;
  --md-light-blue-800: #0277bd;
  --md-light-blue-900: #01579b;
  --md-light-blue-A100: #80d8ff;
  --md-light-blue-A200: #40c4ff;
  --md-light-blue-A400: #00b0ff;
  --md-light-blue-A700: #0091ea;
  --md-cyan-50: #e0f7fa;
  --md-cyan-100: #b2ebf2;
  --md-cyan-200: #80deea;
  --md-cyan-300: #4dd0e1;
  --md-cyan-400: #26c6da;
  --md-cyan-500: #00bcd4;
  --md-cyan-600: #00acc1;
  --md-cyan-700: #0097a7;
  --md-cyan-800: #00838f;
  --md-cyan-900: #006064;
  --md-cyan-A100: #84ffff;
  --md-cyan-A200: #18ffff;
  --md-cyan-A400: #00e5ff;
  --md-cyan-A700: #00b8d4;
  --md-teal-50: #e0f2f1;
  --md-teal-100: #b2dfdb;
  --md-teal-200: #80cbc4;
  --md-teal-300: #4db6ac;
  --md-teal-400: #26a69a;
  --md-teal-500: #009688;
  --md-teal-600: #00897b;
  --md-teal-700: #00796b;
  --md-teal-800: #00695c;
  --md-teal-900: #004d40;
  --md-teal-A100: #a7ffeb;
  --md-teal-A200: #64ffda;
  --md-teal-A400: #1de9b6;
  --md-teal-A700: #00bfa5;
  --md-green-50: #e8f5e9;
  --md-green-100: #c8e6c9;
  --md-green-200: #a5d6a7;
  --md-green-300: #81c784;
  --md-green-400: #66bb6a;
  --md-green-500: #4caf50;
  --md-green-600: #43a047;
  --md-green-700: #388e3c;
  --md-green-800: #2e7d32;
  --md-green-900: #1b5e20;
  --md-green-A100: #b9f6ca;
  --md-green-A200: #69f0ae;
  --md-green-A400: #00e676;
  --md-green-A700: #00c853;
  --md-light-green-50: #f1f8e9;
  --md-light-green-100: #dcedc8;
  --md-light-green-200: #c5e1a5;
  --md-light-green-300: #aed581;
  --md-light-green-400: #9ccc65;
  --md-light-green-500: #8bc34a;
  --md-light-green-600: #7cb342;
  --md-light-green-700: #689f38;
  --md-light-green-800: #558b2f;
  --md-light-green-900: #33691e;
  --md-light-green-A100: #ccff90;
  --md-light-green-A200: #b2ff59;
  --md-light-green-A400: #76ff03;
  --md-light-green-A700: #64dd17;
  --md-lime-50: #f9fbe7;
  --md-lime-100: #f0f4c3;
  --md-lime-200: #e6ee9c;
  --md-lime-300: #dce775;
  --md-lime-400: #d4e157;
  --md-lime-500: #cddc39;
  --md-lime-600: #c0ca33;
  --md-lime-700: #afb42b;
  --md-lime-800: #9e9d24;
  --md-lime-900: #827717;
  --md-lime-A100: #f4ff81;
  --md-lime-A200: #eeff41;
  --md-lime-A400: #c6ff00;
  --md-lime-A700: #aeea00;
  --md-yellow-50: #fffde7;
  --md-yellow-100: #fff9c4;
  --md-yellow-200: #fff59d;
  --md-yellow-300: #fff176;
  --md-yellow-400: #ffee58;
  --md-yellow-500: #ffeb3b;
  --md-yellow-600: #fdd835;
  --md-yellow-700: #fbc02d;
  --md-yellow-800: #f9a825;
  --md-yellow-900: #f57f17;
  --md-yellow-A100: #ffff8d;
  --md-yellow-A200: #ff0;
  --md-yellow-A400: #ffea00;
  --md-yellow-A700: #ffd600;
  --md-amber-50: #fff8e1;
  --md-amber-100: #ffecb3;
  --md-amber-200: #ffe082;
  --md-amber-300: #ffd54f;
  --md-amber-400: #ffca28;
  --md-amber-500: #ffc107;
  --md-amber-600: #ffb300;
  --md-amber-700: #ffa000;
  --md-amber-800: #ff8f00;
  --md-amber-900: #ff6f00;
  --md-amber-A100: #ffe57f;
  --md-amber-A200: #ffd740;
  --md-amber-A400: #ffc400;
  --md-amber-A700: #ffab00;
  --md-orange-50: #fff3e0;
  --md-orange-100: #ffe0b2;
  --md-orange-200: #ffcc80;
  --md-orange-300: #ffb74d;
  --md-orange-400: #ffa726;
  --md-orange-500: #ff9800;
  --md-orange-600: #fb8c00;
  --md-orange-700: #f57c00;
  --md-orange-800: #ef6c00;
  --md-orange-900: #e65100;
  --md-orange-A100: #ffd180;
  --md-orange-A200: #ffab40;
  --md-orange-A400: #ff9100;
  --md-orange-A700: #ff6d00;
  --md-deep-orange-50: #fbe9e7;
  --md-deep-orange-100: #ffccbc;
  --md-deep-orange-200: #ffab91;
  --md-deep-orange-300: #ff8a65;
  --md-deep-orange-400: #ff7043;
  --md-deep-orange-500: #ff5722;
  --md-deep-orange-600: #f4511e;
  --md-deep-orange-700: #e64a19;
  --md-deep-orange-800: #d84315;
  --md-deep-orange-900: #bf360c;
  --md-deep-orange-A100: #ff9e80;
  --md-deep-orange-A200: #ff6e40;
  --md-deep-orange-A400: #ff3d00;
  --md-deep-orange-A700: #dd2c00;
  --md-brown-50: #efebe9;
  --md-brown-100: #d7ccc8;
  --md-brown-200: #bcaaa4;
  --md-brown-300: #a1887f;
  --md-brown-400: #8d6e63;
  --md-brown-500: #795548;
  --md-brown-600: #6d4c41;
  --md-brown-700: #5d4037;
  --md-brown-800: #4e342e;
  --md-brown-900: #3e2723;
  --md-grey-50: #fafafa;
  --md-grey-100: #f5f5f5;
  --md-grey-200: #eee;
  --md-grey-300: #e0e0e0;
  --md-grey-400: #bdbdbd;
  --md-grey-500: #9e9e9e;
  --md-grey-600: #757575;
  --md-grey-700: #616161;
  --md-grey-800: #424242;
  --md-grey-900: #212121;
  --md-blue-grey-50: #eceff1;
  --md-blue-grey-100: #cfd8dc;
  --md-blue-grey-200: #b0bec5;
  --md-blue-grey-300: #90a4ae;
  --md-blue-grey-400: #78909c;
  --md-blue-grey-500: #607d8b;
  --md-blue-grey-600: #546e7a;
  --md-blue-grey-700: #455a64;
  --md-blue-grey-800: #37474f;
  --md-blue-grey-900: #263238;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| RenderedText
|----------------------------------------------------------------------------*/

:root {
  /* This is the padding value to fill the gaps between lines containing spans with background color. */
  --jp-private-code-span-padding: calc(
    (var(--jp-code-line-height) - 1) * var(--jp-code-font-size) / 2
  );
}

.jp-RenderedText {
  text-align: left;
  padding-left: var(--jp-code-padding);
  line-height: var(--jp-code-line-height);
  font-family: var(--jp-code-font-family);
}

.jp-RenderedText pre,
.jp-RenderedJavaScript pre,
.jp-RenderedHTMLCommon pre {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-code-font-size);
  border: none;
  margin: 0;
  padding: 0;
}

.jp-RenderedText pre a:link {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

.jp-RenderedText pre a:hover {
  text-decoration: underline;
  color: var(--jp-content-link-color);
}

.jp-RenderedText pre a:visited {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

/* console foregrounds and backgrounds */
.jp-RenderedText pre .ansi-black-fg {
  color: #3e424d;
}

.jp-RenderedText pre .ansi-red-fg {
  color: #e75c58;
}

.jp-RenderedText pre .ansi-green-fg {
  color: #00a250;
}

.jp-RenderedText pre .ansi-yellow-fg {
  color: #ddb62b;
}

.jp-RenderedText pre .ansi-blue-fg {
  color: #208ffb;
}

.jp-RenderedText pre .ansi-magenta-fg {
  color: #d160c4;
}

.jp-RenderedText pre .ansi-cyan-fg {
  color: #60c6c8;
}

.jp-RenderedText pre .ansi-white-fg {
  color: #c5c1b4;
}

.jp-RenderedText pre .ansi-black-bg {
  background-color: #3e424d;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-red-bg {
  background-color: #e75c58;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-green-bg {
  background-color: #00a250;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-yellow-bg {
  background-color: #ddb62b;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-blue-bg {
  background-color: #208ffb;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-magenta-bg {
  background-color: #d160c4;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-cyan-bg {
  background-color: #60c6c8;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-white-bg {
  background-color: #c5c1b4;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-black-intense-fg {
  color: #282c36;
}

.jp-RenderedText pre .ansi-red-intense-fg {
  color: #b22b31;
}

.jp-RenderedText pre .ansi-green-intense-fg {
  color: #007427;
}

.jp-RenderedText pre .ansi-yellow-intense-fg {
  color: #b27d12;
}

.jp-RenderedText pre .ansi-blue-intense-fg {
  color: #0065ca;
}

.jp-RenderedText pre .ansi-magenta-intense-fg {
  color: #a03196;
}

.jp-RenderedText pre .ansi-cyan-intense-fg {
  color: #258f8f;
}

.jp-RenderedText pre .ansi-white-intense-fg {
  color: #a1a6b2;
}

.jp-RenderedText pre .ansi-black-intense-bg {
  background-color: #282c36;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-red-intense-bg {
  background-color: #b22b31;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-green-intense-bg {
  background-color: #007427;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-yellow-intense-bg {
  background-color: #b27d12;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-blue-intense-bg {
  background-color: #0065ca;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-magenta-intense-bg {
  background-color: #a03196;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-cyan-intense-bg {
  background-color: #258f8f;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-white-intense-bg {
  background-color: #a1a6b2;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-default-inverse-fg {
  color: var(--jp-ui-inverse-font-color0);
}

.jp-RenderedText pre .ansi-default-inverse-bg {
  background-color: var(--jp-inverse-layout-color0);
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-bold {
  font-weight: bold;
}

.jp-RenderedText pre .ansi-underline {
  text-decoration: underline;
}

.jp-RenderedText[data-mime-type='application/vnd.jupyter.stderr'] {
  background: var(--jp-rendermime-error-background);
  padding-top: var(--jp-code-padding);
}

/*-----------------------------------------------------------------------------
| RenderedLatex
|----------------------------------------------------------------------------*/

.jp-RenderedLatex {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-content-font-size1);
  line-height: var(--jp-content-line-height);
}

/* Left-justify outputs.*/
.jp-OutputArea-output.jp-RenderedLatex {
  padding: var(--jp-code-padding);
  text-align: left;
}

/*-----------------------------------------------------------------------------
| RenderedHTML
|----------------------------------------------------------------------------*/

.jp-RenderedHTMLCommon {
  color: var(--jp-content-font-color1);
  font-family: var(--jp-content-font-family);
  font-size: var(--jp-content-font-size1);
  line-height: var(--jp-content-line-height);

  /* Give a bit more R padding on Markdown text to keep line lengths reasonable */
  padding-right: 20px;
}

.jp-RenderedHTMLCommon em {
  font-style: italic;
}

.jp-RenderedHTMLCommon strong {
  font-weight: bold;
}

.jp-RenderedHTMLCommon u {
  text-decoration: underline;
}

.jp-RenderedHTMLCommon a:link {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

.jp-RenderedHTMLCommon a:hover {
  text-decoration: underline;
  color: var(--jp-content-link-color);
}

.jp-RenderedHTMLCommon a:visited {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

/* Headings */

.jp-RenderedHTMLCommon h1,
.jp-RenderedHTMLCommon h2,
.jp-RenderedHTMLCommon h3,
.jp-RenderedHTMLCommon h4,
.jp-RenderedHTMLCommon h5,
.jp-RenderedHTMLCommon h6 {
  line-height: var(--jp-content-heading-line-height);
  font-weight: var(--jp-content-heading-font-weight);
  font-style: normal;
  margin: var(--jp-content-heading-margin-top) 0
    var(--jp-content-heading-margin-bottom) 0;
}

.jp-RenderedHTMLCommon h1:first-child,
.jp-RenderedHTMLCommon h2:first-child,
.jp-RenderedHTMLCommon h3:first-child,
.jp-RenderedHTMLCommon h4:first-child,
.jp-RenderedHTMLCommon h5:first-child,
.jp-RenderedHTMLCommon h6:first-child {
  margin-top: calc(0.5 * var(--jp-content-heading-margin-top));
}

.jp-RenderedHTMLCommon h1:last-child,
.jp-RenderedHTMLCommon h2:last-child,
.jp-RenderedHTMLCommon h3:last-child,
.jp-RenderedHTMLCommon h4:last-child,
.jp-RenderedHTMLCommon h5:last-child,
.jp-RenderedHTMLCommon h6:last-child {
  margin-bottom: calc(0.5 * var(--jp-content-heading-margin-bottom));
}

.jp-RenderedHTMLCommon h1 {
  font-size: var(--jp-content-font-size5);
}

.jp-RenderedHTMLCommon h2 {
  font-size: var(--jp-content-font-size4);
}

.jp-RenderedHTMLCommon h3 {
  font-size: var(--jp-content-font-size3);
}

.jp-RenderedHTMLCommon h4 {
  font-size: var(--jp-content-font-size2);
}

.jp-RenderedHTMLCommon h5 {
  font-size: var(--jp-content-font-size1);
}

.jp-RenderedHTMLCommon h6 {
  font-size: var(--jp-content-font-size0);
}

/* Lists */

/* stylelint-disable selector-max-type, selector-max-compound-selectors */

.jp-RenderedHTMLCommon ul:not(.list-inline),
.jp-RenderedHTMLCommon ol:not(.list-inline) {
  padding-left: 2em;
}

.jp-RenderedHTMLCommon ul {
  list-style: disc;
}

.jp-RenderedHTMLCommon ul ul {
  list-style: square;
}

.jp-RenderedHTMLCommon ul ul ul {
  list-style: circle;
}

.jp-RenderedHTMLCommon ol {
  list-style: decimal;
}

.jp-RenderedHTMLCommon ol ol {
  list-style: upper-alpha;
}

.jp-RenderedHTMLCommon ol ol ol {
  list-style: lower-alpha;
}

.jp-RenderedHTMLCommon ol ol ol ol {
  list-style: lower-roman;
}

.jp-RenderedHTMLCommon ol ol ol ol ol {
  list-style: decimal;
}

.jp-RenderedHTMLCommon ol,
.jp-RenderedHTMLCommon ul {
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon ul ul,
.jp-RenderedHTMLCommon ul ol,
.jp-RenderedHTMLCommon ol ul,
.jp-RenderedHTMLCommon ol ol {
  margin-bottom: 0;
}

/* stylelint-enable selector-max-type, selector-max-compound-selectors */

.jp-RenderedHTMLCommon hr {
  color: var(--jp-border-color2);
  background-color: var(--jp-border-color1);
  margin-top: 1em;
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon > pre {
  margin: 1.5em 2em;
}

.jp-RenderedHTMLCommon pre,
.jp-RenderedHTMLCommon code {
  border: 0;
  background-color: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
  font-family: var(--jp-code-font-family);
  font-size: inherit;
  line-height: var(--jp-code-line-height);
  padding: 0;
  white-space: pre-wrap;
}

.jp-RenderedHTMLCommon :not(pre) > code {
  background-color: var(--jp-layout-color2);
  padding: 1px 5px;
}

/* Tables */

.jp-RenderedHTMLCommon table {
  border-collapse: collapse;
  border-spacing: 0;
  border: none;
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  table-layout: fixed;
  margin-left: auto;
  margin-bottom: 1em;
  margin-right: auto;
}

.jp-RenderedHTMLCommon thead {
  border-bottom: var(--jp-border-width) solid var(--jp-border-color1);
  vertical-align: bottom;
}

.jp-RenderedHTMLCommon td,
.jp-RenderedHTMLCommon th,
.jp-RenderedHTMLCommon tr {
  vertical-align: middle;
  padding: 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}

.jp-RenderedMarkdown.jp-RenderedHTMLCommon td,
.jp-RenderedMarkdown.jp-RenderedHTMLCommon th {
  max-width: none;
}

:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon td,
:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon th,
:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon tr {
  text-align: right;
}

.jp-RenderedHTMLCommon th {
  font-weight: bold;
}

.jp-RenderedHTMLCommon tbody tr:nth-child(odd) {
  background: var(--jp-layout-color0);
}

.jp-RenderedHTMLCommon tbody tr:nth-child(even) {
  background: var(--jp-rendermime-table-row-background);
}

.jp-RenderedHTMLCommon tbody tr:hover {
  background: var(--jp-rendermime-table-row-hover-background);
}

.jp-RenderedHTMLCommon p {
  text-align: left;
  margin: 0;
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon img {
  -moz-force-broken-image-icon: 1;
}

/* Restrict to direct children as other images could be nested in other content. */
.jp-RenderedHTMLCommon > img {
  display: block;
  margin-left: 0;
  margin-right: 0;
  margin-bottom: 1em;
}

/* Change color behind transparent images if they need it... */
[data-jp-theme-light='false'] .jp-RenderedImage img.jp-needs-light-background {
  background-color: var(--jp-inverse-layout-color1);
}

[data-jp-theme-light='true'] .jp-RenderedImage img.jp-needs-dark-background {
  background-color: var(--jp-inverse-layout-color1);
}

.jp-RenderedHTMLCommon img,
.jp-RenderedImage img,
.jp-RenderedHTMLCommon svg,
.jp-RenderedSVG svg {
  max-width: 100%;
  height: auto;
}

.jp-RenderedHTMLCommon img.jp-mod-unconfined,
.jp-RenderedImage img.jp-mod-unconfined,
.jp-RenderedHTMLCommon svg.jp-mod-unconfined,
.jp-RenderedSVG svg.jp-mod-unconfined {
  max-width: none;
}

.jp-RenderedHTMLCommon .alert {
  padding: var(--jp-notebook-padding);
  border: var(--jp-border-width) solid transparent;
  border-radius: var(--jp-border-radius);
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon .alert-info {
  color: var(--jp-info-color0);
  background-color: var(--jp-info-color3);
  border-color: var(--jp-info-color2);
}

.jp-RenderedHTMLCommon .alert-info hr {
  border-color: var(--jp-info-color3);
}

.jp-RenderedHTMLCommon .alert-info > p:last-child,
.jp-RenderedHTMLCommon .alert-info > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-warning {
  color: var(--jp-warn-color0);
  background-color: var(--jp-warn-color3);
  border-color: var(--jp-warn-color2);
}

.jp-RenderedHTMLCommon .alert-warning hr {
  border-color: var(--jp-warn-color3);
}

.jp-RenderedHTMLCommon .alert-warning > p:last-child,
.jp-RenderedHTMLCommon .alert-warning > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-success {
  color: var(--jp-success-color0);
  background-color: var(--jp-success-color3);
  border-color: var(--jp-success-color2);
}

.jp-RenderedHTMLCommon .alert-success hr {
  border-color: var(--jp-success-color3);
}

.jp-RenderedHTMLCommon .alert-success > p:last-child,
.jp-RenderedHTMLCommon .alert-success > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-danger {
  color: var(--jp-error-color0);
  background-color: var(--jp-error-color3);
  border-color: var(--jp-error-color2);
}

.jp-RenderedHTMLCommon .alert-danger hr {
  border-color: var(--jp-error-color3);
}

.jp-RenderedHTMLCommon .alert-danger > p:last-child,
.jp-RenderedHTMLCommon .alert-danger > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon blockquote {
  margin: 1em 2em;
  padding: 0 1em;
  border-left: 5px solid var(--jp-border-color2);
}

a.jp-InternalAnchorLink {
  visibility: hidden;
  margin-left: 8px;
  color: var(--md-blue-800);
}

h1:hover .jp-InternalAnchorLink,
h2:hover .jp-InternalAnchorLink,
h3:hover .jp-InternalAnchorLink,
h4:hover .jp-InternalAnchorLink,
h5:hover .jp-InternalAnchorLink,
h6:hover .jp-InternalAnchorLink {
  visibility: visible;
}

.jp-RenderedHTMLCommon kbd {
  background-color: var(--jp-rendermime-table-row-background);
  border: 1px solid var(--jp-border-color0);
  border-bottom-color: var(--jp-border-color2);
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.25);
  display: inline-block;
  font-size: var(--jp-ui-font-size0);
  line-height: 1em;
  padding: 0.2em 0.5em;
}

/* Most direct children of .jp-RenderedHTMLCommon have a margin-bottom of 1.0.
 * At the bottom of cells this is a bit too much as there is also spacing
 * between cells. Going all the way to 0 gets too tight between markdown and
 * code cells.
 */
.jp-RenderedHTMLCommon > *:last-child {
  margin-bottom: 0.5em;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-cursor-backdrop {
  position: fixed;
  width: 200px;
  height: 200px;
  margin-top: -100px;
  margin-left: -100px;
  will-change: transform;
  z-index: 100;
}

.lm-mod-drag-image {
  will-change: transform;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-lineFormSearch {
  padding: 4px 12px;
  background-color: var(--jp-layout-color2);
  box-shadow: var(--jp-toolbar-box-shadow);
  z-index: 2;
  font-size: var(--jp-ui-font-size1);
}

.jp-lineFormCaption {
  font-size: var(--jp-ui-font-size0);
  line-height: var(--jp-ui-font-size1);
  margin-top: 4px;
  color: var(--jp-ui-font-color0);
}

.jp-baseLineForm {
  border: none;
  border-radius: 0;
  position: absolute;
  background-size: 16px;
  background-repeat: no-repeat;
  background-position: center;
  outline: none;
}

.jp-lineFormButtonContainer {
  top: 4px;
  right: 8px;
  height: 24px;
  padding: 0 12px;
  width: 12px;
}

.jp-lineFormButtonIcon {
  top: 0;
  right: 0;
  background-color: var(--jp-brand-color1);
  height: 100%;
  width: 100%;
  box-sizing: border-box;
  padding: 4px 6px;
}

.jp-lineFormButton {
  top: 0;
  right: 0;
  background-color: transparent;
  height: 100%;
  width: 100%;
  box-sizing: border-box;
}

.jp-lineFormWrapper {
  overflow: hidden;
  padding: 0 8px;
  border: 1px solid var(--jp-border-color0);
  background-color: var(--jp-input-active-background);
  height: 22px;
}

.jp-lineFormWrapperFocusWithin {
  border: var(--jp-border-width) solid var(--md-blue-500);
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-lineFormInput {
  background: transparent;
  width: 200px;
  height: 100%;
  border: none;
  outline: none;
  color: var(--jp-ui-font-color0);
  line-height: 28px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-JSONEditor {
  display: flex;
  flex-direction: column;
  width: 100%;
}

.jp-JSONEditor-host {
  flex: 1 1 auto;
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  border-radius: 0;
  background: var(--jp-layout-color0);
  min-height: 50px;
  padding: 1px;
}

.jp-JSONEditor.jp-mod-error .jp-JSONEditor-host {
  border-color: red;
  outline-color: red;
}

.jp-JSONEditor-header {
  display: flex;
  flex: 1 0 auto;
  padding: 0 0 0 12px;
}

.jp-JSONEditor-header label {
  flex: 0 0 auto;
}

.jp-JSONEditor-commitButton {
  height: 16px;
  width: 16px;
  background-size: 18px;
  background-repeat: no-repeat;
  background-position: center;
}

.jp-JSONEditor-host.jp-mod-focused {
  background-color: var(--jp-input-active-background);
  border: 1px solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
}

.jp-Editor.jp-mod-dropTarget {
  border: var(--jp-border-width) solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/
.jp-DocumentSearch-input {
  border: none;
  outline: none;
  color: var(--jp-ui-font-color0);
  font-size: var(--jp-ui-font-size1);
  background-color: var(--jp-layout-color0);
  font-family: var(--jp-ui-font-family);
  padding: 2px 1px;
  resize: none;
}

.jp-DocumentSearch-overlay {
  position: absolute;
  background-color: var(--jp-toolbar-background);
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  border-left: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  top: 0;
  right: 0;
  z-index: 7;
  min-width: 405px;
  padding: 2px;
  font-size: var(--jp-ui-font-size1);

  --jp-private-document-search-button-height: 20px;
}

.jp-DocumentSearch-overlay button {
  background-color: var(--jp-toolbar-background);
  outline: 0;
}

.jp-DocumentSearch-overlay button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-overlay button:active {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-overlay-row {
  display: flex;
  align-items: center;
  margin-bottom: 2px;
}

.jp-DocumentSearch-button-content {
  display: inline-block;
  cursor: pointer;
  box-sizing: border-box;
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-button-content svg {
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-input-wrapper {
  border: var(--jp-border-width) solid var(--jp-border-color0);
  display: flex;
  background-color: var(--jp-layout-color0);
  margin: 2px;
}

.jp-DocumentSearch-input-wrapper:focus-within {
  border-color: var(--jp-cell-editor-active-border-color);
}

.jp-DocumentSearch-toggle-wrapper,
.jp-DocumentSearch-button-wrapper {
  all: initial;
  overflow: hidden;
  display: inline-block;
  border: none;
  box-sizing: border-box;
}

.jp-DocumentSearch-toggle-wrapper {
  width: 14px;
  height: 14px;
}

.jp-DocumentSearch-button-wrapper {
  width: var(--jp-private-document-search-button-height);
  height: var(--jp-private-document-search-button-height);
}

.jp-DocumentSearch-toggle-wrapper:focus,
.jp-DocumentSearch-button-wrapper:focus {
  outline: var(--jp-border-width) solid
    var(--jp-cell-editor-active-border-color);
  outline-offset: -1px;
}

.jp-DocumentSearch-toggle-wrapper,
.jp-DocumentSearch-button-wrapper,
.jp-DocumentSearch-button-content:focus {
  outline: none;
}

.jp-DocumentSearch-toggle-placeholder {
  width: 5px;
}

.jp-DocumentSearch-input-button::before {
  display: block;
  padding-top: 100%;
}

.jp-DocumentSearch-input-button-off {
  opacity: var(--jp-search-toggle-off-opacity);
}

.jp-DocumentSearch-input-button-off:hover {
  opacity: var(--jp-search-toggle-hover-opacity);
}

.jp-DocumentSearch-input-button-on {
  opacity: var(--jp-search-toggle-on-opacity);
}

.jp-DocumentSearch-index-counter {
  padding-left: 10px;
  padding-right: 10px;
  user-select: none;
  min-width: 35px;
  display: inline-block;
}

.jp-DocumentSearch-up-down-wrapper {
  display: inline-block;
  padding-right: 2px;
  margin-left: auto;
  white-space: nowrap;
}

.jp-DocumentSearch-spacer {
  margin-left: auto;
}

.jp-DocumentSearch-up-down-wrapper button {
  outline: 0;
  border: none;
  width: var(--jp-private-document-search-button-height);
  height: var(--jp-private-document-search-button-height);
  vertical-align: middle;
  margin: 1px 5px 2px;
}

.jp-DocumentSearch-up-down-button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-up-down-button:active {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-filter-button {
  border-radius: var(--jp-border-radius);
}

.jp-DocumentSearch-filter-button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-filter-button-enabled {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-filter-button-enabled:hover {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-search-options {
  padding: 0 8px;
  margin-left: 3px;
  width: 100%;
  display: grid;
  justify-content: start;
  grid-template-columns: 1fr 1fr;
  align-items: center;
  justify-items: stretch;
}

.jp-DocumentSearch-search-filter-disabled {
  color: var(--jp-ui-font-color2);
}

.jp-DocumentSearch-search-filter {
  display: flex;
  align-items: center;
  user-select: none;
}

.jp-DocumentSearch-regex-error {
  color: var(--jp-error-color0);
}

.jp-DocumentSearch-replace-button-wrapper {
  overflow: hidden;
  display: inline-block;
  box-sizing: border-box;
  border: var(--jp-border-width) solid var(--jp-border-color0);
  margin: auto 2px;
  padding: 1px 4px;
  height: calc(var(--jp-private-document-search-button-height) + 2px);
}

.jp-DocumentSearch-replace-button-wrapper:focus {
  border: var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);
}

.jp-DocumentSearch-replace-button {
  display: inline-block;
  text-align: center;
  cursor: pointer;
  box-sizing: border-box;
  color: var(--jp-ui-font-color1);

  /* height - 2 * (padding of wrapper) */
  line-height: calc(var(--jp-private-document-search-button-height) - 2px);
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-replace-button:focus {
  outline: none;
}

.jp-DocumentSearch-replace-wrapper-class {
  margin-left: 14px;
  display: flex;
}

.jp-DocumentSearch-replace-toggle {
  border: none;
  background-color: var(--jp-toolbar-background);
  border-radius: var(--jp-border-radius);
}

.jp-DocumentSearch-replace-toggle:hover {
  background-color: var(--jp-layout-color2);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.cm-editor {
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  font-family: var(--jp-code-font-family);
  border: 0;
  border-radius: 0;
  height: auto;

  /* Changed to auto to autogrow */
}

.cm-editor pre {
  padding: 0 var(--jp-code-padding);
}

.jp-CodeMirrorEditor[data-type='inline'] .cm-dialog {
  background-color: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
}

.jp-CodeMirrorEditor {
  cursor: text;
}

/* When zoomed out 67% and 33% on a screen of 1440 width x 900 height */
@media screen and (min-width: 2138px) and (max-width: 4319px) {
  .jp-CodeMirrorEditor[data-type='inline'] .cm-cursor {
    border-left: var(--jp-code-cursor-width1) solid
      var(--jp-editor-cursor-color);
  }
}

/* When zoomed out less than 33% */
@media screen and (min-width: 4320px) {
  .jp-CodeMirrorEditor[data-type='inline'] .cm-cursor {
    border-left: var(--jp-code-cursor-width2) solid
      var(--jp-editor-cursor-color);
  }
}

.cm-editor.jp-mod-readOnly .cm-cursor {
  display: none;
}

.jp-CollaboratorCursor {
  border-left: 5px solid transparent;
  border-right: 5px solid transparent;
  border-top: none;
  border-bottom: 3px solid;
  background-clip: content-box;
  margin-left: -5px;
  margin-right: -5px;
}

.cm-searching,
.cm-searching span {
  /* `.cm-searching span`: we need to override syntax highlighting */
  background-color: var(--jp-search-unselected-match-background-color);
  color: var(--jp-search-unselected-match-color);
}

.cm-searching::selection,
.cm-searching span::selection {
  background-color: var(--jp-search-unselected-match-background-color);
  color: var(--jp-search-unselected-match-color);
}

.jp-current-match > .cm-searching,
.jp-current-match > .cm-searching span,
.cm-searching > .jp-current-match,
.cm-searching > .jp-current-match span {
  background-color: var(--jp-search-selected-match-background-color);
  color: var(--jp-search-selected-match-color);
}

.jp-current-match > .cm-searching::selection,
.cm-searching > .jp-current-match::selection,
.jp-current-match > .cm-searching span::selection {
  background-color: var(--jp-search-selected-match-background-color);
  color: var(--jp-search-selected-match-color);
}

.cm-trailingspace {
  background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAYAAAB4ka1VAAAAsElEQVQIHQGlAFr/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA7+r3zKmT0/+pk9P/7+r3zAAAAAAAAAAABAAAAAAAAAAA6OPzM+/q9wAAAAAA6OPzMwAAAAAAAAAAAgAAAAAAAAAAGR8NiRQaCgAZIA0AGR8NiQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQyoYJ/SY80UAAAAASUVORK5CYII=);
  background-position: center left;
  background-repeat: repeat-x;
}

.jp-CollaboratorCursor-hover {
  position: absolute;
  z-index: 1;
  transform: translateX(-50%);
  color: white;
  border-radius: 3px;
  padding-left: 4px;
  padding-right: 4px;
  padding-top: 1px;
  padding-bottom: 1px;
  text-align: center;
  font-size: var(--jp-ui-font-size1);
  white-space: nowrap;
}

.jp-CodeMirror-ruler {
  border-left: 1px dashed var(--jp-border-color2);
}

/* Styles for shared cursors (remote cursor locations and selected ranges) */
.jp-CodeMirrorEditor .cm-ySelectionCaret {
  position: relative;
  border-left: 1px solid black;
  margin-left: -1px;
  margin-right: -1px;
  box-sizing: border-box;
}

.jp-CodeMirrorEditor .cm-ySelectionCaret > .cm-ySelectionInfo {
  white-space: nowrap;
  position: absolute;
  top: -1.15em;
  padding-bottom: 0.05em;
  left: -1px;
  font-size: 0.95em;
  font-family: var(--jp-ui-font-family);
  font-weight: bold;
  line-height: normal;
  user-select: none;
  color: white;
  padding-left: 2px;
  padding-right: 2px;
  z-index: 101;
  transition: opacity 0.3s ease-in-out;
}

.jp-CodeMirrorEditor .cm-ySelectionInfo {
  transition-delay: 0.7s;
  opacity: 0;
}

.jp-CodeMirrorEditor .cm-ySelectionCaret:hover > .cm-ySelectionInfo {
  opacity: 1;
  transition-delay: 0s;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-MimeDocument {
  outline: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-filebrowser-button-height: 28px;
  --jp-private-filebrowser-button-width: 48px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-FileBrowser .jp-SidePanel-content {
  display: flex;
  flex-direction: column;
}

.jp-FileBrowser-toolbar.jp-Toolbar {
  flex-wrap: wrap;
  row-gap: 12px;
  border-bottom: none;
  height: auto;
  margin: 8px 12px 0;
  box-shadow: none;
  padding: 0;
  justify-content: flex-start;
}

.jp-FileBrowser-Panel {
  flex: 1 1 auto;
  display: flex;
  flex-direction: column;
}

.jp-BreadCrumbs {
  flex: 0 0 auto;
  margin: 8px 12px;
}

.jp-BreadCrumbs-item {
  margin: 0 2px;
  padding: 0 2px;
  border-radius: var(--jp-border-radius);
  cursor: pointer;
}

.jp-BreadCrumbs-item:hover {
  background-color: var(--jp-layout-color2);
}

.jp-BreadCrumbs-item:first-child {
  margin-left: 0;
}

.jp-BreadCrumbs-item.jp-mod-dropTarget {
  background-color: var(--jp-brand-color2);
  opacity: 0.7;
}

/*-----------------------------------------------------------------------------
| Buttons
|----------------------------------------------------------------------------*/

.jp-FileBrowser-toolbar > .jp-Toolbar-item {
  flex: 0 0 auto;
  padding-left: 0;
  padding-right: 2px;
  align-items: center;
  height: unset;
}

.jp-FileBrowser-toolbar > .jp-Toolbar-item .jp-ToolbarButtonComponent {
  width: 40px;
}

/*-----------------------------------------------------------------------------
| Other styles
|----------------------------------------------------------------------------*/

.jp-FileDialog.jp-mod-conflict input {
  color: var(--jp-error-color1);
}

.jp-FileDialog .jp-new-name-title {
  margin-top: 12px;
}

.jp-LastModified-hidden {
  display: none;
}

.jp-FileSize-hidden {
  display: none;
}

.jp-FileBrowser .lm-AccordionPanel > h3:first-child {
  display: none;
}

/*-----------------------------------------------------------------------------
| DirListing
|----------------------------------------------------------------------------*/

.jp-DirListing {
  flex: 1 1 auto;
  display: flex;
  flex-direction: column;
  outline: 0;
}

.jp-DirListing-header {
  flex: 0 0 auto;
  display: flex;
  flex-direction: row;
  align-items: center;
  overflow: hidden;
  border-top: var(--jp-border-width) solid var(--jp-border-color2);
  border-bottom: var(--jp-border-width) solid var(--jp-border-color1);
  box-shadow: var(--jp-toolbar-box-shadow);
  z-index: 2;
}

.jp-DirListing-headerItem {
  padding: 4px 12px 2px;
  font-weight: 500;
}

.jp-DirListing-headerItem:hover {
  background: var(--jp-layout-color2);
}

.jp-DirListing-headerItem.jp-id-name {
  flex: 1 0 84px;
}

.jp-DirListing-headerItem.jp-id-modified {
  flex: 0 0 112px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
}

.jp-DirListing-headerItem.jp-id-filesize {
  flex: 0 0 75px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
}

.jp-id-narrow {
  display: none;
  flex: 0 0 5px;
  padding: 4px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
  color: var(--jp-border-color2);
}

.jp-DirListing-narrow .jp-id-narrow {
  display: block;
}

.jp-DirListing-narrow .jp-id-modified,
.jp-DirListing-narrow .jp-DirListing-itemModified {
  display: none;
}

.jp-DirListing-headerItem.jp-mod-selected {
  font-weight: 600;
}

/* increase specificity to override bundled default */
.jp-DirListing-content {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  list-style-type: none;
  overflow: auto;
  background-color: var(--jp-layout-color1);
}

.jp-DirListing-content mark {
  color: var(--jp-ui-font-color0);
  background-color: transparent;
  font-weight: bold;
}

.jp-DirListing-content .jp-DirListing-item.jp-mod-selected mark {
  color: var(--jp-ui-inverse-font-color0);
}

/* Style the directory listing content when a user drops a file to upload */
.jp-DirListing.jp-mod-native-drop .jp-DirListing-content {
  outline: 5px dashed rgba(128, 128, 128, 0.5);
  outline-offset: -10px;
  cursor: copy;
}

.jp-DirListing-item {
  display: flex;
  flex-direction: row;
  align-items: center;
  padding: 4px 12px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-DirListing-checkboxWrapper {
  /* Increases hit area of checkbox. */
  padding: 4px;
}

.jp-DirListing-header
  .jp-DirListing-checkboxWrapper
  + .jp-DirListing-headerItem {
  padding-left: 4px;
}

.jp-DirListing-content .jp-DirListing-checkboxWrapper {
  position: relative;
  left: -4px;
  margin: -4px 0 -4px -8px;
}

.jp-DirListing-checkboxWrapper.jp-mod-visible {
  visibility: visible;
}

/* For devices that support hovering, hide checkboxes until hovered, selected...
*/
@media (hover: hover) {
  .jp-DirListing-checkboxWrapper {
    visibility: hidden;
  }

  .jp-DirListing-item:hover .jp-DirListing-checkboxWrapper,
  .jp-DirListing-item.jp-mod-selected .jp-DirListing-checkboxWrapper {
    visibility: visible;
  }
}

.jp-DirListing-item[data-is-dot] {
  opacity: 75%;
}

.jp-DirListing-item.jp-mod-selected {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.jp-DirListing-item.jp-mod-dropTarget {
  background: var(--jp-brand-color3);
}

.jp-DirListing-item:hover:not(.jp-mod-selected) {
  background: var(--jp-layout-color2);
}

.jp-DirListing-itemIcon {
  flex: 0 0 20px;
  margin-right: 4px;
}

.jp-DirListing-itemText {
  flex: 1 0 64px;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  user-select: none;
}

.jp-DirListing-itemText:focus {
  outline-width: 2px;
  outline-color: var(--jp-inverse-layout-color1);
  outline-style: solid;
  outline-offset: 1px;
}

.jp-DirListing-item.jp-mod-selected .jp-DirListing-itemText:focus {
  outline-color: var(--jp-layout-color1);
}

.jp-DirListing-itemModified {
  flex: 0 0 125px;
  text-align: right;
}

.jp-DirListing-itemFileSize {
  flex: 0 0 90px;
  text-align: right;
}

.jp-DirListing-editor {
  flex: 1 0 64px;
  outline: none;
  border: none;
  color: var(--jp-ui-font-color1);
  background-color: var(--jp-layout-color1);
}

.jp-DirListing-item.jp-mod-running .jp-DirListing-itemIcon::before {
  color: var(--jp-success-color1);
  content: '\25CF';
  font-size: 8px;
  position: absolute;
  left: -8px;
}

.jp-DirListing-item.jp-mod-running.jp-mod-selected
  .jp-DirListing-itemIcon::before {
  color: var(--jp-ui-inverse-font-color1);
}

.jp-DirListing-item.lm-mod-drag-image,
.jp-DirListing-item.jp-mod-selected.lm-mod-drag-image {
  font-size: var(--jp-ui-font-size1);
  padding-left: 4px;
  margin-left: 4px;
  width: 160px;
  background-color: var(--jp-ui-inverse-font-color2);
  box-shadow: var(--jp-elevation-z2);
  border-radius: 0;
  color: var(--jp-ui-font-color1);
  transform: translateX(-40%) translateY(-58%);
}

.jp-Document {
  min-width: 120px;
  min-height: 120px;
  outline: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Main OutputArea
| OutputArea has a list of Outputs
|----------------------------------------------------------------------------*/

.jp-OutputArea {
  overflow-y: auto;
}

.jp-OutputArea-child {
  display: table;
  table-layout: fixed;
  width: 100%;
  overflow: hidden;
}

.jp-OutputPrompt {
  width: var(--jp-cell-prompt-width);
  color: var(--jp-cell-outprompt-font-color);
  font-family: var(--jp-cell-prompt-font-family);
  padding: var(--jp-code-padding);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
  opacity: var(--jp-cell-prompt-opacity);

  /* Right align prompt text, don't wrap to handle large prompt numbers */
  text-align: right;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;

  /* Disable text selection */
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-OutputArea-prompt {
  display: table-cell;
  vertical-align: top;
}

.jp-OutputArea-output {
  display: table-cell;
  width: 100%;
  height: auto;
  overflow: auto;
  user-select: text;
  -moz-user-select: text;
  -webkit-user-select: text;
  -ms-user-select: text;
}

.jp-OutputArea .jp-RenderedText {
  padding-left: 1ch;
}

/**
 * Prompt overlay.
 */

.jp-OutputArea-promptOverlay {
  position: absolute;
  top: 0;
  width: var(--jp-cell-prompt-width);
  height: 100%;
  opacity: 0.5;
}

.jp-OutputArea-promptOverlay:hover {
  background: var(--jp-layout-color2);
  box-shadow: inset 0 0 1px var(--jp-inverse-layout-color0);
  cursor: zoom-out;
}

.jp-mod-outputsScrolled .jp-OutputArea-promptOverlay:hover {
  cursor: zoom-in;
}

/**
 * Isolated output.
 */
.jp-OutputArea-output.jp-mod-isolated {
  width: 100%;
  display: block;
}

/*
When drag events occur, `lm-mod-override-cursor` is added to the body.
Because iframes steal all cursor events, the following two rules are necessary
to suppress pointer events while resize drags are occurring. There may be a
better solution to this problem.
*/
body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated {
  position: relative;
}

body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: transparent;
}

/* pre */

.jp-OutputArea-output pre {
  border: none;
  margin: 0;
  padding: 0;
  overflow-x: auto;
  overflow-y: auto;
  word-break: break-all;
  word-wrap: break-word;
  white-space: pre-wrap;
}

/* tables */

.jp-OutputArea-output.jp-RenderedHTMLCommon table {
  margin-left: 0;
  margin-right: 0;
}

/* description lists */

.jp-OutputArea-output dl,
.jp-OutputArea-output dt,
.jp-OutputArea-output dd {
  display: block;
}

.jp-OutputArea-output dl {
  width: 100%;
  overflow: hidden;
  padding: 0;
  margin: 0;
}

.jp-OutputArea-output dt {
  font-weight: bold;
  float: left;
  width: 20%;
  padding: 0;
  margin: 0;
}

.jp-OutputArea-output dd {
  float: left;
  width: 80%;
  padding: 0;
  margin: 0;
}

.jp-TrimmedOutputs pre {
  background: var(--jp-layout-color3);
  font-size: calc(var(--jp-code-font-size) * 1.4);
  text-align: center;
  text-transform: uppercase;
}

/* Hide the gutter in case of
 *  - nested output areas (e.g. in the case of output widgets)
 *  - mirrored output areas
 */
.jp-OutputArea .jp-OutputArea .jp-OutputArea-prompt {
  display: none;
}

/* Hide empty lines in the output area, for instance due to cleared widgets */
.jp-OutputArea-prompt:empty {
  padding: 0;
  border: 0;
}

/*-----------------------------------------------------------------------------
| executeResult is added to any Output-result for the display of the object
| returned by a cell
|----------------------------------------------------------------------------*/

.jp-OutputArea-output.jp-OutputArea-executeResult {
  margin-left: 0;
  width: 100%;
}

/* Text output with the Out[] prompt needs a top padding to match the
 * alignment of the Out[] prompt itself.
 */
.jp-OutputArea-executeResult .jp-RenderedText.jp-OutputArea-output {
  padding-top: var(--jp-code-padding);
  border-top: var(--jp-border-width) solid transparent;
}

/*-----------------------------------------------------------------------------
| The Stdin output
|----------------------------------------------------------------------------*/

.jp-Stdin-prompt {
  color: var(--jp-content-font-color0);
  padding-right: var(--jp-code-padding);
  vertical-align: baseline;
  flex: 0 0 auto;
}

.jp-Stdin-input {
  font-family: var(--jp-code-font-family);
  font-size: inherit;
  color: inherit;
  background-color: inherit;
  width: 42%;
  min-width: 200px;

  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;

  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0 0.25em;
  margin: 0 0.25em;
  flex: 0 0 70%;
}

.jp-Stdin-input::placeholder {
  opacity: 0;
}

.jp-Stdin-input:focus {
  box-shadow: none;
}

.jp-Stdin-input:focus::placeholder {
  opacity: 1;
}

/*-----------------------------------------------------------------------------
| Output Area View
|----------------------------------------------------------------------------*/

.jp-LinkedOutputView .jp-OutputArea {
  height: 100%;
  display: block;
}

.jp-LinkedOutputView .jp-OutputArea-output:only-child {
  height: 100%;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

@media print {
  .jp-OutputArea-child {
    break-inside: avoid-page;
  }
}

/*-----------------------------------------------------------------------------
| Mobile
|----------------------------------------------------------------------------*/
@media only screen and (max-width: 760px) {
  .jp-OutputPrompt {
    display: table-row;
    text-align: left;
  }

  .jp-OutputArea-child .jp-OutputArea-output {
    display: table-row;
    margin-left: var(--jp-notebook-padding);
  }
}

/* Trimmed outputs warning */
.jp-TrimmedOutputs > a {
  margin: 10px;
  text-decoration: none;
  cursor: pointer;
}

.jp-TrimmedOutputs > a:hover {
  text-decoration: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Table of Contents
|----------------------------------------------------------------------------*/

:root {
  --jp-private-toc-active-width: 4px;
}

.jp-TableOfContents {
  display: flex;
  flex-direction: column;
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  height: 100%;
}

.jp-TableOfContents-placeholder {
  text-align: center;
}

.jp-TableOfContents-placeholderContent {
  color: var(--jp-content-font-color2);
  padding: 8px;
}

.jp-TableOfContents-placeholderContent > h3 {
  margin-bottom: var(--jp-content-heading-margin-bottom);
}

.jp-TableOfContents .jp-SidePanel-content {
  overflow-y: auto;
}

.jp-TableOfContents-tree {
  margin: 4px;
}

.jp-TableOfContents ol {
  list-style-type: none;
}

/* stylelint-disable-next-line selector-max-type */
.jp-TableOfContents li > ol {
  /* Align left border with triangle icon center */
  padding-left: 11px;
}

.jp-TableOfContents-content {
  /* left margin for the active heading indicator */
  margin: 0 0 0 var(--jp-private-toc-active-width);
  padding: 0;
  background-color: var(--jp-layout-color1);
}

.jp-tocItem {
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-tocItem-heading {
  display: flex;
  cursor: pointer;
}

.jp-tocItem-heading:hover {
  background-color: var(--jp-layout-color2);
}

.jp-tocItem-content {
  display: block;
  padding: 4px 0;
  white-space: nowrap;
  text-overflow: ellipsis;
  overflow-x: hidden;
}

.jp-tocItem-collapser {
  height: 20px;
  margin: 2px 2px 0;
  padding: 0;
  background: none;
  border: none;
  cursor: pointer;
}

.jp-tocItem-collapser:hover {
  background-color: var(--jp-layout-color3);
}

/* Active heading indicator */

.jp-tocItem-heading::before {
  content: ' ';
  background: transparent;
  width: var(--jp-private-toc-active-width);
  height: 24px;
  position: absolute;
  left: 0;
  border-radius: var(--jp-border-radius);
}

.jp-tocItem-heading.jp-tocItem-active::before {
  background-color: var(--jp-brand-color1);
}

.jp-tocItem-heading:hover.jp-tocItem-active::before {
  background: var(--jp-brand-color0);
  opacity: 1;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Collapser {
  flex: 0 0 var(--jp-cell-collapser-width);
  padding: 0;
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
  border-radius: var(--jp-border-radius);
  opacity: 1;
}

.jp-Collapser-child {
  display: block;
  width: 100%;
  box-sizing: border-box;

  /* height: 100% doesn't work because the height of its parent is computed from content */
  position: absolute;
  top: 0;
  bottom: 0;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

/*
Hiding collapsers in print mode.

Note: input and output wrappers have "display: block" propery in print mode.
*/

@media print {
  .jp-Collapser {
    display: none;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Header/Footer
|----------------------------------------------------------------------------*/

/* Hidden by zero height by default */
.jp-CellHeader,
.jp-CellFooter {
  height: 0;
  width: 100%;
  padding: 0;
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Input
|----------------------------------------------------------------------------*/

/* All input areas */
.jp-InputArea {
  display: table;
  table-layout: fixed;
  width: 100%;
  overflow: hidden;
}

.jp-InputArea-editor {
  display: table-cell;
  overflow: hidden;
  vertical-align: top;

  /* This is the non-active, default styling */
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  border-radius: 0;
  background: var(--jp-cell-editor-background);
}

.jp-InputPrompt {
  display: table-cell;
  vertical-align: top;
  width: var(--jp-cell-prompt-width);
  color: var(--jp-cell-inprompt-font-color);
  font-family: var(--jp-cell-prompt-font-family);
  padding: var(--jp-code-padding);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  opacity: var(--jp-cell-prompt-opacity);
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;

  /* Right align prompt text, don't wrap to handle large prompt numbers */
  text-align: right;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;

  /* Disable text selection */
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

/*-----------------------------------------------------------------------------
| Mobile
|----------------------------------------------------------------------------*/
@media only screen and (max-width: 760px) {
  .jp-InputArea-editor {
    display: table-row;
    margin-left: var(--jp-notebook-padding);
  }

  .jp-InputPrompt {
    display: table-row;
    text-align: left;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Placeholder
|----------------------------------------------------------------------------*/

.jp-Placeholder {
  display: table;
  table-layout: fixed;
  width: 100%;
}

.jp-Placeholder-prompt {
  display: table-cell;
  box-sizing: border-box;
}

.jp-Placeholder-content {
  display: table-cell;
  padding: 4px 6px;
  border: 1px solid transparent;
  border-radius: 0;
  background: none;
  box-sizing: border-box;
  cursor: pointer;
}

.jp-Placeholder-contentContainer {
  display: flex;
}

.jp-Placeholder-content:hover,
.jp-InputPlaceholder > .jp-Placeholder-content:hover {
  border-color: var(--jp-layout-color3);
}

.jp-Placeholder-content .jp-MoreHorizIcon {
  width: 32px;
  height: 16px;
  border: 1px solid transparent;
  border-radius: var(--jp-border-radius);
}

.jp-Placeholder-content .jp-MoreHorizIcon:hover {
  border: 1px solid var(--jp-border-color1);
  box-shadow: 0 0 2px 0 rgba(0, 0, 0, 0.25);
  background-color: var(--jp-layout-color0);
}

.jp-PlaceholderText {
  white-space: nowrap;
  overflow-x: hidden;
  color: var(--jp-inverse-layout-color3);
  font-family: var(--jp-code-font-family);
}

.jp-InputPlaceholder > .jp-Placeholder-content {
  border-color: var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Private CSS variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-cell-scrolling-output-offset: 5px;
}

/*-----------------------------------------------------------------------------
| Cell
|----------------------------------------------------------------------------*/

.jp-Cell {
  padding: var(--jp-cell-padding);
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Common input/output
|----------------------------------------------------------------------------*/

.jp-Cell-inputWrapper,
.jp-Cell-outputWrapper {
  display: flex;
  flex-direction: row;
  padding: 0;
  margin: 0;

  /* Added to reveal the box-shadow on the input and output collapsers. */
  overflow: visible;
}

/* Only input/output areas inside cells */
.jp-Cell-inputArea,
.jp-Cell-outputArea {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Collapser
|----------------------------------------------------------------------------*/

/* Make the output collapser disappear when there is not output, but do so
 * in a manner that leaves it in the layout and preserves its width.
 */
.jp-Cell.jp-mod-noOutputs .jp-Cell-outputCollapser {
  border: none !important;
  background: transparent !important;
}

.jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputCollapser {
  min-height: var(--jp-cell-collapser-min-height);
}

/*-----------------------------------------------------------------------------
| Output
|----------------------------------------------------------------------------*/

/* Put a space between input and output when there IS output */
.jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputWrapper {
  margin-top: 5px;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea {
  overflow-y: auto;
  max-height: 24em;
  margin-left: var(--jp-private-cell-scrolling-output-offset);
  resize: vertical;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea[style*='height'] {
  max-height: unset;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea::after {
  content: ' ';
  box-shadow: inset 0 0 6px 2px rgb(0 0 0 / 30%);
  width: 100%;
  height: 100%;
  position: sticky;
  bottom: 0;
  top: 0;
  margin-top: -50%;
  float: left;
  display: block;
  pointer-events: none;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-child {
  padding-top: 6px;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-prompt {
  width: calc(
    var(--jp-cell-prompt-width) - var(--jp-private-cell-scrolling-output-offset)
  );
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-promptOverlay {
  left: calc(-1 * var(--jp-private-cell-scrolling-output-offset));
}

/*-----------------------------------------------------------------------------
| CodeCell
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| MarkdownCell
|----------------------------------------------------------------------------*/

.jp-MarkdownOutput {
  display: table-cell;
  width: 100%;
  margin-top: 0;
  margin-bottom: 0;
  padding-left: var(--jp-code-padding);
}

.jp-MarkdownOutput.jp-RenderedHTMLCommon {
  overflow: auto;
}

/* collapseHeadingButton (show always if hiddenCellsButton is _not_ shown) */
.jp-collapseHeadingButton {
  display: flex;
  min-height: var(--jp-cell-collapser-min-height);
  font-size: var(--jp-code-font-size);
  position: absolute;
  background-color: transparent;
  background-size: 25px;
  background-repeat: no-repeat;
  background-position-x: center;
  background-position-y: top;
  background-image: var(--jp-icon-caret-down);
  right: 0;
  top: 0;
  bottom: 0;
}

.jp-collapseHeadingButton.jp-mod-collapsed {
  background-image: var(--jp-icon-caret-right);
}

/*
 set the container font size to match that of content
 so that the nested collapse buttons have the right size
*/
.jp-MarkdownCell .jp-InputPrompt {
  font-size: var(--jp-content-font-size1);
}

/*
  Align collapseHeadingButton with cell top header
  The font sizes are identical to the ones in packages/rendermime/style/base.css
*/
.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='1'] {
  font-size: var(--jp-content-font-size5);
  background-position-y: calc(0.3 * var(--jp-content-font-size5));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='2'] {
  font-size: var(--jp-content-font-size4);
  background-position-y: calc(0.3 * var(--jp-content-font-size4));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='3'] {
  font-size: var(--jp-content-font-size3);
  background-position-y: calc(0.3 * var(--jp-content-font-size3));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='4'] {
  font-size: var(--jp-content-font-size2);
  background-position-y: calc(0.3 * var(--jp-content-font-size2));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='5'] {
  font-size: var(--jp-content-font-size1);
  background-position-y: top;
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='6'] {
  font-size: var(--jp-content-font-size0);
  background-position-y: top;
}

/* collapseHeadingButton (show only on (hover,active) if hiddenCellsButton is shown) */
.jp-Notebook.jp-mod-showHiddenCellsButton .jp-collapseHeadingButton {
  display: none;
}

.jp-Notebook.jp-mod-showHiddenCellsButton
  :is(.jp-MarkdownCell:hover, .jp-mod-active)
  .jp-collapseHeadingButton {
  display: flex;
}

/* showHiddenCellsButton (only show if jp-mod-showHiddenCellsButton is set, which
is a consequence of the showHiddenCellsButton option in Notebook Settings)*/
.jp-Notebook.jp-mod-showHiddenCellsButton .jp-showHiddenCellsButton {
  margin-left: calc(var(--jp-cell-prompt-width) + 2 * var(--jp-code-padding));
  margin-top: var(--jp-code-padding);
  border: 1px solid var(--jp-border-color2);
  background-color: var(--jp-border-color3) !important;
  color: var(--jp-content-font-color0) !important;
  display: flex;
}

.jp-Notebook.jp-mod-showHiddenCellsButton .jp-showHiddenCellsButton:hover {
  background-color: var(--jp-border-color2) !important;
}

.jp-showHiddenCellsButton {
  display: none;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

/*
Using block instead of flex to allow the use of the break-inside CSS property for
cell outputs.
*/

@media print {
  .jp-Cell-inputWrapper,
  .jp-Cell-outputWrapper {
    display: block;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-notebook-toolbar-padding: 2px 5px 2px 2px;
}

/*-----------------------------------------------------------------------------

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-NotebookPanel-toolbar {
  padding: var(--jp-notebook-toolbar-padding);

  /* disable paint containment from lumino 2.0 default strict CSS containment */
  contain: style size !important;
}

.jp-Toolbar-item.jp-Notebook-toolbarCellType .jp-select-wrapper.jp-mod-focused {
  border: none;
  box-shadow: none;
}

.jp-Notebook-toolbarCellTypeDropdown select {
  height: 24px;
  font-size: var(--jp-ui-font-size1);
  line-height: 14px;
  border-radius: 0;
  display: block;
}

.jp-Notebook-toolbarCellTypeDropdown span {
  top: 5px !important;
}

.jp-Toolbar-responsive-popup {
  position: absolute;
  height: fit-content;
  display: flex;
  flex-direction: row;
  flex-wrap: wrap;
  justify-content: flex-end;
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  background: var(--jp-toolbar-background);
  min-height: var(--jp-toolbar-micro-height);
  padding: var(--jp-notebook-toolbar-padding);
  z-index: 1;
  right: 0;
  top: 0;
}

.jp-Toolbar > .jp-Toolbar-responsive-opener {
  margin-left: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-Notebook-ExecutionIndicator {
  position: relative;
  display: inline-block;
  height: 100%;
  z-index: 9997;
}

.jp-Notebook-ExecutionIndicator-tooltip {
  visibility: hidden;
  height: auto;
  width: max-content;
  width: -moz-max-content;
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color1);
  text-align: justify;
  border-radius: 6px;
  padding: 0 5px;
  position: fixed;
  display: table;
}

.jp-Notebook-ExecutionIndicator-tooltip.up {
  transform: translateX(-50%) translateY(-100%) translateY(-32px);
}

.jp-Notebook-ExecutionIndicator-tooltip.down {
  transform: translateX(calc(-100% + 16px)) translateY(5px);
}

.jp-Notebook-ExecutionIndicator-tooltip.hidden {
  display: none;
}

.jp-Notebook-ExecutionIndicator:hover .jp-Notebook-ExecutionIndicator-tooltip {
  visibility: visible;
}

.jp-Notebook-ExecutionIndicator span {
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  color: var(--jp-ui-font-color1);
  line-height: 24px;
  display: block;
}

.jp-Notebook-ExecutionIndicator-progress-bar {
  display: flex;
  justify-content: center;
  height: 100%;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*
 * Execution indicator
 */
.jp-tocItem-content::after {
  content: '';

  /* Must be identical to form a circle */
  width: 12px;
  height: 12px;
  background: none;
  border: none;
  position: absolute;
  right: 0;
}

.jp-tocItem-content[data-running='0']::after {
  border-radius: 50%;
  border: var(--jp-border-width) solid var(--jp-inverse-layout-color3);
  background: none;
}

.jp-tocItem-content[data-running='1']::after {
  border-radius: 50%;
  border: var(--jp-border-width) solid var(--jp-inverse-layout-color3);
  background-color: var(--jp-inverse-layout-color3);
}

.jp-tocItem-content[data-running='0'],
.jp-tocItem-content[data-running='1'] {
  margin-right: 12px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-Notebook-footer {
  height: 27px;
  margin-left: calc(
    var(--jp-cell-prompt-width) + var(--jp-cell-collapser-width) +
      var(--jp-cell-padding)
  );
  width: calc(
    100% -
      (
        var(--jp-cell-prompt-width) + var(--jp-cell-collapser-width) +
          var(--jp-cell-padding) + var(--jp-cell-padding)
      )
  );
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  color: var(--jp-ui-font-color3);
  margin-top: 6px;
  background: none;
  cursor: pointer;
}

.jp-Notebook-footer:focus {
  border-color: var(--jp-cell-editor-active-border-color);
}

/* For devices that support hovering, hide footer until hover */
@media (hover: hover) {
  .jp-Notebook-footer {
    opacity: 0;
  }

  .jp-Notebook-footer:focus,
  .jp-Notebook-footer:hover {
    opacity: 1;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Imports
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| CSS variables
|----------------------------------------------------------------------------*/

:root {
  --jp-side-by-side-output-size: 1fr;
  --jp-side-by-side-resized-cell: var(--jp-side-by-side-output-size);
  --jp-private-notebook-dragImage-width: 304px;
  --jp-private-notebook-dragImage-height: 36px;
  --jp-private-notebook-selected-color: var(--md-blue-400);
  --jp-private-notebook-active-color: var(--md-green-400);
}

/*-----------------------------------------------------------------------------
| Notebook
|----------------------------------------------------------------------------*/

/* stylelint-disable selector-max-class */

.jp-NotebookPanel {
  display: block;
  height: 100%;
}

.jp-NotebookPanel.jp-Document {
  min-width: 240px;
  min-height: 120px;
}

.jp-Notebook {
  padding: var(--jp-notebook-padding);
  outline: none;
  overflow: auto;
  background: var(--jp-layout-color0);
}

.jp-Notebook.jp-mod-scrollPastEnd::after {
  display: block;
  content: '';
  min-height: var(--jp-notebook-scroll-padding);
}

.jp-MainAreaWidget-ContainStrict .jp-Notebook * {
  contain: strict;
}

.jp-Notebook .jp-Cell {
  overflow: visible;
}

.jp-Notebook .jp-Cell .jp-InputPrompt {
  cursor: move;
}

/*-----------------------------------------------------------------------------
| Notebook state related styling
|
| The notebook and cells each have states, here are the possibilities:
|
| - Notebook
|   - Command
|   - Edit
| - Cell
|   - None
|   - Active (only one can be active)
|   - Selected (the cells actions are applied to)
|   - Multiselected (when multiple selected, the cursor)
|   - No outputs
|----------------------------------------------------------------------------*/

/* Command or edit modes */

.jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-InputPrompt {
  opacity: var(--jp-cell-prompt-not-active-opacity);
  color: var(--jp-cell-prompt-not-active-font-color);
}

.jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-OutputPrompt {
  opacity: var(--jp-cell-prompt-not-active-opacity);
  color: var(--jp-cell-prompt-not-active-font-color);
}

/* cell is active */
.jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser {
  background: var(--jp-brand-color1);
}

/* cell is dirty */
.jp-Notebook .jp-Cell.jp-mod-dirty .jp-InputPrompt {
  color: var(--jp-warn-color1);
}

.jp-Notebook .jp-Cell.jp-mod-dirty .jp-InputPrompt::before {
  color: var(--jp-warn-color1);
  content: '•';
}

.jp-Notebook .jp-Cell.jp-mod-active.jp-mod-dirty .jp-Collapser {
  background: var(--jp-warn-color1);
}

/* collapser is hovered */
.jp-Notebook .jp-Cell .jp-Collapser:hover {
  box-shadow: var(--jp-elevation-z2);
  background: var(--jp-brand-color1);
  opacity: var(--jp-cell-collapser-not-active-hover-opacity);
}

/* cell is active and collapser is hovered */
.jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser:hover {
  background: var(--jp-brand-color0);
  opacity: 1;
}

/* Command mode */

.jp-Notebook.jp-mod-commandMode .jp-Cell.jp-mod-selected {
  background: var(--jp-notebook-multiselected-color);
}

.jp-Notebook.jp-mod-commandMode
  .jp-Cell.jp-mod-active.jp-mod-selected:not(.jp-mod-multiSelected) {
  background: transparent;
}

/* Edit mode */

.jp-Notebook.jp-mod-editMode .jp-Cell.jp-mod-active .jp-InputArea-editor {
  border: var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
  background-color: var(--jp-cell-editor-active-background);
}

/*-----------------------------------------------------------------------------
| Notebook drag and drop
|----------------------------------------------------------------------------*/

.jp-Notebook-cell.jp-mod-dropSource {
  opacity: 0.5;
}

.jp-Notebook-cell.jp-mod-dropTarget,
.jp-Notebook.jp-mod-commandMode
  .jp-Notebook-cell.jp-mod-active.jp-mod-selected.jp-mod-dropTarget {
  border-top-color: var(--jp-private-notebook-selected-color);
  border-top-style: solid;
  border-top-width: 2px;
}

.jp-dragImage {
  display: block;
  flex-direction: row;
  width: var(--jp-private-notebook-dragImage-width);
  height: var(--jp-private-notebook-dragImage-height);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background);
  overflow: visible;
}

.jp-dragImage-singlePrompt {
  box-shadow: 2px 2px 4px 0 rgba(0, 0, 0, 0.12);
}

.jp-dragImage .jp-dragImage-content {
  flex: 1 1 auto;
  z-index: 2;
  font-size: var(--jp-code-font-size);
  font-family: var(--jp-code-font-family);
  line-height: var(--jp-code-line-height);
  padding: var(--jp-code-padding);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background-color);
  color: var(--jp-content-font-color3);
  text-align: left;
  margin: 4px 4px 4px 0;
}

.jp-dragImage .jp-dragImage-prompt {
  flex: 0 0 auto;
  min-width: 36px;
  color: var(--jp-cell-inprompt-font-color);
  padding: var(--jp-code-padding);
  padding-left: 12px;
  font-family: var(--jp-cell-prompt-font-family);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  line-height: 1.9;
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
}

.jp-dragImage-multipleBack {
  z-index: -1;
  position: absolute;
  height: 32px;
  width: 300px;
  top: 8px;
  left: 8px;
  background: var(--jp-layout-color2);
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  box-shadow: 2px 2px 4px 0 rgba(0, 0, 0, 0.12);
}

/*-----------------------------------------------------------------------------
| Cell toolbar
|----------------------------------------------------------------------------*/

.jp-NotebookTools {
  display: block;
  min-width: var(--jp-sidebar-min-width);
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);

  /* This is needed so that all font sizing of children done in ems is
    * relative to this base size */
  font-size: var(--jp-ui-font-size1);
  overflow: auto;
}

.jp-ActiveCellTool {
  padding: 12px 0;
  display: flex;
}

.jp-ActiveCellTool-Content {
  flex: 1 1 auto;
}

.jp-ActiveCellTool .jp-ActiveCellTool-CellContent {
  background: var(--jp-cell-editor-background);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  border-radius: 0;
  min-height: 29px;
}

.jp-ActiveCellTool .jp-InputPrompt {
  min-width: calc(var(--jp-cell-prompt-width) * 0.75);
}

.jp-ActiveCellTool-CellContent > pre {
  padding: 5px 4px;
  margin: 0;
  white-space: normal;
}

.jp-MetadataEditorTool {
  flex-direction: column;
  padding: 12px 0;
}

.jp-RankedPanel > :not(:first-child) {
  margin-top: 12px;
}

.jp-KeySelector select.jp-mod-styled {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  border: var(--jp-border-width) solid var(--jp-border-color1);
}

.jp-KeySelector label,
.jp-MetadataEditorTool label,
.jp-NumberSetter label {
  line-height: 1.4;
}

.jp-NotebookTools .jp-select-wrapper {
  margin-top: 4px;
  margin-bottom: 0;
}

.jp-NumberSetter input {
  width: 100%;
  margin-top: 4px;
}

.jp-NotebookTools .jp-Collapse {
  margin-top: 16px;
}

/*-----------------------------------------------------------------------------
| Presentation Mode (.jp-mod-presentationMode)
|----------------------------------------------------------------------------*/

.jp-mod-presentationMode .jp-Notebook {
  --jp-content-font-size1: var(--jp-content-presentation-font-size1);
  --jp-code-font-size: var(--jp-code-presentation-font-size);
}

.jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-InputPrompt,
.jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-OutputPrompt {
  flex: 0 0 110px;
}

/*-----------------------------------------------------------------------------
| Side-by-side Mode (.jp-mod-sideBySide)
|----------------------------------------------------------------------------*/
.jp-mod-sideBySide.jp-Notebook .jp-Notebook-cell {
  margin-top: 3em;
  margin-bottom: 3em;
  margin-left: 5%;
  margin-right: 5%;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell {
  display: grid;
  grid-template-columns: minmax(0, 1fr) min-content minmax(
      0,
      var(--jp-side-by-side-output-size)
    );
  grid-template-rows: auto minmax(0, 1fr) auto;
  grid-template-areas:
    'header header header'
    'input handle output'
    'footer footer footer';
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell.jp-mod-resizedCell {
  grid-template-columns: minmax(0, 1fr) min-content minmax(
      0,
      var(--jp-side-by-side-resized-cell)
    );
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellHeader {
  grid-area: header;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-Cell-inputWrapper {
  grid-area: input;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-Cell-outputWrapper {
  /* overwrite the default margin (no vertical separation needed in side by side move */
  margin-top: 0;
  grid-area: output;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellFooter {
  grid-area: footer;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellResizeHandle {
  grid-area: handle;
  user-select: none;
  display: block;
  height: 100%;
  cursor: ew-resize;
  padding: 0 var(--jp-cell-padding);
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellResizeHandle::after {
  content: '';
  display: block;
  background: var(--jp-border-color2);
  height: 100%;
  width: 5px;
}

.jp-mod-sideBySide.jp-Notebook
  .jp-CodeCell.jp-mod-resizedCell
  .jp-CellResizeHandle::after {
  background: var(--jp-border-color0);
}

.jp-CellResizeHandle {
  display: none;
}

/*-----------------------------------------------------------------------------
| Placeholder
|----------------------------------------------------------------------------*/

.jp-Cell-Placeholder {
  padding-left: 55px;
}

.jp-Cell-Placeholder-wrapper {
  background: #fff;
  border: 1px solid;
  border-color: #e5e6e9 #dfe0e4 #d0d1d5;
  border-radius: 4px;
  -webkit-border-radius: 4px;
  margin: 10px 15px;
}

.jp-Cell-Placeholder-wrapper-inner {
  padding: 15px;
  position: relative;
}

.jp-Cell-Placeholder-wrapper-body {
  background-repeat: repeat;
  background-size: 50% auto;
}

.jp-Cell-Placeholder-wrapper-body div {
  background: #f6f7f8;
  background-image: -webkit-linear-gradient(
    left,
    #f6f7f8 0%,
    #edeef1 20%,
    #f6f7f8 40%,
    #f6f7f8 100%
  );
  background-repeat: no-repeat;
  background-size: 800px 104px;
  height: 104px;
  position: absolute;
  right: 15px;
  left: 15px;
  top: 15px;
}

div.jp-Cell-Placeholder-h1 {
  top: 20px;
  height: 20px;
  left: 15px;
  width: 150px;
}

div.jp-Cell-Placeholder-h2 {
  left: 15px;
  top: 50px;
  height: 10px;
  width: 100px;
}

div.jp-Cell-Placeholder-content-1,
div.jp-Cell-Placeholder-content-2,
div.jp-Cell-Placeholder-content-3 {
  left: 15px;
  right: 15px;
  height: 10px;
}

div.jp-Cell-Placeholder-content-1 {
  top: 100px;
}

div.jp-Cell-Placeholder-content-2 {
  top: 120px;
}

div.jp-Cell-Placeholder-content-3 {
  top: 140px;
}

</style>
<style type="text/css">
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*
The following CSS variables define the main, public API for styling JupyterLab.
These variables should be used by all plugins wherever possible. In other
words, plugins should not define custom colors, sizes, etc unless absolutely
necessary. This enables users to change the visual theme of JupyterLab
by changing these variables.

Many variables appear in an ordered sequence (0,1,2,3). These sequences
are designed to work well together, so for example, `--jp-border-color1` should
be used with `--jp-layout-color1`. The numbers have the following meanings:

* 0: super-primary, reserved for special emphasis
* 1: primary, most important under normal situations
* 2: secondary, next most important under normal situations
* 3: tertiary, next most important under normal situations

Throughout JupyterLab, we are mostly following principles from Google's
Material Design when selecting colors. We are not, however, following
all of MD as it is not optimized for dense, information rich UIs.
*/

:root {
  /* Elevation
   *
   * We style box-shadows using Material Design's idea of elevation. These particular numbers are taken from here:
   *
   * https://github.com/material-components/material-components-web
   * https://material-components-web.appspot.com/elevation.html
   */

  --jp-shadow-base-lightness: 0;
  --jp-shadow-umbra-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.2
  );
  --jp-shadow-penumbra-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.14
  );
  --jp-shadow-ambient-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.12
  );
  --jp-elevation-z0: none;
  --jp-elevation-z1: 0 2px 1px -1px var(--jp-shadow-umbra-color),
    0 1px 1px 0 var(--jp-shadow-penumbra-color),
    0 1px 3px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z2: 0 3px 1px -2px var(--jp-shadow-umbra-color),
    0 2px 2px 0 var(--jp-shadow-penumbra-color),
    0 1px 5px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z4: 0 2px 4px -1px var(--jp-shadow-umbra-color),
    0 4px 5px 0 var(--jp-shadow-penumbra-color),
    0 1px 10px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z6: 0 3px 5px -1px var(--jp-shadow-umbra-color),
    0 6px 10px 0 var(--jp-shadow-penumbra-color),
    0 1px 18px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z8: 0 5px 5px -3px var(--jp-shadow-umbra-color),
    0 8px 10px 1px var(--jp-shadow-penumbra-color),
    0 3px 14px 2px var(--jp-shadow-ambient-color);
  --jp-elevation-z12: 0 7px 8px -4px var(--jp-shadow-umbra-color),
    0 12px 17px 2px var(--jp-shadow-penumbra-color),
    0 5px 22px 4px var(--jp-shadow-ambient-color);
  --jp-elevation-z16: 0 8px 10px -5px var(--jp-shadow-umbra-color),
    0 16px 24px 2px var(--jp-shadow-penumbra-color),
    0 6px 30px 5px var(--jp-shadow-ambient-color);
  --jp-elevation-z20: 0 10px 13px -6px var(--jp-shadow-umbra-color),
    0 20px 31px 3px var(--jp-shadow-penumbra-color),
    0 8px 38px 7px var(--jp-shadow-ambient-color);
  --jp-elevation-z24: 0 11px 15px -7px var(--jp-shadow-umbra-color),
    0 24px 38px 3px var(--jp-shadow-penumbra-color),
    0 9px 46px 8px var(--jp-shadow-ambient-color);

  /* Borders
   *
   * The following variables, specify the visual styling of borders in JupyterLab.
   */

  --jp-border-width: 1px;
  --jp-border-color0: var(--md-grey-400);
  --jp-border-color1: var(--md-grey-400);
  --jp-border-color2: var(--md-grey-300);
  --jp-border-color3: var(--md-grey-200);
  --jp-inverse-border-color: var(--md-grey-600);
  --jp-border-radius: 2px;

  /* UI Fonts
   *
   * The UI font CSS variables are used for the typography all of the JupyterLab
   * user interface elements that are not directly user generated content.
   *
   * The font sizing here is done assuming that the body font size of --jp-ui-font-size1
   * is applied to a parent element. When children elements, such as headings, are sized
   * in em all things will be computed relative to that body size.
   */

  --jp-ui-font-scale-factor: 1.2;
  --jp-ui-font-size0: 0.83333em;
  --jp-ui-font-size1: 13px; /* Base font size */
  --jp-ui-font-size2: 1.2em;
  --jp-ui-font-size3: 1.44em;
  --jp-ui-font-family: system-ui, -apple-system, blinkmacsystemfont, 'Segoe UI',
    helvetica, arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji',
    'Segoe UI Symbol';

  /*
   * Use these font colors against the corresponding main layout colors.
   * In a light theme, these go from dark to light.
   */

  /* Defaults use Material Design specification */
  --jp-ui-font-color0: rgba(0, 0, 0, 1);
  --jp-ui-font-color1: rgba(0, 0, 0, 0.87);
  --jp-ui-font-color2: rgba(0, 0, 0, 0.54);
  --jp-ui-font-color3: rgba(0, 0, 0, 0.38);

  /*
   * Use these against the brand/accent/warn/error colors.
   * These will typically go from light to darker, in both a dark and light theme.
   */

  --jp-ui-inverse-font-color0: rgba(255, 255, 255, 1);
  --jp-ui-inverse-font-color1: rgba(255, 255, 255, 1);
  --jp-ui-inverse-font-color2: rgba(255, 255, 255, 0.7);
  --jp-ui-inverse-font-color3: rgba(255, 255, 255, 0.5);

  /* Content Fonts
   *
   * Content font variables are used for typography of user generated content.
   *
   * The font sizing here is done assuming that the body font size of --jp-content-font-size1
   * is applied to a parent element. When children elements, such as headings, are sized
   * in em all things will be computed relative to that body size.
   */

  --jp-content-line-height: 1.6;
  --jp-content-font-scale-factor: 1.2;
  --jp-content-font-size0: 0.83333em;
  --jp-content-font-size1: 14px; /* Base font size */
  --jp-content-font-size2: 1.2em;
  --jp-content-font-size3: 1.44em;
  --jp-content-font-size4: 1.728em;
  --jp-content-font-size5: 2.0736em;

  /* This gives a magnification of about 125% in presentation mode over normal. */
  --jp-content-presentation-font-size1: 17px;
  --jp-content-heading-line-height: 1;
  --jp-content-heading-margin-top: 1.2em;
  --jp-content-heading-margin-bottom: 0.8em;
  --jp-content-heading-font-weight: 500;

  /* Defaults use Material Design specification */
  --jp-content-font-color0: rgba(0, 0, 0, 1);
  --jp-content-font-color1: rgba(0, 0, 0, 0.87);
  --jp-content-font-color2: rgba(0, 0, 0, 0.54);
  --jp-content-font-color3: rgba(0, 0, 0, 0.38);
  --jp-content-link-color: var(--md-blue-900);
  --jp-content-font-family: system-ui, -apple-system, blinkmacsystemfont,
    'Segoe UI', helvetica, arial, sans-serif, 'Apple Color Emoji',
    'Segoe UI Emoji', 'Segoe UI Symbol';

  /*
   * Code Fonts
   *
   * Code font variables are used for typography of code and other monospaces content.
   */

  --jp-code-font-size: 13px;
  --jp-code-line-height: 1.3077; /* 17px for 13px base */
  --jp-code-padding: 5px; /* 5px for 13px base, codemirror highlighting needs integer px value */
  --jp-code-font-family-default: menlo, consolas, 'DejaVu Sans Mono', monospace;
  --jp-code-font-family: var(--jp-code-font-family-default);

  /* This gives a magnification of about 125% in presentation mode over normal. */
  --jp-code-presentation-font-size: 16px;

  /* may need to tweak cursor width if you change font size */
  --jp-code-cursor-width0: 1.4px;
  --jp-code-cursor-width1: 2px;
  --jp-code-cursor-width2: 4px;

  /* Layout
   *
   * The following are the main layout colors use in JupyterLab. In a light
   * theme these would go from light to dark.
   */

  --jp-layout-color0: white;
  --jp-layout-color1: white;
  --jp-layout-color2: var(--md-grey-200);
  --jp-layout-color3: var(--md-grey-400);
  --jp-layout-color4: var(--md-grey-600);

  /* Inverse Layout
   *
   * The following are the inverse layout colors use in JupyterLab. In a light
   * theme these would go from dark to light.
   */

  --jp-inverse-layout-color0: #111;
  --jp-inverse-layout-color1: var(--md-grey-900);
  --jp-inverse-layout-color2: var(--md-grey-800);
  --jp-inverse-layout-color3: var(--md-grey-700);
  --jp-inverse-layout-color4: var(--md-grey-600);

  /* Brand/accent */

  --jp-brand-color0: var(--md-blue-900);
  --jp-brand-color1: var(--md-blue-700);
  --jp-brand-color2: var(--md-blue-300);
  --jp-brand-color3: var(--md-blue-100);
  --jp-brand-color4: var(--md-blue-50);
  --jp-accent-color0: var(--md-green-900);
  --jp-accent-color1: var(--md-green-700);
  --jp-accent-color2: var(--md-green-300);
  --jp-accent-color3: var(--md-green-100);

  /* State colors (warn, error, success, info) */

  --jp-warn-color0: var(--md-orange-900);
  --jp-warn-color1: var(--md-orange-700);
  --jp-warn-color2: var(--md-orange-300);
  --jp-warn-color3: var(--md-orange-100);
  --jp-error-color0: var(--md-red-900);
  --jp-error-color1: var(--md-red-700);
  --jp-error-color2: var(--md-red-300);
  --jp-error-color3: var(--md-red-100);
  --jp-success-color0: var(--md-green-900);
  --jp-success-color1: var(--md-green-700);
  --jp-success-color2: var(--md-green-300);
  --jp-success-color3: var(--md-green-100);
  --jp-info-color0: var(--md-cyan-900);
  --jp-info-color1: var(--md-cyan-700);
  --jp-info-color2: var(--md-cyan-300);
  --jp-info-color3: var(--md-cyan-100);

  /* Cell specific styles */

  --jp-cell-padding: 5px;
  --jp-cell-collapser-width: 8px;
  --jp-cell-collapser-min-height: 20px;
  --jp-cell-collapser-not-active-hover-opacity: 0.6;
  --jp-cell-editor-background: var(--md-grey-100);
  --jp-cell-editor-border-color: var(--md-grey-300);
  --jp-cell-editor-box-shadow: inset 0 0 2px var(--md-blue-300);
  --jp-cell-editor-active-background: var(--jp-layout-color0);
  --jp-cell-editor-active-border-color: var(--jp-brand-color1);
  --jp-cell-prompt-width: 64px;
  --jp-cell-prompt-font-family: var(--jp-code-font-family-default);
  --jp-cell-prompt-letter-spacing: 0;
  --jp-cell-prompt-opacity: 1;
  --jp-cell-prompt-not-active-opacity: 0.5;
  --jp-cell-prompt-not-active-font-color: var(--md-grey-700);

  /* A custom blend of MD grey and blue 600
   * See https://meyerweb.com/eric/tools/color-blend/#546E7A:1E88E5:5:hex */
  --jp-cell-inprompt-font-color: #307fc1;

  /* A custom blend of MD grey and orange 600
   * https://meyerweb.com/eric/tools/color-blend/#546E7A:F4511E:5:hex */
  --jp-cell-outprompt-font-color: #bf5b3d;

  /* Notebook specific styles */

  --jp-notebook-padding: 10px;
  --jp-notebook-select-background: var(--jp-layout-color1);
  --jp-notebook-multiselected-color: var(--md-blue-50);

  /* The scroll padding is calculated to fill enough space at the bottom of the
  notebook to show one single-line cell (with appropriate padding) at the top
  when the notebook is scrolled all the way to the bottom. We also subtract one
  pixel so that no scrollbar appears if we have just one single-line cell in the
  notebook. This padding is to enable a 'scroll past end' feature in a notebook.
  */
  --jp-notebook-scroll-padding: calc(
    100% - var(--jp-code-font-size) * var(--jp-code-line-height) -
      var(--jp-code-padding) - var(--jp-cell-padding) - 1px
  );

  /* Rendermime styles */

  --jp-rendermime-error-background: #fdd;
  --jp-rendermime-table-row-background: var(--md-grey-100);
  --jp-rendermime-table-row-hover-background: var(--md-light-blue-50);

  /* Dialog specific styles */

  --jp-dialog-background: rgba(0, 0, 0, 0.25);

  /* Console specific styles */

  --jp-console-padding: 10px;

  /* Toolbar specific styles */

  --jp-toolbar-border-color: var(--jp-border-color1);
  --jp-toolbar-micro-height: 8px;
  --jp-toolbar-background: var(--jp-layout-color1);
  --jp-toolbar-box-shadow: 0 0 2px 0 rgba(0, 0, 0, 0.24);
  --jp-toolbar-header-margin: 4px 4px 0 4px;
  --jp-toolbar-active-background: var(--md-grey-300);

  /* Statusbar specific styles */

  --jp-statusbar-height: 24px;

  /* Input field styles */

  --jp-input-box-shadow: inset 0 0 2px var(--md-blue-300);
  --jp-input-active-background: var(--jp-layout-color1);
  --jp-input-hover-background: var(--jp-layout-color1);
  --jp-input-background: var(--md-grey-100);
  --jp-input-border-color: var(--jp-inverse-border-color);
  --jp-input-active-border-color: var(--jp-brand-color1);
  --jp-input-active-box-shadow-color: rgba(19, 124, 189, 0.3);

  /* General editor styles */

  --jp-editor-selected-background: #d9d9d9;
  --jp-editor-selected-focused-background: #d7d4f0;
  --jp-editor-cursor-color: var(--jp-ui-font-color0);

  /* Code mirror specific styles */

  --jp-mirror-editor-keyword-color: #008000;
  --jp-mirror-editor-atom-color: #88f;
  --jp-mirror-editor-number-color: #080;
  --jp-mirror-editor-def-color: #00f;
  --jp-mirror-editor-variable-color: var(--md-grey-900);
  --jp-mirror-editor-variable-2-color: rgb(0, 54, 109);
  --jp-mirror-editor-variable-3-color: #085;
  --jp-mirror-editor-punctuation-color: #05a;
  --jp-mirror-editor-property-color: #05a;
  --jp-mirror-editor-operator-color: #a2f;
  --jp-mirror-editor-comment-color: #408080;
  --jp-mirror-editor-string-color: #ba2121;
  --jp-mirror-editor-string-2-color: #708;
  --jp-mirror-editor-meta-color: #a2f;
  --jp-mirror-editor-qualifier-color: #555;
  --jp-mirror-editor-builtin-color: #008000;
  --jp-mirror-editor-bracket-color: #997;
  --jp-mirror-editor-tag-color: #170;
  --jp-mirror-editor-attribute-color: #00c;
  --jp-mirror-editor-header-color: blue;
  --jp-mirror-editor-quote-color: #090;
  --jp-mirror-editor-link-color: #00c;
  --jp-mirror-editor-error-color: #f00;
  --jp-mirror-editor-hr-color: #999;

  /*
    RTC user specific colors.
    These colors are used for the cursor, username in the editor,
    and the icon of the user.
  */

  --jp-collaborator-color1: #ffad8e;
  --jp-collaborator-color2: #dac83d;
  --jp-collaborator-color3: #72dd76;
  --jp-collaborator-color4: #00e4d0;
  --jp-collaborator-color5: #45d4ff;
  --jp-collaborator-color6: #e2b1ff;
  --jp-collaborator-color7: #ff9de6;

  /* Vega extension styles */

  --jp-vega-background: white;

  /* Sidebar-related styles */

  --jp-sidebar-min-width: 250px;

  /* Search-related styles */

  --jp-search-toggle-off-opacity: 0.5;
  --jp-search-toggle-hover-opacity: 0.8;
  --jp-search-toggle-on-opacity: 1;
  --jp-search-selected-match-background-color: rgb(245, 200, 0);
  --jp-search-selected-match-color: black;
  --jp-search-unselected-match-background-color: var(
    --jp-inverse-layout-color0
  );
  --jp-search-unselected-match-color: var(--jp-ui-inverse-font-color0);

  /* Icon colors that work well with light or dark backgrounds */
  --jp-icon-contrast-color0: var(--md-purple-600);
  --jp-icon-contrast-color1: var(--md-green-600);
  --jp-icon-contrast-color2: var(--md-pink-600);
  --jp-icon-contrast-color3: var(--md-blue-600);

  /* Button colors */
  --jp-accept-color-normal: var(--md-blue-700);
  --jp-accept-color-hover: var(--md-blue-800);
  --jp-accept-color-active: var(--md-blue-900);
  --jp-warn-color-normal: var(--md-red-700);
  --jp-warn-color-hover: var(--md-red-800);
  --jp-warn-color-active: var(--md-red-900);
  --jp-reject-color-normal: var(--md-grey-600);
  --jp-reject-color-hover: var(--md-grey-700);
  --jp-reject-color-active: var(--md-grey-800);

  /* File or activity icons and switch semantic variables */
  --jp-jupyter-icon-color: #f37626;
  --jp-notebook-icon-color: #f37626;
  --jp-json-icon-color: var(--md-orange-700);
  --jp-console-icon-background-color: var(--md-blue-700);
  --jp-console-icon-color: white;
  --jp-terminal-icon-background-color: var(--md-grey-800);
  --jp-terminal-icon-color: var(--md-grey-200);
  --jp-text-editor-icon-color: var(--md-grey-700);
  --jp-inspector-icon-color: var(--md-grey-700);
  --jp-switch-color: var(--md-grey-400);
  --jp-switch-true-position-color: var(--md-orange-900);
}
</style>
<style type="text/css">
/* Force rendering true colors when outputing to pdf */
* {
  -webkit-print-color-adjust: exact;
}

/* Misc */
a.anchor-link {
  display: none;
}

/* Input area styling */
.jp-InputArea {
  overflow: hidden;
}

.jp-InputArea-editor {
  overflow: hidden;
}

.cm-editor.cm-s-jupyter .highlight pre {
/* weird, but --jp-code-padding defined to be 5px but 4px horizontal padding is hardcoded for pre.cm-line */
  padding: var(--jp-code-padding) 4px;
  margin: 0;

  font-family: inherit;
  font-size: inherit;
  line-height: inherit;
  color: inherit;

}

.jp-OutputArea-output pre {
  line-height: inherit;
  font-family: inherit;
}

.jp-RenderedText pre {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-code-font-size);
}

/* Hiding the collapser by default */
.jp-Collapser {
  display: none;
}

@page {
    margin: 0.5in; /* Margin for each printed piece of paper */
}

@media print {
  .jp-Cell-inputWrapper,
  .jp-Cell-outputWrapper {
    display: block;
  }
}
</style>
<!-- Load mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML-full,Safe"> </script>
<!-- MathJax configuration -->
<script type="text/x-mathjax-config">
    init_mathjax = function() {
        if (window.MathJax) {
        // MathJax loaded
            MathJax.Hub.Config({
                TeX: {
                    equationNumbers: {
                    autoNumber: "AMS",
                    useLabelIds: true
                    }
                },
                tex2jax: {
                    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                    processEscapes: true,
                    processEnvironments: true
                },
                displayAlign: 'center',
                CommonHTML: {
                    linebreaks: {
                    automatic: true
                    }
                }
            });

            MathJax.Hub.Queue(["Typeset", MathJax.Hub]);
        }
    }
    init_mathjax();
    </script>
<!-- End of mathjax configuration --><script type="module">
  document.addEventListener("DOMContentLoaded", async () => {
    const diagrams = document.querySelectorAll(".jp-Mermaid > pre.mermaid");
    // do not load mermaidjs if not needed
    if (!diagrams.length) {
      return;
    }
    const mermaid = (await import("https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.6.0/mermaid.esm.min.mjs")).default;
    const parser = new DOMParser();

    mermaid.initialize({
      maxTextSize: 100000,
      startOnLoad: false,
      fontFamily: window
        .getComputedStyle(document.body)
        .getPropertyValue("--jp-ui-font-family"),
      theme: document.querySelector("body[data-jp-theme-light='true']")
        ? "default"
        : "dark",
    });

    let _nextMermaidId = 0;

    function makeMermaidImage(svg) {
      const img = document.createElement("img");
      const doc = parser.parseFromString(svg, "image/svg+xml");
      const svgEl = doc.querySelector("svg");
      const { maxWidth } = svgEl?.style || {};
      const firstTitle = doc.querySelector("title");
      const firstDesc = doc.querySelector("desc");

      img.setAttribute("src", `data:image/svg+xml,${encodeURIComponent(svg)}`);
      if (maxWidth) {
        img.width = parseInt(maxWidth);
      }
      if (firstTitle) {
        img.setAttribute("alt", firstTitle.textContent);
      }
      if (firstDesc) {
        const caption = document.createElement("figcaption");
        caption.className = "sr-only";
        caption.textContent = firstDesc.textContent;
        return [img, caption];
      }
      return [img];
    }

    async function makeMermaidError(text) {
      let errorMessage = "";
      try {
        await mermaid.parse(text);
      } catch (err) {
        errorMessage = `${err}`;
      }

      const result = document.createElement("details");
      result.className = 'jp-RenderedMermaid-Details';
      const summary = document.createElement("summary");
      summary.className = 'jp-RenderedMermaid-Summary';
      const pre = document.createElement("pre");
      const code = document.createElement("code");
      code.innerText = text;
      pre.appendChild(code);
      summary.appendChild(pre);
      result.appendChild(summary);

      const warning = document.createElement("pre");
      warning.innerText = errorMessage;
      result.appendChild(warning);
      return [result];
    }

    async function renderOneMarmaid(src) {
      const id = `jp-mermaid-${_nextMermaidId++}`;
      const parent = src.parentNode;
      let raw = src.textContent.trim();
      const el = document.createElement("div");
      el.style.visibility = "hidden";
      document.body.appendChild(el);
      let results = null;
      let output = null;
      try {
        const { svg } = await mermaid.render(id, raw, el);
        results = makeMermaidImage(svg);
        output = document.createElement("figure");
        results.map(output.appendChild, output);
      } catch (err) {
        parent.classList.add("jp-mod-warning");
        results = await makeMermaidError(raw);
        output = results[0];
      } finally {
        el.remove();
      }
      parent.classList.add("jp-RenderedMermaid");
      parent.appendChild(output);
    }

    void Promise.all([...diagrams].map(renderOneMarmaid));
  });
</script>
<style>
  .jp-Mermaid:not(.jp-RenderedMermaid) {
    display: none;
  }

  .jp-RenderedMermaid {
    overflow: auto;
    display: flex;
  }

  .jp-RenderedMermaid.jp-mod-warning {
    width: auto;
    padding: 0.5em;
    margin-top: 0.5em;
    border: var(--jp-border-width) solid var(--jp-warn-color2);
    border-radius: var(--jp-border-radius);
    color: var(--jp-ui-font-color1);
    font-size: var(--jp-ui-font-size1);
    white-space: pre-wrap;
    word-wrap: break-word;
  }

  .jp-RenderedMermaid figure {
    margin: 0;
    overflow: auto;
    max-width: 100%;
  }

  .jp-RenderedMermaid img {
    max-width: 100%;
  }

  .jp-RenderedMermaid-Details > pre {
    margin-top: 1em;
  }

  .jp-RenderedMermaid-Summary {
    color: var(--jp-warn-color2);
  }

  .jp-RenderedMermaid:not(.jp-mod-warning) pre {
    display: none;
  }

  .jp-RenderedMermaid-Summary > pre {
    display: inline-block;
    white-space: normal;
  }
</style>
<!-- End of mermaid configuration --></head>
<body class="jp-Notebook" data-jp-theme-light="true" data-jp-theme-name="JupyterLab Light">
<main>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=38d056bb-061f-460c-a2eb-7226fe89027f">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="Banking-Binary-Classification-Problem">Banking Binary Classification Problem<a class="anchor-link" href="#Banking-Binary-Classification-Problem">¶</a></h2>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=fa5b55c3-7fb0-4f80-ae6b-48620875032a">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Problem">Problem<a class="anchor-link" href="#Problem">¶</a></h3><p>We would like to predict if given a bank client, are they likely to make a term deposit or not at the bank.</p>
<p>Dataset: bankingInfo.csv
     - if output == yes, then the client has made a term deposit at this bank, else they have not.</p>
<p>Project was built using the bc_ml_breast_cancer_pred.ipynb and the deep_learning.ipynb files provided by Dr. Sambriddhi Mainali along with the <em>A hands-on introduction to feed-forward neural networks using Tensorflow and Keras</em> Github Repository by Dr. Badri Adhikari found <a href="https://badriadhikari.github.io/AI-2022spring/NN-using-TF.html">here.</a></p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=15279ce2-13b8-44a1-bc30-44877a89e140">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Phase-2:-Build-a-Model-to-Overfit-the-Entire-Dataset">Phase 2: Build a Model to Overfit the Entire Dataset<a class="anchor-link" href="#Phase-2:-Build-a-Model-to-Overfit-the-Entire-Dataset">¶</a></h3><hr/>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=020f6e3b-943f-4513-9f06-cbc2333260e1">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [23]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="o">%</span><span class="k">matplotlib</span> inline

<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">accuracy_score</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">precision_score</span><span class="p">,</span> <span class="n">recall_score</span><span class="p">,</span> <span class="n">f1_score</span>
<span class="kn">import</span> <span class="nn">keras</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> 
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.callbacks</span> <span class="kn">import</span> <span class="n">EarlyStopping</span><span class="p">,</span> <span class="n">ModelCheckpoint</span>
<span class="kn">import</span> <span class="nn">os</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=68479ec1-9d39-4d9e-8c9d-b5499f75c812">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [24]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1">#low_memory=False used to remove the warning for mixed data types in each columns</span>
<span class="n">banking_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'./cleaned_banking_data.csv'</span><span class="p">,</span> <span class="n">low_memory</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">banking_data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[24]:</div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html" tabindex="0">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>age</th>
<th>education</th>
<th>housing</th>
<th>loan</th>
<th>campaign</th>
<th>pdays</th>
<th>previous</th>
<th>poutcome</th>
<th>emp.var.rate</th>
<th>cons.price.idx</th>
<th>...</th>
<th>entrepreneur</th>
<th>housemaid</th>
<th>management</th>
<th>retired</th>
<th>self-employed</th>
<th>services</th>
<th>student</th>
<th>technician</th>
<th>unemployed</th>
<th>output</th>
</tr>
</thead>
<tbody>
<tr>
<th>0</th>
<td>36</td>
<td>2</td>
<td>0</td>
<td>0</td>
<td>9</td>
<td>999</td>
<td>0</td>
<td>0</td>
<td>1.4</td>
<td>93.444</td>
<td>...</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<th>1</th>
<td>43</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>2</td>
<td>999</td>
<td>0</td>
<td>0</td>
<td>1.1</td>
<td>93.994</td>
<td>...</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<th>2</th>
<td>27</td>
<td>4</td>
<td>1</td>
<td>0</td>
<td>3</td>
<td>999</td>
<td>0</td>
<td>0</td>
<td>1.4</td>
<td>93.918</td>
<td>...</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<th>3</th>
<td>28</td>
<td>3</td>
<td>1</td>
<td>0</td>
<td>3</td>
<td>999</td>
<td>0</td>
<td>0</td>
<td>1.1</td>
<td>93.994</td>
<td>...</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<th>4</th>
<td>25</td>
<td>4</td>
<td>1</td>
<td>0</td>
<td>5</td>
<td>999</td>
<td>0</td>
<td>0</td>
<td>1.4</td>
<td>93.918</td>
<td>...</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>5 rows × 27 columns</p>
</div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=315a683f-126a-402f-a2c8-0d65b4344f91">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [25]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">original_dataset</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[25]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>(8516, 28)</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=5fa76bcc-5914-471f-a9ae-42f17f61bd12">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="2.1-Normalize-the-Data">2.1 Normalize the Data<a class="anchor-link" href="#2.1-Normalize-the-Data">¶</a></h4>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=3b713db9-b5df-47bd-bbe8-c95b564176c5">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [28]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Shuffle dataframe in-place and reset the index</span>
<span class="n">banking_data</span> <span class="o">=</span> <span class="n">banking_data</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">frac</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">banking_data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[28]:</div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html" tabindex="0">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>age</th>
<th>education</th>
<th>housing</th>
<th>loan</th>
<th>campaign</th>
<th>pdays</th>
<th>previous</th>
<th>poutcome</th>
<th>emp.var.rate</th>
<th>cons.price.idx</th>
<th>...</th>
<th>entrepreneur</th>
<th>housemaid</th>
<th>management</th>
<th>retired</th>
<th>self-employed</th>
<th>services</th>
<th>student</th>
<th>technician</th>
<th>unemployed</th>
<th>output</th>
</tr>
</thead>
<tbody>
<tr>
<th>472</th>
<td>59</td>
<td>4</td>
<td>0</td>
<td>0</td>
<td>2</td>
<td>999</td>
<td>0</td>
<td>0</td>
<td>1.4</td>
<td>93.918</td>
<td>...</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<th>7489</th>
<td>44</td>
<td>4</td>
<td>1</td>
<td>0</td>
<td>2</td>
<td>6</td>
<td>1</td>
<td>1</td>
<td>-3.4</td>
<td>92.649</td>
<td>...</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<th>7481</th>
<td>48</td>
<td>6</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>3</td>
<td>1</td>
<td>1</td>
<td>-3.4</td>
<td>92.649</td>
<td>...</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<th>4677</th>
<td>37</td>
<td>4</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>999</td>
<td>0</td>
<td>0</td>
<td>1.4</td>
<td>93.918</td>
<td>...</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<th>7017</th>
<td>74</td>
<td>6</td>
<td>1</td>
<td>1</td>
<td>3</td>
<td>999</td>
<td>0</td>
<td>0</td>
<td>-2.9</td>
<td>92.201</td>
<td>...</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>5 rows × 27 columns</p>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=82a075a7-d861-4088-be12-4c2b8c31518c">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="2.2-Build-a-Model-with-1-Neuron">2.2 Build a Model with 1 Neuron<a class="anchor-link" href="#2.2-Build-a-Model-with-1-Neuron">¶</a></h4>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=fc80df28-a886-4afd-8c66-c3e609826e93">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [43]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">banking_data</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">'output'</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="s1">'columns'</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">banking_data</span><span class="p">[</span><span class="s1">'output'</span><span class="p">]</span>

<span class="c1"># Normalization of the data </span>
<span class="nb">min</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> 
<span class="nb">max</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> 
<span class="n">X</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="nb">min</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="nb">max</span> <span class="o">-</span> <span class="nb">min</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>3570    0
1245    0
4458    1
4600    1
894     0
       ..
2310    0
3969    0
5215    1
7200    1
3333    0
Name: output, Length: 8516, dtype: int64
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=0b1ca9b0-f24b-4450-9f6e-20e53d643063">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [48]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">myadam</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

<span class="c1"># constructing a single neuron model</span>
<span class="n">single_neuron</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">single_neuron</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'sigmoid'</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">single_neuron</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Model: "sequential_10"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense_9 (Dense)             (None, 1)                 27        
                                                                 
=================================================================
Total params: 27 (108.00 Byte)
Trainable params: 27 (108.00 Byte)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
None
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=00e9fb84-7b2e-4f59-a51f-d67ada0bbfa9">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [50]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1">#Check for cycles in the Sequential Model</span>
<span class="n">single_neuron</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">'binary_crossentropy'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">myadam</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'accuracy'</span><span class="p">])</span>
<span class="n">callback_a</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">filepath</span> <span class="o">=</span> <span class="s1">'single_model.hdf5'</span><span class="p">,</span> <span class="n">monitor</span><span class="o">=</span><span class="s1">'val_loss'</span><span class="p">,</span> <span class="n">save_best_only</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">save_weights_only</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">verbose</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">callback_b</span> <span class="o">=</span> <span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s1">'val_loss'</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">'min'</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">history_single</span> <span class="o">=</span> <span class="n">single_neuron</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">callback_a</span><span class="p">,</span> <span class="n">callback_b</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.7636 - accuracy: 0.4660
Epoch 1: val_loss improved from inf to 0.66898, saving model to single_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.7624 - accuracy: 0.4677 - val_loss: 0.6690 - val_accuracy: 0.5969
Epoch 2/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.6339 - accuracy: 0.6620
Epoch 2: val_loss improved from 0.66898 to 0.60968, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.6334 - accuracy: 0.6628 - val_loss: 0.6097 - val_accuracy: 0.6896
Epoch 3/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.5959 - accuracy: 0.7072
Epoch 3: val_loss improved from 0.60968 to 0.58593, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5955 - accuracy: 0.7069 - val_loss: 0.5859 - val_accuracy: 0.7112
Epoch 4/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.5813 - accuracy: 0.7131
Epoch 4: val_loss improved from 0.58593 to 0.57716, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5815 - accuracy: 0.7132 - val_loss: 0.5772 - val_accuracy: 0.7179
Epoch 5/150
852/852 [==============================] - ETA: 0s - loss: 0.5756 - accuracy: 0.7159
Epoch 5: val_loss improved from 0.57716 to 0.57322, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5756 - accuracy: 0.7159 - val_loss: 0.5732 - val_accuracy: 0.7186
Epoch 6/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.5722 - accuracy: 0.7170
Epoch 6: val_loss improved from 0.57322 to 0.57088, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5725 - accuracy: 0.7175 - val_loss: 0.5709 - val_accuracy: 0.7164
Epoch 7/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.5708 - accuracy: 0.7159
Epoch 7: val_loss improved from 0.57088 to 0.57014, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5708 - accuracy: 0.7157 - val_loss: 0.5701 - val_accuracy: 0.7195
Epoch 8/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.5679 - accuracy: 0.7190
Epoch 8: val_loss improved from 0.57014 to 0.56853, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5697 - accuracy: 0.7168 - val_loss: 0.5685 - val_accuracy: 0.7162
Epoch 9/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.5689 - accuracy: 0.7159
Epoch 9: val_loss improved from 0.56853 to 0.56851, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5687 - accuracy: 0.7161 - val_loss: 0.5685 - val_accuracy: 0.7118
Epoch 10/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.5676 - accuracy: 0.7153
Epoch 10: val_loss improved from 0.56851 to 0.56746, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5680 - accuracy: 0.7154 - val_loss: 0.5675 - val_accuracy: 0.7175
Epoch 11/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.5681 - accuracy: 0.7145
Epoch 11: val_loss improved from 0.56746 to 0.56657, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5675 - accuracy: 0.7152 - val_loss: 0.5666 - val_accuracy: 0.7162
Epoch 12/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.5660 - accuracy: 0.7155
Epoch 12: val_loss improved from 0.56657 to 0.56644, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5669 - accuracy: 0.7151 - val_loss: 0.5664 - val_accuracy: 0.7178
Epoch 13/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.5664 - accuracy: 0.7148
Epoch 13: val_loss improved from 0.56644 to 0.56586, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5664 - accuracy: 0.7157 - val_loss: 0.5659 - val_accuracy: 0.7134
Epoch 14/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.5646 - accuracy: 0.7164
Epoch 14: val_loss improved from 0.56586 to 0.56526, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5662 - accuracy: 0.7155 - val_loss: 0.5653 - val_accuracy: 0.7152
Epoch 15/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5656 - accuracy: 0.7160
Epoch 15: val_loss improved from 0.56526 to 0.56491, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5657 - accuracy: 0.7159 - val_loss: 0.5649 - val_accuracy: 0.7170
Epoch 16/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.5650 - accuracy: 0.7171
Epoch 16: val_loss improved from 0.56491 to 0.56455, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5653 - accuracy: 0.7163 - val_loss: 0.5646 - val_accuracy: 0.7162
Epoch 17/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.5646 - accuracy: 0.7176
Epoch 17: val_loss improved from 0.56455 to 0.56448, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5651 - accuracy: 0.7172 - val_loss: 0.5645 - val_accuracy: 0.7152
Epoch 18/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.5650 - accuracy: 0.7158
Epoch 18: val_loss improved from 0.56448 to 0.56396, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5647 - accuracy: 0.7163 - val_loss: 0.5640 - val_accuracy: 0.7178
Epoch 19/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.5650 - accuracy: 0.7174
Epoch 19: val_loss improved from 0.56396 to 0.56372, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5643 - accuracy: 0.7179 - val_loss: 0.5637 - val_accuracy: 0.7195
Epoch 20/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.5644 - accuracy: 0.7170
Epoch 20: val_loss improved from 0.56372 to 0.56361, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5641 - accuracy: 0.7177 - val_loss: 0.5636 - val_accuracy: 0.7208
Epoch 21/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.5646 - accuracy: 0.7198
Epoch 21: val_loss improved from 0.56361 to 0.56325, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5640 - accuracy: 0.7202 - val_loss: 0.5633 - val_accuracy: 0.7191
Epoch 22/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.5637 - accuracy: 0.7188
Epoch 22: val_loss did not improve from 0.56325
852/852 [==============================] - 2s 3ms/step - loss: 0.5637 - accuracy: 0.7188 - val_loss: 0.5633 - val_accuracy: 0.7181
Epoch 23/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.5634 - accuracy: 0.7185
Epoch 23: val_loss did not improve from 0.56325
852/852 [==============================] - 2s 3ms/step - loss: 0.5634 - accuracy: 0.7183 - val_loss: 0.5635 - val_accuracy: 0.7157
Epoch 24/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.5638 - accuracy: 0.7180
Epoch 24: val_loss improved from 0.56325 to 0.56283, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5635 - accuracy: 0.7184 - val_loss: 0.5628 - val_accuracy: 0.7189
Epoch 25/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.5634 - accuracy: 0.7205
Epoch 25: val_loss improved from 0.56283 to 0.56260, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5631 - accuracy: 0.7206 - val_loss: 0.5626 - val_accuracy: 0.7205
Epoch 26/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.5628 - accuracy: 0.7191
Epoch 26: val_loss did not improve from 0.56260
852/852 [==============================] - 2s 3ms/step - loss: 0.5629 - accuracy: 0.7196 - val_loss: 0.5627 - val_accuracy: 0.7224
Epoch 27/150
819/852 [===========================&gt;..] - ETA: 0s - loss: 0.5615 - accuracy: 0.7217
Epoch 27: val_loss improved from 0.56260 to 0.56229, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5630 - accuracy: 0.7204 - val_loss: 0.5623 - val_accuracy: 0.7205
Epoch 28/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.5636 - accuracy: 0.7176
Epoch 28: val_loss improved from 0.56229 to 0.56225, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5628 - accuracy: 0.7190 - val_loss: 0.5622 - val_accuracy: 0.7215
Epoch 29/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.5620 - accuracy: 0.7222
Epoch 29: val_loss improved from 0.56225 to 0.56204, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5627 - accuracy: 0.7215 - val_loss: 0.5620 - val_accuracy: 0.7203
Epoch 30/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.5619 - accuracy: 0.7209
Epoch 30: val_loss did not improve from 0.56204
852/852 [==============================] - 2s 3ms/step - loss: 0.5624 - accuracy: 0.7202 - val_loss: 0.5637 - val_accuracy: 0.7161
Epoch 31/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.5638 - accuracy: 0.7184
Epoch 31: val_loss improved from 0.56204 to 0.56189, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5626 - accuracy: 0.7197 - val_loss: 0.5619 - val_accuracy: 0.7216
Epoch 32/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.5620 - accuracy: 0.7209
Epoch 32: val_loss improved from 0.56189 to 0.56182, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5624 - accuracy: 0.7209 - val_loss: 0.5618 - val_accuracy: 0.7205
Epoch 33/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.5627 - accuracy: 0.7201
Epoch 33: val_loss improved from 0.56182 to 0.56169, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5623 - accuracy: 0.7201 - val_loss: 0.5617 - val_accuracy: 0.7212
Epoch 34/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.5619 - accuracy: 0.7213
Epoch 34: val_loss improved from 0.56169 to 0.56166, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5621 - accuracy: 0.7208 - val_loss: 0.5617 - val_accuracy: 0.7218
Epoch 35/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.5622 - accuracy: 0.7212
Epoch 35: val_loss did not improve from 0.56166
852/852 [==============================] - 2s 3ms/step - loss: 0.5619 - accuracy: 0.7215 - val_loss: 0.5622 - val_accuracy: 0.7226
Epoch 36/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.5620 - accuracy: 0.7194
Epoch 36: val_loss did not improve from 0.56166
852/852 [==============================] - 2s 3ms/step - loss: 0.5616 - accuracy: 0.7202 - val_loss: 0.5625 - val_accuracy: 0.7221
Epoch 37/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.5624 - accuracy: 0.7200
Epoch 37: val_loss improved from 0.56166 to 0.56137, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5621 - accuracy: 0.7203 - val_loss: 0.5614 - val_accuracy: 0.7208
Epoch 38/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.5603 - accuracy: 0.7222
Epoch 38: val_loss did not improve from 0.56137
852/852 [==============================] - 2s 3ms/step - loss: 0.5621 - accuracy: 0.7202 - val_loss: 0.5614 - val_accuracy: 0.7216
Epoch 39/150
820/852 [===========================&gt;..] - ETA: 0s - loss: 0.5633 - accuracy: 0.7199
Epoch 39: val_loss improved from 0.56137 to 0.56128, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5618 - accuracy: 0.7211 - val_loss: 0.5613 - val_accuracy: 0.7211
Epoch 40/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.5612 - accuracy: 0.7215
Epoch 40: val_loss did not improve from 0.56128
852/852 [==============================] - 2s 3ms/step - loss: 0.5616 - accuracy: 0.7213 - val_loss: 0.5615 - val_accuracy: 0.7202
Epoch 41/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.5634 - accuracy: 0.7200
Epoch 41: val_loss improved from 0.56128 to 0.56119, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5617 - accuracy: 0.7203 - val_loss: 0.5612 - val_accuracy: 0.7213
Epoch 42/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.5634 - accuracy: 0.7189
Epoch 42: val_loss improved from 0.56119 to 0.56114, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5618 - accuracy: 0.7209 - val_loss: 0.5611 - val_accuracy: 0.7213
Epoch 43/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.5615 - accuracy: 0.7221
Epoch 43: val_loss improved from 0.56114 to 0.56107, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5617 - accuracy: 0.7221 - val_loss: 0.5611 - val_accuracy: 0.7213
Epoch 44/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.5617 - accuracy: 0.7206
Epoch 44: val_loss did not improve from 0.56107
852/852 [==============================] - 2s 3ms/step - loss: 0.5616 - accuracy: 0.7206 - val_loss: 0.5612 - val_accuracy: 0.7222
Epoch 45/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5612 - accuracy: 0.7219
Epoch 45: val_loss did not improve from 0.56107
852/852 [==============================] - 2s 3ms/step - loss: 0.5615 - accuracy: 0.7216 - val_loss: 0.5617 - val_accuracy: 0.7199
Epoch 46/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.5624 - accuracy: 0.7215
Epoch 46: val_loss improved from 0.56107 to 0.56096, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5617 - accuracy: 0.7216 - val_loss: 0.5610 - val_accuracy: 0.7217
Epoch 47/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5614 - accuracy: 0.7228
Epoch 47: val_loss improved from 0.56096 to 0.56093, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5615 - accuracy: 0.7225 - val_loss: 0.5609 - val_accuracy: 0.7210
Epoch 48/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.5634 - accuracy: 0.7181
Epoch 48: val_loss improved from 0.56093 to 0.56092, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5616 - accuracy: 0.7205 - val_loss: 0.5609 - val_accuracy: 0.7210
Epoch 49/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.5617 - accuracy: 0.7215
Epoch 49: val_loss did not improve from 0.56092
852/852 [==============================] - 2s 3ms/step - loss: 0.5615 - accuracy: 0.7215 - val_loss: 0.5612 - val_accuracy: 0.7208
Epoch 50/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.5609 - accuracy: 0.7216
Epoch 50: val_loss did not improve from 0.56092
852/852 [==============================] - 2s 3ms/step - loss: 0.5614 - accuracy: 0.7210 - val_loss: 0.5612 - val_accuracy: 0.7228
Epoch 51/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.5616 - accuracy: 0.7213
Epoch 51: val_loss did not improve from 0.56092
852/852 [==============================] - 2s 3ms/step - loss: 0.5613 - accuracy: 0.7209 - val_loss: 0.5613 - val_accuracy: 0.7199
Epoch 52/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5620 - accuracy: 0.7187
Epoch 52: val_loss improved from 0.56092 to 0.56082, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5614 - accuracy: 0.7194 - val_loss: 0.5608 - val_accuracy: 0.7218
Epoch 53/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.5613 - accuracy: 0.7216
Epoch 53: val_loss did not improve from 0.56082
852/852 [==============================] - 2s 3ms/step - loss: 0.5613 - accuracy: 0.7216 - val_loss: 0.5609 - val_accuracy: 0.7222
Epoch 54/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.5619 - accuracy: 0.7219
Epoch 54: val_loss did not improve from 0.56082
852/852 [==============================] - 2s 3ms/step - loss: 0.5615 - accuracy: 0.7219 - val_loss: 0.5609 - val_accuracy: 0.7225
Epoch 55/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5616 - accuracy: 0.7207
Epoch 55: val_loss did not improve from 0.56082
852/852 [==============================] - 2s 3ms/step - loss: 0.5614 - accuracy: 0.7210 - val_loss: 0.5609 - val_accuracy: 0.7224
Epoch 56/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.5611 - accuracy: 0.7210
Epoch 56: val_loss improved from 0.56082 to 0.56074, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5614 - accuracy: 0.7208 - val_loss: 0.5607 - val_accuracy: 0.7213
Epoch 57/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.5613 - accuracy: 0.7214
Epoch 57: val_loss did not improve from 0.56074
852/852 [==============================] - 3s 4ms/step - loss: 0.5612 - accuracy: 0.7213 - val_loss: 0.5611 - val_accuracy: 0.7202
Epoch 58/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.5619 - accuracy: 0.7203
Epoch 58: val_loss did not improve from 0.56074
852/852 [==============================] - 3s 3ms/step - loss: 0.5613 - accuracy: 0.7208 - val_loss: 0.5610 - val_accuracy: 0.7204
Epoch 59/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5609 - accuracy: 0.7225
Epoch 59: val_loss improved from 0.56074 to 0.56068, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5613 - accuracy: 0.7219 - val_loss: 0.5607 - val_accuracy: 0.7216
Epoch 60/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.5596 - accuracy: 0.7241
Epoch 60: val_loss did not improve from 0.56068
852/852 [==============================] - 2s 3ms/step - loss: 0.5613 - accuracy: 0.7216 - val_loss: 0.5608 - val_accuracy: 0.7228
Epoch 61/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.5607 - accuracy: 0.7222
Epoch 61: val_loss did not improve from 0.56068
852/852 [==============================] - 2s 3ms/step - loss: 0.5611 - accuracy: 0.7217 - val_loss: 0.5613 - val_accuracy: 0.7228
Epoch 62/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.5614 - accuracy: 0.7219
Epoch 62: val_loss improved from 0.56068 to 0.56063, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5613 - accuracy: 0.7221 - val_loss: 0.5606 - val_accuracy: 0.7222
Epoch 63/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.5601 - accuracy: 0.7213
Epoch 63: val_loss improved from 0.56063 to 0.56062, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5612 - accuracy: 0.7206 - val_loss: 0.5606 - val_accuracy: 0.7223
Epoch 64/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.5617 - accuracy: 0.7219
Epoch 64: val_loss did not improve from 0.56062
852/852 [==============================] - 2s 3ms/step - loss: 0.5613 - accuracy: 0.7213 - val_loss: 0.5607 - val_accuracy: 0.7210
Epoch 65/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5611 - accuracy: 0.7204
Epoch 65: val_loss did not improve from 0.56062
852/852 [==============================] - 2s 3ms/step - loss: 0.5612 - accuracy: 0.7204 - val_loss: 0.5606 - val_accuracy: 0.7213
Epoch 66/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.5625 - accuracy: 0.7202
Epoch 66: val_loss improved from 0.56062 to 0.56058, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5612 - accuracy: 0.7215 - val_loss: 0.5606 - val_accuracy: 0.7226
Epoch 67/150
818/852 [===========================&gt;..] - ETA: 0s - loss: 0.5608 - accuracy: 0.7236
Epoch 67: val_loss did not improve from 0.56058
852/852 [==============================] - 2s 3ms/step - loss: 0.5612 - accuracy: 0.7230 - val_loss: 0.5606 - val_accuracy: 0.7228
Epoch 68/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.5617 - accuracy: 0.7209
Epoch 68: val_loss did not improve from 0.56058
852/852 [==============================] - 2s 3ms/step - loss: 0.5612 - accuracy: 0.7218 - val_loss: 0.5607 - val_accuracy: 0.7215
Epoch 69/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.5624 - accuracy: 0.7203
Epoch 69: val_loss improved from 0.56058 to 0.56055, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5612 - accuracy: 0.7215 - val_loss: 0.5606 - val_accuracy: 0.7218
Epoch 70/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.5602 - accuracy: 0.7222
Epoch 70: val_loss improved from 0.56055 to 0.56053, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5612 - accuracy: 0.7213 - val_loss: 0.5605 - val_accuracy: 0.7225
Epoch 71/150
852/852 [==============================] - ETA: 0s - loss: 0.5613 - accuracy: 0.7222
Epoch 71: val_loss did not improve from 0.56053
852/852 [==============================] - 2s 3ms/step - loss: 0.5613 - accuracy: 0.7222 - val_loss: 0.5607 - val_accuracy: 0.7206
Epoch 72/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.5603 - accuracy: 0.7218
Epoch 72: val_loss improved from 0.56053 to 0.56053, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5611 - accuracy: 0.7215 - val_loss: 0.5605 - val_accuracy: 0.7221
Epoch 73/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.5611 - accuracy: 0.7214
Epoch 73: val_loss did not improve from 0.56053
852/852 [==============================] - 2s 3ms/step - loss: 0.5611 - accuracy: 0.7215 - val_loss: 0.5605 - val_accuracy: 0.7226
Epoch 74/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.5608 - accuracy: 0.7227
Epoch 74: val_loss did not improve from 0.56053
852/852 [==============================] - 2s 3ms/step - loss: 0.5611 - accuracy: 0.7226 - val_loss: 0.5606 - val_accuracy: 0.7215
Epoch 75/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.5610 - accuracy: 0.7223
Epoch 75: val_loss improved from 0.56053 to 0.56049, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5611 - accuracy: 0.7223 - val_loss: 0.5605 - val_accuracy: 0.7230
Epoch 76/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.5606 - accuracy: 0.7234
Epoch 76: val_loss did not improve from 0.56049
852/852 [==============================] - 2s 3ms/step - loss: 0.5611 - accuracy: 0.7230 - val_loss: 0.5607 - val_accuracy: 0.7209
Epoch 77/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.5602 - accuracy: 0.7238
Epoch 77: val_loss did not improve from 0.56049
852/852 [==============================] - 2s 3ms/step - loss: 0.5611 - accuracy: 0.7231 - val_loss: 0.5605 - val_accuracy: 0.7213
Epoch 78/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.5613 - accuracy: 0.7205
Epoch 78: val_loss did not improve from 0.56049
852/852 [==============================] - 2s 3ms/step - loss: 0.5611 - accuracy: 0.7204 - val_loss: 0.5610 - val_accuracy: 0.7232
Epoch 79/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.5628 - accuracy: 0.7203
Epoch 79: val_loss did not improve from 0.56049
852/852 [==============================] - 2s 3ms/step - loss: 0.5611 - accuracy: 0.7222 - val_loss: 0.5610 - val_accuracy: 0.7232
Epoch 80/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.5608 - accuracy: 0.7229
Epoch 80: val_loss did not improve from 0.56049
852/852 [==============================] - 2s 3ms/step - loss: 0.5610 - accuracy: 0.7223 - val_loss: 0.5605 - val_accuracy: 0.7228
Epoch 81/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5621 - accuracy: 0.7225
Epoch 81: val_loss did not improve from 0.56049
852/852 [==============================] - 2s 3ms/step - loss: 0.5609 - accuracy: 0.7235 - val_loss: 0.5608 - val_accuracy: 0.7209
Epoch 82/150
852/852 [==============================] - ETA: 0s - loss: 0.5612 - accuracy: 0.7225
Epoch 82: val_loss improved from 0.56049 to 0.56046, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5612 - accuracy: 0.7225 - val_loss: 0.5605 - val_accuracy: 0.7232
Epoch 83/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.5620 - accuracy: 0.7200
Epoch 83: val_loss did not improve from 0.56046
852/852 [==============================] - 2s 3ms/step - loss: 0.5609 - accuracy: 0.7212 - val_loss: 0.5608 - val_accuracy: 0.7232
Epoch 84/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.5601 - accuracy: 0.7236
Epoch 84: val_loss did not improve from 0.56046
852/852 [==============================] - 2s 3ms/step - loss: 0.5610 - accuracy: 0.7224 - val_loss: 0.5605 - val_accuracy: 0.7221
Epoch 85/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.5610 - accuracy: 0.7216
Epoch 85: val_loss did not improve from 0.56046
852/852 [==============================] - 2s 3ms/step - loss: 0.5610 - accuracy: 0.7222 - val_loss: 0.5605 - val_accuracy: 0.7236
Epoch 86/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5613 - accuracy: 0.7211
Epoch 86: val_loss did not improve from 0.56046
852/852 [==============================] - 2s 3ms/step - loss: 0.5607 - accuracy: 0.7218 - val_loss: 0.5608 - val_accuracy: 0.7231
Epoch 87/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.5608 - accuracy: 0.7218
Epoch 87: val_loss improved from 0.56046 to 0.56041, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5611 - accuracy: 0.7218 - val_loss: 0.5604 - val_accuracy: 0.7229
Epoch 88/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.5610 - accuracy: 0.7216
Epoch 88: val_loss did not improve from 0.56041
852/852 [==============================] - 2s 3ms/step - loss: 0.5611 - accuracy: 0.7215 - val_loss: 0.5604 - val_accuracy: 0.7224
Epoch 89/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.5609 - accuracy: 0.7231
Epoch 89: val_loss improved from 0.56041 to 0.56039, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5610 - accuracy: 0.7230 - val_loss: 0.5604 - val_accuracy: 0.7232
Epoch 90/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.5593 - accuracy: 0.7238
Epoch 90: val_loss did not improve from 0.56039
852/852 [==============================] - 2s 3ms/step - loss: 0.5608 - accuracy: 0.7222 - val_loss: 0.5609 - val_accuracy: 0.7233
Epoch 91/150
852/852 [==============================] - ETA: 0s - loss: 0.5609 - accuracy: 0.7224
Epoch 91: val_loss did not improve from 0.56039
852/852 [==============================] - 2s 3ms/step - loss: 0.5609 - accuracy: 0.7224 - val_loss: 0.5606 - val_accuracy: 0.7210
Epoch 92/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.5603 - accuracy: 0.7234
Epoch 92: val_loss did not improve from 0.56039
852/852 [==============================] - 2s 3ms/step - loss: 0.5609 - accuracy: 0.7233 - val_loss: 0.5604 - val_accuracy: 0.7225
Epoch 93/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.5602 - accuracy: 0.7231
Epoch 93: val_loss did not improve from 0.56039
852/852 [==============================] - 2s 3ms/step - loss: 0.5609 - accuracy: 0.7223 - val_loss: 0.5604 - val_accuracy: 0.7230
Epoch 94/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.5609 - accuracy: 0.7228
Epoch 94: val_loss improved from 0.56039 to 0.56038, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5609 - accuracy: 0.7229 - val_loss: 0.5604 - val_accuracy: 0.7229
Epoch 95/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.5609 - accuracy: 0.7220
Epoch 95: val_loss did not improve from 0.56038
852/852 [==============================] - 2s 3ms/step - loss: 0.5610 - accuracy: 0.7216 - val_loss: 0.5604 - val_accuracy: 0.7233
Epoch 96/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.5617 - accuracy: 0.7227
Epoch 96: val_loss did not improve from 0.56038
852/852 [==============================] - 2s 3ms/step - loss: 0.5611 - accuracy: 0.7225 - val_loss: 0.5604 - val_accuracy: 0.7225
Epoch 97/150
819/852 [===========================&gt;..] - ETA: 0s - loss: 0.5603 - accuracy: 0.7217
Epoch 97: val_loss improved from 0.56038 to 0.56035, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5609 - accuracy: 0.7218 - val_loss: 0.5604 - val_accuracy: 0.7233
Epoch 98/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.5619 - accuracy: 0.7197
Epoch 98: val_loss did not improve from 0.56035
852/852 [==============================] - 2s 3ms/step - loss: 0.5607 - accuracy: 0.7209 - val_loss: 0.5610 - val_accuracy: 0.7237
Epoch 99/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.5617 - accuracy: 0.7219
Epoch 99: val_loss did not improve from 0.56035
852/852 [==============================] - 2s 3ms/step - loss: 0.5610 - accuracy: 0.7228 - val_loss: 0.5605 - val_accuracy: 0.7217
Epoch 100/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.5626 - accuracy: 0.7204
Epoch 100: val_loss did not improve from 0.56035
852/852 [==============================] - 2s 3ms/step - loss: 0.5607 - accuracy: 0.7219 - val_loss: 0.5610 - val_accuracy: 0.7237
Epoch 101/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.5608 - accuracy: 0.7222
Epoch 101: val_loss did not improve from 0.56035
852/852 [==============================] - 2s 3ms/step - loss: 0.5610 - accuracy: 0.7222 - val_loss: 0.5607 - val_accuracy: 0.7236
Epoch 102/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.5605 - accuracy: 0.7229
Epoch 102: val_loss did not improve from 0.56035
852/852 [==============================] - 2s 3ms/step - loss: 0.5609 - accuracy: 0.7225 - val_loss: 0.5604 - val_accuracy: 0.7223
Epoch 103/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.5610 - accuracy: 0.7229
Epoch 103: val_loss did not improve from 0.56035
852/852 [==============================] - 2s 3ms/step - loss: 0.5609 - accuracy: 0.7229 - val_loss: 0.5604 - val_accuracy: 0.7237
Epoch 104/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.5612 - accuracy: 0.7230
Epoch 104: val_loss improved from 0.56035 to 0.56032, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5610 - accuracy: 0.7229 - val_loss: 0.5603 - val_accuracy: 0.7226
Epoch 105/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5610 - accuracy: 0.7227
Epoch 105: val_loss did not improve from 0.56032
852/852 [==============================] - 2s 3ms/step - loss: 0.5608 - accuracy: 0.7229 - val_loss: 0.5608 - val_accuracy: 0.7237
Epoch 106/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.5618 - accuracy: 0.7220
Epoch 106: val_loss improved from 0.56032 to 0.56030, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5610 - accuracy: 0.7229 - val_loss: 0.5603 - val_accuracy: 0.7236
Epoch 107/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.5613 - accuracy: 0.7225
Epoch 107: val_loss improved from 0.56030 to 0.56030, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5609 - accuracy: 0.7231 - val_loss: 0.5603 - val_accuracy: 0.7232
Epoch 108/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.5598 - accuracy: 0.7242
Epoch 108: val_loss did not improve from 0.56030
852/852 [==============================] - 2s 3ms/step - loss: 0.5609 - accuracy: 0.7229 - val_loss: 0.5603 - val_accuracy: 0.7235
Epoch 109/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.5614 - accuracy: 0.7224
Epoch 109: val_loss did not improve from 0.56030
852/852 [==============================] - 2s 3ms/step - loss: 0.5610 - accuracy: 0.7228 - val_loss: 0.5605 - val_accuracy: 0.7236
Epoch 110/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.5600 - accuracy: 0.7229
Epoch 110: val_loss improved from 0.56030 to 0.56029, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5608 - accuracy: 0.7224 - val_loss: 0.5603 - val_accuracy: 0.7238
Epoch 111/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.5609 - accuracy: 0.7214
Epoch 111: val_loss did not improve from 0.56029
852/852 [==============================] - 2s 3ms/step - loss: 0.5608 - accuracy: 0.7213 - val_loss: 0.5603 - val_accuracy: 0.7228
Epoch 112/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.5605 - accuracy: 0.7240
Epoch 112: val_loss did not improve from 0.56029
852/852 [==============================] - 2s 3ms/step - loss: 0.5607 - accuracy: 0.7238 - val_loss: 0.5603 - val_accuracy: 0.7238
Epoch 113/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.5608 - accuracy: 0.7235
Epoch 113: val_loss improved from 0.56029 to 0.56027, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5609 - accuracy: 0.7231 - val_loss: 0.5603 - val_accuracy: 0.7235
Epoch 114/150
852/852 [==============================] - ETA: 0s - loss: 0.5609 - accuracy: 0.7222
Epoch 114: val_loss did not improve from 0.56027
852/852 [==============================] - 2s 3ms/step - loss: 0.5609 - accuracy: 0.7222 - val_loss: 0.5604 - val_accuracy: 0.7238
Epoch 115/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.5616 - accuracy: 0.7218
Epoch 115: val_loss improved from 0.56027 to 0.56027, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5608 - accuracy: 0.7229 - val_loss: 0.5603 - val_accuracy: 0.7233
Epoch 116/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.5611 - accuracy: 0.7228
Epoch 116: val_loss did not improve from 0.56027
852/852 [==============================] - 2s 3ms/step - loss: 0.5610 - accuracy: 0.7228 - val_loss: 0.5603 - val_accuracy: 0.7238
Epoch 117/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.5621 - accuracy: 0.7214
Epoch 117: val_loss improved from 0.56027 to 0.56026, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5607 - accuracy: 0.7224 - val_loss: 0.5603 - val_accuracy: 0.7230
Epoch 118/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.5608 - accuracy: 0.7239
Epoch 118: val_loss improved from 0.56026 to 0.56025, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5608 - accuracy: 0.7236 - val_loss: 0.5603 - val_accuracy: 0.7238
Epoch 119/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.5608 - accuracy: 0.7238
Epoch 119: val_loss did not improve from 0.56025
852/852 [==============================] - 2s 3ms/step - loss: 0.5608 - accuracy: 0.7239 - val_loss: 0.5603 - val_accuracy: 0.7224
Epoch 120/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.5608 - accuracy: 0.7234
Epoch 120: val_loss improved from 0.56025 to 0.56024, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5610 - accuracy: 0.7232 - val_loss: 0.5602 - val_accuracy: 0.7232
Epoch 121/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.5615 - accuracy: 0.7220
Epoch 121: val_loss did not improve from 0.56024
852/852 [==============================] - 2s 3ms/step - loss: 0.5608 - accuracy: 0.7228 - val_loss: 0.5604 - val_accuracy: 0.7240
Epoch 122/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.5597 - accuracy: 0.7249
Epoch 122: val_loss did not improve from 0.56024
852/852 [==============================] - 2s 3ms/step - loss: 0.5608 - accuracy: 0.7231 - val_loss: 0.5604 - val_accuracy: 0.7239
Epoch 123/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.5602 - accuracy: 0.7226
Epoch 123: val_loss did not improve from 0.56024
852/852 [==============================] - 2s 3ms/step - loss: 0.5608 - accuracy: 0.7224 - val_loss: 0.5607 - val_accuracy: 0.7237
Epoch 124/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.5614 - accuracy: 0.7221
Epoch 124: val_loss improved from 0.56024 to 0.56023, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5609 - accuracy: 0.7231 - val_loss: 0.5602 - val_accuracy: 0.7238
Epoch 125/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.5620 - accuracy: 0.7222
Epoch 125: val_loss did not improve from 0.56023
852/852 [==============================] - 2s 3ms/step - loss: 0.5609 - accuracy: 0.7228 - val_loss: 0.5602 - val_accuracy: 0.7238
Epoch 126/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.5604 - accuracy: 0.7251
Epoch 126: val_loss did not improve from 0.56023
852/852 [==============================] - 2s 3ms/step - loss: 0.5608 - accuracy: 0.7239 - val_loss: 0.5604 - val_accuracy: 0.7218
Epoch 127/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.5608 - accuracy: 0.7229
Epoch 127: val_loss did not improve from 0.56023
852/852 [==============================] - 2s 3ms/step - loss: 0.5607 - accuracy: 0.7229 - val_loss: 0.5608 - val_accuracy: 0.7239
Epoch 128/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.5610 - accuracy: 0.7242
Epoch 128: val_loss did not improve from 0.56023
852/852 [==============================] - 2s 3ms/step - loss: 0.5609 - accuracy: 0.7238 - val_loss: 0.5602 - val_accuracy: 0.7239
Epoch 129/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.5612 - accuracy: 0.7230
Epoch 129: val_loss did not improve from 0.56023
852/852 [==============================] - 2s 3ms/step - loss: 0.5608 - accuracy: 0.7229 - val_loss: 0.5603 - val_accuracy: 0.7233
Epoch 130/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.5617 - accuracy: 0.7208
Epoch 130: val_loss did not improve from 0.56023
852/852 [==============================] - 2s 3ms/step - loss: 0.5609 - accuracy: 0.7222 - val_loss: 0.5604 - val_accuracy: 0.7238
Epoch 131/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.5618 - accuracy: 0.7225
Epoch 131: val_loss did not improve from 0.56023
852/852 [==============================] - 2s 3ms/step - loss: 0.5609 - accuracy: 0.7228 - val_loss: 0.5602 - val_accuracy: 0.7225
Epoch 132/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.5600 - accuracy: 0.7240
Epoch 132: val_loss did not improve from 0.56023
852/852 [==============================] - 2s 3ms/step - loss: 0.5607 - accuracy: 0.7233 - val_loss: 0.5608 - val_accuracy: 0.7212
Epoch 133/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.5606 - accuracy: 0.7223
Epoch 133: val_loss did not improve from 0.56023
852/852 [==============================] - 2s 3ms/step - loss: 0.5609 - accuracy: 0.7222 - val_loss: 0.5603 - val_accuracy: 0.7240
Epoch 134/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.5597 - accuracy: 0.7239
Epoch 134: val_loss did not improve from 0.56023
852/852 [==============================] - 2s 3ms/step - loss: 0.5609 - accuracy: 0.7229 - val_loss: 0.5602 - val_accuracy: 0.7242
Epoch 135/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.5608 - accuracy: 0.7230
Epoch 135: val_loss did not improve from 0.56023
852/852 [==============================] - 2s 3ms/step - loss: 0.5609 - accuracy: 0.7237 - val_loss: 0.5605 - val_accuracy: 0.7219
Epoch 136/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.5605 - accuracy: 0.7235
Epoch 136: val_loss did not improve from 0.56023
852/852 [==============================] - 2s 3ms/step - loss: 0.5609 - accuracy: 0.7229 - val_loss: 0.5603 - val_accuracy: 0.7226
Epoch 137/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.5600 - accuracy: 0.7231
Epoch 137: val_loss did not improve from 0.56023
852/852 [==============================] - 3s 3ms/step - loss: 0.5609 - accuracy: 0.7222 - val_loss: 0.5603 - val_accuracy: 0.7246
Epoch 138/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.5613 - accuracy: 0.7252
Epoch 138: val_loss did not improve from 0.56023
852/852 [==============================] - 3s 3ms/step - loss: 0.5608 - accuracy: 0.7253 - val_loss: 0.5603 - val_accuracy: 0.7240
Epoch 139/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.5599 - accuracy: 0.7225
Epoch 139: val_loss did not improve from 0.56023
852/852 [==============================] - 3s 3ms/step - loss: 0.5608 - accuracy: 0.7224 - val_loss: 0.5603 - val_accuracy: 0.7242
Epoch 140/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.5608 - accuracy: 0.7235
Epoch 140: val_loss improved from 0.56023 to 0.56020, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5608 - accuracy: 0.7230 - val_loss: 0.5602 - val_accuracy: 0.7243
Epoch 141/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.5605 - accuracy: 0.7228
Epoch 141: val_loss improved from 0.56020 to 0.56018, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5608 - accuracy: 0.7225 - val_loss: 0.5602 - val_accuracy: 0.7236
Epoch 142/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.5611 - accuracy: 0.7220
Epoch 142: val_loss improved from 0.56018 to 0.56017, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5608 - accuracy: 0.7225 - val_loss: 0.5602 - val_accuracy: 0.7245
Epoch 143/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.5611 - accuracy: 0.7231
Epoch 143: val_loss did not improve from 0.56017
852/852 [==============================] - 2s 3ms/step - loss: 0.5608 - accuracy: 0.7236 - val_loss: 0.5604 - val_accuracy: 0.7244
Epoch 144/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.5608 - accuracy: 0.7246
Epoch 144: val_loss did not improve from 0.56017
852/852 [==============================] - 2s 3ms/step - loss: 0.5607 - accuracy: 0.7240 - val_loss: 0.5603 - val_accuracy: 0.7243
Epoch 145/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.5602 - accuracy: 0.7232
Epoch 145: val_loss did not improve from 0.56017
852/852 [==============================] - 2s 3ms/step - loss: 0.5608 - accuracy: 0.7225 - val_loss: 0.5602 - val_accuracy: 0.7242
Epoch 146/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.5614 - accuracy: 0.7233
Epoch 146: val_loss did not improve from 0.56017
852/852 [==============================] - 2s 3ms/step - loss: 0.5608 - accuracy: 0.7229 - val_loss: 0.5602 - val_accuracy: 0.7246
Epoch 147/150
820/852 [===========================&gt;..] - ETA: 0s - loss: 0.5593 - accuracy: 0.7259
Epoch 147: val_loss did not improve from 0.56017
852/852 [==============================] - 2s 3ms/step - loss: 0.5607 - accuracy: 0.7243 - val_loss: 0.5603 - val_accuracy: 0.7228
Epoch 148/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.5606 - accuracy: 0.7231
Epoch 148: val_loss improved from 0.56017 to 0.56016, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5609 - accuracy: 0.7226 - val_loss: 0.5602 - val_accuracy: 0.7232
Epoch 149/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.5603 - accuracy: 0.7242
Epoch 149: val_loss improved from 0.56016 to 0.56015, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5608 - accuracy: 0.7237 - val_loss: 0.5601 - val_accuracy: 0.7243
Epoch 150/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.5607 - accuracy: 0.7225
Epoch 150: val_loss did not improve from 0.56015
852/852 [==============================] - 2s 3ms/step - loss: 0.5606 - accuracy: 0.7226 - val_loss: 0.5606 - val_accuracy: 0.7243
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=e3384d89-d99e-45f6-bc5f-9e19abf2aa4d">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [51]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1">#Check for cycles in the Sequential Model</span>
<span class="n">history_single</span> <span class="o">=</span> <span class="n">single_neuron</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">callback_a</span><span class="p">,</span> <span class="n">callback_b</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.5614 - accuracy: 0.7215
Epoch 1: val_loss improved from 0.56015 to 0.56014, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5609 - accuracy: 0.7226 - val_loss: 0.5601 - val_accuracy: 0.7236
Epoch 2/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.5610 - accuracy: 0.7212
Epoch 2: val_loss did not improve from 0.56014
852/852 [==============================] - 2s 3ms/step - loss: 0.5606 - accuracy: 0.7219 - val_loss: 0.5608 - val_accuracy: 0.7246
Epoch 3/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.5611 - accuracy: 0.7228
Epoch 3: val_loss did not improve from 0.56014
852/852 [==============================] - 2s 3ms/step - loss: 0.5609 - accuracy: 0.7232 - val_loss: 0.5602 - val_accuracy: 0.7244
Epoch 4/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.5609 - accuracy: 0.7232
Epoch 4: val_loss did not improve from 0.56014
852/852 [==============================] - 2s 3ms/step - loss: 0.5607 - accuracy: 0.7235 - val_loss: 0.5604 - val_accuracy: 0.7222
Epoch 5/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.5613 - accuracy: 0.7216
Epoch 5: val_loss improved from 0.56014 to 0.56013, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5608 - accuracy: 0.7224 - val_loss: 0.5601 - val_accuracy: 0.7249
Epoch 6/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.5602 - accuracy: 0.7235
Epoch 6: val_loss did not improve from 0.56013
852/852 [==============================] - 2s 3ms/step - loss: 0.5607 - accuracy: 0.7231 - val_loss: 0.5601 - val_accuracy: 0.7249
Epoch 7/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.5603 - accuracy: 0.7247
Epoch 7: val_loss did not improve from 0.56013
852/852 [==============================] - 2s 3ms/step - loss: 0.5608 - accuracy: 0.7240 - val_loss: 0.5604 - val_accuracy: 0.7226
Epoch 8/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.5600 - accuracy: 0.7226
Epoch 8: val_loss did not improve from 0.56013
852/852 [==============================] - 2s 3ms/step - loss: 0.5605 - accuracy: 0.7224 - val_loss: 0.5608 - val_accuracy: 0.7209
Epoch 9/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.5609 - accuracy: 0.7221
Epoch 9: val_loss did not improve from 0.56013
852/852 [==============================] - 2s 3ms/step - loss: 0.5607 - accuracy: 0.7223 - val_loss: 0.5604 - val_accuracy: 0.7244
Epoch 10/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.5612 - accuracy: 0.7221
Epoch 10: val_loss improved from 0.56013 to 0.56012, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5607 - accuracy: 0.7229 - val_loss: 0.5601 - val_accuracy: 0.7244
Epoch 11/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5605 - accuracy: 0.7238
Epoch 11: val_loss did not improve from 0.56012
852/852 [==============================] - 2s 3ms/step - loss: 0.5606 - accuracy: 0.7238 - val_loss: 0.5606 - val_accuracy: 0.7242
Epoch 12/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.5599 - accuracy: 0.7236
Epoch 12: val_loss did not improve from 0.56012
852/852 [==============================] - 2s 3ms/step - loss: 0.5607 - accuracy: 0.7232 - val_loss: 0.5601 - val_accuracy: 0.7232
Epoch 13/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.5600 - accuracy: 0.7246
Epoch 13: val_loss improved from 0.56012 to 0.56012, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5607 - accuracy: 0.7236 - val_loss: 0.5601 - val_accuracy: 0.7244
Epoch 14/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.5602 - accuracy: 0.7232
Epoch 14: val_loss did not improve from 0.56012
852/852 [==============================] - 2s 3ms/step - loss: 0.5606 - accuracy: 0.7226 - val_loss: 0.5601 - val_accuracy: 0.7233
Epoch 15/150
852/852 [==============================] - ETA: 0s - loss: 0.5607 - accuracy: 0.7237
Epoch 15: val_loss did not improve from 0.56012
852/852 [==============================] - 2s 3ms/step - loss: 0.5607 - accuracy: 0.7237 - val_loss: 0.5601 - val_accuracy: 0.7231
Epoch 16/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.5620 - accuracy: 0.7227
Epoch 16: val_loss did not improve from 0.56012
852/852 [==============================] - 2s 3ms/step - loss: 0.5605 - accuracy: 0.7244 - val_loss: 0.5610 - val_accuracy: 0.7204
Epoch 17/150
852/852 [==============================] - ETA: 0s - loss: 0.5608 - accuracy: 0.7226
Epoch 17: val_loss did not improve from 0.56012
852/852 [==============================] - 2s 3ms/step - loss: 0.5608 - accuracy: 0.7226 - val_loss: 0.5602 - val_accuracy: 0.7222
Epoch 18/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.5611 - accuracy: 0.7232
Epoch 18: val_loss improved from 0.56012 to 0.56010, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5607 - accuracy: 0.7238 - val_loss: 0.5601 - val_accuracy: 0.7237
Epoch 19/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.5598 - accuracy: 0.7243
Epoch 19: val_loss did not improve from 0.56010
852/852 [==============================] - 2s 3ms/step - loss: 0.5607 - accuracy: 0.7230 - val_loss: 0.5603 - val_accuracy: 0.7243
Epoch 20/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.5609 - accuracy: 0.7233
Epoch 20: val_loss improved from 0.56010 to 0.56008, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5607 - accuracy: 0.7235 - val_loss: 0.5601 - val_accuracy: 0.7252
Epoch 21/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5606 - accuracy: 0.7235
Epoch 21: val_loss did not improve from 0.56008
852/852 [==============================] - 2s 3ms/step - loss: 0.5607 - accuracy: 0.7235 - val_loss: 0.5601 - val_accuracy: 0.7249
Epoch 22/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.5598 - accuracy: 0.7240
Epoch 22: val_loss improved from 0.56008 to 0.56008, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5607 - accuracy: 0.7232 - val_loss: 0.5601 - val_accuracy: 0.7246
Epoch 23/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.5617 - accuracy: 0.7224
Epoch 23: val_loss did not improve from 0.56008
852/852 [==============================] - 2s 3ms/step - loss: 0.5607 - accuracy: 0.7240 - val_loss: 0.5601 - val_accuracy: 0.7239
Epoch 24/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.5611 - accuracy: 0.7226
Epoch 24: val_loss did not improve from 0.56008
852/852 [==============================] - 2s 3ms/step - loss: 0.5606 - accuracy: 0.7228 - val_loss: 0.5601 - val_accuracy: 0.7235
Epoch 25/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.5606 - accuracy: 0.7237
Epoch 25: val_loss did not improve from 0.56008
852/852 [==============================] - 2s 3ms/step - loss: 0.5607 - accuracy: 0.7235 - val_loss: 0.5603 - val_accuracy: 0.7224
Epoch 26/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.5606 - accuracy: 0.7240
Epoch 26: val_loss did not improve from 0.56008
852/852 [==============================] - 2s 3ms/step - loss: 0.5606 - accuracy: 0.7240 - val_loss: 0.5603 - val_accuracy: 0.7221
Epoch 27/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.5613 - accuracy: 0.7231
Epoch 27: val_loss improved from 0.56008 to 0.56008, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5607 - accuracy: 0.7231 - val_loss: 0.5601 - val_accuracy: 0.7249
Epoch 28/150
818/852 [===========================&gt;..] - ETA: 0s - loss: 0.5600 - accuracy: 0.7232
Epoch 28: val_loss did not improve from 0.56008
852/852 [==============================] - 2s 3ms/step - loss: 0.5607 - accuracy: 0.7226 - val_loss: 0.5601 - val_accuracy: 0.7246
Epoch 29/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.5608 - accuracy: 0.7234
Epoch 29: val_loss improved from 0.56008 to 0.56006, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5607 - accuracy: 0.7235 - val_loss: 0.5601 - val_accuracy: 0.7239
Epoch 30/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.5612 - accuracy: 0.7226
Epoch 30: val_loss did not improve from 0.56006
852/852 [==============================] - 2s 3ms/step - loss: 0.5607 - accuracy: 0.7233 - val_loss: 0.5601 - val_accuracy: 0.7230
Epoch 31/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.5594 - accuracy: 0.7231
Epoch 31: val_loss did not improve from 0.56006
852/852 [==============================] - 2s 3ms/step - loss: 0.5606 - accuracy: 0.7221 - val_loss: 0.5601 - val_accuracy: 0.7244
Epoch 32/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.5605 - accuracy: 0.7230
Epoch 32: val_loss did not improve from 0.56006
852/852 [==============================] - 2s 3ms/step - loss: 0.5606 - accuracy: 0.7230 - val_loss: 0.5601 - val_accuracy: 0.7238
Epoch 33/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.5608 - accuracy: 0.7228
Epoch 33: val_loss did not improve from 0.56006
852/852 [==============================] - 2s 3ms/step - loss: 0.5606 - accuracy: 0.7229 - val_loss: 0.5606 - val_accuracy: 0.7245
Epoch 34/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.5607 - accuracy: 0.7222
Epoch 34: val_loss did not improve from 0.56006
852/852 [==============================] - 2s 3ms/step - loss: 0.5608 - accuracy: 0.7223 - val_loss: 0.5601 - val_accuracy: 0.7245
Epoch 35/150
818/852 [===========================&gt;..] - ETA: 0s - loss: 0.5614 - accuracy: 0.7232
Epoch 35: val_loss did not improve from 0.56006
852/852 [==============================] - 2s 3ms/step - loss: 0.5607 - accuracy: 0.7235 - val_loss: 0.5604 - val_accuracy: 0.7242
Epoch 36/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.5602 - accuracy: 0.7257
Epoch 36: val_loss improved from 0.56006 to 0.56006, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5607 - accuracy: 0.7256 - val_loss: 0.5601 - val_accuracy: 0.7235
Epoch 37/150
818/852 [===========================&gt;..] - ETA: 0s - loss: 0.5613 - accuracy: 0.7227
Epoch 37: val_loss improved from 0.56006 to 0.56005, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5607 - accuracy: 0.7237 - val_loss: 0.5600 - val_accuracy: 0.7250
Epoch 38/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5607 - accuracy: 0.7237
Epoch 38: val_loss did not improve from 0.56005
852/852 [==============================] - 2s 3ms/step - loss: 0.5607 - accuracy: 0.7239 - val_loss: 0.5606 - val_accuracy: 0.7210
Epoch 39/150
818/852 [===========================&gt;..] - ETA: 0s - loss: 0.5615 - accuracy: 0.7224
Epoch 39: val_loss improved from 0.56005 to 0.56004, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5607 - accuracy: 0.7224 - val_loss: 0.5600 - val_accuracy: 0.7240
Epoch 40/150
819/852 [===========================&gt;..] - ETA: 0s - loss: 0.5623 - accuracy: 0.7230
Epoch 40: val_loss did not improve from 0.56004
852/852 [==============================] - 2s 3ms/step - loss: 0.5606 - accuracy: 0.7242 - val_loss: 0.5607 - val_accuracy: 0.7248
Epoch 41/150
852/852 [==============================] - ETA: 0s - loss: 0.5607 - accuracy: 0.7233
Epoch 41: val_loss did not improve from 0.56004
852/852 [==============================] - 2s 3ms/step - loss: 0.5607 - accuracy: 0.7233 - val_loss: 0.5601 - val_accuracy: 0.7233
Epoch 42/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.5610 - accuracy: 0.7237
Epoch 42: val_loss did not improve from 0.56004
852/852 [==============================] - 2s 3ms/step - loss: 0.5607 - accuracy: 0.7242 - val_loss: 0.5603 - val_accuracy: 0.7246
Epoch 43/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.5603 - accuracy: 0.7229
Epoch 43: val_loss did not improve from 0.56004
852/852 [==============================] - 2s 3ms/step - loss: 0.5606 - accuracy: 0.7226 - val_loss: 0.5602 - val_accuracy: 0.7226
Epoch 44/150
820/852 [===========================&gt;..] - ETA: 0s - loss: 0.5595 - accuracy: 0.7259
Epoch 44: val_loss did not improve from 0.56004
852/852 [==============================] - 2s 3ms/step - loss: 0.5607 - accuracy: 0.7246 - val_loss: 0.5601 - val_accuracy: 0.7230
Epoch 45/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.5611 - accuracy: 0.7224
Epoch 45: val_loss improved from 0.56004 to 0.56003, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5606 - accuracy: 0.7230 - val_loss: 0.5600 - val_accuracy: 0.7253
Epoch 46/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5606 - accuracy: 0.7244
Epoch 46: val_loss did not improve from 0.56003
852/852 [==============================] - 2s 3ms/step - loss: 0.5605 - accuracy: 0.7245 - val_loss: 0.5602 - val_accuracy: 0.7224
Epoch 47/150
819/852 [===========================&gt;..] - ETA: 0s - loss: 0.5609 - accuracy: 0.7232
Epoch 47: val_loss did not improve from 0.56003
852/852 [==============================] - 2s 3ms/step - loss: 0.5605 - accuracy: 0.7228 - val_loss: 0.5606 - val_accuracy: 0.7210
Epoch 48/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.5605 - accuracy: 0.7233
Epoch 48: val_loss did not improve from 0.56003
852/852 [==============================] - 2s 3ms/step - loss: 0.5607 - accuracy: 0.7228 - val_loss: 0.5602 - val_accuracy: 0.7226
Epoch 49/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5606 - accuracy: 0.7249
Epoch 49: val_loss did not improve from 0.56003
852/852 [==============================] - 2s 3ms/step - loss: 0.5607 - accuracy: 0.7246 - val_loss: 0.5600 - val_accuracy: 0.7248
Epoch 50/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.5607 - accuracy: 0.7229
Epoch 50: val_loss did not improve from 0.56003
852/852 [==============================] - 2s 3ms/step - loss: 0.5606 - accuracy: 0.7231 - val_loss: 0.5605 - val_accuracy: 0.7245
Epoch 51/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.5604 - accuracy: 0.7237
Epoch 51: val_loss did not improve from 0.56003
852/852 [==============================] - 2s 3ms/step - loss: 0.5606 - accuracy: 0.7229 - val_loss: 0.5601 - val_accuracy: 0.7221
Epoch 52/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.5608 - accuracy: 0.7234
Epoch 52: val_loss did not improve from 0.56003
852/852 [==============================] - 2s 3ms/step - loss: 0.5606 - accuracy: 0.7235 - val_loss: 0.5601 - val_accuracy: 0.7228
Epoch 53/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.5613 - accuracy: 0.7236
Epoch 53: val_loss did not improve from 0.56003
852/852 [==============================] - 2s 3ms/step - loss: 0.5607 - accuracy: 0.7243 - val_loss: 0.5602 - val_accuracy: 0.7240
Epoch 54/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.5610 - accuracy: 0.7238
Epoch 54: val_loss improved from 0.56003 to 0.56001, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5606 - accuracy: 0.7238 - val_loss: 0.5600 - val_accuracy: 0.7243
Epoch 55/150
818/852 [===========================&gt;..] - ETA: 0s - loss: 0.5603 - accuracy: 0.7259
Epoch 55: val_loss did not improve from 0.56001
852/852 [==============================] - 2s 3ms/step - loss: 0.5607 - accuracy: 0.7245 - val_loss: 0.5602 - val_accuracy: 0.7223
Epoch 56/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.5597 - accuracy: 0.7236
Epoch 56: val_loss did not improve from 0.56001
852/852 [==============================] - 2s 3ms/step - loss: 0.5606 - accuracy: 0.7232 - val_loss: 0.5604 - val_accuracy: 0.7218
Epoch 57/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.5610 - accuracy: 0.7226
Epoch 57: val_loss did not improve from 0.56001
852/852 [==============================] - 2s 3ms/step - loss: 0.5606 - accuracy: 0.7232 - val_loss: 0.5603 - val_accuracy: 0.7223
Epoch 58/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.5605 - accuracy: 0.7238
Epoch 58: val_loss did not improve from 0.56001
852/852 [==============================] - 2s 3ms/step - loss: 0.5607 - accuracy: 0.7236 - val_loss: 0.5600 - val_accuracy: 0.7239
Epoch 59/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.5609 - accuracy: 0.7228
Epoch 59: val_loss improved from 0.56001 to 0.56000, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5606 - accuracy: 0.7230 - val_loss: 0.5600 - val_accuracy: 0.7251
Epoch 60/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.5603 - accuracy: 0.7234
Epoch 60: val_loss did not improve from 0.56000
852/852 [==============================] - 2s 3ms/step - loss: 0.5606 - accuracy: 0.7233 - val_loss: 0.5601 - val_accuracy: 0.7250
Epoch 61/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.5603 - accuracy: 0.7237
Epoch 61: val_loss improved from 0.56000 to 0.55999, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5607 - accuracy: 0.7233 - val_loss: 0.5600 - val_accuracy: 0.7249
Epoch 62/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.5607 - accuracy: 0.7236
Epoch 62: val_loss did not improve from 0.55999
852/852 [==============================] - 2s 3ms/step - loss: 0.5605 - accuracy: 0.7237 - val_loss: 0.5600 - val_accuracy: 0.7249
Epoch 63/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.5608 - accuracy: 0.7231
Epoch 63: val_loss did not improve from 0.55999
852/852 [==============================] - 3s 4ms/step - loss: 0.5604 - accuracy: 0.7235 - val_loss: 0.5603 - val_accuracy: 0.7224
Epoch 64/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.5598 - accuracy: 0.7227
Epoch 64: val_loss did not improve from 0.55999
852/852 [==============================] - 3s 3ms/step - loss: 0.5606 - accuracy: 0.7229 - val_loss: 0.5600 - val_accuracy: 0.7250
Epoch 65/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5605 - accuracy: 0.7243
Epoch 65: val_loss did not improve from 0.55999
852/852 [==============================] - 3s 3ms/step - loss: 0.5607 - accuracy: 0.7243 - val_loss: 0.5608 - val_accuracy: 0.7213
Epoch 66/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.5604 - accuracy: 0.7236
Epoch 66: val_loss did not improve from 0.55999
852/852 [==============================] - 2s 3ms/step - loss: 0.5603 - accuracy: 0.7237 - val_loss: 0.5602 - val_accuracy: 0.7224
Epoch 67/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.5604 - accuracy: 0.7254
Epoch 67: val_loss did not improve from 0.55999
852/852 [==============================] - 2s 3ms/step - loss: 0.5606 - accuracy: 0.7248 - val_loss: 0.5600 - val_accuracy: 0.7237
Epoch 68/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.5595 - accuracy: 0.7234
Epoch 68: val_loss did not improve from 0.55999
852/852 [==============================] - 3s 3ms/step - loss: 0.5605 - accuracy: 0.7228 - val_loss: 0.5604 - val_accuracy: 0.7244
Epoch 69/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.5609 - accuracy: 0.7239
Epoch 69: val_loss did not improve from 0.55999
852/852 [==============================] - 3s 3ms/step - loss: 0.5606 - accuracy: 0.7239 - val_loss: 0.5600 - val_accuracy: 0.7248
Epoch 70/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.5598 - accuracy: 0.7228
Epoch 70: val_loss did not improve from 0.55999
852/852 [==============================] - 3s 3ms/step - loss: 0.5606 - accuracy: 0.7226 - val_loss: 0.5602 - val_accuracy: 0.7251
Epoch 71/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.5604 - accuracy: 0.7245
Epoch 71: val_loss did not improve from 0.55999
852/852 [==============================] - 2s 3ms/step - loss: 0.5606 - accuracy: 0.7242 - val_loss: 0.5603 - val_accuracy: 0.7248
Epoch 72/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.5599 - accuracy: 0.7234
Epoch 72: val_loss did not improve from 0.55999
852/852 [==============================] - 2s 3ms/step - loss: 0.5606 - accuracy: 0.7236 - val_loss: 0.5604 - val_accuracy: 0.7243
Epoch 73/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.5611 - accuracy: 0.7226
Epoch 73: val_loss did not improve from 0.55999
852/852 [==============================] - 2s 3ms/step - loss: 0.5604 - accuracy: 0.7230 - val_loss: 0.5601 - val_accuracy: 0.7226
Epoch 74/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.5599 - accuracy: 0.7239
Epoch 74: val_loss did not improve from 0.55999
852/852 [==============================] - 2s 3ms/step - loss: 0.5602 - accuracy: 0.7232 - val_loss: 0.5613 - val_accuracy: 0.7255
Epoch 75/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.5612 - accuracy: 0.7249
Epoch 75: val_loss did not improve from 0.55999
852/852 [==============================] - 2s 3ms/step - loss: 0.5607 - accuracy: 0.7253 - val_loss: 0.5600 - val_accuracy: 0.7246
Epoch 76/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.5603 - accuracy: 0.7233
Epoch 76: val_loss did not improve from 0.55999
852/852 [==============================] - 2s 3ms/step - loss: 0.5605 - accuracy: 0.7236 - val_loss: 0.5608 - val_accuracy: 0.7204
Epoch 77/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.5613 - accuracy: 0.7231
Epoch 77: val_loss improved from 0.55999 to 0.55997, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5607 - accuracy: 0.7232 - val_loss: 0.5600 - val_accuracy: 0.7250
Epoch 78/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.5613 - accuracy: 0.7226
Epoch 78: val_loss did not improve from 0.55997
852/852 [==============================] - 2s 3ms/step - loss: 0.5605 - accuracy: 0.7233 - val_loss: 0.5602 - val_accuracy: 0.7255
Epoch 79/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.5598 - accuracy: 0.7247
Epoch 79: val_loss improved from 0.55997 to 0.55997, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5606 - accuracy: 0.7243 - val_loss: 0.5600 - val_accuracy: 0.7239
Epoch 80/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.5601 - accuracy: 0.7233
Epoch 80: val_loss did not improve from 0.55997
852/852 [==============================] - 2s 3ms/step - loss: 0.5605 - accuracy: 0.7230 - val_loss: 0.5601 - val_accuracy: 0.7230
Epoch 81/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.5593 - accuracy: 0.7246
Epoch 81: val_loss did not improve from 0.55997
852/852 [==============================] - 2s 3ms/step - loss: 0.5606 - accuracy: 0.7237 - val_loss: 0.5600 - val_accuracy: 0.7239
Epoch 82/150
820/852 [===========================&gt;..] - ETA: 0s - loss: 0.5602 - accuracy: 0.7224
Epoch 82: val_loss did not improve from 0.55997
852/852 [==============================] - 2s 3ms/step - loss: 0.5606 - accuracy: 0.7224 - val_loss: 0.5600 - val_accuracy: 0.7231
Epoch 83/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.5613 - accuracy: 0.7232
Epoch 83: val_loss did not improve from 0.55997
852/852 [==============================] - 2s 3ms/step - loss: 0.5605 - accuracy: 0.7237 - val_loss: 0.5600 - val_accuracy: 0.7230
Epoch 84/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.5602 - accuracy: 0.7248
Epoch 84: val_loss did not improve from 0.55997
852/852 [==============================] - 2s 3ms/step - loss: 0.5605 - accuracy: 0.7244 - val_loss: 0.5601 - val_accuracy: 0.7223
Epoch 85/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.5611 - accuracy: 0.7231
Epoch 85: val_loss did not improve from 0.55997
852/852 [==============================] - 2s 3ms/step - loss: 0.5605 - accuracy: 0.7240 - val_loss: 0.5602 - val_accuracy: 0.7253
Epoch 86/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.5615 - accuracy: 0.7226
Epoch 86: val_loss improved from 0.55997 to 0.55995, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5604 - accuracy: 0.7236 - val_loss: 0.5600 - val_accuracy: 0.7251
Epoch 87/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.5602 - accuracy: 0.7231
Epoch 87: val_loss did not improve from 0.55995
852/852 [==============================] - 2s 3ms/step - loss: 0.5605 - accuracy: 0.7231 - val_loss: 0.5601 - val_accuracy: 0.7249
Epoch 88/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.5608 - accuracy: 0.7227
Epoch 88: val_loss did not improve from 0.55995
852/852 [==============================] - 2s 3ms/step - loss: 0.5605 - accuracy: 0.7230 - val_loss: 0.5601 - val_accuracy: 0.7245
Epoch 89/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.5611 - accuracy: 0.7229
Epoch 89: val_loss did not improve from 0.55995
852/852 [==============================] - 2s 3ms/step - loss: 0.5605 - accuracy: 0.7233 - val_loss: 0.5605 - val_accuracy: 0.7246
Epoch 90/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.5612 - accuracy: 0.7226
Epoch 90: val_loss did not improve from 0.55995
852/852 [==============================] - 2s 3ms/step - loss: 0.5605 - accuracy: 0.7237 - val_loss: 0.5600 - val_accuracy: 0.7248
Epoch 91/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.5604 - accuracy: 0.7237
Epoch 91: val_loss did not improve from 0.55995
852/852 [==============================] - 2s 3ms/step - loss: 0.5604 - accuracy: 0.7230 - val_loss: 0.5604 - val_accuracy: 0.7216
Epoch 92/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.5607 - accuracy: 0.7235
Epoch 92: val_loss did not improve from 0.55995
852/852 [==============================] - 2s 3ms/step - loss: 0.5606 - accuracy: 0.7233 - val_loss: 0.5601 - val_accuracy: 0.7230
Epoch 93/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.5615 - accuracy: 0.7221
Epoch 93: val_loss did not improve from 0.55995
852/852 [==============================] - 2s 3ms/step - loss: 0.5606 - accuracy: 0.7236 - val_loss: 0.5601 - val_accuracy: 0.7245
Epoch 94/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.5589 - accuracy: 0.7256
Epoch 94: val_loss improved from 0.55995 to 0.55994, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5607 - accuracy: 0.7243 - val_loss: 0.5599 - val_accuracy: 0.7239
Epoch 95/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.5602 - accuracy: 0.7242
Epoch 95: val_loss improved from 0.55994 to 0.55994, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5605 - accuracy: 0.7237 - val_loss: 0.5599 - val_accuracy: 0.7253
Epoch 96/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.5604 - accuracy: 0.7231
Epoch 96: val_loss improved from 0.55994 to 0.55993, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5606 - accuracy: 0.7232 - val_loss: 0.5599 - val_accuracy: 0.7250
Epoch 97/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.5602 - accuracy: 0.7236
Epoch 97: val_loss improved from 0.55993 to 0.55993, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5605 - accuracy: 0.7231 - val_loss: 0.5599 - val_accuracy: 0.7242
Epoch 98/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.5608 - accuracy: 0.7229
Epoch 98: val_loss did not improve from 0.55993
852/852 [==============================] - 2s 3ms/step - loss: 0.5604 - accuracy: 0.7232 - val_loss: 0.5604 - val_accuracy: 0.7249
Epoch 99/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.5608 - accuracy: 0.7229
Epoch 99: val_loss did not improve from 0.55993
852/852 [==============================] - 2s 3ms/step - loss: 0.5605 - accuracy: 0.7236 - val_loss: 0.5601 - val_accuracy: 0.7248
Epoch 100/150
819/852 [===========================&gt;..] - ETA: 0s - loss: 0.5608 - accuracy: 0.7236
Epoch 100: val_loss did not improve from 0.55993
852/852 [==============================] - 2s 3ms/step - loss: 0.5606 - accuracy: 0.7243 - val_loss: 0.5600 - val_accuracy: 0.7250
Epoch 101/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.5603 - accuracy: 0.7246
Epoch 101: val_loss did not improve from 0.55993
852/852 [==============================] - 2s 3ms/step - loss: 0.5604 - accuracy: 0.7245 - val_loss: 0.5600 - val_accuracy: 0.7232
Epoch 102/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.5624 - accuracy: 0.7220
Epoch 102: val_loss did not improve from 0.55993
852/852 [==============================] - 2s 3ms/step - loss: 0.5606 - accuracy: 0.7233 - val_loss: 0.5601 - val_accuracy: 0.7250
Epoch 103/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.5607 - accuracy: 0.7246
Epoch 103: val_loss did not improve from 0.55993
852/852 [==============================] - 2s 3ms/step - loss: 0.5607 - accuracy: 0.7245 - val_loss: 0.5600 - val_accuracy: 0.7248
Epoch 104/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.5601 - accuracy: 0.7247
Epoch 104: val_loss improved from 0.55993 to 0.55992, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5606 - accuracy: 0.7238 - val_loss: 0.5599 - val_accuracy: 0.7248
Epoch 105/150
818/852 [===========================&gt;..] - ETA: 0s - loss: 0.5605 - accuracy: 0.7237
Epoch 105: val_loss did not improve from 0.55992
852/852 [==============================] - 2s 3ms/step - loss: 0.5605 - accuracy: 0.7237 - val_loss: 0.5602 - val_accuracy: 0.7245
Epoch 106/150
820/852 [===========================&gt;..] - ETA: 0s - loss: 0.5629 - accuracy: 0.7209
Epoch 106: val_loss did not improve from 0.55992
852/852 [==============================] - 2s 3ms/step - loss: 0.5604 - accuracy: 0.7235 - val_loss: 0.5600 - val_accuracy: 0.7233
Epoch 107/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.5605 - accuracy: 0.7234
Epoch 107: val_loss did not improve from 0.55992
852/852 [==============================] - 2s 3ms/step - loss: 0.5604 - accuracy: 0.7235 - val_loss: 0.5599 - val_accuracy: 0.7246
Epoch 108/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.5602 - accuracy: 0.7240
Epoch 108: val_loss did not improve from 0.55992
852/852 [==============================] - 2s 3ms/step - loss: 0.5606 - accuracy: 0.7237 - val_loss: 0.5600 - val_accuracy: 0.7248
Epoch 109/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.5613 - accuracy: 0.7238
Epoch 109: val_loss did not improve from 0.55992
852/852 [==============================] - 2s 3ms/step - loss: 0.5605 - accuracy: 0.7242 - val_loss: 0.5599 - val_accuracy: 0.7242
Epoch 110/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5605 - accuracy: 0.7239
Epoch 110: val_loss did not improve from 0.55992
852/852 [==============================] - 2s 3ms/step - loss: 0.5604 - accuracy: 0.7238 - val_loss: 0.5605 - val_accuracy: 0.7250
Epoch 111/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.5594 - accuracy: 0.7253
Epoch 111: val_loss improved from 0.55992 to 0.55992, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5606 - accuracy: 0.7248 - val_loss: 0.5599 - val_accuracy: 0.7252
Epoch 112/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.5598 - accuracy: 0.7249
Epoch 112: val_loss did not improve from 0.55992
852/852 [==============================] - 2s 3ms/step - loss: 0.5605 - accuracy: 0.7242 - val_loss: 0.5601 - val_accuracy: 0.7224
Epoch 113/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5611 - accuracy: 0.7237
Epoch 113: val_loss improved from 0.55992 to 0.55990, saving model to single_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5606 - accuracy: 0.7240 - val_loss: 0.5599 - val_accuracy: 0.7251
Epoch 114/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.5603 - accuracy: 0.7243
Epoch 114: val_loss did not improve from 0.55990
852/852 [==============================] - 2s 3ms/step - loss: 0.5604 - accuracy: 0.7244 - val_loss: 0.5603 - val_accuracy: 0.7246
Epoch 115/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.5604 - accuracy: 0.7234
Epoch 115: val_loss did not improve from 0.55990
852/852 [==============================] - 2s 3ms/step - loss: 0.5606 - accuracy: 0.7232 - val_loss: 0.5601 - val_accuracy: 0.7252
Epoch 116/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.5601 - accuracy: 0.7235
Epoch 116: val_loss did not improve from 0.55990
852/852 [==============================] - 2s 3ms/step - loss: 0.5606 - accuracy: 0.7231 - val_loss: 0.5599 - val_accuracy: 0.7251
Epoch 117/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.5606 - accuracy: 0.7237
Epoch 117: val_loss improved from 0.55990 to 0.55990, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5605 - accuracy: 0.7238 - val_loss: 0.5599 - val_accuracy: 0.7253
Epoch 118/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.5604 - accuracy: 0.7244
Epoch 118: val_loss did not improve from 0.55990
852/852 [==============================] - 2s 3ms/step - loss: 0.5605 - accuracy: 0.7244 - val_loss: 0.5601 - val_accuracy: 0.7250
Epoch 119/150
818/852 [===========================&gt;..] - ETA: 0s - loss: 0.5610 - accuracy: 0.7238
Epoch 119: val_loss improved from 0.55990 to 0.55990, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5605 - accuracy: 0.7250 - val_loss: 0.5599 - val_accuracy: 0.7252
Epoch 120/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.5601 - accuracy: 0.7241
Epoch 120: val_loss did not improve from 0.55990
852/852 [==============================] - 2s 3ms/step - loss: 0.5604 - accuracy: 0.7239 - val_loss: 0.5600 - val_accuracy: 0.7228
Epoch 121/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.5605 - accuracy: 0.7257
Epoch 121: val_loss did not improve from 0.55990
852/852 [==============================] - 2s 3ms/step - loss: 0.5605 - accuracy: 0.7256 - val_loss: 0.5607 - val_accuracy: 0.7205
Epoch 122/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.5604 - accuracy: 0.7237
Epoch 122: val_loss did not improve from 0.55990
852/852 [==============================] - 2s 3ms/step - loss: 0.5606 - accuracy: 0.7236 - val_loss: 0.5601 - val_accuracy: 0.7251
Epoch 123/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.5605 - accuracy: 0.7246
Epoch 123: val_loss did not improve from 0.55990
852/852 [==============================] - 2s 3ms/step - loss: 0.5606 - accuracy: 0.7248 - val_loss: 0.5600 - val_accuracy: 0.7230
Epoch 124/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.5606 - accuracy: 0.7246
Epoch 124: val_loss did not improve from 0.55990
852/852 [==============================] - 2s 3ms/step - loss: 0.5604 - accuracy: 0.7248 - val_loss: 0.5604 - val_accuracy: 0.7222
Epoch 125/150
818/852 [===========================&gt;..] - ETA: 0s - loss: 0.5586 - accuracy: 0.7264
Epoch 125: val_loss did not improve from 0.55990
852/852 [==============================] - 3s 3ms/step - loss: 0.5605 - accuracy: 0.7246 - val_loss: 0.5599 - val_accuracy: 0.7236
Epoch 126/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.5591 - accuracy: 0.7229
Epoch 126: val_loss did not improve from 0.55990
852/852 [==============================] - 3s 4ms/step - loss: 0.5602 - accuracy: 0.7223 - val_loss: 0.5602 - val_accuracy: 0.7251
Epoch 127/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.5616 - accuracy: 0.7226
Epoch 127: val_loss did not improve from 0.55990
852/852 [==============================] - 2s 3ms/step - loss: 0.5605 - accuracy: 0.7238 - val_loss: 0.5599 - val_accuracy: 0.7252
Epoch 128/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.5608 - accuracy: 0.7227
Epoch 128: val_loss improved from 0.55990 to 0.55989, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5606 - accuracy: 0.7235 - val_loss: 0.5599 - val_accuracy: 0.7251
Epoch 129/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.5604 - accuracy: 0.7250
Epoch 129: val_loss did not improve from 0.55989
852/852 [==============================] - 2s 3ms/step - loss: 0.5605 - accuracy: 0.7249 - val_loss: 0.5601 - val_accuracy: 0.7223
Epoch 130/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.5614 - accuracy: 0.7238
Epoch 130: val_loss improved from 0.55989 to 0.55988, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5605 - accuracy: 0.7240 - val_loss: 0.5599 - val_accuracy: 0.7249
Epoch 131/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.5602 - accuracy: 0.7252
Epoch 131: val_loss did not improve from 0.55988
852/852 [==============================] - 3s 3ms/step - loss: 0.5605 - accuracy: 0.7248 - val_loss: 0.5599 - val_accuracy: 0.7253
Epoch 132/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.5602 - accuracy: 0.7239
Epoch 132: val_loss did not improve from 0.55988
852/852 [==============================] - 3s 3ms/step - loss: 0.5605 - accuracy: 0.7237 - val_loss: 0.5599 - val_accuracy: 0.7243
Epoch 133/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.5611 - accuracy: 0.7238
Epoch 133: val_loss improved from 0.55988 to 0.55988, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5604 - accuracy: 0.7246 - val_loss: 0.5599 - val_accuracy: 0.7255
Epoch 134/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.5603 - accuracy: 0.7231
Epoch 134: val_loss did not improve from 0.55988
852/852 [==============================] - 2s 3ms/step - loss: 0.5603 - accuracy: 0.7231 - val_loss: 0.5600 - val_accuracy: 0.7248
Epoch 135/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5601 - accuracy: 0.7240
Epoch 135: val_loss did not improve from 0.55988
852/852 [==============================] - 2s 3ms/step - loss: 0.5605 - accuracy: 0.7246 - val_loss: 0.5600 - val_accuracy: 0.7253
Epoch 136/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.5614 - accuracy: 0.7244
Epoch 136: val_loss did not improve from 0.55988
852/852 [==============================] - 2s 3ms/step - loss: 0.5605 - accuracy: 0.7250 - val_loss: 0.5600 - val_accuracy: 0.7255
Epoch 137/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5616 - accuracy: 0.7227
Epoch 137: val_loss did not improve from 0.55988
852/852 [==============================] - 2s 3ms/step - loss: 0.5605 - accuracy: 0.7240 - val_loss: 0.5600 - val_accuracy: 0.7228
Epoch 138/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.5601 - accuracy: 0.7263
Epoch 138: val_loss did not improve from 0.55988
852/852 [==============================] - 2s 3ms/step - loss: 0.5604 - accuracy: 0.7253 - val_loss: 0.5609 - val_accuracy: 0.7255
Epoch 139/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.5596 - accuracy: 0.7267
Epoch 139: val_loss did not improve from 0.55988
852/852 [==============================] - 2s 3ms/step - loss: 0.5606 - accuracy: 0.7255 - val_loss: 0.5599 - val_accuracy: 0.7246
Epoch 140/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.5597 - accuracy: 0.7255
Epoch 140: val_loss did not improve from 0.55988
852/852 [==============================] - 2s 3ms/step - loss: 0.5605 - accuracy: 0.7242 - val_loss: 0.5604 - val_accuracy: 0.7221
Epoch 141/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.5605 - accuracy: 0.7230
Epoch 141: val_loss did not improve from 0.55988
852/852 [==============================] - 2s 3ms/step - loss: 0.5604 - accuracy: 0.7235 - val_loss: 0.5600 - val_accuracy: 0.7232
Epoch 142/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.5613 - accuracy: 0.7250
Epoch 142: val_loss did not improve from 0.55988
852/852 [==============================] - 2s 3ms/step - loss: 0.5605 - accuracy: 0.7250 - val_loss: 0.5599 - val_accuracy: 0.7242
Epoch 143/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.5602 - accuracy: 0.7238
Epoch 143: val_loss did not improve from 0.55988
852/852 [==============================] - 2s 3ms/step - loss: 0.5605 - accuracy: 0.7235 - val_loss: 0.5602 - val_accuracy: 0.7248
Epoch 144/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.5613 - accuracy: 0.7232
Epoch 144: val_loss did not improve from 0.55988
852/852 [==============================] - 2s 3ms/step - loss: 0.5605 - accuracy: 0.7236 - val_loss: 0.5603 - val_accuracy: 0.7250
Epoch 145/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.5613 - accuracy: 0.7237
Epoch 145: val_loss did not improve from 0.55988
852/852 [==============================] - 2s 3ms/step - loss: 0.5605 - accuracy: 0.7245 - val_loss: 0.5600 - val_accuracy: 0.7250
Epoch 146/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.5589 - accuracy: 0.7255
Epoch 146: val_loss did not improve from 0.55988
852/852 [==============================] - 2s 3ms/step - loss: 0.5605 - accuracy: 0.7244 - val_loss: 0.5599 - val_accuracy: 0.7246
Epoch 147/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.5596 - accuracy: 0.7249
Epoch 147: val_loss did not improve from 0.55988
852/852 [==============================] - 2s 3ms/step - loss: 0.5602 - accuracy: 0.7251 - val_loss: 0.5603 - val_accuracy: 0.7217
Epoch 148/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.5604 - accuracy: 0.7246
Epoch 148: val_loss did not improve from 0.55988
852/852 [==============================] - 2s 3ms/step - loss: 0.5604 - accuracy: 0.7245 - val_loss: 0.5601 - val_accuracy: 0.7250
Epoch 149/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.5598 - accuracy: 0.7249
Epoch 149: val_loss did not improve from 0.55988
852/852 [==============================] - 2s 3ms/step - loss: 0.5605 - accuracy: 0.7240 - val_loss: 0.5599 - val_accuracy: 0.7248
Epoch 150/150
818/852 [===========================&gt;..] - ETA: 0s - loss: 0.5605 - accuracy: 0.7242
Epoch 150: val_loss improved from 0.55988 to 0.55988, saving model to single_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5605 - accuracy: 0.7243 - val_loss: 0.5599 - val_accuracy: 0.7253
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=62f14b8d-857d-4506-9abd-68ada01cbe3c">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [52]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">single_neuron</span><span class="o">.</span><span class="n">load_weights</span><span class="p">(</span><span class="s1">'single_model.hdf5'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=b7bb948f-cf2e-4939-9238-849a5eccbd41">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [53]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Other Metrics</span>
<span class="n">single_neuron_preds</span> <span class="o">=</span> <span class="n">single_neuron</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">single_neuron_preds</span><span class="o">.</span><span class="n">round</span><span class="p">())</span>
<span class="n">precision</span> <span class="o">=</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">single_neuron_preds</span><span class="o">.</span><span class="n">round</span><span class="p">())</span>
<span class="n">recall</span> <span class="o">=</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">single_neuron_preds</span><span class="o">.</span><span class="n">round</span><span class="p">())</span>
<span class="n">f1score</span> <span class="o">=</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">single_neuron_preds</span><span class="o">.</span><span class="n">round</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Accuracy: </span><span class="si">%.2f</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">accuracy</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Precision: </span><span class="si">%.2f</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">precision</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Recall: </span><span class="si">%.2f</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">recall</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"F1-score: </span><span class="si">%.2f</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">f1score</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>267/267 [==============================] - 0s 1ms/step
Accuracy: 0.73
Precision: 0.75
Recall: 0.67
F1-score: 0.71
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=55332cd5-18cd-41ea-86ca-d1b52cc77087">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="2.3-Build-a-Model-with-3-Neurons-in-2-Layers">2.3 Build a Model with 3 Neurons in 2 Layers<a class="anchor-link" href="#2.3-Build-a-Model-with-3-Neurons-in-2-Layers">¶</a></h4>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=312328dc-5742-448d-a6cc-52824676879f">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [54]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># constructing a 3 neuron model</span>
<span class="n">three_neuron</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">three_neuron</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">input_dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">))</span>
<span class="n">three_neuron</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'sigmoid'</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">three_neuron</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>

<span class="c1">#Check for cycles in the Sequential Model</span>
<span class="n">three_neuron</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">'binary_crossentropy'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s1">'adam'</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'accuracy'</span><span class="p">])</span>
<span class="n">callback_a</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">filepath</span> <span class="o">=</span> <span class="s1">'three_neuron_model.hdf5'</span><span class="p">,</span> <span class="n">monitor</span><span class="o">=</span><span class="s1">'val_loss'</span><span class="p">,</span> <span class="n">save_best_only</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">save_weights_only</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">verbose</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">callback_b</span> <span class="o">=</span> <span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s1">'val_loss'</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">'min'</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">history_3</span> <span class="o">=</span> <span class="n">three_neuron</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">callback_a</span><span class="p">,</span> <span class="n">callback_b</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Model: "sequential_11"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense_10 (Dense)            (None, 2)                 54        
                                                                 
 dense_11 (Dense)            (None, 1)                 3         
                                                                 
=================================================================
Total params: 57 (228.00 Byte)
Trainable params: 57 (228.00 Byte)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
None
Epoch 1/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.6471 - accuracy: 0.6296
Epoch 1: val_loss improved from inf to 0.61003, saving model to three_neuron_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.6471 - accuracy: 0.6301 - val_loss: 0.6100 - val_accuracy: 0.7053
Epoch 2/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.5934 - accuracy: 0.7104
Epoch 2: val_loss improved from 0.61003 to 0.57993, saving model to three_neuron_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5928 - accuracy: 0.7111 - val_loss: 0.5799 - val_accuracy: 0.7165
Epoch 3/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.5745 - accuracy: 0.7184
Epoch 3: val_loss improved from 0.57993 to 0.56849, saving model to three_neuron_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5742 - accuracy: 0.7189 - val_loss: 0.5685 - val_accuracy: 0.7212
Epoch 4/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.5663 - accuracy: 0.7222
Epoch 4: val_loss improved from 0.56849 to 0.56332, saving model to three_neuron_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5666 - accuracy: 0.7215 - val_loss: 0.5633 - val_accuracy: 0.7223
Epoch 5/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5621 - accuracy: 0.7217
Epoch 5: val_loss improved from 0.56332 to 0.56064, saving model to three_neuron_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5628 - accuracy: 0.7213 - val_loss: 0.5606 - val_accuracy: 0.7224
Epoch 6/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.5595 - accuracy: 0.7239
Epoch 6: val_loss improved from 0.56064 to 0.55925, saving model to three_neuron_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5604 - accuracy: 0.7230 - val_loss: 0.5593 - val_accuracy: 0.7228
Epoch 7/150
852/852 [==============================] - ETA: 0s - loss: 0.5593 - accuracy: 0.7223
Epoch 7: val_loss improved from 0.55925 to 0.55778, saving model to three_neuron_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5593 - accuracy: 0.7223 - val_loss: 0.5578 - val_accuracy: 0.7245
Epoch 8/150
818/852 [===========================&gt;..] - ETA: 0s - loss: 0.5584 - accuracy: 0.7244
Epoch 8: val_loss improved from 0.55778 to 0.55764, saving model to three_neuron_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5581 - accuracy: 0.7244 - val_loss: 0.5576 - val_accuracy: 0.7231
Epoch 9/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5578 - accuracy: 0.7220
Epoch 9: val_loss improved from 0.55764 to 0.55612, saving model to three_neuron_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5576 - accuracy: 0.7222 - val_loss: 0.5561 - val_accuracy: 0.7244
Epoch 10/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.5567 - accuracy: 0.7252
Epoch 10: val_loss improved from 0.55612 to 0.55588, saving model to three_neuron_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5567 - accuracy: 0.7249 - val_loss: 0.5559 - val_accuracy: 0.7238
Epoch 11/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.5566 - accuracy: 0.7250
Epoch 11: val_loss did not improve from 0.55588
852/852 [==============================] - 2s 3ms/step - loss: 0.5564 - accuracy: 0.7248 - val_loss: 0.5560 - val_accuracy: 0.7256
Epoch 12/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.5553 - accuracy: 0.7257
Epoch 12: val_loss improved from 0.55588 to 0.55477, saving model to three_neuron_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5560 - accuracy: 0.7250 - val_loss: 0.5548 - val_accuracy: 0.7264
Epoch 13/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.5562 - accuracy: 0.7238
Epoch 13: val_loss improved from 0.55477 to 0.55431, saving model to three_neuron_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5558 - accuracy: 0.7243 - val_loss: 0.5543 - val_accuracy: 0.7263
Epoch 14/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.5559 - accuracy: 0.7254
Epoch 14: val_loss improved from 0.55431 to 0.55420, saving model to three_neuron_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5551 - accuracy: 0.7260 - val_loss: 0.5542 - val_accuracy: 0.7265
Epoch 15/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.5562 - accuracy: 0.7270
Epoch 15: val_loss improved from 0.55420 to 0.55396, saving model to three_neuron_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5549 - accuracy: 0.7273 - val_loss: 0.5540 - val_accuracy: 0.7238
Epoch 16/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5550 - accuracy: 0.7260
Epoch 16: val_loss improved from 0.55396 to 0.55342, saving model to three_neuron_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5547 - accuracy: 0.7262 - val_loss: 0.5534 - val_accuracy: 0.7277
Epoch 17/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.5545 - accuracy: 0.7265
Epoch 17: val_loss improved from 0.55342 to 0.55338, saving model to three_neuron_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5544 - accuracy: 0.7260 - val_loss: 0.5534 - val_accuracy: 0.7231
Epoch 18/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5542 - accuracy: 0.7259
Epoch 18: val_loss improved from 0.55338 to 0.55303, saving model to three_neuron_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5541 - accuracy: 0.7259 - val_loss: 0.5530 - val_accuracy: 0.7279
Epoch 19/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.5541 - accuracy: 0.7262
Epoch 19: val_loss improved from 0.55303 to 0.55295, saving model to three_neuron_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5537 - accuracy: 0.7264 - val_loss: 0.5530 - val_accuracy: 0.7283
Epoch 20/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.5525 - accuracy: 0.7270
Epoch 20: val_loss improved from 0.55295 to 0.55283, saving model to three_neuron_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5539 - accuracy: 0.7259 - val_loss: 0.5528 - val_accuracy: 0.7263
Epoch 21/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.5536 - accuracy: 0.7269
Epoch 21: val_loss improved from 0.55283 to 0.55282, saving model to three_neuron_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5538 - accuracy: 0.7264 - val_loss: 0.5528 - val_accuracy: 0.7239
Epoch 22/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5536 - accuracy: 0.7256
Epoch 22: val_loss did not improve from 0.55282
852/852 [==============================] - 2s 3ms/step - loss: 0.5536 - accuracy: 0.7258 - val_loss: 0.5531 - val_accuracy: 0.7236
Epoch 23/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.5541 - accuracy: 0.7272
Epoch 23: val_loss improved from 0.55282 to 0.55274, saving model to three_neuron_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5535 - accuracy: 0.7279 - val_loss: 0.5527 - val_accuracy: 0.7296
Epoch 24/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.5530 - accuracy: 0.7285
Epoch 24: val_loss did not improve from 0.55274
852/852 [==============================] - 2s 3ms/step - loss: 0.5533 - accuracy: 0.7279 - val_loss: 0.5528 - val_accuracy: 0.7245
Epoch 25/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.5533 - accuracy: 0.7279
Epoch 25: val_loss improved from 0.55274 to 0.55220, saving model to three_neuron_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5532 - accuracy: 0.7282 - val_loss: 0.5522 - val_accuracy: 0.7297
Epoch 26/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5528 - accuracy: 0.7287
Epoch 26: val_loss did not improve from 0.55220
852/852 [==============================] - 2s 3ms/step - loss: 0.5534 - accuracy: 0.7286 - val_loss: 0.5522 - val_accuracy: 0.7280
Epoch 27/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.5536 - accuracy: 0.7274
Epoch 27: val_loss did not improve from 0.55220
852/852 [==============================] - 2s 3ms/step - loss: 0.5532 - accuracy: 0.7276 - val_loss: 0.5523 - val_accuracy: 0.7306
Epoch 28/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.5537 - accuracy: 0.7275
Epoch 28: val_loss improved from 0.55220 to 0.55198, saving model to three_neuron_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5531 - accuracy: 0.7279 - val_loss: 0.5520 - val_accuracy: 0.7279
Epoch 29/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.5522 - accuracy: 0.7272
Epoch 29: val_loss did not improve from 0.55198
852/852 [==============================] - 3s 3ms/step - loss: 0.5530 - accuracy: 0.7265 - val_loss: 0.5521 - val_accuracy: 0.7287
Epoch 30/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.5542 - accuracy: 0.7264
Epoch 30: val_loss did not improve from 0.55198
852/852 [==============================] - 2s 3ms/step - loss: 0.5531 - accuracy: 0.7278 - val_loss: 0.5531 - val_accuracy: 0.7229
Epoch 31/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.5537 - accuracy: 0.7261
Epoch 31: val_loss did not improve from 0.55198
852/852 [==============================] - 2s 3ms/step - loss: 0.5528 - accuracy: 0.7267 - val_loss: 0.5520 - val_accuracy: 0.7270
Epoch 32/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.5523 - accuracy: 0.7275
Epoch 32: val_loss did not improve from 0.55198
852/852 [==============================] - 2s 3ms/step - loss: 0.5531 - accuracy: 0.7265 - val_loss: 0.5520 - val_accuracy: 0.7265
Epoch 33/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.5517 - accuracy: 0.7285
Epoch 33: val_loss improved from 0.55198 to 0.55188, saving model to three_neuron_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5526 - accuracy: 0.7275 - val_loss: 0.5519 - val_accuracy: 0.7264
Epoch 34/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.5540 - accuracy: 0.7283
Epoch 34: val_loss improved from 0.55188 to 0.55175, saving model to three_neuron_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5529 - accuracy: 0.7293 - val_loss: 0.5518 - val_accuracy: 0.7271
Epoch 35/150
819/852 [===========================&gt;..] - ETA: 0s - loss: 0.5523 - accuracy: 0.7292
Epoch 35: val_loss improved from 0.55175 to 0.55175, saving model to three_neuron_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5528 - accuracy: 0.7286 - val_loss: 0.5518 - val_accuracy: 0.7295
Epoch 36/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.5523 - accuracy: 0.7281
Epoch 36: val_loss did not improve from 0.55175
852/852 [==============================] - 2s 3ms/step - loss: 0.5526 - accuracy: 0.7273 - val_loss: 0.5518 - val_accuracy: 0.7265
Epoch 37/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.5522 - accuracy: 0.7286
Epoch 37: val_loss improved from 0.55175 to 0.55163, saving model to three_neuron_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5524 - accuracy: 0.7282 - val_loss: 0.5516 - val_accuracy: 0.7299
Epoch 38/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.5524 - accuracy: 0.7280
Epoch 38: val_loss did not improve from 0.55163
852/852 [==============================] - 2s 3ms/step - loss: 0.5525 - accuracy: 0.7279 - val_loss: 0.5521 - val_accuracy: 0.7310
Epoch 39/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.5540 - accuracy: 0.7260
Epoch 39: val_loss did not improve from 0.55163
852/852 [==============================] - 2s 3ms/step - loss: 0.5525 - accuracy: 0.7270 - val_loss: 0.5523 - val_accuracy: 0.7260
Epoch 40/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.5518 - accuracy: 0.7294
Epoch 40: val_loss improved from 0.55163 to 0.55150, saving model to three_neuron_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5526 - accuracy: 0.7278 - val_loss: 0.5515 - val_accuracy: 0.7295
Epoch 41/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.5523 - accuracy: 0.7279
Epoch 41: val_loss improved from 0.55150 to 0.55143, saving model to three_neuron_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5524 - accuracy: 0.7278 - val_loss: 0.5514 - val_accuracy: 0.7297
Epoch 42/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.5543 - accuracy: 0.7268
Epoch 42: val_loss did not improve from 0.55143
852/852 [==============================] - 2s 3ms/step - loss: 0.5526 - accuracy: 0.7284 - val_loss: 0.5518 - val_accuracy: 0.7318
Epoch 43/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.5516 - accuracy: 0.7314
Epoch 43: val_loss did not improve from 0.55143
852/852 [==============================] - 3s 3ms/step - loss: 0.5523 - accuracy: 0.7298 - val_loss: 0.5516 - val_accuracy: 0.7300
Epoch 44/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.5521 - accuracy: 0.7273
Epoch 44: val_loss improved from 0.55143 to 0.55138, saving model to three_neuron_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5527 - accuracy: 0.7276 - val_loss: 0.5514 - val_accuracy: 0.7295
Epoch 45/150
819/852 [===========================&gt;..] - ETA: 0s - loss: 0.5511 - accuracy: 0.7289
Epoch 45: val_loss did not improve from 0.55138
852/852 [==============================] - 3s 3ms/step - loss: 0.5522 - accuracy: 0.7285 - val_loss: 0.5523 - val_accuracy: 0.7313
Epoch 46/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.5537 - accuracy: 0.7283
Epoch 46: val_loss improved from 0.55138 to 0.55135, saving model to three_neuron_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5525 - accuracy: 0.7292 - val_loss: 0.5514 - val_accuracy: 0.7272
Epoch 47/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.5541 - accuracy: 0.7264
Epoch 47: val_loss did not improve from 0.55135
852/852 [==============================] - 2s 3ms/step - loss: 0.5524 - accuracy: 0.7286 - val_loss: 0.5517 - val_accuracy: 0.7313
Epoch 48/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.5503 - accuracy: 0.7303
Epoch 48: val_loss did not improve from 0.55135
852/852 [==============================] - 2s 3ms/step - loss: 0.5519 - accuracy: 0.7295 - val_loss: 0.5521 - val_accuracy: 0.7319
Epoch 49/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.5528 - accuracy: 0.7301
Epoch 49: val_loss improved from 0.55135 to 0.55129, saving model to three_neuron_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5524 - accuracy: 0.7299 - val_loss: 0.5513 - val_accuracy: 0.7266
Epoch 50/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.5525 - accuracy: 0.7284
Epoch 50: val_loss improved from 0.55129 to 0.55114, saving model to three_neuron_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5525 - accuracy: 0.7291 - val_loss: 0.5511 - val_accuracy: 0.7291
Epoch 51/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.5517 - accuracy: 0.7278
Epoch 51: val_loss did not improve from 0.55114
852/852 [==============================] - 2s 3ms/step - loss: 0.5522 - accuracy: 0.7277 - val_loss: 0.5512 - val_accuracy: 0.7309
Epoch 52/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.5514 - accuracy: 0.7276
Epoch 52: val_loss did not improve from 0.55114
852/852 [==============================] - 2s 3ms/step - loss: 0.5520 - accuracy: 0.7270 - val_loss: 0.5513 - val_accuracy: 0.7279
Epoch 53/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.5523 - accuracy: 0.7282
Epoch 53: val_loss did not improve from 0.55114
852/852 [==============================] - 2s 3ms/step - loss: 0.5520 - accuracy: 0.7284 - val_loss: 0.5518 - val_accuracy: 0.7319
Epoch 54/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.5513 - accuracy: 0.7285
Epoch 54: val_loss improved from 0.55114 to 0.55105, saving model to three_neuron_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5522 - accuracy: 0.7283 - val_loss: 0.5510 - val_accuracy: 0.7310
Epoch 55/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.5513 - accuracy: 0.7295
Epoch 55: val_loss did not improve from 0.55105
852/852 [==============================] - 2s 3ms/step - loss: 0.5522 - accuracy: 0.7285 - val_loss: 0.5512 - val_accuracy: 0.7272
Epoch 56/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.5511 - accuracy: 0.7302
Epoch 56: val_loss did not improve from 0.55105
852/852 [==============================] - 2s 3ms/step - loss: 0.5514 - accuracy: 0.7299 - val_loss: 0.5522 - val_accuracy: 0.7310
Epoch 57/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.5530 - accuracy: 0.7282
Epoch 57: val_loss did not improve from 0.55105
852/852 [==============================] - 2s 3ms/step - loss: 0.5521 - accuracy: 0.7289 - val_loss: 0.5512 - val_accuracy: 0.7266
Epoch 58/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.5509 - accuracy: 0.7304
Epoch 58: val_loss improved from 0.55105 to 0.55079, saving model to three_neuron_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5517 - accuracy: 0.7293 - val_loss: 0.5508 - val_accuracy: 0.7306
Epoch 59/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.5521 - accuracy: 0.7291
Epoch 59: val_loss did not improve from 0.55079
852/852 [==============================] - 2s 3ms/step - loss: 0.5517 - accuracy: 0.7295 - val_loss: 0.5509 - val_accuracy: 0.7309
Epoch 60/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.5530 - accuracy: 0.7270
Epoch 60: val_loss improved from 0.55079 to 0.55074, saving model to three_neuron_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5519 - accuracy: 0.7283 - val_loss: 0.5507 - val_accuracy: 0.7312
Epoch 61/150
820/852 [===========================&gt;..] - ETA: 0s - loss: 0.5543 - accuracy: 0.7270
Epoch 61: val_loss improved from 0.55074 to 0.55073, saving model to three_neuron_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5519 - accuracy: 0.7293 - val_loss: 0.5507 - val_accuracy: 0.7303
Epoch 62/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5509 - accuracy: 0.7298
Epoch 62: val_loss did not improve from 0.55073
852/852 [==============================] - 2s 3ms/step - loss: 0.5517 - accuracy: 0.7289 - val_loss: 0.5510 - val_accuracy: 0.7285
Epoch 63/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.5518 - accuracy: 0.7294
Epoch 63: val_loss improved from 0.55073 to 0.55068, saving model to three_neuron_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5517 - accuracy: 0.7303 - val_loss: 0.5507 - val_accuracy: 0.7297
Epoch 64/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.5515 - accuracy: 0.7305
Epoch 64: val_loss did not improve from 0.55068
852/852 [==============================] - 2s 3ms/step - loss: 0.5517 - accuracy: 0.7306 - val_loss: 0.5514 - val_accuracy: 0.7273
Epoch 65/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.5525 - accuracy: 0.7288
Epoch 65: val_loss did not improve from 0.55068
852/852 [==============================] - 2s 3ms/step - loss: 0.5516 - accuracy: 0.7297 - val_loss: 0.5521 - val_accuracy: 0.7304
Epoch 66/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.5522 - accuracy: 0.7279
Epoch 66: val_loss did not improve from 0.55068
852/852 [==============================] - 2s 3ms/step - loss: 0.5517 - accuracy: 0.7282 - val_loss: 0.5509 - val_accuracy: 0.7291
Epoch 67/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.5518 - accuracy: 0.7280
Epoch 67: val_loss improved from 0.55068 to 0.55066, saving model to three_neuron_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5517 - accuracy: 0.7278 - val_loss: 0.5507 - val_accuracy: 0.7320
Epoch 68/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.5515 - accuracy: 0.7307
Epoch 68: val_loss improved from 0.55066 to 0.55058, saving model to three_neuron_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5514 - accuracy: 0.7304 - val_loss: 0.5506 - val_accuracy: 0.7322
Epoch 69/150
852/852 [==============================] - ETA: 0s - loss: 0.5517 - accuracy: 0.7289
Epoch 69: val_loss did not improve from 0.55058
852/852 [==============================] - 3s 3ms/step - loss: 0.5517 - accuracy: 0.7289 - val_loss: 0.5508 - val_accuracy: 0.7275
Epoch 70/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.5515 - accuracy: 0.7271
Epoch 70: val_loss improved from 0.55058 to 0.55052, saving model to three_neuron_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5515 - accuracy: 0.7276 - val_loss: 0.5505 - val_accuracy: 0.7323
Epoch 71/150
820/852 [===========================&gt;..] - ETA: 0s - loss: 0.5518 - accuracy: 0.7299
Epoch 71: val_loss did not improve from 0.55052
852/852 [==============================] - 2s 3ms/step - loss: 0.5512 - accuracy: 0.7302 - val_loss: 0.5506 - val_accuracy: 0.7312
Epoch 72/150
820/852 [===========================&gt;..] - ETA: 0s - loss: 0.5518 - accuracy: 0.7309
Epoch 72: val_loss did not improve from 0.55052
852/852 [==============================] - 2s 3ms/step - loss: 0.5516 - accuracy: 0.7299 - val_loss: 0.5509 - val_accuracy: 0.7279
Epoch 73/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.5507 - accuracy: 0.7318
Epoch 73: val_loss improved from 0.55052 to 0.55032, saving model to three_neuron_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5512 - accuracy: 0.7312 - val_loss: 0.5503 - val_accuracy: 0.7307
Epoch 74/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.5504 - accuracy: 0.7283
Epoch 74: val_loss did not improve from 0.55032
852/852 [==============================] - 2s 3ms/step - loss: 0.5513 - accuracy: 0.7279 - val_loss: 0.5505 - val_accuracy: 0.7307
Epoch 75/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.5517 - accuracy: 0.7287
Epoch 75: val_loss did not improve from 0.55032
852/852 [==============================] - 2s 3ms/step - loss: 0.5514 - accuracy: 0.7289 - val_loss: 0.5506 - val_accuracy: 0.7291
Epoch 76/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.5510 - accuracy: 0.7301
Epoch 76: val_loss improved from 0.55032 to 0.55014, saving model to three_neuron_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5513 - accuracy: 0.7297 - val_loss: 0.5501 - val_accuracy: 0.7322
Epoch 77/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.5515 - accuracy: 0.7303
Epoch 77: val_loss did not improve from 0.55014
852/852 [==============================] - 2s 3ms/step - loss: 0.5512 - accuracy: 0.7303 - val_loss: 0.5502 - val_accuracy: 0.7320
Epoch 78/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.5495 - accuracy: 0.7303
Epoch 78: val_loss did not improve from 0.55014
852/852 [==============================] - 2s 3ms/step - loss: 0.5510 - accuracy: 0.7298 - val_loss: 0.5508 - val_accuracy: 0.7306
Epoch 79/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.5512 - accuracy: 0.7313
Epoch 79: val_loss did not improve from 0.55014
852/852 [==============================] - 2s 3ms/step - loss: 0.5513 - accuracy: 0.7309 - val_loss: 0.5504 - val_accuracy: 0.7316
Epoch 80/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.5509 - accuracy: 0.7300
Epoch 80: val_loss improved from 0.55014 to 0.55006, saving model to three_neuron_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5510 - accuracy: 0.7300 - val_loss: 0.5501 - val_accuracy: 0.7296
Epoch 81/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5523 - accuracy: 0.7287
Epoch 81: val_loss did not improve from 0.55006
852/852 [==============================] - 2s 3ms/step - loss: 0.5511 - accuracy: 0.7296 - val_loss: 0.5503 - val_accuracy: 0.7279
Epoch 82/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5509 - accuracy: 0.7323
Epoch 82: val_loss did not improve from 0.55006
852/852 [==============================] - 2s 3ms/step - loss: 0.5510 - accuracy: 0.7320 - val_loss: 0.5508 - val_accuracy: 0.7325
Epoch 83/150
820/852 [===========================&gt;..] - ETA: 0s - loss: 0.5523 - accuracy: 0.7305
Epoch 83: val_loss improved from 0.55006 to 0.54990, saving model to three_neuron_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5511 - accuracy: 0.7312 - val_loss: 0.5499 - val_accuracy: 0.7322
Epoch 84/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.5512 - accuracy: 0.7306
Epoch 84: val_loss did not improve from 0.54990
852/852 [==============================] - 2s 3ms/step - loss: 0.5512 - accuracy: 0.7304 - val_loss: 0.5500 - val_accuracy: 0.7317
Epoch 85/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.5514 - accuracy: 0.7284
Epoch 85: val_loss did not improve from 0.54990
852/852 [==============================] - 2s 3ms/step - loss: 0.5510 - accuracy: 0.7292 - val_loss: 0.5500 - val_accuracy: 0.7313
Epoch 86/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.5511 - accuracy: 0.7298
Epoch 86: val_loss did not improve from 0.54990
852/852 [==============================] - 2s 3ms/step - loss: 0.5508 - accuracy: 0.7296 - val_loss: 0.5500 - val_accuracy: 0.7304
Epoch 87/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.5516 - accuracy: 0.7296
Epoch 87: val_loss did not improve from 0.54990
852/852 [==============================] - 2s 3ms/step - loss: 0.5512 - accuracy: 0.7300 - val_loss: 0.5501 - val_accuracy: 0.7324
Epoch 88/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.5521 - accuracy: 0.7298
Epoch 88: val_loss improved from 0.54990 to 0.54976, saving model to three_neuron_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5508 - accuracy: 0.7307 - val_loss: 0.5498 - val_accuracy: 0.7327
Epoch 89/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.5492 - accuracy: 0.7330
Epoch 89: val_loss did not improve from 0.54976
852/852 [==============================] - 3s 3ms/step - loss: 0.5506 - accuracy: 0.7316 - val_loss: 0.5505 - val_accuracy: 0.7319
Epoch 90/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5508 - accuracy: 0.7299
Epoch 90: val_loss did not improve from 0.54976
852/852 [==============================] - 2s 3ms/step - loss: 0.5510 - accuracy: 0.7298 - val_loss: 0.5498 - val_accuracy: 0.7307
Epoch 91/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.5510 - accuracy: 0.7303
Epoch 91: val_loss improved from 0.54976 to 0.54968, saving model to three_neuron_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5510 - accuracy: 0.7304 - val_loss: 0.5497 - val_accuracy: 0.7324
Epoch 92/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.5508 - accuracy: 0.7302
Epoch 92: val_loss did not improve from 0.54968
852/852 [==============================] - 2s 3ms/step - loss: 0.5508 - accuracy: 0.7300 - val_loss: 0.5497 - val_accuracy: 0.7325
Epoch 93/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.5504 - accuracy: 0.7323
Epoch 93: val_loss did not improve from 0.54968
852/852 [==============================] - 2s 3ms/step - loss: 0.5510 - accuracy: 0.7317 - val_loss: 0.5498 - val_accuracy: 0.7323
Epoch 94/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.5511 - accuracy: 0.7310
Epoch 94: val_loss did not improve from 0.54968
852/852 [==============================] - 2s 3ms/step - loss: 0.5506 - accuracy: 0.7312 - val_loss: 0.5497 - val_accuracy: 0.7309
Epoch 95/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.5497 - accuracy: 0.7322
Epoch 95: val_loss did not improve from 0.54968
852/852 [==============================] - 2s 3ms/step - loss: 0.5508 - accuracy: 0.7312 - val_loss: 0.5518 - val_accuracy: 0.7319
Epoch 96/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.5509 - accuracy: 0.7305
Epoch 96: val_loss did not improve from 0.54968
852/852 [==============================] - 2s 3ms/step - loss: 0.5506 - accuracy: 0.7306 - val_loss: 0.5498 - val_accuracy: 0.7319
Epoch 97/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.5510 - accuracy: 0.7292
Epoch 97: val_loss did not improve from 0.54968
852/852 [==============================] - 2s 3ms/step - loss: 0.5507 - accuracy: 0.7296 - val_loss: 0.5498 - val_accuracy: 0.7295
Epoch 98/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5512 - accuracy: 0.7302
Epoch 98: val_loss improved from 0.54968 to 0.54960, saving model to three_neuron_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5508 - accuracy: 0.7304 - val_loss: 0.5496 - val_accuracy: 0.7324
Epoch 99/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.5516 - accuracy: 0.7293
Epoch 99: val_loss did not improve from 0.54960
852/852 [==============================] - 2s 3ms/step - loss: 0.5505 - accuracy: 0.7298 - val_loss: 0.5504 - val_accuracy: 0.7277
Epoch 100/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.5503 - accuracy: 0.7300
Epoch 100: val_loss did not improve from 0.54960
852/852 [==============================] - 2s 3ms/step - loss: 0.5507 - accuracy: 0.7296 - val_loss: 0.5497 - val_accuracy: 0.7325
Epoch 101/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.5495 - accuracy: 0.7312
Epoch 101: val_loss did not improve from 0.54960
852/852 [==============================] - 2s 3ms/step - loss: 0.5505 - accuracy: 0.7306 - val_loss: 0.5498 - val_accuracy: 0.7319
Epoch 102/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5504 - accuracy: 0.7292
Epoch 102: val_loss improved from 0.54960 to 0.54953, saving model to three_neuron_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5506 - accuracy: 0.7293 - val_loss: 0.5495 - val_accuracy: 0.7330
Epoch 103/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.5508 - accuracy: 0.7298
Epoch 103: val_loss did not improve from 0.54953
852/852 [==============================] - 2s 3ms/step - loss: 0.5508 - accuracy: 0.7298 - val_loss: 0.5498 - val_accuracy: 0.7320
Epoch 104/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.5493 - accuracy: 0.7316
Epoch 104: val_loss did not improve from 0.54953
852/852 [==============================] - 2s 3ms/step - loss: 0.5506 - accuracy: 0.7305 - val_loss: 0.5496 - val_accuracy: 0.7304
Epoch 105/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5508 - accuracy: 0.7293
Epoch 105: val_loss did not improve from 0.54953
852/852 [==============================] - 2s 3ms/step - loss: 0.5506 - accuracy: 0.7293 - val_loss: 0.5504 - val_accuracy: 0.7298
Epoch 106/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.5502 - accuracy: 0.7309
Epoch 106: val_loss improved from 0.54953 to 0.54938, saving model to three_neuron_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5507 - accuracy: 0.7303 - val_loss: 0.5494 - val_accuracy: 0.7318
Epoch 107/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.5506 - accuracy: 0.7305
Epoch 107: val_loss did not improve from 0.54938
852/852 [==============================] - 2s 3ms/step - loss: 0.5505 - accuracy: 0.7304 - val_loss: 0.5499 - val_accuracy: 0.7322
Epoch 108/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.5510 - accuracy: 0.7303
Epoch 108: val_loss did not improve from 0.54938
852/852 [==============================] - 2s 3ms/step - loss: 0.5506 - accuracy: 0.7303 - val_loss: 0.5496 - val_accuracy: 0.7302
Epoch 109/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.5503 - accuracy: 0.7325
Epoch 109: val_loss did not improve from 0.54938
852/852 [==============================] - 3s 3ms/step - loss: 0.5504 - accuracy: 0.7325 - val_loss: 0.5494 - val_accuracy: 0.7322
Epoch 110/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.5510 - accuracy: 0.7295
Epoch 110: val_loss did not improve from 0.54938
852/852 [==============================] - 2s 3ms/step - loss: 0.5505 - accuracy: 0.7295 - val_loss: 0.5495 - val_accuracy: 0.7339
Epoch 111/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.5496 - accuracy: 0.7328
Epoch 111: val_loss improved from 0.54938 to 0.54932, saving model to three_neuron_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5502 - accuracy: 0.7327 - val_loss: 0.5493 - val_accuracy: 0.7332
Epoch 112/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.5515 - accuracy: 0.7312
Epoch 112: val_loss improved from 0.54932 to 0.54928, saving model to three_neuron_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5502 - accuracy: 0.7320 - val_loss: 0.5493 - val_accuracy: 0.7344
Epoch 113/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.5506 - accuracy: 0.7312
Epoch 113: val_loss did not improve from 0.54928
852/852 [==============================] - 2s 3ms/step - loss: 0.5505 - accuracy: 0.7316 - val_loss: 0.5494 - val_accuracy: 0.7336
Epoch 114/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.5497 - accuracy: 0.7306
Epoch 114: val_loss did not improve from 0.54928
852/852 [==============================] - 2s 3ms/step - loss: 0.5502 - accuracy: 0.7307 - val_loss: 0.5497 - val_accuracy: 0.7327
Epoch 115/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.5516 - accuracy: 0.7297
Epoch 115: val_loss did not improve from 0.54928
852/852 [==============================] - 3s 3ms/step - loss: 0.5504 - accuracy: 0.7306 - val_loss: 0.5499 - val_accuracy: 0.7297
Epoch 116/150
819/852 [===========================&gt;..] - ETA: 0s - loss: 0.5507 - accuracy: 0.7311
Epoch 116: val_loss improved from 0.54928 to 0.54918, saving model to three_neuron_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5506 - accuracy: 0.7316 - val_loss: 0.5492 - val_accuracy: 0.7329
Epoch 117/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.5501 - accuracy: 0.7301
Epoch 117: val_loss did not improve from 0.54918
852/852 [==============================] - 2s 3ms/step - loss: 0.5504 - accuracy: 0.7307 - val_loss: 0.5494 - val_accuracy: 0.7340
Epoch 118/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.5503 - accuracy: 0.7317
Epoch 118: val_loss improved from 0.54918 to 0.54914, saving model to three_neuron_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5505 - accuracy: 0.7319 - val_loss: 0.5491 - val_accuracy: 0.7337
Epoch 119/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.5494 - accuracy: 0.7331
Epoch 119: val_loss did not improve from 0.54914
852/852 [==============================] - 2s 3ms/step - loss: 0.5504 - accuracy: 0.7317 - val_loss: 0.5502 - val_accuracy: 0.7297
Epoch 120/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.5502 - accuracy: 0.7329
Epoch 120: val_loss did not improve from 0.54914
852/852 [==============================] - 2s 3ms/step - loss: 0.5503 - accuracy: 0.7327 - val_loss: 0.5492 - val_accuracy: 0.7345
Epoch 121/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5512 - accuracy: 0.7324
Epoch 121: val_loss did not improve from 0.54914
852/852 [==============================] - 2s 3ms/step - loss: 0.5505 - accuracy: 0.7329 - val_loss: 0.5492 - val_accuracy: 0.7324
Epoch 122/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.5508 - accuracy: 0.7302
Epoch 122: val_loss did not improve from 0.54914
852/852 [==============================] - 3s 3ms/step - loss: 0.5504 - accuracy: 0.7305 - val_loss: 0.5492 - val_accuracy: 0.7350
Epoch 123/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.5507 - accuracy: 0.7335
Epoch 123: val_loss did not improve from 0.54914
852/852 [==============================] - 2s 3ms/step - loss: 0.5503 - accuracy: 0.7333 - val_loss: 0.5492 - val_accuracy: 0.7327
Epoch 124/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.5504 - accuracy: 0.7316
Epoch 124: val_loss did not improve from 0.54914
852/852 [==============================] - 2s 3ms/step - loss: 0.5504 - accuracy: 0.7318 - val_loss: 0.5492 - val_accuracy: 0.7347
Epoch 125/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.5493 - accuracy: 0.7335
Epoch 125: val_loss did not improve from 0.54914
852/852 [==============================] - 2s 3ms/step - loss: 0.5504 - accuracy: 0.7329 - val_loss: 0.5493 - val_accuracy: 0.7329
Epoch 126/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.5512 - accuracy: 0.7319
Epoch 126: val_loss did not improve from 0.54914
852/852 [==============================] - 2s 3ms/step - loss: 0.5503 - accuracy: 0.7327 - val_loss: 0.5493 - val_accuracy: 0.7320
Epoch 127/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.5499 - accuracy: 0.7308
Epoch 127: val_loss did not improve from 0.54914
852/852 [==============================] - 2s 3ms/step - loss: 0.5502 - accuracy: 0.7304 - val_loss: 0.5495 - val_accuracy: 0.7307
Epoch 128/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.5509 - accuracy: 0.7316
Epoch 128: val_loss improved from 0.54914 to 0.54905, saving model to three_neuron_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5503 - accuracy: 0.7319 - val_loss: 0.5491 - val_accuracy: 0.7354
Epoch 129/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.5519 - accuracy: 0.7310
Epoch 129: val_loss did not improve from 0.54905
852/852 [==============================] - 2s 3ms/step - loss: 0.5504 - accuracy: 0.7324 - val_loss: 0.5492 - val_accuracy: 0.7337
Epoch 130/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5501 - accuracy: 0.7326
Epoch 130: val_loss did not improve from 0.54905
852/852 [==============================] - 2s 3ms/step - loss: 0.5504 - accuracy: 0.7327 - val_loss: 0.5491 - val_accuracy: 0.7341
Epoch 131/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.5505 - accuracy: 0.7320
Epoch 131: val_loss did not improve from 0.54905
852/852 [==============================] - 2s 3ms/step - loss: 0.5500 - accuracy: 0.7323 - val_loss: 0.5499 - val_accuracy: 0.7309
Epoch 132/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.5507 - accuracy: 0.7322
Epoch 132: val_loss did not improve from 0.54905
852/852 [==============================] - 2s 3ms/step - loss: 0.5499 - accuracy: 0.7324 - val_loss: 0.5491 - val_accuracy: 0.7345
Epoch 133/150
818/852 [===========================&gt;..] - ETA: 0s - loss: 0.5501 - accuracy: 0.7320
Epoch 133: val_loss did not improve from 0.54905
852/852 [==============================] - 2s 3ms/step - loss: 0.5502 - accuracy: 0.7312 - val_loss: 0.5492 - val_accuracy: 0.7341
Epoch 134/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.5509 - accuracy: 0.7342
Epoch 134: val_loss did not improve from 0.54905
852/852 [==============================] - 2s 3ms/step - loss: 0.5502 - accuracy: 0.7350 - val_loss: 0.5496 - val_accuracy: 0.7317
Epoch 135/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.5495 - accuracy: 0.7325
Epoch 135: val_loss did not improve from 0.54905
852/852 [==============================] - 2s 3ms/step - loss: 0.5504 - accuracy: 0.7316 - val_loss: 0.5492 - val_accuracy: 0.7338
Epoch 136/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.5503 - accuracy: 0.7309
Epoch 136: val_loss did not improve from 0.54905
852/852 [==============================] - 2s 3ms/step - loss: 0.5503 - accuracy: 0.7310 - val_loss: 0.5493 - val_accuracy: 0.7325
Epoch 137/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5497 - accuracy: 0.7306
Epoch 137: val_loss did not improve from 0.54905
852/852 [==============================] - 2s 3ms/step - loss: 0.5502 - accuracy: 0.7300 - val_loss: 0.5497 - val_accuracy: 0.7311
Epoch 138/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.5504 - accuracy: 0.7325
Epoch 138: val_loss did not improve from 0.54905
852/852 [==============================] - 2s 3ms/step - loss: 0.5500 - accuracy: 0.7320 - val_loss: 0.5492 - val_accuracy: 0.7323
Epoch 139/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.5518 - accuracy: 0.7291
Epoch 139: val_loss did not improve from 0.54905
852/852 [==============================] - 2s 3ms/step - loss: 0.5501 - accuracy: 0.7306 - val_loss: 0.5491 - val_accuracy: 0.7338
Epoch 140/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.5514 - accuracy: 0.7305
Epoch 140: val_loss improved from 0.54905 to 0.54892, saving model to three_neuron_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5502 - accuracy: 0.7325 - val_loss: 0.5489 - val_accuracy: 0.7347
Epoch 141/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5501 - accuracy: 0.7324
Epoch 141: val_loss did not improve from 0.54892
852/852 [==============================] - 3s 3ms/step - loss: 0.5501 - accuracy: 0.7322 - val_loss: 0.5490 - val_accuracy: 0.7340
Epoch 142/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.5500 - accuracy: 0.7351
Epoch 142: val_loss did not improve from 0.54892
852/852 [==============================] - 2s 3ms/step - loss: 0.5502 - accuracy: 0.7343 - val_loss: 0.5490 - val_accuracy: 0.7330
Epoch 143/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.5505 - accuracy: 0.7338
Epoch 143: val_loss improved from 0.54892 to 0.54885, saving model to three_neuron_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5504 - accuracy: 0.7336 - val_loss: 0.5489 - val_accuracy: 0.7344
Epoch 144/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.5504 - accuracy: 0.7310
Epoch 144: val_loss did not improve from 0.54885
852/852 [==============================] - 3s 3ms/step - loss: 0.5501 - accuracy: 0.7311 - val_loss: 0.5491 - val_accuracy: 0.7334
Epoch 145/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.5501 - accuracy: 0.7332
Epoch 145: val_loss did not improve from 0.54885
852/852 [==============================] - 2s 3ms/step - loss: 0.5502 - accuracy: 0.7334 - val_loss: 0.5493 - val_accuracy: 0.7337
Epoch 146/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.5490 - accuracy: 0.7336
Epoch 146: val_loss did not improve from 0.54885
852/852 [==============================] - 2s 3ms/step - loss: 0.5499 - accuracy: 0.7330 - val_loss: 0.5491 - val_accuracy: 0.7327
Epoch 147/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.5498 - accuracy: 0.7328
Epoch 147: val_loss did not improve from 0.54885
852/852 [==============================] - 2s 3ms/step - loss: 0.5501 - accuracy: 0.7327 - val_loss: 0.5491 - val_accuracy: 0.7336
Epoch 148/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.5501 - accuracy: 0.7337
Epoch 148: val_loss did not improve from 0.54885
852/852 [==============================] - 2s 3ms/step - loss: 0.5501 - accuracy: 0.7338 - val_loss: 0.5490 - val_accuracy: 0.7338
Epoch 149/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.5505 - accuracy: 0.7329
Epoch 149: val_loss did not improve from 0.54885
852/852 [==============================] - 2s 3ms/step - loss: 0.5501 - accuracy: 0.7332 - val_loss: 0.5494 - val_accuracy: 0.7324
Epoch 150/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.5510 - accuracy: 0.7321
Epoch 150: val_loss did not improve from 0.54885
852/852 [==============================] - 2s 3ms/step - loss: 0.5502 - accuracy: 0.7318 - val_loss: 0.5493 - val_accuracy: 0.7339
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=205b19ab-b782-42c3-bae5-9e61edd02e8e">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [65]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># constructing a 3 neuron model</span>
<span class="n">history_3</span> <span class="o">=</span> <span class="n">three_neuron</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">callback_a</span><span class="p">,</span> <span class="n">callback_b</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.5490 - accuracy: 0.7327
Epoch 1: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5497 - accuracy: 0.7317 - val_loss: 0.5508 - val_accuracy: 0.7318
Epoch 2/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.5511 - accuracy: 0.7329
Epoch 2: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5503 - accuracy: 0.7334 - val_loss: 0.5488 - val_accuracy: 0.7351
Epoch 3/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.5486 - accuracy: 0.7350
Epoch 3: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5501 - accuracy: 0.7336 - val_loss: 0.5490 - val_accuracy: 0.7346
Epoch 4/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.5509 - accuracy: 0.7316
Epoch 4: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5501 - accuracy: 0.7326 - val_loss: 0.5495 - val_accuracy: 0.7329
Epoch 5/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.5504 - accuracy: 0.7325
Epoch 5: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5502 - accuracy: 0.7332 - val_loss: 0.5488 - val_accuracy: 0.7361
Epoch 6/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5502 - accuracy: 0.7328
Epoch 6: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5498 - accuracy: 0.7332 - val_loss: 0.5492 - val_accuracy: 0.7359
Epoch 7/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.5495 - accuracy: 0.7375
Epoch 7: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5502 - accuracy: 0.7365 - val_loss: 0.5488 - val_accuracy: 0.7344
Epoch 8/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.5502 - accuracy: 0.7328
Epoch 8: val_loss did not improve from 0.42561
852/852 [==============================] - 3s 3ms/step - loss: 0.5502 - accuracy: 0.7332 - val_loss: 0.5489 - val_accuracy: 0.7340
Epoch 9/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.5487 - accuracy: 0.7346
Epoch 9: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5497 - accuracy: 0.7344 - val_loss: 0.5488 - val_accuracy: 0.7347
Epoch 10/150
820/852 [===========================&gt;..] - ETA: 0s - loss: 0.5497 - accuracy: 0.7340
Epoch 10: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5501 - accuracy: 0.7339 - val_loss: 0.5488 - val_accuracy: 0.7351
Epoch 11/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.5507 - accuracy: 0.7305
Epoch 11: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5501 - accuracy: 0.7311 - val_loss: 0.5487 - val_accuracy: 0.7350
Epoch 12/150
803/852 [===========================&gt;..] - ETA: 0s - loss: 0.5504 - accuracy: 0.7316
Epoch 12: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5498 - accuracy: 0.7322 - val_loss: 0.5488 - val_accuracy: 0.7347
Epoch 13/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.5495 - accuracy: 0.7327
Epoch 13: val_loss did not improve from 0.42561
852/852 [==============================] - 3s 4ms/step - loss: 0.5497 - accuracy: 0.7324 - val_loss: 0.5496 - val_accuracy: 0.7305
Epoch 14/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.5506 - accuracy: 0.7318
Epoch 14: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 3ms/step - loss: 0.5498 - accuracy: 0.7324 - val_loss: 0.5494 - val_accuracy: 0.7312
Epoch 15/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.5503 - accuracy: 0.7324
Epoch 15: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5501 - accuracy: 0.7327 - val_loss: 0.5493 - val_accuracy: 0.7341
Epoch 16/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.5491 - accuracy: 0.7338
Epoch 16: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5499 - accuracy: 0.7336 - val_loss: 0.5490 - val_accuracy: 0.7339
Epoch 17/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.5503 - accuracy: 0.7328
Epoch 17: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5500 - accuracy: 0.7331 - val_loss: 0.5490 - val_accuracy: 0.7343
Epoch 18/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5498 - accuracy: 0.7341
Epoch 18: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5498 - accuracy: 0.7344 - val_loss: 0.5488 - val_accuracy: 0.7352
Epoch 19/150
809/852 [===========================&gt;..] - ETA: 0s - loss: 0.5518 - accuracy: 0.7297
Epoch 19: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5496 - accuracy: 0.7319 - val_loss: 0.5495 - val_accuracy: 0.7309
Epoch 20/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.5511 - accuracy: 0.7322
Epoch 20: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5500 - accuracy: 0.7336 - val_loss: 0.5492 - val_accuracy: 0.7318
Epoch 21/150
809/852 [===========================&gt;..] - ETA: 0s - loss: 0.5512 - accuracy: 0.7330
Epoch 21: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5499 - accuracy: 0.7337 - val_loss: 0.5493 - val_accuracy: 0.7343
Epoch 22/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.5499 - accuracy: 0.7333
Epoch 22: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5500 - accuracy: 0.7331 - val_loss: 0.5488 - val_accuracy: 0.7343
Epoch 23/150
806/852 [===========================&gt;..] - ETA: 0s - loss: 0.5496 - accuracy: 0.7336
Epoch 23: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5497 - accuracy: 0.7344 - val_loss: 0.5487 - val_accuracy: 0.7347
Epoch 24/150
805/852 [===========================&gt;..] - ETA: 0s - loss: 0.5492 - accuracy: 0.7334
Epoch 24: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5497 - accuracy: 0.7333 - val_loss: 0.5494 - val_accuracy: 0.7345
Epoch 25/150
802/852 [===========================&gt;..] - ETA: 0s - loss: 0.5489 - accuracy: 0.7325
Epoch 25: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5500 - accuracy: 0.7316 - val_loss: 0.5492 - val_accuracy: 0.7339
Epoch 26/150
812/852 [===========================&gt;..] - ETA: 0s - loss: 0.5489 - accuracy: 0.7335
Epoch 26: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5496 - accuracy: 0.7329 - val_loss: 0.5503 - val_accuracy: 0.7334
Epoch 27/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.5506 - accuracy: 0.7335
Epoch 27: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5500 - accuracy: 0.7336 - val_loss: 0.5494 - val_accuracy: 0.7307
Epoch 28/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.5499 - accuracy: 0.7325
Epoch 28: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5497 - accuracy: 0.7330 - val_loss: 0.5488 - val_accuracy: 0.7344
Epoch 29/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.5496 - accuracy: 0.7334
Epoch 29: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5496 - accuracy: 0.7333 - val_loss: 0.5487 - val_accuracy: 0.7353
Epoch 30/150
809/852 [===========================&gt;..] - ETA: 0s - loss: 0.5511 - accuracy: 0.7329
Epoch 30: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5496 - accuracy: 0.7337 - val_loss: 0.5489 - val_accuracy: 0.7341
Epoch 31/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.5508 - accuracy: 0.7334
Epoch 31: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5500 - accuracy: 0.7339 - val_loss: 0.5489 - val_accuracy: 0.7340
Epoch 32/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.5476 - accuracy: 0.7336
Epoch 32: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5498 - accuracy: 0.7327 - val_loss: 0.5488 - val_accuracy: 0.7341
Epoch 33/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.5500 - accuracy: 0.7340
Epoch 33: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5499 - accuracy: 0.7343 - val_loss: 0.5489 - val_accuracy: 0.7350
Epoch 34/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.5506 - accuracy: 0.7315
Epoch 34: val_loss did not improve from 0.42561
852/852 [==============================] - 3s 3ms/step - loss: 0.5499 - accuracy: 0.7326 - val_loss: 0.5488 - val_accuracy: 0.7346
Epoch 35/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.5500 - accuracy: 0.7326
Epoch 35: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5499 - accuracy: 0.7329 - val_loss: 0.5486 - val_accuracy: 0.7352
Epoch 36/150
808/852 [===========================&gt;..] - ETA: 0s - loss: 0.5494 - accuracy: 0.7337
Epoch 36: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5499 - accuracy: 0.7339 - val_loss: 0.5492 - val_accuracy: 0.7349
Epoch 37/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.5490 - accuracy: 0.7334
Epoch 37: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5498 - accuracy: 0.7332 - val_loss: 0.5488 - val_accuracy: 0.7346
Epoch 38/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.5499 - accuracy: 0.7356
Epoch 38: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5499 - accuracy: 0.7356 - val_loss: 0.5490 - val_accuracy: 0.7341
Epoch 39/150
809/852 [===========================&gt;..] - ETA: 0s - loss: 0.5495 - accuracy: 0.7344
Epoch 39: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5499 - accuracy: 0.7339 - val_loss: 0.5488 - val_accuracy: 0.7354
Epoch 40/150
819/852 [===========================&gt;..] - ETA: 0s - loss: 0.5470 - accuracy: 0.7348
Epoch 40: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5499 - accuracy: 0.7332 - val_loss: 0.5496 - val_accuracy: 0.7361
Epoch 41/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.5503 - accuracy: 0.7330
Epoch 41: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5493 - accuracy: 0.7340 - val_loss: 0.5506 - val_accuracy: 0.7284
Epoch 42/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5499 - accuracy: 0.7333
Epoch 42: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5499 - accuracy: 0.7330 - val_loss: 0.5490 - val_accuracy: 0.7340
Epoch 43/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.5499 - accuracy: 0.7339
Epoch 43: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5498 - accuracy: 0.7341 - val_loss: 0.5490 - val_accuracy: 0.7332
Epoch 44/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.5491 - accuracy: 0.7344
Epoch 44: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5498 - accuracy: 0.7343 - val_loss: 0.5491 - val_accuracy: 0.7357
Epoch 45/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.5498 - accuracy: 0.7336
Epoch 45: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5494 - accuracy: 0.7340 - val_loss: 0.5491 - val_accuracy: 0.7339
Epoch 46/150
813/852 [===========================&gt;..] - ETA: 0s - loss: 0.5508 - accuracy: 0.7336
Epoch 46: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5500 - accuracy: 0.7340 - val_loss: 0.5493 - val_accuracy: 0.7344
Epoch 47/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.5489 - accuracy: 0.7346
Epoch 47: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5501 - accuracy: 0.7338 - val_loss: 0.5487 - val_accuracy: 0.7344
Epoch 48/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.5495 - accuracy: 0.7350
Epoch 48: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5498 - accuracy: 0.7350 - val_loss: 0.5486 - val_accuracy: 0.7361
Epoch 49/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.5496 - accuracy: 0.7338
Epoch 49: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5498 - accuracy: 0.7334 - val_loss: 0.5494 - val_accuracy: 0.7331
Epoch 50/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.5498 - accuracy: 0.7342
Epoch 50: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5500 - accuracy: 0.7343 - val_loss: 0.5488 - val_accuracy: 0.7346
Epoch 51/150
801/852 [===========================&gt;..] - ETA: 0s - loss: 0.5496 - accuracy: 0.7363
Epoch 51: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5497 - accuracy: 0.7356 - val_loss: 0.5488 - val_accuracy: 0.7330
Epoch 52/150
808/852 [===========================&gt;..] - ETA: 0s - loss: 0.5490 - accuracy: 0.7344
Epoch 52: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5498 - accuracy: 0.7337 - val_loss: 0.5490 - val_accuracy: 0.7339
Epoch 53/150
819/852 [===========================&gt;..] - ETA: 0s - loss: 0.5506 - accuracy: 0.7335
Epoch 53: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5498 - accuracy: 0.7344 - val_loss: 0.5486 - val_accuracy: 0.7353
Epoch 54/150
815/852 [===========================&gt;..] - ETA: 0s - loss: 0.5500 - accuracy: 0.7334
Epoch 54: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5498 - accuracy: 0.7341 - val_loss: 0.5486 - val_accuracy: 0.7350
Epoch 55/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.5499 - accuracy: 0.7339
Epoch 55: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5499 - accuracy: 0.7333 - val_loss: 0.5485 - val_accuracy: 0.7353
Epoch 56/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.5502 - accuracy: 0.7339
Epoch 56: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5497 - accuracy: 0.7344 - val_loss: 0.5486 - val_accuracy: 0.7360
Epoch 57/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.5508 - accuracy: 0.7318
Epoch 57: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5499 - accuracy: 0.7323 - val_loss: 0.5485 - val_accuracy: 0.7363
Epoch 58/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.5496 - accuracy: 0.7335
Epoch 58: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5498 - accuracy: 0.7334 - val_loss: 0.5486 - val_accuracy: 0.7356
Epoch 59/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.5499 - accuracy: 0.7346
Epoch 59: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5498 - accuracy: 0.7344 - val_loss: 0.5485 - val_accuracy: 0.7356
Epoch 60/150
813/852 [===========================&gt;..] - ETA: 0s - loss: 0.5496 - accuracy: 0.7338
Epoch 60: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5495 - accuracy: 0.7343 - val_loss: 0.5488 - val_accuracy: 0.7343
Epoch 61/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.5483 - accuracy: 0.7326
Epoch 61: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5494 - accuracy: 0.7322 - val_loss: 0.5491 - val_accuracy: 0.7358
Epoch 62/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.5513 - accuracy: 0.7322
Epoch 62: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5496 - accuracy: 0.7337 - val_loss: 0.5486 - val_accuracy: 0.7358
Epoch 63/150
820/852 [===========================&gt;..] - ETA: 0s - loss: 0.5484 - accuracy: 0.7371
Epoch 63: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5496 - accuracy: 0.7358 - val_loss: 0.5495 - val_accuracy: 0.7341
Epoch 64/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5501 - accuracy: 0.7335
Epoch 64: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5498 - accuracy: 0.7338 - val_loss: 0.5487 - val_accuracy: 0.7349
Epoch 65/150
813/852 [===========================&gt;..] - ETA: 0s - loss: 0.5493 - accuracy: 0.7342
Epoch 65: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5497 - accuracy: 0.7339 - val_loss: 0.5485 - val_accuracy: 0.7359
Epoch 66/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.5496 - accuracy: 0.7345
Epoch 66: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5498 - accuracy: 0.7336 - val_loss: 0.5495 - val_accuracy: 0.7310
Epoch 67/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.5491 - accuracy: 0.7352
Epoch 67: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5496 - accuracy: 0.7340 - val_loss: 0.5497 - val_accuracy: 0.7319
Epoch 68/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.5470 - accuracy: 0.7368
Epoch 68: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5494 - accuracy: 0.7347 - val_loss: 0.5506 - val_accuracy: 0.7327
Epoch 69/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.5503 - accuracy: 0.7351
Epoch 69: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5497 - accuracy: 0.7354 - val_loss: 0.5488 - val_accuracy: 0.7343
Epoch 70/150
852/852 [==============================] - ETA: 0s - loss: 0.5498 - accuracy: 0.7343
Epoch 70: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5498 - accuracy: 0.7343 - val_loss: 0.5487 - val_accuracy: 0.7359
Epoch 71/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.5501 - accuracy: 0.7345
Epoch 71: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5497 - accuracy: 0.7345 - val_loss: 0.5490 - val_accuracy: 0.7338
Epoch 72/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.5502 - accuracy: 0.7350
Epoch 72: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5498 - accuracy: 0.7351 - val_loss: 0.5486 - val_accuracy: 0.7352
Epoch 73/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.5494 - accuracy: 0.7333
Epoch 73: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5496 - accuracy: 0.7332 - val_loss: 0.5488 - val_accuracy: 0.7345
Epoch 74/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.5514 - accuracy: 0.7311
Epoch 74: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5498 - accuracy: 0.7327 - val_loss: 0.5485 - val_accuracy: 0.7351
Epoch 75/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.5513 - accuracy: 0.7346
Epoch 75: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5499 - accuracy: 0.7354 - val_loss: 0.5486 - val_accuracy: 0.7359
Epoch 76/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.5511 - accuracy: 0.7331
Epoch 76: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5496 - accuracy: 0.7343 - val_loss: 0.5485 - val_accuracy: 0.7356
Epoch 77/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.5481 - accuracy: 0.7361
Epoch 77: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5494 - accuracy: 0.7350 - val_loss: 0.5497 - val_accuracy: 0.7351
Epoch 78/150
808/852 [===========================&gt;..] - ETA: 0s - loss: 0.5512 - accuracy: 0.7335
Epoch 78: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5499 - accuracy: 0.7352 - val_loss: 0.5488 - val_accuracy: 0.7347
Epoch 79/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.5491 - accuracy: 0.7341
Epoch 79: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5497 - accuracy: 0.7338 - val_loss: 0.5489 - val_accuracy: 0.7347
Epoch 80/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.5492 - accuracy: 0.7341
Epoch 80: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5497 - accuracy: 0.7341 - val_loss: 0.5485 - val_accuracy: 0.7361
Epoch 81/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.5490 - accuracy: 0.7347
Epoch 81: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5493 - accuracy: 0.7346 - val_loss: 0.5489 - val_accuracy: 0.7345
Epoch 82/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.5487 - accuracy: 0.7368
Epoch 82: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5495 - accuracy: 0.7366 - val_loss: 0.5485 - val_accuracy: 0.7353
Epoch 83/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.5496 - accuracy: 0.7350
Epoch 83: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5497 - accuracy: 0.7347 - val_loss: 0.5494 - val_accuracy: 0.7357
Epoch 84/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.5486 - accuracy: 0.7366
Epoch 84: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5497 - accuracy: 0.7356 - val_loss: 0.5484 - val_accuracy: 0.7363
Epoch 85/150
811/852 [===========================&gt;..] - ETA: 0s - loss: 0.5490 - accuracy: 0.7346
Epoch 85: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5495 - accuracy: 0.7347 - val_loss: 0.5488 - val_accuracy: 0.7351
Epoch 86/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.5492 - accuracy: 0.7344
Epoch 86: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5498 - accuracy: 0.7339 - val_loss: 0.5485 - val_accuracy: 0.7363
Epoch 87/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.5490 - accuracy: 0.7323
Epoch 87: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5495 - accuracy: 0.7322 - val_loss: 0.5504 - val_accuracy: 0.7303
Epoch 88/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.5475 - accuracy: 0.7360
Epoch 88: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5496 - accuracy: 0.7346 - val_loss: 0.5485 - val_accuracy: 0.7360
Epoch 89/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.5496 - accuracy: 0.7340
Epoch 89: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5495 - accuracy: 0.7339 - val_loss: 0.5485 - val_accuracy: 0.7352
Epoch 90/150
810/852 [===========================&gt;..] - ETA: 0s - loss: 0.5500 - accuracy: 0.7342
Epoch 90: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5497 - accuracy: 0.7343 - val_loss: 0.5485 - val_accuracy: 0.7356
Epoch 91/150
808/852 [===========================&gt;..] - ETA: 0s - loss: 0.5479 - accuracy: 0.7343
Epoch 91: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5495 - accuracy: 0.7340 - val_loss: 0.5498 - val_accuracy: 0.7349
Epoch 92/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.5483 - accuracy: 0.7344
Epoch 92: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5493 - accuracy: 0.7341 - val_loss: 0.5504 - val_accuracy: 0.7334
Epoch 93/150
806/852 [===========================&gt;..] - ETA: 0s - loss: 0.5504 - accuracy: 0.7364
Epoch 93: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5495 - accuracy: 0.7363 - val_loss: 0.5490 - val_accuracy: 0.7323
Epoch 94/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5499 - accuracy: 0.7336
Epoch 94: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5497 - accuracy: 0.7338 - val_loss: 0.5489 - val_accuracy: 0.7357
Epoch 95/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5500 - accuracy: 0.7355
Epoch 95: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5496 - accuracy: 0.7359 - val_loss: 0.5485 - val_accuracy: 0.7356
Epoch 96/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.5488 - accuracy: 0.7356
Epoch 96: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5495 - accuracy: 0.7351 - val_loss: 0.5490 - val_accuracy: 0.7356
Epoch 97/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.5489 - accuracy: 0.7341
Epoch 97: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5496 - accuracy: 0.7336 - val_loss: 0.5485 - val_accuracy: 0.7349
Epoch 98/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.5492 - accuracy: 0.7344
Epoch 98: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5497 - accuracy: 0.7346 - val_loss: 0.5483 - val_accuracy: 0.7358
Epoch 99/150
811/852 [===========================&gt;..] - ETA: 0s - loss: 0.5479 - accuracy: 0.7349
Epoch 99: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5492 - accuracy: 0.7337 - val_loss: 0.5491 - val_accuracy: 0.7322
Epoch 100/150
803/852 [===========================&gt;..] - ETA: 0s - loss: 0.5508 - accuracy: 0.7329
Epoch 100: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5496 - accuracy: 0.7341 - val_loss: 0.5496 - val_accuracy: 0.7351
Epoch 101/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.5489 - accuracy: 0.7343
Epoch 101: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5497 - accuracy: 0.7337 - val_loss: 0.5488 - val_accuracy: 0.7356
Epoch 102/150
812/852 [===========================&gt;..] - ETA: 0s - loss: 0.5473 - accuracy: 0.7369
Epoch 102: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5496 - accuracy: 0.7360 - val_loss: 0.5484 - val_accuracy: 0.7356
Epoch 103/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.5499 - accuracy: 0.7336
Epoch 103: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5495 - accuracy: 0.7338 - val_loss: 0.5483 - val_accuracy: 0.7363
Epoch 104/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5492 - accuracy: 0.7360
Epoch 104: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5495 - accuracy: 0.7359 - val_loss: 0.5483 - val_accuracy: 0.7359
Epoch 105/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.5493 - accuracy: 0.7360
Epoch 105: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5496 - accuracy: 0.7356 - val_loss: 0.5489 - val_accuracy: 0.7356
Epoch 106/150
819/852 [===========================&gt;..] - ETA: 0s - loss: 0.5496 - accuracy: 0.7353
Epoch 106: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5494 - accuracy: 0.7346 - val_loss: 0.5488 - val_accuracy: 0.7336
Epoch 107/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.5489 - accuracy: 0.7330
Epoch 107: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5493 - accuracy: 0.7333 - val_loss: 0.5494 - val_accuracy: 0.7363
Epoch 108/150
810/852 [===========================&gt;..] - ETA: 0s - loss: 0.5480 - accuracy: 0.7362
Epoch 108: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5494 - accuracy: 0.7352 - val_loss: 0.5501 - val_accuracy: 0.7343
Epoch 109/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.5497 - accuracy: 0.7355
Epoch 109: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5496 - accuracy: 0.7356 - val_loss: 0.5488 - val_accuracy: 0.7347
Epoch 110/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.5487 - accuracy: 0.7343
Epoch 110: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5495 - accuracy: 0.7331 - val_loss: 0.5488 - val_accuracy: 0.7352
Epoch 111/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.5496 - accuracy: 0.7343
Epoch 111: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5495 - accuracy: 0.7347 - val_loss: 0.5483 - val_accuracy: 0.7358
Epoch 112/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.5488 - accuracy: 0.7370
Epoch 112: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5494 - accuracy: 0.7368 - val_loss: 0.5484 - val_accuracy: 0.7352
Epoch 113/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.5493 - accuracy: 0.7354
Epoch 113: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5493 - accuracy: 0.7361 - val_loss: 0.5485 - val_accuracy: 0.7358
Epoch 114/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5496 - accuracy: 0.7334
Epoch 114: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5496 - accuracy: 0.7333 - val_loss: 0.5498 - val_accuracy: 0.7312
Epoch 115/150
852/852 [==============================] - ETA: 0s - loss: 0.5496 - accuracy: 0.7346
Epoch 115: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5496 - accuracy: 0.7346 - val_loss: 0.5483 - val_accuracy: 0.7356
Epoch 116/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5491 - accuracy: 0.7333
Epoch 116: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5496 - accuracy: 0.7331 - val_loss: 0.5486 - val_accuracy: 0.7343
Epoch 117/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5492 - accuracy: 0.7340
Epoch 117: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5494 - accuracy: 0.7338 - val_loss: 0.5487 - val_accuracy: 0.7344
Epoch 118/150
852/852 [==============================] - ETA: 0s - loss: 0.5497 - accuracy: 0.7341
Epoch 118: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5497 - accuracy: 0.7341 - val_loss: 0.5488 - val_accuracy: 0.7345
Epoch 119/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.5484 - accuracy: 0.7355
Epoch 119: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5496 - accuracy: 0.7350 - val_loss: 0.5486 - val_accuracy: 0.7333
Epoch 120/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.5485 - accuracy: 0.7343
Epoch 120: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5494 - accuracy: 0.7339 - val_loss: 0.5485 - val_accuracy: 0.7350
Epoch 121/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.5497 - accuracy: 0.7340
Epoch 121: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5495 - accuracy: 0.7341 - val_loss: 0.5483 - val_accuracy: 0.7366
Epoch 122/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.5500 - accuracy: 0.7348
Epoch 122: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5494 - accuracy: 0.7353 - val_loss: 0.5493 - val_accuracy: 0.7357
Epoch 123/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.5490 - accuracy: 0.7354
Epoch 123: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5496 - accuracy: 0.7340 - val_loss: 0.5487 - val_accuracy: 0.7344
Epoch 124/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.5493 - accuracy: 0.7338
Epoch 124: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5492 - accuracy: 0.7334 - val_loss: 0.5491 - val_accuracy: 0.7349
Epoch 125/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.5503 - accuracy: 0.7346
Epoch 125: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5495 - accuracy: 0.7347 - val_loss: 0.5483 - val_accuracy: 0.7364
Epoch 126/150
812/852 [===========================&gt;..] - ETA: 0s - loss: 0.5484 - accuracy: 0.7345
Epoch 126: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5493 - accuracy: 0.7345 - val_loss: 0.5500 - val_accuracy: 0.7337
Epoch 127/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.5492 - accuracy: 0.7344
Epoch 127: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5495 - accuracy: 0.7340 - val_loss: 0.5488 - val_accuracy: 0.7352
Epoch 128/150
809/852 [===========================&gt;..] - ETA: 0s - loss: 0.5490 - accuracy: 0.7350
Epoch 128: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5494 - accuracy: 0.7340 - val_loss: 0.5495 - val_accuracy: 0.7329
Epoch 129/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.5504 - accuracy: 0.7328
Epoch 129: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5494 - accuracy: 0.7337 - val_loss: 0.5485 - val_accuracy: 0.7357
Epoch 130/150
820/852 [===========================&gt;..] - ETA: 0s - loss: 0.5505 - accuracy: 0.7335
Epoch 130: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5495 - accuracy: 0.7343 - val_loss: 0.5484 - val_accuracy: 0.7352
Epoch 131/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.5494 - accuracy: 0.7360
Epoch 131: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.5494 - accuracy: 0.7353 - val_loss: 0.5491 - val_accuracy: 0.7329
Epoch 131: early stopping
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=e20b88db-095c-4421-a12d-809e172c20e4">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [66]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">three_neuron</span><span class="o">.</span><span class="n">load_weights</span><span class="p">(</span><span class="s1">'three_neuron_model.hdf5'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=b9ba21f0-c2b3-4b5c-9b04-48b95c359b71">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [67]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Other Metrics</span>
<span class="n">three_neuron_preds</span> <span class="o">=</span> <span class="n">three_neuron</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">three_neuron_preds</span><span class="o">.</span><span class="n">round</span><span class="p">())</span>
<span class="n">precision</span> <span class="o">=</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">three_neuron_preds</span><span class="o">.</span><span class="n">round</span><span class="p">())</span>
<span class="n">recall</span> <span class="o">=</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">three_neuron_preds</span><span class="o">.</span><span class="n">round</span><span class="p">())</span>
<span class="n">f1score</span> <span class="o">=</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">three_neuron_preds</span><span class="o">.</span><span class="n">round</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Accuracy: </span><span class="si">%.2f</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">accuracy</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Precision: </span><span class="si">%.2f</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">precision</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Recall: </span><span class="si">%.2f</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">recall</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"F1-score: </span><span class="si">%.2f</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">f1score</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>267/267 [==============================] - 0s 867us/step
Accuracy: 0.73
Precision: 0.79
Recall: 0.64
F1-score: 0.71
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=09c46474-646f-452b-81a1-945da0815dfc">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="2.4-Build-a-Model-with-7-Neurons-in-3-Layers">2.4 Build a Model with 7 Neurons in 3 Layers<a class="anchor-link" href="#2.4-Build-a-Model-with-7-Neurons-in-3-Layers">¶</a></h4>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=76ff7a0d-a907-45b5-bed3-b9363cc022be">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [55]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># constructing a 7 neuron model</span>
<span class="n">seven_neuron</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">seven_neuron</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">input_dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">))</span>
<span class="n">seven_neuron</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">))</span>
<span class="n">seven_neuron</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'sigmoid'</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">seven_neuron</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>

<span class="c1">#Check for cycles in the Sequential Model</span>
<span class="n">seven_neuron</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">'binary_crossentropy'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s1">'adam'</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'accuracy'</span><span class="p">])</span>
<span class="n">callback_a</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">filepath</span> <span class="o">=</span> <span class="s1">'seven_model.hdf5'</span><span class="p">,</span> <span class="n">monitor</span><span class="o">=</span><span class="s1">'val_loss'</span><span class="p">,</span> <span class="n">save_best_only</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">save_weights_only</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">verbose</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">callback_b</span> <span class="o">=</span> <span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s1">'val_loss'</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">'min'</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">history_7</span> <span class="o">=</span> <span class="n">seven_neuron</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">callback_a</span><span class="p">,</span> <span class="n">callback_b</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Model: "sequential_12"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense_12 (Dense)            (None, 4)                 108       
                                                                 
 dense_13 (Dense)            (None, 2)                 10        
                                                                 
 dense_14 (Dense)            (None, 1)                 3         
                                                                 
=================================================================
Total params: 121 (484.00 Byte)
Trainable params: 121 (484.00 Byte)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
None
Epoch 1/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.6499 - accuracy: 0.6627
Epoch 1: val_loss improved from inf to 0.61962, saving model to seven_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.6493 - accuracy: 0.6640 - val_loss: 0.6196 - val_accuracy: 0.7118
Epoch 2/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.6092 - accuracy: 0.7154
Epoch 2: val_loss improved from 0.61962 to 0.59870, saving model to seven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.6093 - accuracy: 0.7156 - val_loss: 0.5987 - val_accuracy: 0.7172
Epoch 3/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.5940 - accuracy: 0.7152
Epoch 3: val_loss improved from 0.59870 to 0.58753, saving model to seven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5939 - accuracy: 0.7154 - val_loss: 0.5875 - val_accuracy: 0.7158
Epoch 4/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.5858 - accuracy: 0.7166
Epoch 4: val_loss improved from 0.58753 to 0.58104, saving model to seven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5854 - accuracy: 0.7170 - val_loss: 0.5810 - val_accuracy: 0.7189
Epoch 5/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.5785 - accuracy: 0.7195
Epoch 5: val_loss improved from 0.58104 to 0.57720, saving model to seven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5797 - accuracy: 0.7184 - val_loss: 0.5772 - val_accuracy: 0.7148
Epoch 6/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.5778 - accuracy: 0.7149
Epoch 6: val_loss improved from 0.57720 to 0.57340, saving model to seven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5766 - accuracy: 0.7162 - val_loss: 0.5734 - val_accuracy: 0.7213
Epoch 7/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.5726 - accuracy: 0.7185
Epoch 7: val_loss improved from 0.57340 to 0.57071, saving model to seven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5733 - accuracy: 0.7174 - val_loss: 0.5707 - val_accuracy: 0.7203
Epoch 8/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.5710 - accuracy: 0.7200
Epoch 8: val_loss improved from 0.57071 to 0.56973, saving model to seven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5712 - accuracy: 0.7196 - val_loss: 0.5697 - val_accuracy: 0.7255
Epoch 9/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.5687 - accuracy: 0.7224
Epoch 9: val_loss improved from 0.56973 to 0.56695, saving model to seven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5692 - accuracy: 0.7217 - val_loss: 0.5669 - val_accuracy: 0.7237
Epoch 10/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5672 - accuracy: 0.7233
Epoch 10: val_loss improved from 0.56695 to 0.56588, saving model to seven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5677 - accuracy: 0.7228 - val_loss: 0.5659 - val_accuracy: 0.7199
Epoch 11/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.5650 - accuracy: 0.7250
Epoch 11: val_loss did not improve from 0.56588
852/852 [==============================] - 3s 3ms/step - loss: 0.5660 - accuracy: 0.7244 - val_loss: 0.5669 - val_accuracy: 0.7194
Epoch 12/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.5650 - accuracy: 0.7241
Epoch 12: val_loss improved from 0.56588 to 0.56203, saving model to seven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5648 - accuracy: 0.7245 - val_loss: 0.5620 - val_accuracy: 0.7245
Epoch 13/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.5637 - accuracy: 0.7250
Epoch 13: val_loss improved from 0.56203 to 0.56066, saving model to seven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5631 - accuracy: 0.7259 - val_loss: 0.5607 - val_accuracy: 0.7297
Epoch 14/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.5617 - accuracy: 0.7266
Epoch 14: val_loss improved from 0.56066 to 0.55992, saving model to seven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5611 - accuracy: 0.7272 - val_loss: 0.5599 - val_accuracy: 0.7256
Epoch 15/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5598 - accuracy: 0.7281
Epoch 15: val_loss improved from 0.55992 to 0.55804, saving model to seven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5598 - accuracy: 0.7276 - val_loss: 0.5580 - val_accuracy: 0.7306
Epoch 16/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5587 - accuracy: 0.7285
Epoch 16: val_loss improved from 0.55804 to 0.55673, saving model to seven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5589 - accuracy: 0.7283 - val_loss: 0.5567 - val_accuracy: 0.7296
Epoch 17/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.5575 - accuracy: 0.7298
Epoch 17: val_loss improved from 0.55673 to 0.55578, saving model to seven_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5582 - accuracy: 0.7289 - val_loss: 0.5558 - val_accuracy: 0.7292
Epoch 18/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.5574 - accuracy: 0.7283
Epoch 18: val_loss improved from 0.55578 to 0.55451, saving model to seven_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5573 - accuracy: 0.7290 - val_loss: 0.5545 - val_accuracy: 0.7303
Epoch 19/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.5564 - accuracy: 0.7290
Epoch 19: val_loss improved from 0.55451 to 0.55385, saving model to seven_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5562 - accuracy: 0.7291 - val_loss: 0.5539 - val_accuracy: 0.7307
Epoch 20/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.5552 - accuracy: 0.7301
Epoch 20: val_loss improved from 0.55385 to 0.55333, saving model to seven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5553 - accuracy: 0.7300 - val_loss: 0.5533 - val_accuracy: 0.7310
Epoch 21/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.5546 - accuracy: 0.7300
Epoch 21: val_loss did not improve from 0.55333
852/852 [==============================] - 2s 3ms/step - loss: 0.5547 - accuracy: 0.7300 - val_loss: 0.5565 - val_accuracy: 0.7299
Epoch 22/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.5538 - accuracy: 0.7302
Epoch 22: val_loss improved from 0.55333 to 0.55204, saving model to seven_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5543 - accuracy: 0.7292 - val_loss: 0.5520 - val_accuracy: 0.7303
Epoch 23/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.5511 - accuracy: 0.7309
Epoch 23: val_loss did not improve from 0.55204
852/852 [==============================] - 2s 3ms/step - loss: 0.5532 - accuracy: 0.7293 - val_loss: 0.5529 - val_accuracy: 0.7289
Epoch 24/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.5532 - accuracy: 0.7298
Epoch 24: val_loss improved from 0.55204 to 0.55110, saving model to seven_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5531 - accuracy: 0.7298 - val_loss: 0.5511 - val_accuracy: 0.7313
Epoch 25/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.5525 - accuracy: 0.7318
Epoch 25: val_loss did not improve from 0.55110
852/852 [==============================] - 2s 3ms/step - loss: 0.5525 - accuracy: 0.7318 - val_loss: 0.5520 - val_accuracy: 0.7278
Epoch 26/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.5521 - accuracy: 0.7304
Epoch 26: val_loss improved from 0.55110 to 0.55103, saving model to seven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5521 - accuracy: 0.7305 - val_loss: 0.5510 - val_accuracy: 0.7292
Epoch 27/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.5514 - accuracy: 0.7304
Epoch 27: val_loss improved from 0.55103 to 0.54942, saving model to seven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5516 - accuracy: 0.7300 - val_loss: 0.5494 - val_accuracy: 0.7322
Epoch 28/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.5504 - accuracy: 0.7319
Epoch 28: val_loss improved from 0.54942 to 0.54869, saving model to seven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5513 - accuracy: 0.7319 - val_loss: 0.5487 - val_accuracy: 0.7311
Epoch 29/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.5502 - accuracy: 0.7309
Epoch 29: val_loss did not improve from 0.54869
852/852 [==============================] - 2s 3ms/step - loss: 0.5502 - accuracy: 0.7311 - val_loss: 0.5537 - val_accuracy: 0.7329
Epoch 30/150
818/852 [===========================&gt;..] - ETA: 0s - loss: 0.5508 - accuracy: 0.7293
Epoch 30: val_loss did not improve from 0.54869
852/852 [==============================] - 3s 3ms/step - loss: 0.5498 - accuracy: 0.7311 - val_loss: 0.5504 - val_accuracy: 0.7316
Epoch 31/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.5496 - accuracy: 0.7305
Epoch 31: val_loss improved from 0.54869 to 0.54818, saving model to seven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5496 - accuracy: 0.7311 - val_loss: 0.5482 - val_accuracy: 0.7302
Epoch 32/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.5498 - accuracy: 0.7325
Epoch 32: val_loss improved from 0.54818 to 0.54716, saving model to seven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5498 - accuracy: 0.7324 - val_loss: 0.5472 - val_accuracy: 0.7349
Epoch 33/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.5490 - accuracy: 0.7314
Epoch 33: val_loss did not improve from 0.54716
852/852 [==============================] - 2s 3ms/step - loss: 0.5489 - accuracy: 0.7311 - val_loss: 0.5491 - val_accuracy: 0.7302
Epoch 34/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.5487 - accuracy: 0.7344
Epoch 34: val_loss improved from 0.54716 to 0.54618, saving model to seven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5487 - accuracy: 0.7346 - val_loss: 0.5462 - val_accuracy: 0.7341
Epoch 35/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.5480 - accuracy: 0.7348
Epoch 35: val_loss did not improve from 0.54618
852/852 [==============================] - 2s 3ms/step - loss: 0.5476 - accuracy: 0.7347 - val_loss: 0.5494 - val_accuracy: 0.7298
Epoch 36/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.5476 - accuracy: 0.7335
Epoch 36: val_loss did not improve from 0.54618
852/852 [==============================] - 2s 3ms/step - loss: 0.5478 - accuracy: 0.7330 - val_loss: 0.5463 - val_accuracy: 0.7360
Epoch 37/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.5482 - accuracy: 0.7349
Epoch 37: val_loss improved from 0.54618 to 0.54523, saving model to seven_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5478 - accuracy: 0.7354 - val_loss: 0.5452 - val_accuracy: 0.7331
Epoch 38/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.5468 - accuracy: 0.7324
Epoch 38: val_loss did not improve from 0.54523
852/852 [==============================] - 2s 3ms/step - loss: 0.5471 - accuracy: 0.7325 - val_loss: 0.5479 - val_accuracy: 0.7356
Epoch 39/150
852/852 [==============================] - ETA: 0s - loss: 0.5466 - accuracy: 0.7334
Epoch 39: val_loss did not improve from 0.54523
852/852 [==============================] - 2s 3ms/step - loss: 0.5466 - accuracy: 0.7334 - val_loss: 0.5464 - val_accuracy: 0.7334
Epoch 40/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.5457 - accuracy: 0.7357
Epoch 40: val_loss improved from 0.54523 to 0.54497, saving model to seven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5471 - accuracy: 0.7339 - val_loss: 0.5450 - val_accuracy: 0.7366
Epoch 41/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5463 - accuracy: 0.7337
Epoch 41: val_loss did not improve from 0.54497
852/852 [==============================] - 3s 3ms/step - loss: 0.5466 - accuracy: 0.7331 - val_loss: 0.5451 - val_accuracy: 0.7325
Epoch 42/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.5468 - accuracy: 0.7341
Epoch 42: val_loss did not improve from 0.54497
852/852 [==============================] - 2s 3ms/step - loss: 0.5466 - accuracy: 0.7338 - val_loss: 0.5454 - val_accuracy: 0.7352
Epoch 43/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.5464 - accuracy: 0.7361
Epoch 43: val_loss improved from 0.54497 to 0.54440, saving model to seven_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5464 - accuracy: 0.7359 - val_loss: 0.5444 - val_accuracy: 0.7377
Epoch 44/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.5470 - accuracy: 0.7333
Epoch 44: val_loss improved from 0.54440 to 0.54392, saving model to seven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5457 - accuracy: 0.7345 - val_loss: 0.5439 - val_accuracy: 0.7370
Epoch 45/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.5459 - accuracy: 0.7354
Epoch 45: val_loss improved from 0.54392 to 0.54338, saving model to seven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5460 - accuracy: 0.7354 - val_loss: 0.5434 - val_accuracy: 0.7373
Epoch 46/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.5460 - accuracy: 0.7351
Epoch 46: val_loss did not improve from 0.54338
852/852 [==============================] - 2s 3ms/step - loss: 0.5459 - accuracy: 0.7352 - val_loss: 0.5454 - val_accuracy: 0.7363
Epoch 47/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.5459 - accuracy: 0.7345
Epoch 47: val_loss improved from 0.54338 to 0.54318, saving model to seven_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5455 - accuracy: 0.7345 - val_loss: 0.5432 - val_accuracy: 0.7384
Epoch 48/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.5442 - accuracy: 0.7380
Epoch 48: val_loss did not improve from 0.54318
852/852 [==============================] - 2s 3ms/step - loss: 0.5458 - accuracy: 0.7370 - val_loss: 0.5433 - val_accuracy: 0.7385
Epoch 49/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.5461 - accuracy: 0.7361
Epoch 49: val_loss did not improve from 0.54318
852/852 [==============================] - 2s 3ms/step - loss: 0.5454 - accuracy: 0.7373 - val_loss: 0.5434 - val_accuracy: 0.7378
Epoch 50/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.5446 - accuracy: 0.7397
Epoch 50: val_loss improved from 0.54318 to 0.54314, saving model to seven_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5453 - accuracy: 0.7386 - val_loss: 0.5431 - val_accuracy: 0.7370
Epoch 51/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.5461 - accuracy: 0.7362
Epoch 51: val_loss improved from 0.54314 to 0.54294, saving model to seven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5454 - accuracy: 0.7365 - val_loss: 0.5429 - val_accuracy: 0.7379
Epoch 52/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.5457 - accuracy: 0.7359
Epoch 52: val_loss did not improve from 0.54294
852/852 [==============================] - 2s 3ms/step - loss: 0.5449 - accuracy: 0.7366 - val_loss: 0.5441 - val_accuracy: 0.7365
Epoch 53/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.5447 - accuracy: 0.7363
Epoch 53: val_loss did not improve from 0.54294
852/852 [==============================] - 2s 3ms/step - loss: 0.5445 - accuracy: 0.7363 - val_loss: 0.5437 - val_accuracy: 0.7368
Epoch 54/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.5432 - accuracy: 0.7389
Epoch 54: val_loss did not improve from 0.54294
852/852 [==============================] - 2s 3ms/step - loss: 0.5444 - accuracy: 0.7374 - val_loss: 0.5431 - val_accuracy: 0.7351
Epoch 55/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.5437 - accuracy: 0.7389
Epoch 55: val_loss improved from 0.54294 to 0.54216, saving model to seven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5448 - accuracy: 0.7377 - val_loss: 0.5422 - val_accuracy: 0.7398
Epoch 56/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.5426 - accuracy: 0.7399
Epoch 56: val_loss improved from 0.54216 to 0.54174, saving model to seven_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5445 - accuracy: 0.7384 - val_loss: 0.5417 - val_accuracy: 0.7385
Epoch 57/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.5434 - accuracy: 0.7371
Epoch 57: val_loss did not improve from 0.54174
852/852 [==============================] - 3s 3ms/step - loss: 0.5442 - accuracy: 0.7372 - val_loss: 0.5431 - val_accuracy: 0.7394
Epoch 58/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.5455 - accuracy: 0.7357
Epoch 58: val_loss did not improve from 0.54174
852/852 [==============================] - 2s 3ms/step - loss: 0.5441 - accuracy: 0.7367 - val_loss: 0.5424 - val_accuracy: 0.7374
Epoch 59/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.5441 - accuracy: 0.7361
Epoch 59: val_loss did not improve from 0.54174
852/852 [==============================] - 2s 3ms/step - loss: 0.5442 - accuracy: 0.7358 - val_loss: 0.5419 - val_accuracy: 0.7381
Epoch 60/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.5445 - accuracy: 0.7373
Epoch 60: val_loss did not improve from 0.54174
852/852 [==============================] - 2s 3ms/step - loss: 0.5440 - accuracy: 0.7377 - val_loss: 0.5422 - val_accuracy: 0.7395
Epoch 61/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.5431 - accuracy: 0.7371
Epoch 61: val_loss improved from 0.54174 to 0.54145, saving model to seven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5436 - accuracy: 0.7370 - val_loss: 0.5415 - val_accuracy: 0.7383
Epoch 62/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.5429 - accuracy: 0.7377
Epoch 62: val_loss did not improve from 0.54145
852/852 [==============================] - 2s 3ms/step - loss: 0.5435 - accuracy: 0.7373 - val_loss: 0.5418 - val_accuracy: 0.7378
Epoch 63/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.5437 - accuracy: 0.7349
Epoch 63: val_loss did not improve from 0.54145
852/852 [==============================] - 2s 3ms/step - loss: 0.5432 - accuracy: 0.7356 - val_loss: 0.5448 - val_accuracy: 0.7379
Epoch 64/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.5443 - accuracy: 0.7364
Epoch 64: val_loss did not improve from 0.54145
852/852 [==============================] - 2s 3ms/step - loss: 0.5438 - accuracy: 0.7368 - val_loss: 0.5422 - val_accuracy: 0.7358
Epoch 65/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.5432 - accuracy: 0.7380
Epoch 65: val_loss did not improve from 0.54145
852/852 [==============================] - 2s 3ms/step - loss: 0.5430 - accuracy: 0.7383 - val_loss: 0.5420 - val_accuracy: 0.7386
Epoch 66/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.5428 - accuracy: 0.7388
Epoch 66: val_loss did not improve from 0.54145
852/852 [==============================] - 2s 3ms/step - loss: 0.5428 - accuracy: 0.7387 - val_loss: 0.5420 - val_accuracy: 0.7397
Epoch 67/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.5424 - accuracy: 0.7377
Epoch 67: val_loss improved from 0.54145 to 0.54042, saving model to seven_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5433 - accuracy: 0.7370 - val_loss: 0.5404 - val_accuracy: 0.7392
Epoch 68/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.5417 - accuracy: 0.7372
Epoch 68: val_loss did not improve from 0.54042
852/852 [==============================] - 2s 3ms/step - loss: 0.5428 - accuracy: 0.7364 - val_loss: 0.5431 - val_accuracy: 0.7394
Epoch 69/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.5430 - accuracy: 0.7363
Epoch 69: val_loss did not improve from 0.54042
852/852 [==============================] - 2s 3ms/step - loss: 0.5434 - accuracy: 0.7359 - val_loss: 0.5425 - val_accuracy: 0.7377
Epoch 70/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.5426 - accuracy: 0.7381
Epoch 70: val_loss did not improve from 0.54042
852/852 [==============================] - 2s 3ms/step - loss: 0.5427 - accuracy: 0.7381 - val_loss: 0.5412 - val_accuracy: 0.7397
Epoch 71/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.5424 - accuracy: 0.7380
Epoch 71: val_loss did not improve from 0.54042
852/852 [==============================] - 2s 3ms/step - loss: 0.5427 - accuracy: 0.7372 - val_loss: 0.5409 - val_accuracy: 0.7390
Epoch 72/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5427 - accuracy: 0.7377
Epoch 72: val_loss did not improve from 0.54042
852/852 [==============================] - 2s 3ms/step - loss: 0.5426 - accuracy: 0.7379 - val_loss: 0.5406 - val_accuracy: 0.7394
Epoch 73/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.5424 - accuracy: 0.7375
Epoch 73: val_loss improved from 0.54042 to 0.54025, saving model to seven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5424 - accuracy: 0.7376 - val_loss: 0.5403 - val_accuracy: 0.7414
Epoch 74/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.5434 - accuracy: 0.7382
Epoch 74: val_loss did not improve from 0.54025
852/852 [==============================] - 3s 3ms/step - loss: 0.5427 - accuracy: 0.7390 - val_loss: 0.5404 - val_accuracy: 0.7387
Epoch 75/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.5429 - accuracy: 0.7372
Epoch 75: val_loss did not improve from 0.54025
852/852 [==============================] - 2s 3ms/step - loss: 0.5425 - accuracy: 0.7377 - val_loss: 0.5403 - val_accuracy: 0.7390
Epoch 76/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.5411 - accuracy: 0.7371
Epoch 76: val_loss did not improve from 0.54025
852/852 [==============================] - 3s 3ms/step - loss: 0.5422 - accuracy: 0.7373 - val_loss: 0.5405 - val_accuracy: 0.7388
Epoch 77/150
852/852 [==============================] - ETA: 0s - loss: 0.5421 - accuracy: 0.7390
Epoch 77: val_loss improved from 0.54025 to 0.53996, saving model to seven_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5421 - accuracy: 0.7390 - val_loss: 0.5400 - val_accuracy: 0.7418
Epoch 78/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.5434 - accuracy: 0.7372
Epoch 78: val_loss improved from 0.53996 to 0.53958, saving model to seven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5426 - accuracy: 0.7378 - val_loss: 0.5396 - val_accuracy: 0.7400
Epoch 79/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.5410 - accuracy: 0.7407
Epoch 79: val_loss improved from 0.53958 to 0.53944, saving model to seven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5422 - accuracy: 0.7398 - val_loss: 0.5394 - val_accuracy: 0.7408
Epoch 80/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5417 - accuracy: 0.7373
Epoch 80: val_loss improved from 0.53944 to 0.53938, saving model to seven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5422 - accuracy: 0.7371 - val_loss: 0.5394 - val_accuracy: 0.7413
Epoch 81/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.5410 - accuracy: 0.7382
Epoch 81: val_loss improved from 0.53938 to 0.53928, saving model to seven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5420 - accuracy: 0.7371 - val_loss: 0.5393 - val_accuracy: 0.7401
Epoch 82/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5415 - accuracy: 0.7374
Epoch 82: val_loss did not improve from 0.53928
852/852 [==============================] - 3s 3ms/step - loss: 0.5415 - accuracy: 0.7377 - val_loss: 0.5400 - val_accuracy: 0.7394
Epoch 83/150
852/852 [==============================] - ETA: 0s - loss: 0.5422 - accuracy: 0.7395
Epoch 83: val_loss did not improve from 0.53928
852/852 [==============================] - 2s 3ms/step - loss: 0.5422 - accuracy: 0.7395 - val_loss: 0.5430 - val_accuracy: 0.7394
Epoch 84/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.5414 - accuracy: 0.7389
Epoch 84: val_loss did not improve from 0.53928
852/852 [==============================] - 3s 3ms/step - loss: 0.5413 - accuracy: 0.7391 - val_loss: 0.5401 - val_accuracy: 0.7393
Epoch 85/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.5427 - accuracy: 0.7371
Epoch 85: val_loss did not improve from 0.53928
852/852 [==============================] - 3s 3ms/step - loss: 0.5423 - accuracy: 0.7374 - val_loss: 0.5402 - val_accuracy: 0.7392
Epoch 86/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.5416 - accuracy: 0.7401
Epoch 86: val_loss did not improve from 0.53928
852/852 [==============================] - 3s 3ms/step - loss: 0.5415 - accuracy: 0.7401 - val_loss: 0.5393 - val_accuracy: 0.7399
Epoch 87/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5398 - accuracy: 0.7400
Epoch 87: val_loss did not improve from 0.53928
852/852 [==============================] - 4s 4ms/step - loss: 0.5416 - accuracy: 0.7384 - val_loss: 0.5395 - val_accuracy: 0.7400
Epoch 88/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.5414 - accuracy: 0.7387
Epoch 88: val_loss did not improve from 0.53928
852/852 [==============================] - 3s 3ms/step - loss: 0.5416 - accuracy: 0.7383 - val_loss: 0.5413 - val_accuracy: 0.7393
Epoch 89/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5409 - accuracy: 0.7389
Epoch 89: val_loss did not improve from 0.53928
852/852 [==============================] - 2s 3ms/step - loss: 0.5415 - accuracy: 0.7387 - val_loss: 0.5394 - val_accuracy: 0.7379
Epoch 90/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5412 - accuracy: 0.7391
Epoch 90: val_loss did not improve from 0.53928
852/852 [==============================] - 3s 3ms/step - loss: 0.5414 - accuracy: 0.7388 - val_loss: 0.5407 - val_accuracy: 0.7370
Epoch 91/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.5420 - accuracy: 0.7378
Epoch 91: val_loss improved from 0.53928 to 0.53893, saving model to seven_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5416 - accuracy: 0.7381 - val_loss: 0.5389 - val_accuracy: 0.7406
Epoch 92/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.5408 - accuracy: 0.7379
Epoch 92: val_loss did not improve from 0.53893
852/852 [==============================] - 3s 3ms/step - loss: 0.5413 - accuracy: 0.7378 - val_loss: 0.5406 - val_accuracy: 0.7417
Epoch 93/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.5426 - accuracy: 0.7390
Epoch 93: val_loss did not improve from 0.53893
852/852 [==============================] - 3s 3ms/step - loss: 0.5412 - accuracy: 0.7403 - val_loss: 0.5412 - val_accuracy: 0.7374
Epoch 94/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5417 - accuracy: 0.7384
Epoch 94: val_loss improved from 0.53893 to 0.53852, saving model to seven_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5412 - accuracy: 0.7386 - val_loss: 0.5385 - val_accuracy: 0.7407
Epoch 95/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.5407 - accuracy: 0.7380
Epoch 95: val_loss did not improve from 0.53852
852/852 [==============================] - 3s 3ms/step - loss: 0.5417 - accuracy: 0.7381 - val_loss: 0.5387 - val_accuracy: 0.7400
Epoch 96/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.5402 - accuracy: 0.7389
Epoch 96: val_loss did not improve from 0.53852
852/852 [==============================] - 3s 3ms/step - loss: 0.5407 - accuracy: 0.7383 - val_loss: 0.5397 - val_accuracy: 0.7408
Epoch 97/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.5399 - accuracy: 0.7395
Epoch 97: val_loss did not improve from 0.53852
852/852 [==============================] - 3s 3ms/step - loss: 0.5412 - accuracy: 0.7384 - val_loss: 0.5386 - val_accuracy: 0.7397
Epoch 98/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5401 - accuracy: 0.7398
Epoch 98: val_loss did not improve from 0.53852
852/852 [==============================] - 3s 3ms/step - loss: 0.5402 - accuracy: 0.7398 - val_loss: 0.5471 - val_accuracy: 0.7280
Epoch 99/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.5413 - accuracy: 0.7373
Epoch 99: val_loss did not improve from 0.53852
852/852 [==============================] - 3s 3ms/step - loss: 0.5410 - accuracy: 0.7378 - val_loss: 0.5396 - val_accuracy: 0.7420
Epoch 100/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5424 - accuracy: 0.7380
Epoch 100: val_loss improved from 0.53852 to 0.53825, saving model to seven_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5411 - accuracy: 0.7390 - val_loss: 0.5383 - val_accuracy: 0.7414
Epoch 101/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.5403 - accuracy: 0.7387
Epoch 101: val_loss improved from 0.53825 to 0.53793, saving model to seven_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5410 - accuracy: 0.7379 - val_loss: 0.5379 - val_accuracy: 0.7403
Epoch 102/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5420 - accuracy: 0.7377
Epoch 102: val_loss did not improve from 0.53793
852/852 [==============================] - 3s 3ms/step - loss: 0.5410 - accuracy: 0.7385 - val_loss: 0.5381 - val_accuracy: 0.7408
Epoch 103/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.5392 - accuracy: 0.7400
Epoch 103: val_loss did not improve from 0.53793
852/852 [==============================] - 3s 3ms/step - loss: 0.5403 - accuracy: 0.7391 - val_loss: 0.5410 - val_accuracy: 0.7405
Epoch 104/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.5399 - accuracy: 0.7389
Epoch 104: val_loss did not improve from 0.53793
852/852 [==============================] - 3s 3ms/step - loss: 0.5402 - accuracy: 0.7385 - val_loss: 0.5389 - val_accuracy: 0.7412
Epoch 105/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.5405 - accuracy: 0.7386
Epoch 105: val_loss did not improve from 0.53793
852/852 [==============================] - 3s 3ms/step - loss: 0.5408 - accuracy: 0.7381 - val_loss: 0.5381 - val_accuracy: 0.7383
Epoch 106/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.5407 - accuracy: 0.7387
Epoch 106: val_loss did not improve from 0.53793
852/852 [==============================] - 3s 3ms/step - loss: 0.5406 - accuracy: 0.7390 - val_loss: 0.5389 - val_accuracy: 0.7391
Epoch 107/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.5431 - accuracy: 0.7358
Epoch 107: val_loss did not improve from 0.53793
852/852 [==============================] - 3s 3ms/step - loss: 0.5412 - accuracy: 0.7378 - val_loss: 0.5380 - val_accuracy: 0.7408
Epoch 108/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.5391 - accuracy: 0.7388
Epoch 108: val_loss did not improve from 0.53793
852/852 [==============================] - 2s 3ms/step - loss: 0.5406 - accuracy: 0.7383 - val_loss: 0.5384 - val_accuracy: 0.7408
Epoch 109/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.5398 - accuracy: 0.7396
Epoch 109: val_loss did not improve from 0.53793
852/852 [==============================] - 3s 3ms/step - loss: 0.5407 - accuracy: 0.7390 - val_loss: 0.5385 - val_accuracy: 0.7400
Epoch 110/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.5390 - accuracy: 0.7396
Epoch 110: val_loss did not improve from 0.53793
852/852 [==============================] - 3s 3ms/step - loss: 0.5400 - accuracy: 0.7386 - val_loss: 0.5388 - val_accuracy: 0.7406
Epoch 111/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.5408 - accuracy: 0.7368
Epoch 111: val_loss improved from 0.53793 to 0.53738, saving model to seven_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5403 - accuracy: 0.7385 - val_loss: 0.5374 - val_accuracy: 0.7406
Epoch 112/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.5401 - accuracy: 0.7391
Epoch 112: val_loss did not improve from 0.53738
852/852 [==============================] - 3s 3ms/step - loss: 0.5404 - accuracy: 0.7386 - val_loss: 0.5387 - val_accuracy: 0.7392
Epoch 113/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5403 - accuracy: 0.7404
Epoch 113: val_loss did not improve from 0.53738
852/852 [==============================] - 3s 3ms/step - loss: 0.5405 - accuracy: 0.7403 - val_loss: 0.5385 - val_accuracy: 0.7407
Epoch 114/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.5411 - accuracy: 0.7377
Epoch 114: val_loss did not improve from 0.53738
852/852 [==============================] - 3s 3ms/step - loss: 0.5400 - accuracy: 0.7383 - val_loss: 0.5376 - val_accuracy: 0.7403
Epoch 115/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.5399 - accuracy: 0.7391
Epoch 115: val_loss did not improve from 0.53738
852/852 [==============================] - 3s 3ms/step - loss: 0.5397 - accuracy: 0.7393 - val_loss: 0.5391 - val_accuracy: 0.7376
Epoch 116/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.5406 - accuracy: 0.7363
Epoch 116: val_loss did not improve from 0.53738
852/852 [==============================] - 3s 3ms/step - loss: 0.5408 - accuracy: 0.7360 - val_loss: 0.5383 - val_accuracy: 0.7392
Epoch 117/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.5414 - accuracy: 0.7384
Epoch 117: val_loss did not improve from 0.53738
852/852 [==============================] - 3s 3ms/step - loss: 0.5399 - accuracy: 0.7398 - val_loss: 0.5383 - val_accuracy: 0.7401
Epoch 118/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.5393 - accuracy: 0.7394
Epoch 118: val_loss did not improve from 0.53738
852/852 [==============================] - 3s 3ms/step - loss: 0.5398 - accuracy: 0.7390 - val_loss: 0.5395 - val_accuracy: 0.7421
Epoch 119/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.5404 - accuracy: 0.7377
Epoch 119: val_loss did not improve from 0.53738
852/852 [==============================] - 3s 3ms/step - loss: 0.5403 - accuracy: 0.7379 - val_loss: 0.5375 - val_accuracy: 0.7394
Epoch 120/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.5402 - accuracy: 0.7392
Epoch 120: val_loss did not improve from 0.53738
852/852 [==============================] - 3s 3ms/step - loss: 0.5398 - accuracy: 0.7388 - val_loss: 0.5393 - val_accuracy: 0.7392
Epoch 121/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.5398 - accuracy: 0.7385
Epoch 121: val_loss did not improve from 0.53738
852/852 [==============================] - 3s 3ms/step - loss: 0.5400 - accuracy: 0.7386 - val_loss: 0.5395 - val_accuracy: 0.7385
Epoch 122/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.5408 - accuracy: 0.7380
Epoch 122: val_loss did not improve from 0.53738
852/852 [==============================] - 2s 3ms/step - loss: 0.5403 - accuracy: 0.7383 - val_loss: 0.5378 - val_accuracy: 0.7407
Epoch 123/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.5401 - accuracy: 0.7364
Epoch 123: val_loss did not improve from 0.53738
852/852 [==============================] - 3s 3ms/step - loss: 0.5405 - accuracy: 0.7367 - val_loss: 0.5392 - val_accuracy: 0.7394
Epoch 124/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.5402 - accuracy: 0.7394
Epoch 124: val_loss did not improve from 0.53738
852/852 [==============================] - 3s 3ms/step - loss: 0.5402 - accuracy: 0.7394 - val_loss: 0.5388 - val_accuracy: 0.7391
Epoch 125/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.5396 - accuracy: 0.7375
Epoch 125: val_loss improved from 0.53738 to 0.53698, saving model to seven_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5397 - accuracy: 0.7378 - val_loss: 0.5370 - val_accuracy: 0.7408
Epoch 126/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.5400 - accuracy: 0.7396
Epoch 126: val_loss did not improve from 0.53698
852/852 [==============================] - 2s 3ms/step - loss: 0.5397 - accuracy: 0.7398 - val_loss: 0.5382 - val_accuracy: 0.7395
Epoch 127/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5399 - accuracy: 0.7396
Epoch 127: val_loss did not improve from 0.53698
852/852 [==============================] - 3s 3ms/step - loss: 0.5399 - accuracy: 0.7398 - val_loss: 0.5394 - val_accuracy: 0.7384
Epoch 128/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5411 - accuracy: 0.7377
Epoch 128: val_loss did not improve from 0.53698
852/852 [==============================] - 3s 3ms/step - loss: 0.5404 - accuracy: 0.7384 - val_loss: 0.5373 - val_accuracy: 0.7412
Epoch 129/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.5397 - accuracy: 0.7374
Epoch 129: val_loss did not improve from 0.53698
852/852 [==============================] - 3s 3ms/step - loss: 0.5398 - accuracy: 0.7374 - val_loss: 0.5382 - val_accuracy: 0.7408
Epoch 130/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.5397 - accuracy: 0.7377
Epoch 130: val_loss did not improve from 0.53698
852/852 [==============================] - 3s 3ms/step - loss: 0.5395 - accuracy: 0.7386 - val_loss: 0.5395 - val_accuracy: 0.7383
Epoch 131/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5401 - accuracy: 0.7402
Epoch 131: val_loss did not improve from 0.53698
852/852 [==============================] - 3s 3ms/step - loss: 0.5399 - accuracy: 0.7404 - val_loss: 0.5397 - val_accuracy: 0.7386
Epoch 132/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.5398 - accuracy: 0.7392
Epoch 132: val_loss did not improve from 0.53698
852/852 [==============================] - 3s 3ms/step - loss: 0.5396 - accuracy: 0.7395 - val_loss: 0.5388 - val_accuracy: 0.7400
Epoch 133/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.5398 - accuracy: 0.7393
Epoch 133: val_loss did not improve from 0.53698
852/852 [==============================] - 3s 3ms/step - loss: 0.5395 - accuracy: 0.7393 - val_loss: 0.5382 - val_accuracy: 0.7385
Epoch 134/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.5397 - accuracy: 0.7365
Epoch 134: val_loss did not improve from 0.53698
852/852 [==============================] - 2s 3ms/step - loss: 0.5399 - accuracy: 0.7366 - val_loss: 0.5390 - val_accuracy: 0.7368
Epoch 135/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.5409 - accuracy: 0.7385
Epoch 135: val_loss did not improve from 0.53698
852/852 [==============================] - 3s 3ms/step - loss: 0.5403 - accuracy: 0.7388 - val_loss: 0.5372 - val_accuracy: 0.7417
Epoch 136/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.5396 - accuracy: 0.7390
Epoch 136: val_loss did not improve from 0.53698
852/852 [==============================] - 3s 3ms/step - loss: 0.5393 - accuracy: 0.7390 - val_loss: 0.5381 - val_accuracy: 0.7415
Epoch 137/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.5399 - accuracy: 0.7395
Epoch 137: val_loss did not improve from 0.53698
852/852 [==============================] - 3s 3ms/step - loss: 0.5391 - accuracy: 0.7404 - val_loss: 0.5431 - val_accuracy: 0.7383
Epoch 138/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5409 - accuracy: 0.7378
Epoch 138: val_loss improved from 0.53698 to 0.53693, saving model to seven_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5401 - accuracy: 0.7387 - val_loss: 0.5369 - val_accuracy: 0.7411
Epoch 139/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.5394 - accuracy: 0.7390
Epoch 139: val_loss did not improve from 0.53693
852/852 [==============================] - 3s 3ms/step - loss: 0.5390 - accuracy: 0.7394 - val_loss: 0.5370 - val_accuracy: 0.7406
Epoch 140/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.5389 - accuracy: 0.7393
Epoch 140: val_loss did not improve from 0.53693
852/852 [==============================] - 3s 3ms/step - loss: 0.5392 - accuracy: 0.7391 - val_loss: 0.5385 - val_accuracy: 0.7404
Epoch 141/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.5399 - accuracy: 0.7376
Epoch 141: val_loss improved from 0.53693 to 0.53686, saving model to seven_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5397 - accuracy: 0.7377 - val_loss: 0.5369 - val_accuracy: 0.7399
Epoch 142/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.5406 - accuracy: 0.7381
Epoch 142: val_loss did not improve from 0.53686
852/852 [==============================] - 2s 3ms/step - loss: 0.5398 - accuracy: 0.7393 - val_loss: 0.5371 - val_accuracy: 0.7420
Epoch 143/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5388 - accuracy: 0.7410
Epoch 143: val_loss improved from 0.53686 to 0.53653, saving model to seven_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5394 - accuracy: 0.7401 - val_loss: 0.5365 - val_accuracy: 0.7425
Epoch 144/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.5407 - accuracy: 0.7389
Epoch 144: val_loss did not improve from 0.53653
852/852 [==============================] - 3s 3ms/step - loss: 0.5400 - accuracy: 0.7397 - val_loss: 0.5375 - val_accuracy: 0.7412
Epoch 145/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.5390 - accuracy: 0.7385
Epoch 145: val_loss did not improve from 0.53653
852/852 [==============================] - 3s 3ms/step - loss: 0.5389 - accuracy: 0.7388 - val_loss: 0.5374 - val_accuracy: 0.7407
Epoch 146/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.5404 - accuracy: 0.7386
Epoch 146: val_loss did not improve from 0.53653
852/852 [==============================] - 3s 3ms/step - loss: 0.5392 - accuracy: 0.7395 - val_loss: 0.5378 - val_accuracy: 0.7392
Epoch 147/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.5391 - accuracy: 0.7385
Epoch 147: val_loss did not improve from 0.53653
852/852 [==============================] - 3s 3ms/step - loss: 0.5393 - accuracy: 0.7388 - val_loss: 0.5371 - val_accuracy: 0.7408
Epoch 148/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.5395 - accuracy: 0.7382
Epoch 148: val_loss did not improve from 0.53653
852/852 [==============================] - 3s 3ms/step - loss: 0.5397 - accuracy: 0.7383 - val_loss: 0.5371 - val_accuracy: 0.7412
Epoch 149/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.5397 - accuracy: 0.7396
Epoch 149: val_loss did not improve from 0.53653
852/852 [==============================] - 3s 3ms/step - loss: 0.5392 - accuracy: 0.7398 - val_loss: 0.5386 - val_accuracy: 0.7386
Epoch 150/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.5400 - accuracy: 0.7382
Epoch 150: val_loss did not improve from 0.53653
852/852 [==============================] - 3s 3ms/step - loss: 0.5391 - accuracy: 0.7393 - val_loss: 0.5387 - val_accuracy: 0.7390
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=4986f2df-c240-4a09-b159-e3e9a7b2c90b">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [56]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">history_7</span> <span class="o">=</span> <span class="n">seven_neuron</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">callback_a</span><span class="p">,</span> <span class="n">callback_b</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.5394 - accuracy: 0.7367
Epoch 1: val_loss did not improve from 0.53653
852/852 [==============================] - 3s 3ms/step - loss: 0.5387 - accuracy: 0.7379 - val_loss: 0.5408 - val_accuracy: 0.7379
Epoch 2/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.5392 - accuracy: 0.7393
Epoch 2: val_loss did not improve from 0.53653
852/852 [==============================] - 3s 3ms/step - loss: 0.5395 - accuracy: 0.7393 - val_loss: 0.5375 - val_accuracy: 0.7406
Epoch 3/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.5394 - accuracy: 0.7387
Epoch 3: val_loss did not improve from 0.53653
852/852 [==============================] - 2s 3ms/step - loss: 0.5394 - accuracy: 0.7388 - val_loss: 0.5373 - val_accuracy: 0.7397
Epoch 4/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.5391 - accuracy: 0.7416
Epoch 4: val_loss did not improve from 0.53653
852/852 [==============================] - 3s 3ms/step - loss: 0.5392 - accuracy: 0.7410 - val_loss: 0.5373 - val_accuracy: 0.7415
Epoch 5/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.5389 - accuracy: 0.7403
Epoch 5: val_loss did not improve from 0.53653
852/852 [==============================] - 3s 3ms/step - loss: 0.5394 - accuracy: 0.7397 - val_loss: 0.5367 - val_accuracy: 0.7400
Epoch 6/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.5386 - accuracy: 0.7411
Epoch 6: val_loss did not improve from 0.53653
852/852 [==============================] - 3s 3ms/step - loss: 0.5389 - accuracy: 0.7406 - val_loss: 0.5375 - val_accuracy: 0.7421
Epoch 7/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.5396 - accuracy: 0.7400
Epoch 7: val_loss did not improve from 0.53653
852/852 [==============================] - 3s 3ms/step - loss: 0.5395 - accuracy: 0.7401 - val_loss: 0.5406 - val_accuracy: 0.7381
Epoch 8/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.5392 - accuracy: 0.7391
Epoch 8: val_loss did not improve from 0.53653
852/852 [==============================] - 2s 3ms/step - loss: 0.5394 - accuracy: 0.7394 - val_loss: 0.5377 - val_accuracy: 0.7405
Epoch 9/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.5400 - accuracy: 0.7386
Epoch 9: val_loss did not improve from 0.53653
852/852 [==============================] - 2s 3ms/step - loss: 0.5395 - accuracy: 0.7381 - val_loss: 0.5373 - val_accuracy: 0.7411
Epoch 10/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.5378 - accuracy: 0.7406
Epoch 10: val_loss did not improve from 0.53653
852/852 [==============================] - 2s 3ms/step - loss: 0.5393 - accuracy: 0.7399 - val_loss: 0.5366 - val_accuracy: 0.7415
Epoch 11/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.5387 - accuracy: 0.7398
Epoch 11: val_loss improved from 0.53653 to 0.53628, saving model to seven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5391 - accuracy: 0.7388 - val_loss: 0.5363 - val_accuracy: 0.7420
Epoch 12/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.5398 - accuracy: 0.7397
Epoch 12: val_loss improved from 0.53628 to 0.53605, saving model to seven_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5392 - accuracy: 0.7400 - val_loss: 0.5361 - val_accuracy: 0.7418
Epoch 13/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5372 - accuracy: 0.7426
Epoch 13: val_loss did not improve from 0.53605
852/852 [==============================] - 2s 3ms/step - loss: 0.5385 - accuracy: 0.7406 - val_loss: 0.5384 - val_accuracy: 0.7420
Epoch 14/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.5396 - accuracy: 0.7393
Epoch 14: val_loss did not improve from 0.53605
852/852 [==============================] - 3s 3ms/step - loss: 0.5394 - accuracy: 0.7393 - val_loss: 0.5374 - val_accuracy: 0.7405
Epoch 15/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.5391 - accuracy: 0.7402
Epoch 15: val_loss did not improve from 0.53605
852/852 [==============================] - 3s 3ms/step - loss: 0.5388 - accuracy: 0.7404 - val_loss: 0.5401 - val_accuracy: 0.7386
Epoch 16/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.5381 - accuracy: 0.7433
Epoch 16: val_loss did not improve from 0.53605
852/852 [==============================] - 2s 3ms/step - loss: 0.5385 - accuracy: 0.7419 - val_loss: 0.5423 - val_accuracy: 0.7397
Epoch 17/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.5380 - accuracy: 0.7417
Epoch 17: val_loss did not improve from 0.53605
852/852 [==============================] - 3s 3ms/step - loss: 0.5386 - accuracy: 0.7405 - val_loss: 0.5365 - val_accuracy: 0.7421
Epoch 18/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.5393 - accuracy: 0.7405
Epoch 18: val_loss did not improve from 0.53605
852/852 [==============================] - 3s 3ms/step - loss: 0.5391 - accuracy: 0.7408 - val_loss: 0.5367 - val_accuracy: 0.7424
Epoch 19/150
852/852 [==============================] - ETA: 0s - loss: 0.5397 - accuracy: 0.7370
Epoch 19: val_loss did not improve from 0.53605
852/852 [==============================] - 3s 3ms/step - loss: 0.5397 - accuracy: 0.7370 - val_loss: 0.5367 - val_accuracy: 0.7417
Epoch 20/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5389 - accuracy: 0.7400
Epoch 20: val_loss did not improve from 0.53605
852/852 [==============================] - 2s 3ms/step - loss: 0.5389 - accuracy: 0.7395 - val_loss: 0.5420 - val_accuracy: 0.7374
Epoch 21/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.5387 - accuracy: 0.7386
Epoch 21: val_loss did not improve from 0.53605
852/852 [==============================] - 3s 3ms/step - loss: 0.5390 - accuracy: 0.7385 - val_loss: 0.5379 - val_accuracy: 0.7418
Epoch 22/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5395 - accuracy: 0.7397
Epoch 22: val_loss did not improve from 0.53605
852/852 [==============================] - 3s 3ms/step - loss: 0.5391 - accuracy: 0.7399 - val_loss: 0.5370 - val_accuracy: 0.7430
Epoch 23/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.5378 - accuracy: 0.7407
Epoch 23: val_loss did not improve from 0.53605
852/852 [==============================] - 3s 3ms/step - loss: 0.5388 - accuracy: 0.7397 - val_loss: 0.5379 - val_accuracy: 0.7424
Epoch 24/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.5409 - accuracy: 0.7372
Epoch 24: val_loss did not improve from 0.53605
852/852 [==============================] - 3s 3ms/step - loss: 0.5393 - accuracy: 0.7386 - val_loss: 0.5367 - val_accuracy: 0.7426
Epoch 25/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.5396 - accuracy: 0.7375
Epoch 25: val_loss did not improve from 0.53605
852/852 [==============================] - 3s 3ms/step - loss: 0.5396 - accuracy: 0.7374 - val_loss: 0.5362 - val_accuracy: 0.7415
Epoch 26/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.5394 - accuracy: 0.7408
Epoch 26: val_loss did not improve from 0.53605
852/852 [==============================] - 2s 3ms/step - loss: 0.5387 - accuracy: 0.7415 - val_loss: 0.5363 - val_accuracy: 0.7417
Epoch 27/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.5395 - accuracy: 0.7381
Epoch 27: val_loss did not improve from 0.53605
852/852 [==============================] - 3s 3ms/step - loss: 0.5392 - accuracy: 0.7380 - val_loss: 0.5370 - val_accuracy: 0.7417
Epoch 28/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.5394 - accuracy: 0.7392
Epoch 28: val_loss did not improve from 0.53605
852/852 [==============================] - 3s 3ms/step - loss: 0.5395 - accuracy: 0.7393 - val_loss: 0.5380 - val_accuracy: 0.7386
Epoch 29/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5372 - accuracy: 0.7409
Epoch 29: val_loss did not improve from 0.53605
852/852 [==============================] - 3s 3ms/step - loss: 0.5382 - accuracy: 0.7400 - val_loss: 0.5361 - val_accuracy: 0.7417
Epoch 30/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.5393 - accuracy: 0.7383
Epoch 30: val_loss did not improve from 0.53605
852/852 [==============================] - 3s 3ms/step - loss: 0.5385 - accuracy: 0.7392 - val_loss: 0.5368 - val_accuracy: 0.7411
Epoch 31/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.5374 - accuracy: 0.7407
Epoch 31: val_loss did not improve from 0.53605
852/852 [==============================] - 3s 3ms/step - loss: 0.5384 - accuracy: 0.7400 - val_loss: 0.5380 - val_accuracy: 0.7404
Epoch 32/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.5392 - accuracy: 0.7361
Epoch 32: val_loss did not improve from 0.53605
852/852 [==============================] - 2s 3ms/step - loss: 0.5389 - accuracy: 0.7370 - val_loss: 0.5369 - val_accuracy: 0.7414
Epoch 32: early stopping
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=c74a6cdb-fa1c-465e-907f-d528f31e6e61">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [59]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">history_7</span> <span class="o">=</span> <span class="n">seven_neuron</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">callback_a</span><span class="p">,</span> <span class="n">callback_b</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.5383 - accuracy: 0.7407
Epoch 1: val_loss did not improve from 0.51923
852/852 [==============================] - 2s 3ms/step - loss: 0.5390 - accuracy: 0.7400 - val_loss: 0.5377 - val_accuracy: 0.7393
Epoch 2/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.5392 - accuracy: 0.7397
Epoch 2: val_loss did not improve from 0.51923
852/852 [==============================] - 3s 3ms/step - loss: 0.5392 - accuracy: 0.7400 - val_loss: 0.5363 - val_accuracy: 0.7420
Epoch 3/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.5393 - accuracy: 0.7416
Epoch 3: val_loss did not improve from 0.51923
852/852 [==============================] - 2s 3ms/step - loss: 0.5394 - accuracy: 0.7415 - val_loss: 0.5362 - val_accuracy: 0.7408
Epoch 4/150
852/852 [==============================] - ETA: 0s - loss: 0.5389 - accuracy: 0.7398
Epoch 4: val_loss did not improve from 0.51923
852/852 [==============================] - 3s 3ms/step - loss: 0.5389 - accuracy: 0.7398 - val_loss: 0.5368 - val_accuracy: 0.7419
Epoch 5/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.5393 - accuracy: 0.7400
Epoch 5: val_loss did not improve from 0.51923
852/852 [==============================] - 3s 3ms/step - loss: 0.5383 - accuracy: 0.7408 - val_loss: 0.5375 - val_accuracy: 0.7403
Epoch 6/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.5383 - accuracy: 0.7389
Epoch 6: val_loss did not improve from 0.51923
852/852 [==============================] - 2s 3ms/step - loss: 0.5387 - accuracy: 0.7378 - val_loss: 0.5374 - val_accuracy: 0.7413
Epoch 7/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.5376 - accuracy: 0.7422
Epoch 7: val_loss did not improve from 0.51923
852/852 [==============================] - 3s 3ms/step - loss: 0.5387 - accuracy: 0.7410 - val_loss: 0.5369 - val_accuracy: 0.7421
Epoch 8/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.5390 - accuracy: 0.7389
Epoch 8: val_loss did not improve from 0.51923
852/852 [==============================] - 2s 3ms/step - loss: 0.5389 - accuracy: 0.7390 - val_loss: 0.5391 - val_accuracy: 0.7391
Epoch 9/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.5382 - accuracy: 0.7416
Epoch 9: val_loss did not improve from 0.51923
852/852 [==============================] - 2s 3ms/step - loss: 0.5383 - accuracy: 0.7414 - val_loss: 0.5458 - val_accuracy: 0.7325
Epoch 10/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.5393 - accuracy: 0.7390
Epoch 10: val_loss did not improve from 0.51923
852/852 [==============================] - 2s 3ms/step - loss: 0.5389 - accuracy: 0.7393 - val_loss: 0.5380 - val_accuracy: 0.7381
Epoch 11/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.5395 - accuracy: 0.7374
Epoch 11: val_loss did not improve from 0.51923
852/852 [==============================] - 3s 3ms/step - loss: 0.5393 - accuracy: 0.7380 - val_loss: 0.5362 - val_accuracy: 0.7425
Epoch 12/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.5377 - accuracy: 0.7412
Epoch 12: val_loss did not improve from 0.51923
852/852 [==============================] - 4s 4ms/step - loss: 0.5380 - accuracy: 0.7410 - val_loss: 0.5379 - val_accuracy: 0.7419
Epoch 13/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5382 - accuracy: 0.7413
Epoch 13: val_loss did not improve from 0.51923
852/852 [==============================] - 3s 3ms/step - loss: 0.5389 - accuracy: 0.7406 - val_loss: 0.5393 - val_accuracy: 0.7405
Epoch 14/150
852/852 [==============================] - ETA: 0s - loss: 0.5389 - accuracy: 0.7404
Epoch 14: val_loss did not improve from 0.51923
852/852 [==============================] - 3s 3ms/step - loss: 0.5389 - accuracy: 0.7404 - val_loss: 0.5372 - val_accuracy: 0.7404
Epoch 15/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.5391 - accuracy: 0.7415
Epoch 15: val_loss did not improve from 0.51923
852/852 [==============================] - 2s 3ms/step - loss: 0.5390 - accuracy: 0.7415 - val_loss: 0.5364 - val_accuracy: 0.7428
Epoch 16/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.5383 - accuracy: 0.7393
Epoch 16: val_loss did not improve from 0.51923
852/852 [==============================] - 3s 4ms/step - loss: 0.5385 - accuracy: 0.7393 - val_loss: 0.5360 - val_accuracy: 0.7431
Epoch 17/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.5379 - accuracy: 0.7407
Epoch 17: val_loss did not improve from 0.51923
852/852 [==============================] - 3s 3ms/step - loss: 0.5384 - accuracy: 0.7404 - val_loss: 0.5387 - val_accuracy: 0.7437
Epoch 18/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.5395 - accuracy: 0.7376
Epoch 18: val_loss did not improve from 0.51923
852/852 [==============================] - 2s 3ms/step - loss: 0.5390 - accuracy: 0.7379 - val_loss: 0.5357 - val_accuracy: 0.7431
Epoch 19/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.5385 - accuracy: 0.7398
Epoch 19: val_loss did not improve from 0.51923
852/852 [==============================] - 2s 3ms/step - loss: 0.5383 - accuracy: 0.7397 - val_loss: 0.5369 - val_accuracy: 0.7418
Epoch 20/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5396 - accuracy: 0.7400
Epoch 20: val_loss did not improve from 0.51923
852/852 [==============================] - 2s 3ms/step - loss: 0.5384 - accuracy: 0.7406 - val_loss: 0.5387 - val_accuracy: 0.7386
Epoch 21/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.5391 - accuracy: 0.7399
Epoch 21: val_loss did not improve from 0.51923
852/852 [==============================] - 2s 3ms/step - loss: 0.5390 - accuracy: 0.7406 - val_loss: 0.5365 - val_accuracy: 0.7425
Epoch 22/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.5385 - accuracy: 0.7411
Epoch 22: val_loss did not improve from 0.51923
852/852 [==============================] - 2s 3ms/step - loss: 0.5389 - accuracy: 0.7408 - val_loss: 0.5361 - val_accuracy: 0.7421
Epoch 23/150
852/852 [==============================] - ETA: 0s - loss: 0.5385 - accuracy: 0.7410
Epoch 23: val_loss did not improve from 0.51923
852/852 [==============================] - 3s 3ms/step - loss: 0.5385 - accuracy: 0.7410 - val_loss: 0.5383 - val_accuracy: 0.7401
Epoch 24/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.5383 - accuracy: 0.7399
Epoch 24: val_loss did not improve from 0.51923
852/852 [==============================] - 2s 3ms/step - loss: 0.5384 - accuracy: 0.7400 - val_loss: 0.5365 - val_accuracy: 0.7411
Epoch 25/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.5390 - accuracy: 0.7399
Epoch 25: val_loss did not improve from 0.51923
852/852 [==============================] - 2s 3ms/step - loss: 0.5384 - accuracy: 0.7405 - val_loss: 0.5376 - val_accuracy: 0.7415
Epoch 26/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.5388 - accuracy: 0.7418
Epoch 26: val_loss did not improve from 0.51923
852/852 [==============================] - 2s 3ms/step - loss: 0.5392 - accuracy: 0.7413 - val_loss: 0.5376 - val_accuracy: 0.7407
Epoch 27/150
852/852 [==============================] - ETA: 0s - loss: 0.5388 - accuracy: 0.7394
Epoch 27: val_loss did not improve from 0.51923
852/852 [==============================] - 2s 3ms/step - loss: 0.5388 - accuracy: 0.7394 - val_loss: 0.5363 - val_accuracy: 0.7421
Epoch 28/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.5381 - accuracy: 0.7389
Epoch 28: val_loss did not improve from 0.51923
852/852 [==============================] - 3s 3ms/step - loss: 0.5386 - accuracy: 0.7384 - val_loss: 0.5359 - val_accuracy: 0.7440
Epoch 29/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.5382 - accuracy: 0.7396
Epoch 29: val_loss did not improve from 0.51923
852/852 [==============================] - 2s 3ms/step - loss: 0.5383 - accuracy: 0.7400 - val_loss: 0.5365 - val_accuracy: 0.7417
Epoch 30/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5381 - accuracy: 0.7402
Epoch 30: val_loss did not improve from 0.51923
852/852 [==============================] - 3s 3ms/step - loss: 0.5382 - accuracy: 0.7403 - val_loss: 0.5367 - val_accuracy: 0.7420
Epoch 31/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.5376 - accuracy: 0.7410
Epoch 31: val_loss did not improve from 0.51923
852/852 [==============================] - 2s 3ms/step - loss: 0.5384 - accuracy: 0.7403 - val_loss: 0.5367 - val_accuracy: 0.7411
Epoch 32/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.5387 - accuracy: 0.7403
Epoch 32: val_loss did not improve from 0.51923
852/852 [==============================] - 2s 3ms/step - loss: 0.5384 - accuracy: 0.7401 - val_loss: 0.5364 - val_accuracy: 0.7410
Epoch 33/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.5391 - accuracy: 0.7365
Epoch 33: val_loss did not improve from 0.51923
852/852 [==============================] - 2s 3ms/step - loss: 0.5392 - accuracy: 0.7366 - val_loss: 0.5376 - val_accuracy: 0.7398
Epoch 34/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.5393 - accuracy: 0.7392
Epoch 34: val_loss did not improve from 0.51923
852/852 [==============================] - 3s 3ms/step - loss: 0.5389 - accuracy: 0.7394 - val_loss: 0.5360 - val_accuracy: 0.7424
Epoch 35/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.5375 - accuracy: 0.7401
Epoch 35: val_loss did not improve from 0.51923
852/852 [==============================] - 2s 3ms/step - loss: 0.5378 - accuracy: 0.7400 - val_loss: 0.5369 - val_accuracy: 0.7418
Epoch 36/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.5380 - accuracy: 0.7405
Epoch 36: val_loss did not improve from 0.51923
852/852 [==============================] - 3s 3ms/step - loss: 0.5390 - accuracy: 0.7400 - val_loss: 0.5358 - val_accuracy: 0.7427
Epoch 37/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.5386 - accuracy: 0.7395
Epoch 37: val_loss did not improve from 0.51923
852/852 [==============================] - 2s 3ms/step - loss: 0.5384 - accuracy: 0.7397 - val_loss: 0.5388 - val_accuracy: 0.7405
Epoch 38/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.5385 - accuracy: 0.7392
Epoch 38: val_loss did not improve from 0.51923
852/852 [==============================] - 2s 3ms/step - loss: 0.5388 - accuracy: 0.7391 - val_loss: 0.5379 - val_accuracy: 0.7430
Epoch 38: early stopping
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=d131d209-1156-4a51-b508-92bdba9a013c">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [83]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">seven_neuron</span><span class="o">.</span><span class="n">load_weights</span><span class="p">(</span><span class="s1">'seven_model.hdf5'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=580912fd-fb9f-4848-a78c-cfb2d067e20a">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [84]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Other Metrics</span>
<span class="n">seven_neuron_preds</span> <span class="o">=</span> <span class="n">seven_neuron</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">seven_neuron_preds</span><span class="o">.</span><span class="n">round</span><span class="p">())</span>
<span class="n">precision</span> <span class="o">=</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">seven_neuron_preds</span><span class="o">.</span><span class="n">round</span><span class="p">())</span>
<span class="n">recall</span> <span class="o">=</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">seven_neuron_preds</span><span class="o">.</span><span class="n">round</span><span class="p">())</span>
<span class="n">f1score</span> <span class="o">=</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">seven_neuron_preds</span><span class="o">.</span><span class="n">round</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Accuracy: </span><span class="si">%.2f</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">accuracy</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Precision: </span><span class="si">%.2f</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">precision</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Recall: </span><span class="si">%.2f</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">recall</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"F1-score: </span><span class="si">%.2f</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">f1score</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>267/267 [==============================] - 0s 946us/step
Accuracy: 0.74
Precision: 0.83
Recall: 0.61
F1-score: 0.70
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=4e9b81ac-766d-45d1-b228-4aab937ac0c3">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="2.5-Build-a-Model-with-15-Neurons-in-4-Layers">2.5 Build a Model with 15 Neurons in 4 Layers<a class="anchor-link" href="#2.5-Build-a-Model-with-15-Neurons-in-4-Layers">¶</a></h4>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=fa00ef68-5b3b-449f-9ab7-1883b9bdc8aa">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [57]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># constructing a 15 neuron model</span>
<span class="n">fifteen_neuron</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">fifteen_neuron</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">input_dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">))</span>
<span class="n">fifteen_neuron</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">))</span>
<span class="n">fifteen_neuron</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">))</span>
<span class="n">fifteen_neuron</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'sigmoid'</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">fifteen_neuron</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>

<span class="c1">#Check for cycles in the Sequential Model</span>
<span class="n">fifteen_neuron</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">'binary_crossentropy'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s1">'adam'</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'accuracy'</span><span class="p">])</span>
<span class="n">callback_a</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">filepath</span> <span class="o">=</span> <span class="s1">'fifteen_model.hdf5'</span><span class="p">,</span> <span class="n">monitor</span><span class="o">=</span><span class="s1">'val_loss'</span><span class="p">,</span> <span class="n">save_best_only</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">save_weights_only</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">verbose</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">callback_b</span> <span class="o">=</span> <span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s1">'val_loss'</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">'min'</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">history_15</span> <span class="o">=</span> <span class="n">fifteen_neuron</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">callback_a</span><span class="p">,</span> <span class="n">callback_b</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Model: "sequential_13"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense_15 (Dense)            (None, 8)                 216       
                                                                 
 dense_16 (Dense)            (None, 4)                 36        
                                                                 
 dense_17 (Dense)            (None, 2)                 10        
                                                                 
 dense_18 (Dense)            (None, 1)                 3         
                                                                 
=================================================================
Total params: 265 (1.04 KB)
Trainable params: 265 (1.04 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
None
Epoch 1/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.6428 - accuracy: 0.6099
Epoch 1: val_loss improved from inf to 0.57055, saving model to fifteen_model.hdf5
852/852 [==============================] - 4s 3ms/step - loss: 0.6404 - accuracy: 0.6137 - val_loss: 0.5705 - val_accuracy: 0.7176
Epoch 2/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.5655 - accuracy: 0.7184
Epoch 2: val_loss improved from 0.57055 to 0.55967, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5647 - accuracy: 0.7194 - val_loss: 0.5597 - val_accuracy: 0.7240
Epoch 3/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.5589 - accuracy: 0.7232
Epoch 3: val_loss improved from 0.55967 to 0.55745, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5605 - accuracy: 0.7219 - val_loss: 0.5574 - val_accuracy: 0.7252
Epoch 4/150
852/852 [==============================] - ETA: 0s - loss: 0.5585 - accuracy: 0.7233
Epoch 4: val_loss improved from 0.55745 to 0.55597, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5585 - accuracy: 0.7233 - val_loss: 0.5560 - val_accuracy: 0.7264
Epoch 5/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.5570 - accuracy: 0.7227
Epoch 5: val_loss improved from 0.55597 to 0.55466, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5574 - accuracy: 0.7228 - val_loss: 0.5547 - val_accuracy: 0.7282
Epoch 6/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.5579 - accuracy: 0.7244
Epoch 6: val_loss improved from 0.55466 to 0.55422, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5569 - accuracy: 0.7250 - val_loss: 0.5542 - val_accuracy: 0.7263
Epoch 7/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5550 - accuracy: 0.7252
Epoch 7: val_loss did not improve from 0.55422
852/852 [==============================] - 3s 3ms/step - loss: 0.5548 - accuracy: 0.7252 - val_loss: 0.5551 - val_accuracy: 0.7267
Epoch 8/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.5570 - accuracy: 0.7234
Epoch 8: val_loss improved from 0.55422 to 0.55261, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5555 - accuracy: 0.7246 - val_loss: 0.5526 - val_accuracy: 0.7266
Epoch 9/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5546 - accuracy: 0.7262
Epoch 9: val_loss did not improve from 0.55261
852/852 [==============================] - 3s 3ms/step - loss: 0.5544 - accuracy: 0.7265 - val_loss: 0.5540 - val_accuracy: 0.7273
Epoch 10/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.5539 - accuracy: 0.7267
Epoch 10: val_loss did not improve from 0.55261
852/852 [==============================] - 3s 3ms/step - loss: 0.5542 - accuracy: 0.7264 - val_loss: 0.5531 - val_accuracy: 0.7226
Epoch 11/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.5531 - accuracy: 0.7270
Epoch 11: val_loss did not improve from 0.55261
852/852 [==============================] - 3s 3ms/step - loss: 0.5533 - accuracy: 0.7273 - val_loss: 0.5553 - val_accuracy: 0.7260
Epoch 12/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.5517 - accuracy: 0.7277
Epoch 12: val_loss improved from 0.55261 to 0.55107, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5530 - accuracy: 0.7271 - val_loss: 0.5511 - val_accuracy: 0.7263
Epoch 13/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.5524 - accuracy: 0.7271
Epoch 13: val_loss improved from 0.55107 to 0.55006, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5526 - accuracy: 0.7269 - val_loss: 0.5501 - val_accuracy: 0.7296
Epoch 14/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.5510 - accuracy: 0.7295
Epoch 14: val_loss improved from 0.55006 to 0.54964, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5520 - accuracy: 0.7284 - val_loss: 0.5496 - val_accuracy: 0.7279
Epoch 15/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.5518 - accuracy: 0.7294
Epoch 15: val_loss improved from 0.54964 to 0.54898, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5517 - accuracy: 0.7293 - val_loss: 0.5490 - val_accuracy: 0.7300
Epoch 16/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5508 - accuracy: 0.7278
Epoch 16: val_loss improved from 0.54898 to 0.54886, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5515 - accuracy: 0.7270 - val_loss: 0.5489 - val_accuracy: 0.7307
Epoch 17/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.5507 - accuracy: 0.7279
Epoch 17: val_loss improved from 0.54886 to 0.54862, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5511 - accuracy: 0.7276 - val_loss: 0.5486 - val_accuracy: 0.7300
Epoch 18/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5516 - accuracy: 0.7279
Epoch 18: val_loss improved from 0.54862 to 0.54821, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5509 - accuracy: 0.7285 - val_loss: 0.5482 - val_accuracy: 0.7297
Epoch 19/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.5502 - accuracy: 0.7279
Epoch 19: val_loss improved from 0.54821 to 0.54753, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5507 - accuracy: 0.7276 - val_loss: 0.5475 - val_accuracy: 0.7320
Epoch 20/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.5502 - accuracy: 0.7285
Epoch 20: val_loss did not improve from 0.54753
852/852 [==============================] - 3s 3ms/step - loss: 0.5499 - accuracy: 0.7286 - val_loss: 0.5480 - val_accuracy: 0.7314
Epoch 21/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.5499 - accuracy: 0.7298
Epoch 21: val_loss improved from 0.54753 to 0.54735, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5499 - accuracy: 0.7298 - val_loss: 0.5473 - val_accuracy: 0.7299
Epoch 22/150
852/852 [==============================] - ETA: 0s - loss: 0.5493 - accuracy: 0.7300
Epoch 22: val_loss improved from 0.54735 to 0.54711, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5493 - accuracy: 0.7300 - val_loss: 0.5471 - val_accuracy: 0.7327
Epoch 23/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.5479 - accuracy: 0.7302
Epoch 23: val_loss did not improve from 0.54711
852/852 [==============================] - 3s 3ms/step - loss: 0.5490 - accuracy: 0.7293 - val_loss: 0.5523 - val_accuracy: 0.7283
Epoch 24/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.5496 - accuracy: 0.7275
Epoch 24: val_loss improved from 0.54711 to 0.54628, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5491 - accuracy: 0.7279 - val_loss: 0.5463 - val_accuracy: 0.7318
Epoch 25/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.5488 - accuracy: 0.7276
Epoch 25: val_loss improved from 0.54628 to 0.54604, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5484 - accuracy: 0.7279 - val_loss: 0.5460 - val_accuracy: 0.7322
Epoch 26/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.5484 - accuracy: 0.7309
Epoch 26: val_loss improved from 0.54604 to 0.54552, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5483 - accuracy: 0.7314 - val_loss: 0.5455 - val_accuracy: 0.7324
Epoch 27/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.5489 - accuracy: 0.7290
Epoch 27: val_loss did not improve from 0.54552
852/852 [==============================] - 3s 3ms/step - loss: 0.5485 - accuracy: 0.7292 - val_loss: 0.5456 - val_accuracy: 0.7316
Epoch 28/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.5472 - accuracy: 0.7293
Epoch 28: val_loss did not improve from 0.54552
852/852 [==============================] - 3s 3ms/step - loss: 0.5478 - accuracy: 0.7293 - val_loss: 0.5479 - val_accuracy: 0.7317
Epoch 29/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.5475 - accuracy: 0.7314
Epoch 29: val_loss improved from 0.54552 to 0.54541, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5483 - accuracy: 0.7305 - val_loss: 0.5454 - val_accuracy: 0.7309
Epoch 30/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.5481 - accuracy: 0.7287
Epoch 30: val_loss improved from 0.54541 to 0.54517, saving model to fifteen_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5479 - accuracy: 0.7292 - val_loss: 0.5452 - val_accuracy: 0.7331
Epoch 31/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.5478 - accuracy: 0.7286
Epoch 31: val_loss did not improve from 0.54517
852/852 [==============================] - 2s 3ms/step - loss: 0.5478 - accuracy: 0.7286 - val_loss: 0.5469 - val_accuracy: 0.7325
Epoch 32/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.5471 - accuracy: 0.7306
Epoch 32: val_loss improved from 0.54517 to 0.54478, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5475 - accuracy: 0.7309 - val_loss: 0.5448 - val_accuracy: 0.7337
Epoch 33/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.5470 - accuracy: 0.7308
Epoch 33: val_loss improved from 0.54478 to 0.54476, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5471 - accuracy: 0.7306 - val_loss: 0.5448 - val_accuracy: 0.7330
Epoch 34/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.5474 - accuracy: 0.7294
Epoch 34: val_loss improved from 0.54476 to 0.54461, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5470 - accuracy: 0.7300 - val_loss: 0.5446 - val_accuracy: 0.7300
Epoch 35/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5461 - accuracy: 0.7302
Epoch 35: val_loss improved from 0.54461 to 0.54399, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5463 - accuracy: 0.7302 - val_loss: 0.5440 - val_accuracy: 0.7316
Epoch 36/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.5474 - accuracy: 0.7292
Epoch 36: val_loss did not improve from 0.54399
852/852 [==============================] - 3s 3ms/step - loss: 0.5463 - accuracy: 0.7302 - val_loss: 0.5446 - val_accuracy: 0.7300
Epoch 37/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.5469 - accuracy: 0.7292
Epoch 37: val_loss did not improve from 0.54399
852/852 [==============================] - 3s 3ms/step - loss: 0.5466 - accuracy: 0.7298 - val_loss: 0.5442 - val_accuracy: 0.7324
Epoch 38/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.5462 - accuracy: 0.7299
Epoch 38: val_loss did not improve from 0.54399
852/852 [==============================] - 3s 3ms/step - loss: 0.5459 - accuracy: 0.7303 - val_loss: 0.5448 - val_accuracy: 0.7341
Epoch 39/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.5453 - accuracy: 0.7316
Epoch 39: val_loss improved from 0.54399 to 0.54389, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5457 - accuracy: 0.7316 - val_loss: 0.5439 - val_accuracy: 0.7334
Epoch 40/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.5455 - accuracy: 0.7316
Epoch 40: val_loss improved from 0.54389 to 0.54292, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5459 - accuracy: 0.7316 - val_loss: 0.5429 - val_accuracy: 0.7338
Epoch 41/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.5452 - accuracy: 0.7341
Epoch 41: val_loss did not improve from 0.54292
852/852 [==============================] - 3s 3ms/step - loss: 0.5451 - accuracy: 0.7343 - val_loss: 0.5459 - val_accuracy: 0.7326
Epoch 42/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.5442 - accuracy: 0.7339
Epoch 42: val_loss did not improve from 0.54292
852/852 [==============================] - 3s 3ms/step - loss: 0.5451 - accuracy: 0.7320 - val_loss: 0.5434 - val_accuracy: 0.7345
Epoch 43/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.5441 - accuracy: 0.7341
Epoch 43: val_loss improved from 0.54292 to 0.54273, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5454 - accuracy: 0.7319 - val_loss: 0.5427 - val_accuracy: 0.7341
Epoch 44/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.5450 - accuracy: 0.7352
Epoch 44: val_loss did not improve from 0.54273
852/852 [==============================] - 3s 3ms/step - loss: 0.5450 - accuracy: 0.7344 - val_loss: 0.5429 - val_accuracy: 0.7347
Epoch 45/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.5450 - accuracy: 0.7341
Epoch 45: val_loss improved from 0.54273 to 0.54232, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5445 - accuracy: 0.7341 - val_loss: 0.5423 - val_accuracy: 0.7331
Epoch 46/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.5446 - accuracy: 0.7323
Epoch 46: val_loss improved from 0.54232 to 0.54217, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5446 - accuracy: 0.7329 - val_loss: 0.5422 - val_accuracy: 0.7379
Epoch 47/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.5437 - accuracy: 0.7357
Epoch 47: val_loss did not improve from 0.54217
852/852 [==============================] - 3s 3ms/step - loss: 0.5445 - accuracy: 0.7349 - val_loss: 0.5429 - val_accuracy: 0.7360
Epoch 48/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.5448 - accuracy: 0.7313
Epoch 48: val_loss improved from 0.54217 to 0.54144, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5444 - accuracy: 0.7317 - val_loss: 0.5414 - val_accuracy: 0.7359
Epoch 49/150
852/852 [==============================] - ETA: 0s - loss: 0.5442 - accuracy: 0.7353
Epoch 49: val_loss did not improve from 0.54144
852/852 [==============================] - 3s 3ms/step - loss: 0.5442 - accuracy: 0.7353 - val_loss: 0.5416 - val_accuracy: 0.7356
Epoch 50/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.5430 - accuracy: 0.7338
Epoch 50: val_loss improved from 0.54144 to 0.54138, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5441 - accuracy: 0.7333 - val_loss: 0.5414 - val_accuracy: 0.7347
Epoch 51/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.5434 - accuracy: 0.7345
Epoch 51: val_loss did not improve from 0.54138
852/852 [==============================] - 3s 3ms/step - loss: 0.5437 - accuracy: 0.7343 - val_loss: 0.5419 - val_accuracy: 0.7364
Epoch 52/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.5437 - accuracy: 0.7349
Epoch 52: val_loss improved from 0.54138 to 0.54092, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5434 - accuracy: 0.7352 - val_loss: 0.5409 - val_accuracy: 0.7330
Epoch 53/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.5442 - accuracy: 0.7336
Epoch 53: val_loss did not improve from 0.54092
852/852 [==============================] - 3s 4ms/step - loss: 0.5435 - accuracy: 0.7341 - val_loss: 0.5423 - val_accuracy: 0.7354
Epoch 54/150
852/852 [==============================] - ETA: 0s - loss: 0.5432 - accuracy: 0.7351
Epoch 54: val_loss did not improve from 0.54092
852/852 [==============================] - 3s 3ms/step - loss: 0.5432 - accuracy: 0.7351 - val_loss: 0.5463 - val_accuracy: 0.7314
Epoch 55/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.5440 - accuracy: 0.7339
Epoch 55: val_loss did not improve from 0.54092
852/852 [==============================] - 2s 3ms/step - loss: 0.5432 - accuracy: 0.7351 - val_loss: 0.5419 - val_accuracy: 0.7358
Epoch 56/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.5440 - accuracy: 0.7324
Epoch 56: val_loss did not improve from 0.54092
852/852 [==============================] - 3s 3ms/step - loss: 0.5423 - accuracy: 0.7343 - val_loss: 0.5427 - val_accuracy: 0.7346
Epoch 57/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.5420 - accuracy: 0.7366
Epoch 57: val_loss improved from 0.54092 to 0.53992, saving model to fifteen_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5431 - accuracy: 0.7357 - val_loss: 0.5399 - val_accuracy: 0.7373
Epoch 58/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.5405 - accuracy: 0.7379
Epoch 58: val_loss did not improve from 0.53992
852/852 [==============================] - 3s 3ms/step - loss: 0.5424 - accuracy: 0.7368 - val_loss: 0.5403 - val_accuracy: 0.7370
Epoch 59/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.5421 - accuracy: 0.7354
Epoch 59: val_loss did not improve from 0.53992
852/852 [==============================] - 3s 3ms/step - loss: 0.5428 - accuracy: 0.7351 - val_loss: 0.5402 - val_accuracy: 0.7378
Epoch 60/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.5429 - accuracy: 0.7342
Epoch 60: val_loss improved from 0.53992 to 0.53920, saving model to fifteen_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5426 - accuracy: 0.7347 - val_loss: 0.5392 - val_accuracy: 0.7384
Epoch 61/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.5419 - accuracy: 0.7355
Epoch 61: val_loss did not improve from 0.53920
852/852 [==============================] - 2s 3ms/step - loss: 0.5419 - accuracy: 0.7350 - val_loss: 0.5406 - val_accuracy: 0.7361
Epoch 62/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.5431 - accuracy: 0.7346
Epoch 62: val_loss did not improve from 0.53920
852/852 [==============================] - 2s 3ms/step - loss: 0.5417 - accuracy: 0.7356 - val_loss: 0.5395 - val_accuracy: 0.7378
Epoch 63/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.5417 - accuracy: 0.7363
Epoch 63: val_loss did not improve from 0.53920
852/852 [==============================] - 2s 3ms/step - loss: 0.5419 - accuracy: 0.7358 - val_loss: 0.5413 - val_accuracy: 0.7371
Epoch 64/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.5420 - accuracy: 0.7359
Epoch 64: val_loss improved from 0.53920 to 0.53896, saving model to fifteen_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5416 - accuracy: 0.7366 - val_loss: 0.5390 - val_accuracy: 0.7378
Epoch 65/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.5411 - accuracy: 0.7382
Epoch 65: val_loss did not improve from 0.53896
852/852 [==============================] - 3s 3ms/step - loss: 0.5417 - accuracy: 0.7379 - val_loss: 0.5423 - val_accuracy: 0.7354
Epoch 66/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.5412 - accuracy: 0.7347
Epoch 66: val_loss did not improve from 0.53896
852/852 [==============================] - 3s 3ms/step - loss: 0.5414 - accuracy: 0.7353 - val_loss: 0.5390 - val_accuracy: 0.7406
Epoch 67/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.5412 - accuracy: 0.7374
Epoch 67: val_loss improved from 0.53896 to 0.53842, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5413 - accuracy: 0.7376 - val_loss: 0.5384 - val_accuracy: 0.7399
Epoch 68/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.5403 - accuracy: 0.7360
Epoch 68: val_loss improved from 0.53842 to 0.53808, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5402 - accuracy: 0.7361 - val_loss: 0.5381 - val_accuracy: 0.7411
Epoch 69/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.5411 - accuracy: 0.7380
Epoch 69: val_loss did not improve from 0.53808
852/852 [==============================] - 3s 3ms/step - loss: 0.5407 - accuracy: 0.7384 - val_loss: 0.5382 - val_accuracy: 0.7395
Epoch 70/150
852/852 [==============================] - ETA: 0s - loss: 0.5408 - accuracy: 0.7353
Epoch 70: val_loss improved from 0.53808 to 0.53776, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5408 - accuracy: 0.7353 - val_loss: 0.5378 - val_accuracy: 0.7398
Epoch 71/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.5412 - accuracy: 0.7340
Epoch 71: val_loss did not improve from 0.53776
852/852 [==============================] - 3s 3ms/step - loss: 0.5406 - accuracy: 0.7346 - val_loss: 0.5378 - val_accuracy: 0.7386
Epoch 72/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5400 - accuracy: 0.7379
Epoch 72: val_loss did not improve from 0.53776
852/852 [==============================] - 3s 3ms/step - loss: 0.5406 - accuracy: 0.7377 - val_loss: 0.5389 - val_accuracy: 0.7405
Epoch 73/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.5401 - accuracy: 0.7376
Epoch 73: val_loss did not improve from 0.53776
852/852 [==============================] - 3s 3ms/step - loss: 0.5399 - accuracy: 0.7373 - val_loss: 0.5390 - val_accuracy: 0.7372
Epoch 74/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.5403 - accuracy: 0.7376
Epoch 74: val_loss improved from 0.53776 to 0.53689, saving model to fifteen_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5398 - accuracy: 0.7378 - val_loss: 0.5369 - val_accuracy: 0.7407
Epoch 75/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.5405 - accuracy: 0.7386
Epoch 75: val_loss improved from 0.53689 to 0.53675, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5399 - accuracy: 0.7390 - val_loss: 0.5368 - val_accuracy: 0.7399
Epoch 76/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.5396 - accuracy: 0.7378
Epoch 76: val_loss did not improve from 0.53675
852/852 [==============================] - 3s 3ms/step - loss: 0.5395 - accuracy: 0.7380 - val_loss: 0.5372 - val_accuracy: 0.7413
Epoch 77/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.5410 - accuracy: 0.7392
Epoch 77: val_loss did not improve from 0.53675
852/852 [==============================] - 3s 3ms/step - loss: 0.5392 - accuracy: 0.7407 - val_loss: 0.5371 - val_accuracy: 0.7411
Epoch 78/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5396 - accuracy: 0.7399
Epoch 78: val_loss improved from 0.53675 to 0.53670, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5392 - accuracy: 0.7407 - val_loss: 0.5367 - val_accuracy: 0.7418
Epoch 79/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.5398 - accuracy: 0.7365
Epoch 79: val_loss improved from 0.53670 to 0.53548, saving model to fifteen_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5391 - accuracy: 0.7367 - val_loss: 0.5355 - val_accuracy: 0.7418
Epoch 80/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.5394 - accuracy: 0.7380
Epoch 80: val_loss did not improve from 0.53548
852/852 [==============================] - 2s 3ms/step - loss: 0.5391 - accuracy: 0.7384 - val_loss: 0.5369 - val_accuracy: 0.7419
Epoch 81/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.5386 - accuracy: 0.7386
Epoch 81: val_loss improved from 0.53548 to 0.53546, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5391 - accuracy: 0.7378 - val_loss: 0.5355 - val_accuracy: 0.7405
Epoch 82/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.5379 - accuracy: 0.7410
Epoch 82: val_loss did not improve from 0.53546
852/852 [==============================] - 2s 3ms/step - loss: 0.5381 - accuracy: 0.7407 - val_loss: 0.5357 - val_accuracy: 0.7425
Epoch 83/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5374 - accuracy: 0.7427
Epoch 83: val_loss did not improve from 0.53546
852/852 [==============================] - 3s 3ms/step - loss: 0.5380 - accuracy: 0.7418 - val_loss: 0.5359 - val_accuracy: 0.7425
Epoch 84/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.5382 - accuracy: 0.7407
Epoch 84: val_loss improved from 0.53546 to 0.53542, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5382 - accuracy: 0.7406 - val_loss: 0.5354 - val_accuracy: 0.7441
Epoch 85/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.5380 - accuracy: 0.7414
Epoch 85: val_loss improved from 0.53542 to 0.53525, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5380 - accuracy: 0.7417 - val_loss: 0.5353 - val_accuracy: 0.7431
Epoch 86/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.5363 - accuracy: 0.7410
Epoch 86: val_loss improved from 0.53525 to 0.53416, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5375 - accuracy: 0.7403 - val_loss: 0.5342 - val_accuracy: 0.7442
Epoch 87/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.5368 - accuracy: 0.7409
Epoch 87: val_loss did not improve from 0.53416
852/852 [==============================] - 2s 3ms/step - loss: 0.5370 - accuracy: 0.7408 - val_loss: 0.5348 - val_accuracy: 0.7447
Epoch 88/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5368 - accuracy: 0.7426
Epoch 88: val_loss improved from 0.53416 to 0.53303, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5369 - accuracy: 0.7427 - val_loss: 0.5330 - val_accuracy: 0.7444
Epoch 89/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.5371 - accuracy: 0.7417
Epoch 89: val_loss did not improve from 0.53303
852/852 [==============================] - 3s 3ms/step - loss: 0.5369 - accuracy: 0.7418 - val_loss: 0.5353 - val_accuracy: 0.7446
Epoch 90/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5357 - accuracy: 0.7428
Epoch 90: val_loss did not improve from 0.53303
852/852 [==============================] - 3s 3ms/step - loss: 0.5356 - accuracy: 0.7428 - val_loss: 0.5366 - val_accuracy: 0.7422
Epoch 91/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.5384 - accuracy: 0.7396
Epoch 91: val_loss improved from 0.53303 to 0.53292, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5365 - accuracy: 0.7407 - val_loss: 0.5329 - val_accuracy: 0.7450
Epoch 92/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.5361 - accuracy: 0.7416
Epoch 92: val_loss did not improve from 0.53292
852/852 [==============================] - 3s 3ms/step - loss: 0.5363 - accuracy: 0.7417 - val_loss: 0.5351 - val_accuracy: 0.7422
Epoch 93/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.5362 - accuracy: 0.7408
Epoch 93: val_loss improved from 0.53292 to 0.53210, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5366 - accuracy: 0.7404 - val_loss: 0.5321 - val_accuracy: 0.7467
Epoch 94/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.5349 - accuracy: 0.7433
Epoch 94: val_loss did not improve from 0.53210
852/852 [==============================] - 2s 3ms/step - loss: 0.5357 - accuracy: 0.7433 - val_loss: 0.5330 - val_accuracy: 0.7459
Epoch 95/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.5354 - accuracy: 0.7448
Epoch 95: val_loss did not improve from 0.53210
852/852 [==============================] - 2s 3ms/step - loss: 0.5352 - accuracy: 0.7448 - val_loss: 0.5333 - val_accuracy: 0.7437
Epoch 96/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.5351 - accuracy: 0.7433
Epoch 96: val_loss improved from 0.53210 to 0.53208, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5348 - accuracy: 0.7433 - val_loss: 0.5321 - val_accuracy: 0.7462
Epoch 97/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.5348 - accuracy: 0.7436
Epoch 97: val_loss did not improve from 0.53208
852/852 [==============================] - 3s 3ms/step - loss: 0.5345 - accuracy: 0.7439 - val_loss: 0.5326 - val_accuracy: 0.7442
Epoch 98/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.5361 - accuracy: 0.7436
Epoch 98: val_loss improved from 0.53208 to 0.53185, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5346 - accuracy: 0.7445 - val_loss: 0.5318 - val_accuracy: 0.7459
Epoch 99/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.5363 - accuracy: 0.7427
Epoch 99: val_loss improved from 0.53185 to 0.53138, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5352 - accuracy: 0.7431 - val_loss: 0.5314 - val_accuracy: 0.7459
Epoch 100/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.5350 - accuracy: 0.7444
Epoch 100: val_loss improved from 0.53138 to 0.53119, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 4ms/step - loss: 0.5350 - accuracy: 0.7442 - val_loss: 0.5312 - val_accuracy: 0.7454
Epoch 101/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.5355 - accuracy: 0.7440
Epoch 101: val_loss improved from 0.53119 to 0.53069, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5347 - accuracy: 0.7445 - val_loss: 0.5307 - val_accuracy: 0.7485
Epoch 102/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5341 - accuracy: 0.7457
Epoch 102: val_loss did not improve from 0.53069
852/852 [==============================] - 3s 3ms/step - loss: 0.5341 - accuracy: 0.7455 - val_loss: 0.5310 - val_accuracy: 0.7473
Epoch 103/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.5332 - accuracy: 0.7458
Epoch 103: val_loss did not improve from 0.53069
852/852 [==============================] - 3s 3ms/step - loss: 0.5339 - accuracy: 0.7447 - val_loss: 0.5309 - val_accuracy: 0.7478
Epoch 104/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.5328 - accuracy: 0.7458
Epoch 104: val_loss improved from 0.53069 to 0.52984, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5334 - accuracy: 0.7453 - val_loss: 0.5298 - val_accuracy: 0.7489
Epoch 105/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5334 - accuracy: 0.7464
Epoch 105: val_loss did not improve from 0.52984
852/852 [==============================] - 3s 3ms/step - loss: 0.5336 - accuracy: 0.7459 - val_loss: 0.5313 - val_accuracy: 0.7494
Epoch 106/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.5332 - accuracy: 0.7463
Epoch 106: val_loss did not improve from 0.52984
852/852 [==============================] - 3s 3ms/step - loss: 0.5333 - accuracy: 0.7461 - val_loss: 0.5303 - val_accuracy: 0.7473
Epoch 107/150
852/852 [==============================] - ETA: 0s - loss: 0.5328 - accuracy: 0.7466
Epoch 107: val_loss did not improve from 0.52984
852/852 [==============================] - 3s 3ms/step - loss: 0.5328 - accuracy: 0.7466 - val_loss: 0.5327 - val_accuracy: 0.7479
Epoch 108/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.5342 - accuracy: 0.7456
Epoch 108: val_loss did not improve from 0.52984
852/852 [==============================] - 3s 3ms/step - loss: 0.5329 - accuracy: 0.7467 - val_loss: 0.5315 - val_accuracy: 0.7482
Epoch 109/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.5331 - accuracy: 0.7452
Epoch 109: val_loss improved from 0.52984 to 0.52940, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5331 - accuracy: 0.7452 - val_loss: 0.5294 - val_accuracy: 0.7488
Epoch 110/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.5331 - accuracy: 0.7458
Epoch 110: val_loss improved from 0.52940 to 0.52870, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5329 - accuracy: 0.7462 - val_loss: 0.5287 - val_accuracy: 0.7502
Epoch 111/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.5340 - accuracy: 0.7417
Epoch 111: val_loss did not improve from 0.52870
852/852 [==============================] - 3s 3ms/step - loss: 0.5328 - accuracy: 0.7430 - val_loss: 0.5296 - val_accuracy: 0.7498
Epoch 112/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.5326 - accuracy: 0.7459
Epoch 112: val_loss did not improve from 0.52870
852/852 [==============================] - 3s 3ms/step - loss: 0.5325 - accuracy: 0.7459 - val_loss: 0.5303 - val_accuracy: 0.7484
Epoch 113/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5322 - accuracy: 0.7472
Epoch 113: val_loss improved from 0.52870 to 0.52806, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5323 - accuracy: 0.7468 - val_loss: 0.5281 - val_accuracy: 0.7511
Epoch 114/150
820/852 [===========================&gt;..] - ETA: 0s - loss: 0.5319 - accuracy: 0.7459
Epoch 114: val_loss did not improve from 0.52806
852/852 [==============================] - 3s 3ms/step - loss: 0.5316 - accuracy: 0.7461 - val_loss: 0.5305 - val_accuracy: 0.7455
Epoch 115/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.5311 - accuracy: 0.7467
Epoch 115: val_loss did not improve from 0.52806
852/852 [==============================] - 2s 3ms/step - loss: 0.5317 - accuracy: 0.7455 - val_loss: 0.5284 - val_accuracy: 0.7487
Epoch 116/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.5341 - accuracy: 0.7424
Epoch 116: val_loss did not improve from 0.52806
852/852 [==============================] - 3s 3ms/step - loss: 0.5321 - accuracy: 0.7444 - val_loss: 0.5288 - val_accuracy: 0.7487
Epoch 117/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.5324 - accuracy: 0.7452
Epoch 117: val_loss did not improve from 0.52806
852/852 [==============================] - 3s 3ms/step - loss: 0.5317 - accuracy: 0.7459 - val_loss: 0.5290 - val_accuracy: 0.7474
Epoch 118/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.5304 - accuracy: 0.7472
Epoch 118: val_loss improved from 0.52806 to 0.52718, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5313 - accuracy: 0.7464 - val_loss: 0.5272 - val_accuracy: 0.7504
Epoch 119/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.5319 - accuracy: 0.7463
Epoch 119: val_loss did not improve from 0.52718
852/852 [==============================] - 3s 3ms/step - loss: 0.5318 - accuracy: 0.7462 - val_loss: 0.5287 - val_accuracy: 0.7486
Epoch 120/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.5312 - accuracy: 0.7468
Epoch 120: val_loss improved from 0.52718 to 0.52669, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5311 - accuracy: 0.7465 - val_loss: 0.5267 - val_accuracy: 0.7478
Epoch 121/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.5285 - accuracy: 0.7494
Epoch 121: val_loss did not improve from 0.52669
852/852 [==============================] - 2s 3ms/step - loss: 0.5301 - accuracy: 0.7485 - val_loss: 0.5321 - val_accuracy: 0.7448
Epoch 122/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.5313 - accuracy: 0.7481
Epoch 122: val_loss did not improve from 0.52669
852/852 [==============================] - 3s 3ms/step - loss: 0.5308 - accuracy: 0.7480 - val_loss: 0.5279 - val_accuracy: 0.7499
Epoch 123/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.5303 - accuracy: 0.7463
Epoch 123: val_loss did not improve from 0.52669
852/852 [==============================] - 3s 3ms/step - loss: 0.5304 - accuracy: 0.7461 - val_loss: 0.5268 - val_accuracy: 0.7487
Epoch 124/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.5311 - accuracy: 0.7467
Epoch 124: val_loss did not improve from 0.52669
852/852 [==============================] - 3s 3ms/step - loss: 0.5313 - accuracy: 0.7464 - val_loss: 0.5292 - val_accuracy: 0.7469
Epoch 125/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.5297 - accuracy: 0.7459
Epoch 125: val_loss did not improve from 0.52669
852/852 [==============================] - 2s 3ms/step - loss: 0.5306 - accuracy: 0.7452 - val_loss: 0.5303 - val_accuracy: 0.7474
Epoch 126/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.5303 - accuracy: 0.7466
Epoch 126: val_loss did not improve from 0.52669
852/852 [==============================] - 3s 3ms/step - loss: 0.5299 - accuracy: 0.7464 - val_loss: 0.5287 - val_accuracy: 0.7469
Epoch 127/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5299 - accuracy: 0.7476
Epoch 127: val_loss did not improve from 0.52669
852/852 [==============================] - 3s 3ms/step - loss: 0.5291 - accuracy: 0.7481 - val_loss: 0.5393 - val_accuracy: 0.7371
Epoch 128/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.5283 - accuracy: 0.7467
Epoch 128: val_loss improved from 0.52669 to 0.52649, saving model to fifteen_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5296 - accuracy: 0.7452 - val_loss: 0.5265 - val_accuracy: 0.7489
Epoch 129/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.5293 - accuracy: 0.7459
Epoch 129: val_loss did not improve from 0.52649
852/852 [==============================] - 3s 3ms/step - loss: 0.5293 - accuracy: 0.7457 - val_loss: 0.5278 - val_accuracy: 0.7479
Epoch 130/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5299 - accuracy: 0.7482
Epoch 130: val_loss improved from 0.52649 to 0.52620, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5294 - accuracy: 0.7486 - val_loss: 0.5262 - val_accuracy: 0.7493
Epoch 131/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5289 - accuracy: 0.7467
Epoch 131: val_loss did not improve from 0.52620
852/852 [==============================] - 3s 3ms/step - loss: 0.5286 - accuracy: 0.7465 - val_loss: 0.5276 - val_accuracy: 0.7498
Epoch 132/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.5306 - accuracy: 0.7466
Epoch 132: val_loss improved from 0.52620 to 0.52509, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5302 - accuracy: 0.7469 - val_loss: 0.5251 - val_accuracy: 0.7493
Epoch 133/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5295 - accuracy: 0.7482
Epoch 133: val_loss did not improve from 0.52509
852/852 [==============================] - 3s 3ms/step - loss: 0.5292 - accuracy: 0.7485 - val_loss: 0.5263 - val_accuracy: 0.7504
Epoch 134/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5296 - accuracy: 0.7464
Epoch 134: val_loss did not improve from 0.52509
852/852 [==============================] - 3s 3ms/step - loss: 0.5293 - accuracy: 0.7468 - val_loss: 0.5253 - val_accuracy: 0.7479
Epoch 135/150
852/852 [==============================] - ETA: 0s - loss: 0.5294 - accuracy: 0.7464
Epoch 135: val_loss improved from 0.52509 to 0.52490, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5294 - accuracy: 0.7464 - val_loss: 0.5249 - val_accuracy: 0.7505
Epoch 136/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5291 - accuracy: 0.7483
Epoch 136: val_loss did not improve from 0.52490
852/852 [==============================] - 3s 3ms/step - loss: 0.5291 - accuracy: 0.7478 - val_loss: 0.5267 - val_accuracy: 0.7492
Epoch 137/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.5291 - accuracy: 0.7475
Epoch 137: val_loss did not improve from 0.52490
852/852 [==============================] - 2s 3ms/step - loss: 0.5286 - accuracy: 0.7473 - val_loss: 0.5275 - val_accuracy: 0.7479
Epoch 138/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.5288 - accuracy: 0.7487
Epoch 138: val_loss did not improve from 0.52490
852/852 [==============================] - 2s 3ms/step - loss: 0.5278 - accuracy: 0.7496 - val_loss: 0.5274 - val_accuracy: 0.7475
Epoch 139/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5279 - accuracy: 0.7492
Epoch 139: val_loss did not improve from 0.52490
852/852 [==============================] - 3s 3ms/step - loss: 0.5290 - accuracy: 0.7481 - val_loss: 0.5274 - val_accuracy: 0.7494
Epoch 140/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.5287 - accuracy: 0.7483
Epoch 140: val_loss did not improve from 0.52490
852/852 [==============================] - 3s 3ms/step - loss: 0.5281 - accuracy: 0.7486 - val_loss: 0.5254 - val_accuracy: 0.7500
Epoch 141/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.5275 - accuracy: 0.7488
Epoch 141: val_loss improved from 0.52490 to 0.52408, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5281 - accuracy: 0.7482 - val_loss: 0.5241 - val_accuracy: 0.7515
Epoch 142/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.5269 - accuracy: 0.7501
Epoch 142: val_loss did not improve from 0.52408
852/852 [==============================] - 3s 3ms/step - loss: 0.5277 - accuracy: 0.7492 - val_loss: 0.5250 - val_accuracy: 0.7523
Epoch 143/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.5282 - accuracy: 0.7489
Epoch 143: val_loss did not improve from 0.52408
852/852 [==============================] - 2s 3ms/step - loss: 0.5283 - accuracy: 0.7487 - val_loss: 0.5282 - val_accuracy: 0.7481
Epoch 144/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.5285 - accuracy: 0.7450
Epoch 144: val_loss did not improve from 0.52408
852/852 [==============================] - 3s 3ms/step - loss: 0.5279 - accuracy: 0.7455 - val_loss: 0.5244 - val_accuracy: 0.7502
Epoch 145/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.5284 - accuracy: 0.7487
Epoch 145: val_loss did not improve from 0.52408
852/852 [==============================] - 3s 3ms/step - loss: 0.5286 - accuracy: 0.7485 - val_loss: 0.5242 - val_accuracy: 0.7513
Epoch 146/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5280 - accuracy: 0.7468
Epoch 146: val_loss did not improve from 0.52408
852/852 [==============================] - 2s 3ms/step - loss: 0.5290 - accuracy: 0.7462 - val_loss: 0.5251 - val_accuracy: 0.7491
Epoch 147/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.5275 - accuracy: 0.7521
Epoch 147: val_loss did not improve from 0.52408
852/852 [==============================] - 3s 3ms/step - loss: 0.5274 - accuracy: 0.7516 - val_loss: 0.5249 - val_accuracy: 0.7507
Epoch 148/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.5280 - accuracy: 0.7472
Epoch 148: val_loss did not improve from 0.52408
852/852 [==============================] - 3s 3ms/step - loss: 0.5281 - accuracy: 0.7469 - val_loss: 0.5252 - val_accuracy: 0.7516
Epoch 149/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.5287 - accuracy: 0.7481
Epoch 149: val_loss did not improve from 0.52408
852/852 [==============================] - 3s 3ms/step - loss: 0.5283 - accuracy: 0.7484 - val_loss: 0.5260 - val_accuracy: 0.7507
Epoch 150/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.5285 - accuracy: 0.7479
Epoch 150: val_loss did not improve from 0.52408
852/852 [==============================] - 3s 3ms/step - loss: 0.5284 - accuracy: 0.7478 - val_loss: 0.5282 - val_accuracy: 0.7477
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=c290034a-7a64-4e1b-bf71-fff0dd10e024">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [58]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">history_15</span> <span class="o">=</span> <span class="n">fifteen_neuron</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">callback_a</span><span class="p">,</span> <span class="n">callback_b</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.5282 - accuracy: 0.7488
Epoch 1: val_loss did not improve from 0.52408
852/852 [==============================] - 3s 3ms/step - loss: 0.5274 - accuracy: 0.7494 - val_loss: 0.5271 - val_accuracy: 0.7501
Epoch 2/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.5286 - accuracy: 0.7473
Epoch 2: val_loss did not improve from 0.52408
852/852 [==============================] - 2s 3ms/step - loss: 0.5284 - accuracy: 0.7474 - val_loss: 0.5271 - val_accuracy: 0.7472
Epoch 3/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.5296 - accuracy: 0.7458
Epoch 3: val_loss improved from 0.52408 to 0.52336, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5285 - accuracy: 0.7469 - val_loss: 0.5234 - val_accuracy: 0.7508
Epoch 4/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.5273 - accuracy: 0.7497
Epoch 4: val_loss did not improve from 0.52336
852/852 [==============================] - 2s 3ms/step - loss: 0.5280 - accuracy: 0.7485 - val_loss: 0.5241 - val_accuracy: 0.7512
Epoch 5/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.5277 - accuracy: 0.7493
Epoch 5: val_loss did not improve from 0.52336
852/852 [==============================] - 3s 3ms/step - loss: 0.5274 - accuracy: 0.7494 - val_loss: 0.5236 - val_accuracy: 0.7507
Epoch 6/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.5283 - accuracy: 0.7473
Epoch 6: val_loss improved from 0.52336 to 0.52304, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5278 - accuracy: 0.7478 - val_loss: 0.5230 - val_accuracy: 0.7513
Epoch 7/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5283 - accuracy: 0.7474
Epoch 7: val_loss did not improve from 0.52304
852/852 [==============================] - 3s 3ms/step - loss: 0.5270 - accuracy: 0.7481 - val_loss: 0.5249 - val_accuracy: 0.7492
Epoch 8/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.5272 - accuracy: 0.7473
Epoch 8: val_loss did not improve from 0.52304
852/852 [==============================] - 3s 3ms/step - loss: 0.5276 - accuracy: 0.7471 - val_loss: 0.5257 - val_accuracy: 0.7484
Epoch 9/150
852/852 [==============================] - ETA: 0s - loss: 0.5277 - accuracy: 0.7472
Epoch 9: val_loss did not improve from 0.52304
852/852 [==============================] - 3s 3ms/step - loss: 0.5277 - accuracy: 0.7472 - val_loss: 0.5241 - val_accuracy: 0.7527
Epoch 10/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.5274 - accuracy: 0.7487
Epoch 10: val_loss did not improve from 0.52304
852/852 [==============================] - 3s 3ms/step - loss: 0.5273 - accuracy: 0.7485 - val_loss: 0.5267 - val_accuracy: 0.7492
Epoch 11/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.5266 - accuracy: 0.7482
Epoch 11: val_loss did not improve from 0.52304
852/852 [==============================] - 2s 3ms/step - loss: 0.5268 - accuracy: 0.7480 - val_loss: 0.5242 - val_accuracy: 0.7487
Epoch 12/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5259 - accuracy: 0.7478
Epoch 12: val_loss improved from 0.52304 to 0.52288, saving model to fifteen_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5269 - accuracy: 0.7469 - val_loss: 0.5229 - val_accuracy: 0.7505
Epoch 13/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.5279 - accuracy: 0.7468
Epoch 13: val_loss did not improve from 0.52288
852/852 [==============================] - 3s 3ms/step - loss: 0.5267 - accuracy: 0.7480 - val_loss: 0.5248 - val_accuracy: 0.7493
Epoch 14/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.5266 - accuracy: 0.7485
Epoch 14: val_loss did not improve from 0.52288
852/852 [==============================] - 3s 3ms/step - loss: 0.5267 - accuracy: 0.7485 - val_loss: 0.5249 - val_accuracy: 0.7487
Epoch 15/150
852/852 [==============================] - ETA: 0s - loss: 0.5265 - accuracy: 0.7492
Epoch 15: val_loss did not improve from 0.52288
852/852 [==============================] - 2s 3ms/step - loss: 0.5265 - accuracy: 0.7492 - val_loss: 0.5248 - val_accuracy: 0.7508
Epoch 16/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.5266 - accuracy: 0.7498
Epoch 16: val_loss did not improve from 0.52288
852/852 [==============================] - 3s 3ms/step - loss: 0.5268 - accuracy: 0.7495 - val_loss: 0.5279 - val_accuracy: 0.7472
Epoch 17/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.5279 - accuracy: 0.7456
Epoch 17: val_loss did not improve from 0.52288
852/852 [==============================] - 2s 3ms/step - loss: 0.5278 - accuracy: 0.7459 - val_loss: 0.5253 - val_accuracy: 0.7486
Epoch 18/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.5276 - accuracy: 0.7464
Epoch 18: val_loss improved from 0.52288 to 0.52273, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5272 - accuracy: 0.7469 - val_loss: 0.5227 - val_accuracy: 0.7512
Epoch 19/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.5271 - accuracy: 0.7474
Epoch 19: val_loss did not improve from 0.52273
852/852 [==============================] - 2s 3ms/step - loss: 0.5265 - accuracy: 0.7479 - val_loss: 0.5233 - val_accuracy: 0.7508
Epoch 20/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5260 - accuracy: 0.7499
Epoch 20: val_loss did not improve from 0.52273
852/852 [==============================] - 3s 3ms/step - loss: 0.5262 - accuracy: 0.7489 - val_loss: 0.5228 - val_accuracy: 0.7531
Epoch 21/150
852/852 [==============================] - ETA: 0s - loss: 0.5265 - accuracy: 0.7494
Epoch 21: val_loss did not improve from 0.52273
852/852 [==============================] - 3s 3ms/step - loss: 0.5265 - accuracy: 0.7494 - val_loss: 0.5229 - val_accuracy: 0.7500
Epoch 22/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.5270 - accuracy: 0.7459
Epoch 22: val_loss did not improve from 0.52273
852/852 [==============================] - 3s 3ms/step - loss: 0.5268 - accuracy: 0.7461 - val_loss: 0.5236 - val_accuracy: 0.7495
Epoch 23/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.5273 - accuracy: 0.7482
Epoch 23: val_loss improved from 0.52273 to 0.52265, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5266 - accuracy: 0.7486 - val_loss: 0.5226 - val_accuracy: 0.7501
Epoch 24/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.5279 - accuracy: 0.7448
Epoch 24: val_loss did not improve from 0.52265
852/852 [==============================] - 3s 3ms/step - loss: 0.5272 - accuracy: 0.7452 - val_loss: 0.5233 - val_accuracy: 0.7501
Epoch 25/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.5254 - accuracy: 0.7488
Epoch 25: val_loss improved from 0.52265 to 0.52205, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5267 - accuracy: 0.7478 - val_loss: 0.5221 - val_accuracy: 0.7516
Epoch 26/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5263 - accuracy: 0.7504
Epoch 26: val_loss did not improve from 0.52205
852/852 [==============================] - 3s 3ms/step - loss: 0.5260 - accuracy: 0.7502 - val_loss: 0.5244 - val_accuracy: 0.7489
Epoch 27/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5261 - accuracy: 0.7489
Epoch 27: val_loss did not improve from 0.52205
852/852 [==============================] - 3s 3ms/step - loss: 0.5260 - accuracy: 0.7489 - val_loss: 0.5234 - val_accuracy: 0.7505
Epoch 28/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5253 - accuracy: 0.7506
Epoch 28: val_loss did not improve from 0.52205
852/852 [==============================] - 2s 3ms/step - loss: 0.5257 - accuracy: 0.7499 - val_loss: 0.5238 - val_accuracy: 0.7495
Epoch 29/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.5283 - accuracy: 0.7464
Epoch 29: val_loss did not improve from 0.52205
852/852 [==============================] - 3s 3ms/step - loss: 0.5269 - accuracy: 0.7472 - val_loss: 0.5223 - val_accuracy: 0.7526
Epoch 30/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.5263 - accuracy: 0.7513
Epoch 30: val_loss did not improve from 0.52205
852/852 [==============================] - 3s 3ms/step - loss: 0.5249 - accuracy: 0.7515 - val_loss: 0.5224 - val_accuracy: 0.7522
Epoch 31/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.5250 - accuracy: 0.7481
Epoch 31: val_loss did not improve from 0.52205
852/852 [==============================] - 2s 3ms/step - loss: 0.5257 - accuracy: 0.7480 - val_loss: 0.5229 - val_accuracy: 0.7511
Epoch 32/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5259 - accuracy: 0.7488
Epoch 32: val_loss did not improve from 0.52205
852/852 [==============================] - 3s 3ms/step - loss: 0.5261 - accuracy: 0.7487 - val_loss: 0.5231 - val_accuracy: 0.7505
Epoch 33/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.5253 - accuracy: 0.7488
Epoch 33: val_loss did not improve from 0.52205
852/852 [==============================] - 3s 3ms/step - loss: 0.5260 - accuracy: 0.7482 - val_loss: 0.5240 - val_accuracy: 0.7493
Epoch 34/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.5263 - accuracy: 0.7471
Epoch 34: val_loss improved from 0.52205 to 0.52154, saving model to fifteen_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5263 - accuracy: 0.7469 - val_loss: 0.5215 - val_accuracy: 0.7527
Epoch 35/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.5258 - accuracy: 0.7477
Epoch 35: val_loss did not improve from 0.52154
852/852 [==============================] - 3s 3ms/step - loss: 0.5248 - accuracy: 0.7486 - val_loss: 0.5236 - val_accuracy: 0.7520
Epoch 36/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.5247 - accuracy: 0.7493
Epoch 36: val_loss improved from 0.52154 to 0.52117, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5256 - accuracy: 0.7494 - val_loss: 0.5212 - val_accuracy: 0.7516
Epoch 37/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.5251 - accuracy: 0.7488
Epoch 37: val_loss did not improve from 0.52117
852/852 [==============================] - 3s 3ms/step - loss: 0.5259 - accuracy: 0.7485 - val_loss: 0.5215 - val_accuracy: 0.7542
Epoch 38/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.5258 - accuracy: 0.7483
Epoch 38: val_loss did not improve from 0.52117
852/852 [==============================] - 3s 3ms/step - loss: 0.5260 - accuracy: 0.7484 - val_loss: 0.5226 - val_accuracy: 0.7508
Epoch 39/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.5260 - accuracy: 0.7489
Epoch 39: val_loss did not improve from 0.52117
852/852 [==============================] - 3s 3ms/step - loss: 0.5256 - accuracy: 0.7495 - val_loss: 0.5214 - val_accuracy: 0.7519
Epoch 40/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.5256 - accuracy: 0.7503
Epoch 40: val_loss did not improve from 0.52117
852/852 [==============================] - 2s 3ms/step - loss: 0.5251 - accuracy: 0.7508 - val_loss: 0.5241 - val_accuracy: 0.7505
Epoch 41/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.5260 - accuracy: 0.7471
Epoch 41: val_loss did not improve from 0.52117
852/852 [==============================] - 2s 3ms/step - loss: 0.5253 - accuracy: 0.7477 - val_loss: 0.5274 - val_accuracy: 0.7458
Epoch 42/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.5249 - accuracy: 0.7498
Epoch 42: val_loss did not improve from 0.52117
852/852 [==============================] - 2s 3ms/step - loss: 0.5250 - accuracy: 0.7498 - val_loss: 0.5266 - val_accuracy: 0.7478
Epoch 43/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.5243 - accuracy: 0.7498
Epoch 43: val_loss did not improve from 0.52117
852/852 [==============================] - 3s 3ms/step - loss: 0.5252 - accuracy: 0.7487 - val_loss: 0.5238 - val_accuracy: 0.7489
Epoch 44/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.5257 - accuracy: 0.7493
Epoch 44: val_loss did not improve from 0.52117
852/852 [==============================] - 3s 3ms/step - loss: 0.5254 - accuracy: 0.7495 - val_loss: 0.5212 - val_accuracy: 0.7519
Epoch 45/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.5250 - accuracy: 0.7498
Epoch 45: val_loss did not improve from 0.52117
852/852 [==============================] - 3s 3ms/step - loss: 0.5254 - accuracy: 0.7500 - val_loss: 0.5227 - val_accuracy: 0.7504
Epoch 46/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.5251 - accuracy: 0.7494
Epoch 46: val_loss did not improve from 0.52117
852/852 [==============================] - 2s 3ms/step - loss: 0.5256 - accuracy: 0.7489 - val_loss: 0.5214 - val_accuracy: 0.7520
Epoch 47/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5246 - accuracy: 0.7499
Epoch 47: val_loss did not improve from 0.52117
852/852 [==============================] - 2s 3ms/step - loss: 0.5247 - accuracy: 0.7496 - val_loss: 0.5236 - val_accuracy: 0.7485
Epoch 48/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.5256 - accuracy: 0.7493
Epoch 48: val_loss did not improve from 0.52117
852/852 [==============================] - 3s 3ms/step - loss: 0.5251 - accuracy: 0.7498 - val_loss: 0.5219 - val_accuracy: 0.7513
Epoch 49/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.5258 - accuracy: 0.7484
Epoch 49: val_loss did not improve from 0.52117
852/852 [==============================] - 3s 3ms/step - loss: 0.5252 - accuracy: 0.7489 - val_loss: 0.5237 - val_accuracy: 0.7518
Epoch 50/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5254 - accuracy: 0.7464
Epoch 50: val_loss did not improve from 0.52117
852/852 [==============================] - 3s 4ms/step - loss: 0.5250 - accuracy: 0.7466 - val_loss: 0.5246 - val_accuracy: 0.7479
Epoch 51/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.5246 - accuracy: 0.7484
Epoch 51: val_loss improved from 0.52117 to 0.51995, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 4ms/step - loss: 0.5249 - accuracy: 0.7482 - val_loss: 0.5199 - val_accuracy: 0.7519
Epoch 52/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5240 - accuracy: 0.7486
Epoch 52: val_loss did not improve from 0.51995
852/852 [==============================] - 3s 4ms/step - loss: 0.5239 - accuracy: 0.7487 - val_loss: 0.5209 - val_accuracy: 0.7519
Epoch 53/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.5248 - accuracy: 0.7482
Epoch 53: val_loss did not improve from 0.51995
852/852 [==============================] - 3s 3ms/step - loss: 0.5249 - accuracy: 0.7484 - val_loss: 0.5232 - val_accuracy: 0.7512
Epoch 54/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.5248 - accuracy: 0.7467
Epoch 54: val_loss did not improve from 0.51995
852/852 [==============================] - 3s 3ms/step - loss: 0.5250 - accuracy: 0.7469 - val_loss: 0.5213 - val_accuracy: 0.7522
Epoch 55/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.5248 - accuracy: 0.7488
Epoch 55: val_loss did not improve from 0.51995
852/852 [==============================] - 3s 3ms/step - loss: 0.5253 - accuracy: 0.7488 - val_loss: 0.5206 - val_accuracy: 0.7528
Epoch 56/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.5239 - accuracy: 0.7495
Epoch 56: val_loss did not improve from 0.51995
852/852 [==============================] - 3s 3ms/step - loss: 0.5240 - accuracy: 0.7493 - val_loss: 0.5215 - val_accuracy: 0.7507
Epoch 57/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.5244 - accuracy: 0.7490
Epoch 57: val_loss did not improve from 0.51995
852/852 [==============================] - 3s 3ms/step - loss: 0.5240 - accuracy: 0.7488 - val_loss: 0.5210 - val_accuracy: 0.7512
Epoch 58/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.5252 - accuracy: 0.7487
Epoch 58: val_loss did not improve from 0.51995
852/852 [==============================] - 3s 3ms/step - loss: 0.5246 - accuracy: 0.7487 - val_loss: 0.5222 - val_accuracy: 0.7506
Epoch 59/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.5244 - accuracy: 0.7492
Epoch 59: val_loss did not improve from 0.51995
852/852 [==============================] - 2s 3ms/step - loss: 0.5249 - accuracy: 0.7485 - val_loss: 0.5224 - val_accuracy: 0.7486
Epoch 60/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.5235 - accuracy: 0.7502
Epoch 60: val_loss did not improve from 0.51995
852/852 [==============================] - 3s 3ms/step - loss: 0.5240 - accuracy: 0.7499 - val_loss: 0.5200 - val_accuracy: 0.7518
Epoch 61/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.5243 - accuracy: 0.7483
Epoch 61: val_loss did not improve from 0.51995
852/852 [==============================] - 3s 3ms/step - loss: 0.5243 - accuracy: 0.7482 - val_loss: 0.5221 - val_accuracy: 0.7531
Epoch 62/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.5241 - accuracy: 0.7496
Epoch 62: val_loss did not improve from 0.51995
852/852 [==============================] - 3s 3ms/step - loss: 0.5241 - accuracy: 0.7501 - val_loss: 0.5205 - val_accuracy: 0.7523
Epoch 63/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.5250 - accuracy: 0.7489
Epoch 63: val_loss did not improve from 0.51995
852/852 [==============================] - 3s 3ms/step - loss: 0.5247 - accuracy: 0.7492 - val_loss: 0.5207 - val_accuracy: 0.7525
Epoch 64/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.5256 - accuracy: 0.7483
Epoch 64: val_loss did not improve from 0.51995
852/852 [==============================] - 3s 3ms/step - loss: 0.5245 - accuracy: 0.7492 - val_loss: 0.5216 - val_accuracy: 0.7512
Epoch 65/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.5245 - accuracy: 0.7476
Epoch 65: val_loss did not improve from 0.51995
852/852 [==============================] - 4s 4ms/step - loss: 0.5241 - accuracy: 0.7479 - val_loss: 0.5224 - val_accuracy: 0.7511
Epoch 66/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.5243 - accuracy: 0.7506
Epoch 66: val_loss did not improve from 0.51995
852/852 [==============================] - 3s 3ms/step - loss: 0.5241 - accuracy: 0.7506 - val_loss: 0.5201 - val_accuracy: 0.7506
Epoch 67/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.5234 - accuracy: 0.7473
Epoch 67: val_loss did not improve from 0.51995
852/852 [==============================] - 3s 3ms/step - loss: 0.5243 - accuracy: 0.7471 - val_loss: 0.5213 - val_accuracy: 0.7509
Epoch 68/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.5235 - accuracy: 0.7490
Epoch 68: val_loss did not improve from 0.51995
852/852 [==============================] - 3s 3ms/step - loss: 0.5239 - accuracy: 0.7493 - val_loss: 0.5209 - val_accuracy: 0.7513
Epoch 69/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.5245 - accuracy: 0.7477
Epoch 69: val_loss did not improve from 0.51995
852/852 [==============================] - 3s 3ms/step - loss: 0.5243 - accuracy: 0.7477 - val_loss: 0.5212 - val_accuracy: 0.7514
Epoch 70/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.5222 - accuracy: 0.7523
Epoch 70: val_loss did not improve from 0.51995
852/852 [==============================] - 2s 3ms/step - loss: 0.5238 - accuracy: 0.7506 - val_loss: 0.5252 - val_accuracy: 0.7481
Epoch 71/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.5241 - accuracy: 0.7473
Epoch 71: val_loss improved from 0.51995 to 0.51961, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5249 - accuracy: 0.7468 - val_loss: 0.5196 - val_accuracy: 0.7520
Epoch 72/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.5224 - accuracy: 0.7474
Epoch 72: val_loss did not improve from 0.51961
852/852 [==============================] - 3s 3ms/step - loss: 0.5232 - accuracy: 0.7467 - val_loss: 0.5213 - val_accuracy: 0.7511
Epoch 73/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.5227 - accuracy: 0.7505
Epoch 73: val_loss did not improve from 0.51961
852/852 [==============================] - 2s 3ms/step - loss: 0.5242 - accuracy: 0.7494 - val_loss: 0.5211 - val_accuracy: 0.7525
Epoch 74/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.5237 - accuracy: 0.7493
Epoch 74: val_loss improved from 0.51961 to 0.51923, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5239 - accuracy: 0.7489 - val_loss: 0.5192 - val_accuracy: 0.7528
Epoch 75/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.5250 - accuracy: 0.7479
Epoch 75: val_loss did not improve from 0.51923
852/852 [==============================] - 3s 3ms/step - loss: 0.5247 - accuracy: 0.7478 - val_loss: 0.5212 - val_accuracy: 0.7515
Epoch 76/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.5237 - accuracy: 0.7498
Epoch 76: val_loss did not improve from 0.51923
852/852 [==============================] - 3s 3ms/step - loss: 0.5242 - accuracy: 0.7494 - val_loss: 0.5201 - val_accuracy: 0.7516
Epoch 77/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.5233 - accuracy: 0.7493
Epoch 77: val_loss did not improve from 0.51923
852/852 [==============================] - 2s 3ms/step - loss: 0.5235 - accuracy: 0.7492 - val_loss: 0.5210 - val_accuracy: 0.7520
Epoch 78/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.5221 - accuracy: 0.7513
Epoch 78: val_loss did not improve from 0.51923
852/852 [==============================] - 2s 3ms/step - loss: 0.5242 - accuracy: 0.7500 - val_loss: 0.5219 - val_accuracy: 0.7504
Epoch 79/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5243 - accuracy: 0.7486
Epoch 79: val_loss did not improve from 0.51923
852/852 [==============================] - 3s 3ms/step - loss: 0.5244 - accuracy: 0.7485 - val_loss: 0.5215 - val_accuracy: 0.7492
Epoch 80/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.5232 - accuracy: 0.7502
Epoch 80: val_loss did not improve from 0.51923
852/852 [==============================] - 3s 3ms/step - loss: 0.5227 - accuracy: 0.7514 - val_loss: 0.5224 - val_accuracy: 0.7525
Epoch 81/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.5251 - accuracy: 0.7503
Epoch 81: val_loss did not improve from 0.51923
852/852 [==============================] - 4s 4ms/step - loss: 0.5241 - accuracy: 0.7511 - val_loss: 0.5223 - val_accuracy: 0.7505
Epoch 82/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.5245 - accuracy: 0.7500
Epoch 82: val_loss did not improve from 0.51923
852/852 [==============================] - 3s 3ms/step - loss: 0.5239 - accuracy: 0.7498 - val_loss: 0.5205 - val_accuracy: 0.7518
Epoch 83/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5224 - accuracy: 0.7519
Epoch 83: val_loss did not improve from 0.51923
852/852 [==============================] - 3s 3ms/step - loss: 0.5234 - accuracy: 0.7509 - val_loss: 0.5387 - val_accuracy: 0.7356
Epoch 84/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.5253 - accuracy: 0.7465
Epoch 84: val_loss did not improve from 0.51923
852/852 [==============================] - 3s 3ms/step - loss: 0.5246 - accuracy: 0.7468 - val_loss: 0.5232 - val_accuracy: 0.7480
Epoch 85/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.5233 - accuracy: 0.7505
Epoch 85: val_loss did not improve from 0.51923
852/852 [==============================] - 3s 3ms/step - loss: 0.5241 - accuracy: 0.7507 - val_loss: 0.5243 - val_accuracy: 0.7479
Epoch 86/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.5245 - accuracy: 0.7479
Epoch 86: val_loss did not improve from 0.51923
852/852 [==============================] - 3s 3ms/step - loss: 0.5244 - accuracy: 0.7479 - val_loss: 0.5218 - val_accuracy: 0.7513
Epoch 87/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.5235 - accuracy: 0.7506
Epoch 87: val_loss did not improve from 0.51923
852/852 [==============================] - 3s 3ms/step - loss: 0.5242 - accuracy: 0.7492 - val_loss: 0.5214 - val_accuracy: 0.7499
Epoch 88/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.5244 - accuracy: 0.7505
Epoch 88: val_loss did not improve from 0.51923
852/852 [==============================] - 3s 3ms/step - loss: 0.5237 - accuracy: 0.7514 - val_loss: 0.5195 - val_accuracy: 0.7513
Epoch 89/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.5248 - accuracy: 0.7466
Epoch 89: val_loss did not improve from 0.51923
852/852 [==============================] - 3s 3ms/step - loss: 0.5244 - accuracy: 0.7468 - val_loss: 0.5213 - val_accuracy: 0.7512
Epoch 90/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.5219 - accuracy: 0.7484
Epoch 90: val_loss did not improve from 0.51923
852/852 [==============================] - 3s 4ms/step - loss: 0.5236 - accuracy: 0.7473 - val_loss: 0.5221 - val_accuracy: 0.7494
Epoch 91/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5236 - accuracy: 0.7499
Epoch 91: val_loss did not improve from 0.51923
852/852 [==============================] - 3s 3ms/step - loss: 0.5236 - accuracy: 0.7498 - val_loss: 0.5214 - val_accuracy: 0.7509
Epoch 92/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.5248 - accuracy: 0.7493
Epoch 92: val_loss did not improve from 0.51923
852/852 [==============================] - 3s 3ms/step - loss: 0.5239 - accuracy: 0.7505 - val_loss: 0.5196 - val_accuracy: 0.7523
Epoch 93/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.5243 - accuracy: 0.7478
Epoch 93: val_loss did not improve from 0.51923
852/852 [==============================] - 3s 4ms/step - loss: 0.5240 - accuracy: 0.7480 - val_loss: 0.5206 - val_accuracy: 0.7518
Epoch 94/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.5234 - accuracy: 0.7507
Epoch 94: val_loss did not improve from 0.51923
852/852 [==============================] - 3s 3ms/step - loss: 0.5235 - accuracy: 0.7501 - val_loss: 0.5217 - val_accuracy: 0.7505
Epoch 94: early stopping
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=2f735644-d2ec-4674-8d46-1f726d34cc7c">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [60]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">history_15</span> <span class="o">=</span> <span class="n">fifteen_neuron</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">callback_a</span><span class="p">,</span> <span class="n">callback_b</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.5220 - accuracy: 0.7508
Epoch 1: val_loss did not improve from 0.51923
852/852 [==============================] - 3s 3ms/step - loss: 0.5232 - accuracy: 0.7502 - val_loss: 0.5217 - val_accuracy: 0.7515
Epoch 2/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5235 - accuracy: 0.7504
Epoch 2: val_loss did not improve from 0.51923
852/852 [==============================] - 3s 3ms/step - loss: 0.5237 - accuracy: 0.7500 - val_loss: 0.5211 - val_accuracy: 0.7508
Epoch 3/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.5234 - accuracy: 0.7479
Epoch 3: val_loss did not improve from 0.51923
852/852 [==============================] - 2s 3ms/step - loss: 0.5235 - accuracy: 0.7480 - val_loss: 0.5206 - val_accuracy: 0.7521
Epoch 4/150
852/852 [==============================] - ETA: 0s - loss: 0.5233 - accuracy: 0.7493
Epoch 4: val_loss did not improve from 0.51923
852/852 [==============================] - 3s 3ms/step - loss: 0.5233 - accuracy: 0.7493 - val_loss: 0.5199 - val_accuracy: 0.7529
Epoch 5/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.5243 - accuracy: 0.7494
Epoch 5: val_loss did not improve from 0.51923
852/852 [==============================] - 2s 3ms/step - loss: 0.5239 - accuracy: 0.7499 - val_loss: 0.5207 - val_accuracy: 0.7509
Epoch 6/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5233 - accuracy: 0.7489
Epoch 6: val_loss did not improve from 0.51923
852/852 [==============================] - 3s 3ms/step - loss: 0.5234 - accuracy: 0.7491 - val_loss: 0.5219 - val_accuracy: 0.7513
Epoch 7/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.5246 - accuracy: 0.7471
Epoch 7: val_loss did not improve from 0.51923
852/852 [==============================] - 2s 3ms/step - loss: 0.5234 - accuracy: 0.7479 - val_loss: 0.5197 - val_accuracy: 0.7522
Epoch 8/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.5234 - accuracy: 0.7494
Epoch 8: val_loss did not improve from 0.51923
852/852 [==============================] - 2s 3ms/step - loss: 0.5233 - accuracy: 0.7499 - val_loss: 0.5197 - val_accuracy: 0.7523
Epoch 9/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.5236 - accuracy: 0.7486
Epoch 9: val_loss did not improve from 0.51923
852/852 [==============================] - 3s 3ms/step - loss: 0.5232 - accuracy: 0.7487 - val_loss: 0.5205 - val_accuracy: 0.7532
Epoch 10/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.5239 - accuracy: 0.7504
Epoch 10: val_loss did not improve from 0.51923
852/852 [==============================] - 2s 3ms/step - loss: 0.5239 - accuracy: 0.7506 - val_loss: 0.5240 - val_accuracy: 0.7474
Epoch 11/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.5241 - accuracy: 0.7502
Epoch 11: val_loss did not improve from 0.51923
852/852 [==============================] - 3s 3ms/step - loss: 0.5235 - accuracy: 0.7506 - val_loss: 0.5204 - val_accuracy: 0.7521
Epoch 12/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.5230 - accuracy: 0.7511
Epoch 12: val_loss did not improve from 0.51923
852/852 [==============================] - 2s 3ms/step - loss: 0.5238 - accuracy: 0.7501 - val_loss: 0.5211 - val_accuracy: 0.7512
Epoch 13/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.5234 - accuracy: 0.7503
Epoch 13: val_loss did not improve from 0.51923
852/852 [==============================] - 2s 3ms/step - loss: 0.5236 - accuracy: 0.7496 - val_loss: 0.5232 - val_accuracy: 0.7488
Epoch 14/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.5243 - accuracy: 0.7493
Epoch 14: val_loss did not improve from 0.51923
852/852 [==============================] - 2s 3ms/step - loss: 0.5232 - accuracy: 0.7502 - val_loss: 0.5232 - val_accuracy: 0.7485
Epoch 15/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.5238 - accuracy: 0.7471
Epoch 15: val_loss did not improve from 0.51923
852/852 [==============================] - 3s 3ms/step - loss: 0.5245 - accuracy: 0.7469 - val_loss: 0.5206 - val_accuracy: 0.7518
Epoch 16/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.5237 - accuracy: 0.7496
Epoch 16: val_loss did not improve from 0.51923
852/852 [==============================] - 3s 3ms/step - loss: 0.5243 - accuracy: 0.7489 - val_loss: 0.5196 - val_accuracy: 0.7522
Epoch 17/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.5226 - accuracy: 0.7493
Epoch 17: val_loss did not improve from 0.51923
852/852 [==============================] - 3s 3ms/step - loss: 0.5225 - accuracy: 0.7494 - val_loss: 0.5234 - val_accuracy: 0.7492
Epoch 18/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5226 - accuracy: 0.7481
Epoch 18: val_loss did not improve from 0.51923
852/852 [==============================] - 3s 3ms/step - loss: 0.5236 - accuracy: 0.7477 - val_loss: 0.5195 - val_accuracy: 0.7520
Epoch 19/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.5241 - accuracy: 0.7494
Epoch 19: val_loss did not improve from 0.51923
852/852 [==============================] - 2s 3ms/step - loss: 0.5240 - accuracy: 0.7495 - val_loss: 0.5209 - val_accuracy: 0.7505
Epoch 20/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.5229 - accuracy: 0.7505
Epoch 20: val_loss did not improve from 0.51923
852/852 [==============================] - 3s 3ms/step - loss: 0.5230 - accuracy: 0.7505 - val_loss: 0.5220 - val_accuracy: 0.7489
Epoch 21/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5228 - accuracy: 0.7498
Epoch 21: val_loss did not improve from 0.51923
852/852 [==============================] - 2s 3ms/step - loss: 0.5230 - accuracy: 0.7496 - val_loss: 0.5215 - val_accuracy: 0.7512
Epoch 22/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.5208 - accuracy: 0.7518
Epoch 22: val_loss improved from 0.51923 to 0.51894, saving model to fifteen_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5229 - accuracy: 0.7500 - val_loss: 0.5189 - val_accuracy: 0.7527
Epoch 23/150
852/852 [==============================] - ETA: 0s - loss: 0.5231 - accuracy: 0.7491
Epoch 23: val_loss did not improve from 0.51894
852/852 [==============================] - 3s 3ms/step - loss: 0.5231 - accuracy: 0.7491 - val_loss: 0.5204 - val_accuracy: 0.7513
Epoch 24/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.5242 - accuracy: 0.7488
Epoch 24: val_loss did not improve from 0.51894
852/852 [==============================] - 2s 3ms/step - loss: 0.5239 - accuracy: 0.7488 - val_loss: 0.5198 - val_accuracy: 0.7511
Epoch 25/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.5231 - accuracy: 0.7501
Epoch 25: val_loss did not improve from 0.51894
852/852 [==============================] - 2s 3ms/step - loss: 0.5230 - accuracy: 0.7501 - val_loss: 0.5200 - val_accuracy: 0.7522
Epoch 26/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5221 - accuracy: 0.7499
Epoch 26: val_loss did not improve from 0.51894
852/852 [==============================] - 3s 3ms/step - loss: 0.5226 - accuracy: 0.7493 - val_loss: 0.5194 - val_accuracy: 0.7512
Epoch 27/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.5224 - accuracy: 0.7523
Epoch 27: val_loss did not improve from 0.51894
852/852 [==============================] - 3s 3ms/step - loss: 0.5226 - accuracy: 0.7521 - val_loss: 0.5228 - val_accuracy: 0.7474
Epoch 28/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5219 - accuracy: 0.7504
Epoch 28: val_loss did not improve from 0.51894
852/852 [==============================] - 2s 3ms/step - loss: 0.5225 - accuracy: 0.7504 - val_loss: 0.5234 - val_accuracy: 0.7485
Epoch 29/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.5238 - accuracy: 0.7510
Epoch 29: val_loss did not improve from 0.51894
852/852 [==============================] - 3s 3ms/step - loss: 0.5232 - accuracy: 0.7511 - val_loss: 0.5214 - val_accuracy: 0.7515
Epoch 30/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.5238 - accuracy: 0.7492
Epoch 30: val_loss did not improve from 0.51894
852/852 [==============================] - 3s 3ms/step - loss: 0.5238 - accuracy: 0.7492 - val_loss: 0.5200 - val_accuracy: 0.7528
Epoch 31/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5225 - accuracy: 0.7492
Epoch 31: val_loss did not improve from 0.51894
852/852 [==============================] - 2s 3ms/step - loss: 0.5233 - accuracy: 0.7482 - val_loss: 0.5196 - val_accuracy: 0.7538
Epoch 32/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.5227 - accuracy: 0.7492
Epoch 32: val_loss did not improve from 0.51894
852/852 [==============================] - 2s 3ms/step - loss: 0.5230 - accuracy: 0.7491 - val_loss: 0.5207 - val_accuracy: 0.7516
Epoch 33/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.5227 - accuracy: 0.7477
Epoch 33: val_loss did not improve from 0.51894
852/852 [==============================] - 3s 3ms/step - loss: 0.5233 - accuracy: 0.7479 - val_loss: 0.5206 - val_accuracy: 0.7511
Epoch 34/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.5246 - accuracy: 0.7478
Epoch 34: val_loss did not improve from 0.51894
852/852 [==============================] - 3s 3ms/step - loss: 0.5242 - accuracy: 0.7479 - val_loss: 0.5201 - val_accuracy: 0.7519
Epoch 35/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.5222 - accuracy: 0.7522
Epoch 35: val_loss did not improve from 0.51894
852/852 [==============================] - 3s 3ms/step - loss: 0.5233 - accuracy: 0.7514 - val_loss: 0.5232 - val_accuracy: 0.7504
Epoch 36/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.5237 - accuracy: 0.7489
Epoch 36: val_loss did not improve from 0.51894
852/852 [==============================] - 3s 3ms/step - loss: 0.5231 - accuracy: 0.7494 - val_loss: 0.5256 - val_accuracy: 0.7448
Epoch 37/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.5240 - accuracy: 0.7502
Epoch 37: val_loss did not improve from 0.51894
852/852 [==============================] - 3s 3ms/step - loss: 0.5237 - accuracy: 0.7504 - val_loss: 0.5192 - val_accuracy: 0.7527
Epoch 38/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.5227 - accuracy: 0.7492
Epoch 38: val_loss did not improve from 0.51894
852/852 [==============================] - 3s 3ms/step - loss: 0.5227 - accuracy: 0.7498 - val_loss: 0.5197 - val_accuracy: 0.7526
Epoch 39/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5222 - accuracy: 0.7508
Epoch 39: val_loss did not improve from 0.51894
852/852 [==============================] - 2s 3ms/step - loss: 0.5226 - accuracy: 0.7506 - val_loss: 0.5195 - val_accuracy: 0.7523
Epoch 40/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.5219 - accuracy: 0.7505
Epoch 40: val_loss did not improve from 0.51894
852/852 [==============================] - 3s 3ms/step - loss: 0.5228 - accuracy: 0.7499 - val_loss: 0.5197 - val_accuracy: 0.7522
Epoch 41/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.5221 - accuracy: 0.7504
Epoch 41: val_loss did not improve from 0.51894
852/852 [==============================] - 2s 3ms/step - loss: 0.5227 - accuracy: 0.7501 - val_loss: 0.5202 - val_accuracy: 0.7511
Epoch 42/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.5243 - accuracy: 0.7472
Epoch 42: val_loss did not improve from 0.51894
852/852 [==============================] - 3s 3ms/step - loss: 0.5237 - accuracy: 0.7477 - val_loss: 0.5204 - val_accuracy: 0.7506
Epoch 42: early stopping
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=ec342463-ed38-47b6-be86-e7666591ca61">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [68]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">fifteen_neuron</span><span class="o">.</span><span class="n">load_weights</span><span class="p">(</span><span class="s1">'fifteen_model.hdf5'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=622d74e8-4e67-40af-a21d-89ac0fc95130">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [69]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Other Metrics</span>
<span class="n">fifteen_neuron_preds</span> <span class="o">=</span> <span class="n">fifteen_neuron</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">fifteen_neuron_preds</span><span class="o">.</span><span class="n">round</span><span class="p">())</span>
<span class="n">precision</span> <span class="o">=</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">fifteen_neuron_preds</span><span class="o">.</span><span class="n">round</span><span class="p">())</span>
<span class="n">recall</span> <span class="o">=</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">fifteen_neuron_preds</span><span class="o">.</span><span class="n">round</span><span class="p">())</span>
<span class="n">f1score</span> <span class="o">=</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">fifteen_neuron_preds</span><span class="o">.</span><span class="n">round</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Accuracy: </span><span class="si">%.2f</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">accuracy</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Precision: </span><span class="si">%.2f</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">precision</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Recall: </span><span class="si">%.2f</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">recall</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"F1-score: </span><span class="si">%.2f</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">f1score</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>267/267 [==============================] - 0s 939us/step
Accuracy: 0.75
Precision: 0.84
Recall: 0.62
F1-score: 0.72
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=25c459b9-b1e6-4527-9bf5-39caafb0fcea">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="2.6-Build-a-Model-with-31-Neurons-in-5-Layers">2.6 Build a Model with 31 Neurons in 5 Layers<a class="anchor-link" href="#2.6-Build-a-Model-with-31-Neurons-in-5-Layers">¶</a></h4>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=bfb43564-c84f-4e85-b87c-78b9a6177236">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [61]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># constructing a 31 neuron model</span>
<span class="n">thirty_one_neuron</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">thirty_one_neuron</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">input_dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">))</span>
<span class="n">thirty_one_neuron</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">))</span>
<span class="n">thirty_one_neuron</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">))</span>
<span class="n">thirty_one_neuron</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">))</span>
<span class="n">thirty_one_neuron</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'sigmoid'</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">thirty_one_neuron</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>

<span class="c1">#Check for cycles in the Sequential Model</span>
<span class="n">thirty_one_neuron</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">'binary_crossentropy'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s1">'adam'</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'accuracy'</span><span class="p">])</span>
<span class="n">callback_a</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">filepath</span> <span class="o">=</span> <span class="s1">'thirty_one_model.hdf5'</span><span class="p">,</span> <span class="n">monitor</span><span class="o">=</span><span class="s1">'val_loss'</span><span class="p">,</span> <span class="n">save_best_only</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">save_weights_only</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">verbose</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">callback_b</span> <span class="o">=</span> <span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s1">'val_loss'</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">'min'</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">history_31</span> <span class="o">=</span> <span class="n">thirty_one_neuron</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">callback_a</span><span class="p">,</span> <span class="n">callback_b</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Model: "sequential_14"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense_19 (Dense)            (None, 16)                432       
                                                                 
 dense_20 (Dense)            (None, 8)                 136       
                                                                 
 dense_21 (Dense)            (None, 4)                 36        
                                                                 
 dense_22 (Dense)            (None, 2)                 10        
                                                                 
 dense_23 (Dense)            (None, 1)                 3         
                                                                 
=================================================================
Total params: 617 (2.41 KB)
Trainable params: 617 (2.41 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
None
Epoch 1/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.6233 - accuracy: 0.6768
Epoch 1: val_loss improved from inf to 0.58202, saving model to thirty_one_model.hdf5
852/852 [==============================] - 4s 3ms/step - loss: 0.6223 - accuracy: 0.6767 - val_loss: 0.5820 - val_accuracy: 0.7198
Epoch 2/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.5737 - accuracy: 0.7216
Epoch 2: val_loss improved from 0.58202 to 0.56364, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5736 - accuracy: 0.7217 - val_loss: 0.5636 - val_accuracy: 0.7238
Epoch 3/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.5631 - accuracy: 0.7230
Epoch 3: val_loss improved from 0.56364 to 0.55879, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5628 - accuracy: 0.7237 - val_loss: 0.5588 - val_accuracy: 0.7282
Epoch 4/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.5594 - accuracy: 0.7213
Epoch 4: val_loss improved from 0.55879 to 0.55502, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5596 - accuracy: 0.7211 - val_loss: 0.5550 - val_accuracy: 0.7278
Epoch 5/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.5563 - accuracy: 0.7260
Epoch 5: val_loss improved from 0.55502 to 0.55324, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5567 - accuracy: 0.7255 - val_loss: 0.5532 - val_accuracy: 0.7282
Epoch 6/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.5553 - accuracy: 0.7264
Epoch 6: val_loss improved from 0.55324 to 0.55239, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5555 - accuracy: 0.7262 - val_loss: 0.5524 - val_accuracy: 0.7282
Epoch 7/150
852/852 [==============================] - ETA: 0s - loss: 0.5536 - accuracy: 0.7298
Epoch 7: val_loss improved from 0.55239 to 0.55131, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5536 - accuracy: 0.7298 - val_loss: 0.5513 - val_accuracy: 0.7292
Epoch 8/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.5532 - accuracy: 0.7271
Epoch 8: val_loss did not improve from 0.55131
852/852 [==============================] - 3s 3ms/step - loss: 0.5540 - accuracy: 0.7266 - val_loss: 0.5515 - val_accuracy: 0.7299
Epoch 9/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.5530 - accuracy: 0.7287
Epoch 9: val_loss did not improve from 0.55131
852/852 [==============================] - 3s 3ms/step - loss: 0.5527 - accuracy: 0.7290 - val_loss: 0.5528 - val_accuracy: 0.7269
Epoch 10/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.5525 - accuracy: 0.7293
Epoch 10: val_loss improved from 0.55131 to 0.54907, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5525 - accuracy: 0.7293 - val_loss: 0.5491 - val_accuracy: 0.7300
Epoch 11/150
852/852 [==============================] - ETA: 0s - loss: 0.5522 - accuracy: 0.7289
Epoch 11: val_loss improved from 0.54907 to 0.54886, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5522 - accuracy: 0.7289 - val_loss: 0.5489 - val_accuracy: 0.7312
Epoch 12/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.5515 - accuracy: 0.7299
Epoch 12: val_loss improved from 0.54886 to 0.54771, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5513 - accuracy: 0.7297 - val_loss: 0.5477 - val_accuracy: 0.7319
Epoch 13/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.5508 - accuracy: 0.7313
Epoch 13: val_loss did not improve from 0.54771
852/852 [==============================] - 3s 3ms/step - loss: 0.5504 - accuracy: 0.7317 - val_loss: 0.5489 - val_accuracy: 0.7309
Epoch 14/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.5507 - accuracy: 0.7321
Epoch 14: val_loss improved from 0.54771 to 0.54706, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5501 - accuracy: 0.7325 - val_loss: 0.5471 - val_accuracy: 0.7313
Epoch 15/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.5501 - accuracy: 0.7314
Epoch 15: val_loss did not improve from 0.54706
852/852 [==============================] - 3s 3ms/step - loss: 0.5507 - accuracy: 0.7306 - val_loss: 0.5503 - val_accuracy: 0.7263
Epoch 16/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.5494 - accuracy: 0.7326
Epoch 16: val_loss improved from 0.54706 to 0.54529, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5493 - accuracy: 0.7324 - val_loss: 0.5453 - val_accuracy: 0.7344
Epoch 17/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5476 - accuracy: 0.7308
Epoch 17: val_loss did not improve from 0.54529
852/852 [==============================] - 3s 3ms/step - loss: 0.5488 - accuracy: 0.7302 - val_loss: 0.5455 - val_accuracy: 0.7306
Epoch 18/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.5477 - accuracy: 0.7331
Epoch 18: val_loss did not improve from 0.54529
852/852 [==============================] - 3s 3ms/step - loss: 0.5485 - accuracy: 0.7320 - val_loss: 0.5473 - val_accuracy: 0.7302
Epoch 19/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.5473 - accuracy: 0.7316
Epoch 19: val_loss improved from 0.54529 to 0.54497, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5482 - accuracy: 0.7310 - val_loss: 0.5450 - val_accuracy: 0.7376
Epoch 20/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.5455 - accuracy: 0.7336
Epoch 20: val_loss improved from 0.54497 to 0.54461, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5473 - accuracy: 0.7325 - val_loss: 0.5446 - val_accuracy: 0.7359
Epoch 21/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.5461 - accuracy: 0.7356
Epoch 21: val_loss improved from 0.54461 to 0.54265, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5463 - accuracy: 0.7354 - val_loss: 0.5426 - val_accuracy: 0.7344
Epoch 22/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.5463 - accuracy: 0.7339
Epoch 22: val_loss did not improve from 0.54265
852/852 [==============================] - 3s 3ms/step - loss: 0.5462 - accuracy: 0.7341 - val_loss: 0.5430 - val_accuracy: 0.7380
Epoch 23/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.5443 - accuracy: 0.7344
Epoch 23: val_loss did not improve from 0.54265
852/852 [==============================] - 3s 3ms/step - loss: 0.5452 - accuracy: 0.7341 - val_loss: 0.5469 - val_accuracy: 0.7354
Epoch 24/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5448 - accuracy: 0.7349
Epoch 24: val_loss improved from 0.54265 to 0.54075, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5448 - accuracy: 0.7354 - val_loss: 0.5407 - val_accuracy: 0.7392
Epoch 25/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.5431 - accuracy: 0.7354
Epoch 25: val_loss did not improve from 0.54075
852/852 [==============================] - 3s 3ms/step - loss: 0.5437 - accuracy: 0.7352 - val_loss: 0.5419 - val_accuracy: 0.7343
Epoch 26/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.5441 - accuracy: 0.7359
Epoch 26: val_loss improved from 0.54075 to 0.53917, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5443 - accuracy: 0.7358 - val_loss: 0.5392 - val_accuracy: 0.7368
Epoch 27/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.5435 - accuracy: 0.7347
Epoch 27: val_loss did not improve from 0.53917
852/852 [==============================] - 3s 3ms/step - loss: 0.5433 - accuracy: 0.7346 - val_loss: 0.5422 - val_accuracy: 0.7350
Epoch 28/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.5432 - accuracy: 0.7371
Epoch 28: val_loss improved from 0.53917 to 0.53810, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5432 - accuracy: 0.7371 - val_loss: 0.5381 - val_accuracy: 0.7368
Epoch 29/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.5415 - accuracy: 0.7331
Epoch 29: val_loss did not improve from 0.53810
852/852 [==============================] - 3s 3ms/step - loss: 0.5410 - accuracy: 0.7334 - val_loss: 0.5426 - val_accuracy: 0.7309
Epoch 30/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.5428 - accuracy: 0.7358
Epoch 30: val_loss improved from 0.53810 to 0.53800, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5426 - accuracy: 0.7360 - val_loss: 0.5380 - val_accuracy: 0.7400
Epoch 31/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.5415 - accuracy: 0.7376
Epoch 31: val_loss did not improve from 0.53800
852/852 [==============================] - 3s 3ms/step - loss: 0.5413 - accuracy: 0.7374 - val_loss: 0.5413 - val_accuracy: 0.7392
Epoch 32/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.5407 - accuracy: 0.7365
Epoch 32: val_loss did not improve from 0.53800
852/852 [==============================] - 3s 3ms/step - loss: 0.5413 - accuracy: 0.7364 - val_loss: 0.5410 - val_accuracy: 0.7393
Epoch 33/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.5412 - accuracy: 0.7358
Epoch 33: val_loss improved from 0.53800 to 0.53745, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5408 - accuracy: 0.7363 - val_loss: 0.5374 - val_accuracy: 0.7380
Epoch 34/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.5405 - accuracy: 0.7385
Epoch 34: val_loss improved from 0.53745 to 0.53596, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5404 - accuracy: 0.7384 - val_loss: 0.5360 - val_accuracy: 0.7393
Epoch 35/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.5384 - accuracy: 0.7388
Epoch 35: val_loss did not improve from 0.53596
852/852 [==============================] - 3s 3ms/step - loss: 0.5398 - accuracy: 0.7378 - val_loss: 0.5403 - val_accuracy: 0.7377
Epoch 36/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.5399 - accuracy: 0.7370
Epoch 36: val_loss improved from 0.53596 to 0.53404, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5396 - accuracy: 0.7371 - val_loss: 0.5340 - val_accuracy: 0.7395
Epoch 37/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.5401 - accuracy: 0.7355
Epoch 37: val_loss did not improve from 0.53404
852/852 [==============================] - 3s 3ms/step - loss: 0.5397 - accuracy: 0.7358 - val_loss: 0.5358 - val_accuracy: 0.7381
Epoch 38/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.5406 - accuracy: 0.7372
Epoch 38: val_loss did not improve from 0.53404
852/852 [==============================] - 3s 3ms/step - loss: 0.5397 - accuracy: 0.7378 - val_loss: 0.5364 - val_accuracy: 0.7368
Epoch 39/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.5374 - accuracy: 0.7401
Epoch 39: val_loss did not improve from 0.53404
852/852 [==============================] - 3s 3ms/step - loss: 0.5378 - accuracy: 0.7392 - val_loss: 0.5478 - val_accuracy: 0.7267
Epoch 40/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.5381 - accuracy: 0.7382
Epoch 40: val_loss improved from 0.53404 to 0.53311, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5380 - accuracy: 0.7377 - val_loss: 0.5331 - val_accuracy: 0.7425
Epoch 41/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.5366 - accuracy: 0.7405
Epoch 41: val_loss did not improve from 0.53311
852/852 [==============================] - 3s 3ms/step - loss: 0.5371 - accuracy: 0.7403 - val_loss: 0.5344 - val_accuracy: 0.7433
Epoch 42/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.5371 - accuracy: 0.7379
Epoch 42: val_loss improved from 0.53311 to 0.53196, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5370 - accuracy: 0.7384 - val_loss: 0.5320 - val_accuracy: 0.7453
Epoch 43/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5366 - accuracy: 0.7399
Epoch 43: val_loss did not improve from 0.53196
852/852 [==============================] - 3s 3ms/step - loss: 0.5370 - accuracy: 0.7394 - val_loss: 0.5366 - val_accuracy: 0.7404
Epoch 44/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.5356 - accuracy: 0.7402
Epoch 44: val_loss did not improve from 0.53196
852/852 [==============================] - 3s 3ms/step - loss: 0.5354 - accuracy: 0.7405 - val_loss: 0.5336 - val_accuracy: 0.7433
Epoch 45/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.5346 - accuracy: 0.7375
Epoch 45: val_loss improved from 0.53196 to 0.53102, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5353 - accuracy: 0.7366 - val_loss: 0.5310 - val_accuracy: 0.7411
Epoch 46/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.5344 - accuracy: 0.7407
Epoch 46: val_loss did not improve from 0.53102
852/852 [==============================] - 3s 3ms/step - loss: 0.5346 - accuracy: 0.7404 - val_loss: 0.5321 - val_accuracy: 0.7422
Epoch 47/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.5351 - accuracy: 0.7401
Epoch 47: val_loss improved from 0.53102 to 0.52886, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5351 - accuracy: 0.7395 - val_loss: 0.5289 - val_accuracy: 0.7432
Epoch 48/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5339 - accuracy: 0.7387
Epoch 48: val_loss did not improve from 0.52886
852/852 [==============================] - 3s 3ms/step - loss: 0.5344 - accuracy: 0.7387 - val_loss: 0.5324 - val_accuracy: 0.7405
Epoch 49/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.5339 - accuracy: 0.7408
Epoch 49: val_loss improved from 0.52886 to 0.52869, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5335 - accuracy: 0.7410 - val_loss: 0.5287 - val_accuracy: 0.7448
Epoch 50/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.5334 - accuracy: 0.7396
Epoch 50: val_loss did not improve from 0.52869
852/852 [==============================] - 3s 3ms/step - loss: 0.5332 - accuracy: 0.7401 - val_loss: 0.5409 - val_accuracy: 0.7357
Epoch 51/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.5341 - accuracy: 0.7420
Epoch 51: val_loss improved from 0.52869 to 0.52841, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5330 - accuracy: 0.7430 - val_loss: 0.5284 - val_accuracy: 0.7435
Epoch 52/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.5328 - accuracy: 0.7415
Epoch 52: val_loss did not improve from 0.52841
852/852 [==============================] - 3s 3ms/step - loss: 0.5329 - accuracy: 0.7414 - val_loss: 0.5365 - val_accuracy: 0.7417
Epoch 53/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.5319 - accuracy: 0.7434
Epoch 53: val_loss did not improve from 0.52841
852/852 [==============================] - 3s 3ms/step - loss: 0.5315 - accuracy: 0.7434 - val_loss: 0.5332 - val_accuracy: 0.7426
Epoch 54/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.5331 - accuracy: 0.7436
Epoch 54: val_loss improved from 0.52841 to 0.52677, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5319 - accuracy: 0.7440 - val_loss: 0.5268 - val_accuracy: 0.7444
Epoch 55/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.5328 - accuracy: 0.7376
Epoch 55: val_loss improved from 0.52677 to 0.52513, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5318 - accuracy: 0.7390 - val_loss: 0.5251 - val_accuracy: 0.7460
Epoch 56/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.5315 - accuracy: 0.7426
Epoch 56: val_loss improved from 0.52513 to 0.52448, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5313 - accuracy: 0.7430 - val_loss: 0.5245 - val_accuracy: 0.7454
Epoch 57/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.5305 - accuracy: 0.7419
Epoch 57: val_loss did not improve from 0.52448
852/852 [==============================] - 3s 3ms/step - loss: 0.5310 - accuracy: 0.7417 - val_loss: 0.5320 - val_accuracy: 0.7451
Epoch 58/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.5315 - accuracy: 0.7445
Epoch 58: val_loss did not improve from 0.52448
852/852 [==============================] - 3s 3ms/step - loss: 0.5315 - accuracy: 0.7444 - val_loss: 0.5270 - val_accuracy: 0.7450
Epoch 59/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.5295 - accuracy: 0.7437
Epoch 59: val_loss did not improve from 0.52448
852/852 [==============================] - 3s 3ms/step - loss: 0.5294 - accuracy: 0.7438 - val_loss: 0.5297 - val_accuracy: 0.7404
Epoch 60/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.5309 - accuracy: 0.7435
Epoch 60: val_loss did not improve from 0.52448
852/852 [==============================] - 3s 3ms/step - loss: 0.5307 - accuracy: 0.7437 - val_loss: 0.5276 - val_accuracy: 0.7420
Epoch 61/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.5289 - accuracy: 0.7442
Epoch 61: val_loss did not improve from 0.52448
852/852 [==============================] - 3s 3ms/step - loss: 0.5293 - accuracy: 0.7441 - val_loss: 0.5281 - val_accuracy: 0.7471
Epoch 62/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.5307 - accuracy: 0.7398
Epoch 62: val_loss did not improve from 0.52448
852/852 [==============================] - 3s 3ms/step - loss: 0.5302 - accuracy: 0.7399 - val_loss: 0.5247 - val_accuracy: 0.7474
Epoch 63/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.5292 - accuracy: 0.7431
Epoch 63: val_loss improved from 0.52448 to 0.52292, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5300 - accuracy: 0.7422 - val_loss: 0.5229 - val_accuracy: 0.7496
Epoch 64/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.5290 - accuracy: 0.7455
Epoch 64: val_loss did not improve from 0.52292
852/852 [==============================] - 3s 3ms/step - loss: 0.5291 - accuracy: 0.7454 - val_loss: 0.5249 - val_accuracy: 0.7481
Epoch 65/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.5295 - accuracy: 0.7440
Epoch 65: val_loss did not improve from 0.52292
852/852 [==============================] - 3s 3ms/step - loss: 0.5288 - accuracy: 0.7445 - val_loss: 0.5265 - val_accuracy: 0.7474
Epoch 66/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.5295 - accuracy: 0.7414
Epoch 66: val_loss improved from 0.52292 to 0.52250, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5291 - accuracy: 0.7418 - val_loss: 0.5225 - val_accuracy: 0.7493
Epoch 67/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.5281 - accuracy: 0.7431
Epoch 67: val_loss did not improve from 0.52250
852/852 [==============================] - 3s 3ms/step - loss: 0.5275 - accuracy: 0.7440 - val_loss: 0.5271 - val_accuracy: 0.7457
Epoch 68/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.5277 - accuracy: 0.7457
Epoch 68: val_loss improved from 0.52250 to 0.52183, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5279 - accuracy: 0.7455 - val_loss: 0.5218 - val_accuracy: 0.7496
Epoch 69/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5279 - accuracy: 0.7446
Epoch 69: val_loss did not improve from 0.52183
852/852 [==============================] - 3s 3ms/step - loss: 0.5281 - accuracy: 0.7445 - val_loss: 0.5225 - val_accuracy: 0.7480
Epoch 70/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.5275 - accuracy: 0.7453
Epoch 70: val_loss did not improve from 0.52183
852/852 [==============================] - 3s 3ms/step - loss: 0.5276 - accuracy: 0.7452 - val_loss: 0.5244 - val_accuracy: 0.7442
Epoch 71/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.5289 - accuracy: 0.7460
Epoch 71: val_loss improved from 0.52183 to 0.52162, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5280 - accuracy: 0.7462 - val_loss: 0.5216 - val_accuracy: 0.7489
Epoch 72/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.5268 - accuracy: 0.7434
Epoch 72: val_loss did not improve from 0.52162
852/852 [==============================] - 3s 3ms/step - loss: 0.5267 - accuracy: 0.7435 - val_loss: 0.5275 - val_accuracy: 0.7481
Epoch 73/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5262 - accuracy: 0.7479
Epoch 73: val_loss improved from 0.52162 to 0.52053, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5272 - accuracy: 0.7477 - val_loss: 0.5205 - val_accuracy: 0.7500
Epoch 74/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.5266 - accuracy: 0.7446
Epoch 74: val_loss did not improve from 0.52053
852/852 [==============================] - 3s 3ms/step - loss: 0.5269 - accuracy: 0.7447 - val_loss: 0.5207 - val_accuracy: 0.7516
Epoch 75/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.5261 - accuracy: 0.7470
Epoch 75: val_loss improved from 0.52053 to 0.51986, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5264 - accuracy: 0.7467 - val_loss: 0.5199 - val_accuracy: 0.7518
Epoch 76/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.5268 - accuracy: 0.7435
Epoch 76: val_loss did not improve from 0.51986
852/852 [==============================] - 3s 3ms/step - loss: 0.5267 - accuracy: 0.7435 - val_loss: 0.5215 - val_accuracy: 0.7487
Epoch 77/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5251 - accuracy: 0.7472
Epoch 77: val_loss did not improve from 0.51986
852/852 [==============================] - 3s 3ms/step - loss: 0.5255 - accuracy: 0.7466 - val_loss: 0.5238 - val_accuracy: 0.7462
Epoch 78/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.5257 - accuracy: 0.7453
Epoch 78: val_loss did not improve from 0.51986
852/852 [==============================] - 3s 3ms/step - loss: 0.5256 - accuracy: 0.7452 - val_loss: 0.5206 - val_accuracy: 0.7514
Epoch 79/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.5269 - accuracy: 0.7454
Epoch 79: val_loss did not improve from 0.51986
852/852 [==============================] - 3s 3ms/step - loss: 0.5262 - accuracy: 0.7466 - val_loss: 0.5200 - val_accuracy: 0.7502
Epoch 80/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.5274 - accuracy: 0.7463
Epoch 80: val_loss improved from 0.51986 to 0.51917, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5268 - accuracy: 0.7468 - val_loss: 0.5192 - val_accuracy: 0.7513
Epoch 81/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.5250 - accuracy: 0.7493
Epoch 81: val_loss did not improve from 0.51917
852/852 [==============================] - 3s 3ms/step - loss: 0.5251 - accuracy: 0.7492 - val_loss: 0.5280 - val_accuracy: 0.7450
Epoch 82/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.5266 - accuracy: 0.7464
Epoch 82: val_loss did not improve from 0.51917
852/852 [==============================] - 3s 3ms/step - loss: 0.5260 - accuracy: 0.7466 - val_loss: 0.5202 - val_accuracy: 0.7508
Epoch 83/150
852/852 [==============================] - ETA: 0s - loss: 0.5254 - accuracy: 0.7454
Epoch 83: val_loss improved from 0.51917 to 0.51887, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5254 - accuracy: 0.7454 - val_loss: 0.5189 - val_accuracy: 0.7527
Epoch 84/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.5255 - accuracy: 0.7464
Epoch 84: val_loss did not improve from 0.51887
852/852 [==============================] - 3s 3ms/step - loss: 0.5255 - accuracy: 0.7465 - val_loss: 0.5204 - val_accuracy: 0.7526
Epoch 85/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.5252 - accuracy: 0.7463
Epoch 85: val_loss did not improve from 0.51887
852/852 [==============================] - 3s 3ms/step - loss: 0.5242 - accuracy: 0.7468 - val_loss: 0.5194 - val_accuracy: 0.7509
Epoch 86/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5257 - accuracy: 0.7442
Epoch 86: val_loss did not improve from 0.51887
852/852 [==============================] - 3s 3ms/step - loss: 0.5249 - accuracy: 0.7452 - val_loss: 0.5220 - val_accuracy: 0.7486
Epoch 87/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.5248 - accuracy: 0.7472
Epoch 87: val_loss did not improve from 0.51887
852/852 [==============================] - 3s 3ms/step - loss: 0.5242 - accuracy: 0.7479 - val_loss: 0.5211 - val_accuracy: 0.7491
Epoch 88/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.5235 - accuracy: 0.7473
Epoch 88: val_loss did not improve from 0.51887
852/852 [==============================] - 3s 3ms/step - loss: 0.5230 - accuracy: 0.7477 - val_loss: 0.5191 - val_accuracy: 0.7511
Epoch 89/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.5222 - accuracy: 0.7484
Epoch 89: val_loss improved from 0.51887 to 0.51713, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5227 - accuracy: 0.7481 - val_loss: 0.5171 - val_accuracy: 0.7516
Epoch 90/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.5236 - accuracy: 0.7461
Epoch 90: val_loss did not improve from 0.51713
852/852 [==============================] - 3s 3ms/step - loss: 0.5234 - accuracy: 0.7459 - val_loss: 0.5232 - val_accuracy: 0.7496
Epoch 91/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.5232 - accuracy: 0.7463
Epoch 91: val_loss improved from 0.51713 to 0.51665, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5233 - accuracy: 0.7462 - val_loss: 0.5166 - val_accuracy: 0.7525
Epoch 92/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.5242 - accuracy: 0.7479
Epoch 92: val_loss did not improve from 0.51665
852/852 [==============================] - 3s 3ms/step - loss: 0.5237 - accuracy: 0.7489 - val_loss: 0.5191 - val_accuracy: 0.7520
Epoch 93/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.5221 - accuracy: 0.7492
Epoch 93: val_loss did not improve from 0.51665
852/852 [==============================] - 3s 3ms/step - loss: 0.5221 - accuracy: 0.7493 - val_loss: 0.5173 - val_accuracy: 0.7520
Epoch 94/150
852/852 [==============================] - ETA: 0s - loss: 0.5224 - accuracy: 0.7477
Epoch 94: val_loss did not improve from 0.51665
852/852 [==============================] - 3s 3ms/step - loss: 0.5224 - accuracy: 0.7477 - val_loss: 0.5174 - val_accuracy: 0.7535
Epoch 95/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.5219 - accuracy: 0.7506
Epoch 95: val_loss improved from 0.51665 to 0.51614, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5217 - accuracy: 0.7506 - val_loss: 0.5161 - val_accuracy: 0.7529
Epoch 96/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.5215 - accuracy: 0.7476
Epoch 96: val_loss did not improve from 0.51614
852/852 [==============================] - 3s 3ms/step - loss: 0.5213 - accuracy: 0.7478 - val_loss: 0.5183 - val_accuracy: 0.7525
Epoch 97/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.5237 - accuracy: 0.7477
Epoch 97: val_loss improved from 0.51614 to 0.51529, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5236 - accuracy: 0.7479 - val_loss: 0.5153 - val_accuracy: 0.7540
Epoch 98/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.5238 - accuracy: 0.7480
Epoch 98: val_loss did not improve from 0.51529
852/852 [==============================] - 3s 3ms/step - loss: 0.5235 - accuracy: 0.7484 - val_loss: 0.5177 - val_accuracy: 0.7507
Epoch 99/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.5210 - accuracy: 0.7487
Epoch 99: val_loss improved from 0.51529 to 0.51457, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5212 - accuracy: 0.7488 - val_loss: 0.5146 - val_accuracy: 0.7541
Epoch 100/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.5224 - accuracy: 0.7492
Epoch 100: val_loss did not improve from 0.51457
852/852 [==============================] - 3s 3ms/step - loss: 0.5220 - accuracy: 0.7492 - val_loss: 0.5200 - val_accuracy: 0.7514
Epoch 101/150
852/852 [==============================] - ETA: 0s - loss: 0.5213 - accuracy: 0.7508
Epoch 101: val_loss improved from 0.51457 to 0.51387, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5213 - accuracy: 0.7508 - val_loss: 0.5139 - val_accuracy: 0.7559
Epoch 102/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.5210 - accuracy: 0.7475
Epoch 102: val_loss did not improve from 0.51387
852/852 [==============================] - 3s 3ms/step - loss: 0.5211 - accuracy: 0.7473 - val_loss: 0.5166 - val_accuracy: 0.7516
Epoch 103/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.5224 - accuracy: 0.7481
Epoch 103: val_loss did not improve from 0.51387
852/852 [==============================] - 3s 3ms/step - loss: 0.5219 - accuracy: 0.7486 - val_loss: 0.5141 - val_accuracy: 0.7545
Epoch 104/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5204 - accuracy: 0.7510
Epoch 104: val_loss did not improve from 0.51387
852/852 [==============================] - 3s 4ms/step - loss: 0.5205 - accuracy: 0.7506 - val_loss: 0.5141 - val_accuracy: 0.7553
Epoch 105/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.5202 - accuracy: 0.7524
Epoch 105: val_loss did not improve from 0.51387
852/852 [==============================] - 4s 5ms/step - loss: 0.5202 - accuracy: 0.7522 - val_loss: 0.5174 - val_accuracy: 0.7505
Epoch 106/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.5201 - accuracy: 0.7471
Epoch 106: val_loss improved from 0.51387 to 0.51207, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5203 - accuracy: 0.7468 - val_loss: 0.5121 - val_accuracy: 0.7568
Epoch 107/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.5191 - accuracy: 0.7507
Epoch 107: val_loss did not improve from 0.51207
852/852 [==============================] - 3s 3ms/step - loss: 0.5192 - accuracy: 0.7505 - val_loss: 0.5129 - val_accuracy: 0.7548
Epoch 108/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.5200 - accuracy: 0.7502
Epoch 108: val_loss did not improve from 0.51207
852/852 [==============================] - 3s 4ms/step - loss: 0.5201 - accuracy: 0.7502 - val_loss: 0.5128 - val_accuracy: 0.7549
Epoch 109/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.5188 - accuracy: 0.7496
Epoch 109: val_loss did not improve from 0.51207
852/852 [==============================] - 3s 4ms/step - loss: 0.5199 - accuracy: 0.7489 - val_loss: 0.5134 - val_accuracy: 0.7565
Epoch 110/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.5199 - accuracy: 0.7519
Epoch 110: val_loss did not improve from 0.51207
852/852 [==============================] - 3s 4ms/step - loss: 0.5197 - accuracy: 0.7521 - val_loss: 0.5154 - val_accuracy: 0.7559
Epoch 111/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.5191 - accuracy: 0.7521
Epoch 111: val_loss did not improve from 0.51207
852/852 [==============================] - 3s 3ms/step - loss: 0.5187 - accuracy: 0.7520 - val_loss: 0.5127 - val_accuracy: 0.7538
Epoch 112/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5197 - accuracy: 0.7483
Epoch 112: val_loss did not improve from 0.51207
852/852 [==============================] - 3s 3ms/step - loss: 0.5185 - accuracy: 0.7493 - val_loss: 0.5160 - val_accuracy: 0.7498
Epoch 113/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5180 - accuracy: 0.7532
Epoch 113: val_loss did not improve from 0.51207
852/852 [==============================] - 3s 3ms/step - loss: 0.5181 - accuracy: 0.7531 - val_loss: 0.5165 - val_accuracy: 0.7540
Epoch 114/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.5178 - accuracy: 0.7505
Epoch 114: val_loss did not improve from 0.51207
852/852 [==============================] - 3s 3ms/step - loss: 0.5175 - accuracy: 0.7506 - val_loss: 0.5132 - val_accuracy: 0.7535
Epoch 115/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.5184 - accuracy: 0.7505
Epoch 115: val_loss did not improve from 0.51207
852/852 [==============================] - 3s 4ms/step - loss: 0.5186 - accuracy: 0.7509 - val_loss: 0.5132 - val_accuracy: 0.7545
Epoch 116/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.5187 - accuracy: 0.7529
Epoch 116: val_loss did not improve from 0.51207
852/852 [==============================] - 3s 3ms/step - loss: 0.5184 - accuracy: 0.7531 - val_loss: 0.5175 - val_accuracy: 0.7522
Epoch 117/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.5172 - accuracy: 0.7524
Epoch 117: val_loss did not improve from 0.51207
852/852 [==============================] - 3s 3ms/step - loss: 0.5172 - accuracy: 0.7523 - val_loss: 0.5145 - val_accuracy: 0.7567
Epoch 118/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.5182 - accuracy: 0.7524
Epoch 118: val_loss improved from 0.51207 to 0.51051, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5176 - accuracy: 0.7529 - val_loss: 0.5105 - val_accuracy: 0.7550
Epoch 119/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.5183 - accuracy: 0.7512
Epoch 119: val_loss did not improve from 0.51051
852/852 [==============================] - 3s 3ms/step - loss: 0.5181 - accuracy: 0.7516 - val_loss: 0.5107 - val_accuracy: 0.7580
Epoch 120/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.5188 - accuracy: 0.7499
Epoch 120: val_loss improved from 0.51051 to 0.50993, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5187 - accuracy: 0.7500 - val_loss: 0.5099 - val_accuracy: 0.7583
Epoch 121/150
852/852 [==============================] - ETA: 0s - loss: 0.5173 - accuracy: 0.7495
Epoch 121: val_loss did not improve from 0.50993
852/852 [==============================] - 3s 3ms/step - loss: 0.5173 - accuracy: 0.7495 - val_loss: 0.5135 - val_accuracy: 0.7528
Epoch 122/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.5156 - accuracy: 0.7514
Epoch 122: val_loss did not improve from 0.50993
852/852 [==============================] - 3s 3ms/step - loss: 0.5174 - accuracy: 0.7500 - val_loss: 0.5107 - val_accuracy: 0.7573
Epoch 123/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.5160 - accuracy: 0.7555
Epoch 123: val_loss did not improve from 0.50993
852/852 [==============================] - 3s 3ms/step - loss: 0.5160 - accuracy: 0.7553 - val_loss: 0.5113 - val_accuracy: 0.7554
Epoch 124/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.5152 - accuracy: 0.7533
Epoch 124: val_loss did not improve from 0.50993
852/852 [==============================] - 3s 3ms/step - loss: 0.5162 - accuracy: 0.7526 - val_loss: 0.5114 - val_accuracy: 0.7543
Epoch 125/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5183 - accuracy: 0.7484
Epoch 125: val_loss did not improve from 0.50993
852/852 [==============================] - 3s 3ms/step - loss: 0.5184 - accuracy: 0.7484 - val_loss: 0.5125 - val_accuracy: 0.7568
Epoch 126/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.5160 - accuracy: 0.7511
Epoch 126: val_loss did not improve from 0.50993
852/852 [==============================] - 3s 3ms/step - loss: 0.5163 - accuracy: 0.7508 - val_loss: 0.5206 - val_accuracy: 0.7514
Epoch 127/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.5163 - accuracy: 0.7498
Epoch 127: val_loss did not improve from 0.50993
852/852 [==============================] - 3s 3ms/step - loss: 0.5162 - accuracy: 0.7500 - val_loss: 0.5105 - val_accuracy: 0.7579
Epoch 128/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.5141 - accuracy: 0.7547
Epoch 128: val_loss did not improve from 0.50993
852/852 [==============================] - 3s 3ms/step - loss: 0.5149 - accuracy: 0.7535 - val_loss: 0.5109 - val_accuracy: 0.7540
Epoch 129/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5148 - accuracy: 0.7555
Epoch 129: val_loss did not improve from 0.50993
852/852 [==============================] - 3s 3ms/step - loss: 0.5155 - accuracy: 0.7546 - val_loss: 0.5109 - val_accuracy: 0.7556
Epoch 130/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.5154 - accuracy: 0.7531
Epoch 130: val_loss did not improve from 0.50993
852/852 [==============================] - 3s 3ms/step - loss: 0.5151 - accuracy: 0.7533 - val_loss: 0.5120 - val_accuracy: 0.7559
Epoch 131/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.5148 - accuracy: 0.7504
Epoch 131: val_loss improved from 0.50993 to 0.50881, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5149 - accuracy: 0.7505 - val_loss: 0.5088 - val_accuracy: 0.7586
Epoch 132/150
820/852 [===========================&gt;..] - ETA: 0s - loss: 0.5165 - accuracy: 0.7539
Epoch 132: val_loss did not improve from 0.50881
852/852 [==============================] - 3s 3ms/step - loss: 0.5155 - accuracy: 0.7545 - val_loss: 0.5090 - val_accuracy: 0.7576
Epoch 133/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.5156 - accuracy: 0.7499
Epoch 133: val_loss did not improve from 0.50881
852/852 [==============================] - 3s 3ms/step - loss: 0.5153 - accuracy: 0.7502 - val_loss: 0.5092 - val_accuracy: 0.7607
Epoch 134/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.5159 - accuracy: 0.7508
Epoch 134: val_loss did not improve from 0.50881
852/852 [==============================] - 3s 3ms/step - loss: 0.5148 - accuracy: 0.7523 - val_loss: 0.5159 - val_accuracy: 0.7534
Epoch 135/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5160 - accuracy: 0.7519
Epoch 135: val_loss did not improve from 0.50881
852/852 [==============================] - 3s 3ms/step - loss: 0.5148 - accuracy: 0.7523 - val_loss: 0.5123 - val_accuracy: 0.7569
Epoch 136/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5156 - accuracy: 0.7499
Epoch 136: val_loss improved from 0.50881 to 0.50874, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5160 - accuracy: 0.7496 - val_loss: 0.5087 - val_accuracy: 0.7587
Epoch 137/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.5137 - accuracy: 0.7531
Epoch 137: val_loss did not improve from 0.50874
852/852 [==============================] - 3s 3ms/step - loss: 0.5144 - accuracy: 0.7533 - val_loss: 0.5196 - val_accuracy: 0.7509
Epoch 138/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.5165 - accuracy: 0.7499
Epoch 138: val_loss improved from 0.50874 to 0.50632, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5161 - accuracy: 0.7507 - val_loss: 0.5063 - val_accuracy: 0.7606
Epoch 139/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.5131 - accuracy: 0.7532
Epoch 139: val_loss did not improve from 0.50632
852/852 [==============================] - 3s 3ms/step - loss: 0.5134 - accuracy: 0.7527 - val_loss: 0.5091 - val_accuracy: 0.7612
Epoch 140/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5161 - accuracy: 0.7513
Epoch 140: val_loss improved from 0.50632 to 0.50503, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5146 - accuracy: 0.7525 - val_loss: 0.5050 - val_accuracy: 0.7619
Epoch 141/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.5125 - accuracy: 0.7565
Epoch 141: val_loss did not improve from 0.50503
852/852 [==============================] - 3s 3ms/step - loss: 0.5128 - accuracy: 0.7566 - val_loss: 0.5083 - val_accuracy: 0.7614
Epoch 142/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.5133 - accuracy: 0.7549
Epoch 142: val_loss did not improve from 0.50503
852/852 [==============================] - 3s 3ms/step - loss: 0.5131 - accuracy: 0.7558 - val_loss: 0.5088 - val_accuracy: 0.7599
Epoch 143/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5136 - accuracy: 0.7569
Epoch 143: val_loss did not improve from 0.50503
852/852 [==============================] - 3s 3ms/step - loss: 0.5151 - accuracy: 0.7549 - val_loss: 0.5119 - val_accuracy: 0.7554
Epoch 144/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.5132 - accuracy: 0.7527
Epoch 144: val_loss did not improve from 0.50503
852/852 [==============================] - 3s 3ms/step - loss: 0.5130 - accuracy: 0.7529 - val_loss: 0.5077 - val_accuracy: 0.7590
Epoch 145/150
852/852 [==============================] - ETA: 0s - loss: 0.5144 - accuracy: 0.7546
Epoch 145: val_loss did not improve from 0.50503
852/852 [==============================] - 3s 3ms/step - loss: 0.5144 - accuracy: 0.7546 - val_loss: 0.5101 - val_accuracy: 0.7592
Epoch 146/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.5132 - accuracy: 0.7527
Epoch 146: val_loss did not improve from 0.50503
852/852 [==============================] - 3s 3ms/step - loss: 0.5130 - accuracy: 0.7527 - val_loss: 0.5110 - val_accuracy: 0.7585
Epoch 147/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.5132 - accuracy: 0.7548
Epoch 147: val_loss did not improve from 0.50503
852/852 [==============================] - 3s 3ms/step - loss: 0.5132 - accuracy: 0.7550 - val_loss: 0.5071 - val_accuracy: 0.7593
Epoch 148/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5123 - accuracy: 0.7539
Epoch 148: val_loss did not improve from 0.50503
852/852 [==============================] - 3s 3ms/step - loss: 0.5123 - accuracy: 0.7538 - val_loss: 0.5068 - val_accuracy: 0.7587
Epoch 149/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.5134 - accuracy: 0.7554
Epoch 149: val_loss did not improve from 0.50503
852/852 [==============================] - 3s 3ms/step - loss: 0.5129 - accuracy: 0.7559 - val_loss: 0.5066 - val_accuracy: 0.7587
Epoch 150/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5132 - accuracy: 0.7549
Epoch 150: val_loss did not improve from 0.50503
852/852 [==============================] - 3s 3ms/step - loss: 0.5141 - accuracy: 0.7543 - val_loss: 0.5055 - val_accuracy: 0.7607
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=84e3a89c-f0e0-4d58-9d04-5bc00b1c8d80">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [62]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">history_31</span> <span class="o">=</span> <span class="n">thirty_one_neuron</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">callback_a</span><span class="p">,</span> <span class="n">callback_b</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5135 - accuracy: 0.7522
Epoch 1: val_loss did not improve from 0.50503
852/852 [==============================] - 3s 3ms/step - loss: 0.5133 - accuracy: 0.7520 - val_loss: 0.5069 - val_accuracy: 0.7609
Epoch 2/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.5125 - accuracy: 0.7560
Epoch 2: val_loss improved from 0.50503 to 0.50360, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5129 - accuracy: 0.7559 - val_loss: 0.5036 - val_accuracy: 0.7612
Epoch 3/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.5121 - accuracy: 0.7547
Epoch 3: val_loss did not improve from 0.50360
852/852 [==============================] - 3s 3ms/step - loss: 0.5124 - accuracy: 0.7546 - val_loss: 0.5097 - val_accuracy: 0.7547
Epoch 4/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.5120 - accuracy: 0.7542
Epoch 4: val_loss did not improve from 0.50360
852/852 [==============================] - 3s 3ms/step - loss: 0.5128 - accuracy: 0.7542 - val_loss: 0.5109 - val_accuracy: 0.7546
Epoch 5/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.5119 - accuracy: 0.7571
Epoch 5: val_loss did not improve from 0.50360
852/852 [==============================] - 3s 3ms/step - loss: 0.5121 - accuracy: 0.7568 - val_loss: 0.5059 - val_accuracy: 0.7595
Epoch 6/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5118 - accuracy: 0.7540
Epoch 6: val_loss did not improve from 0.50360
852/852 [==============================] - 3s 3ms/step - loss: 0.5120 - accuracy: 0.7538 - val_loss: 0.5048 - val_accuracy: 0.7605
Epoch 7/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.5127 - accuracy: 0.7584
Epoch 7: val_loss did not improve from 0.50360
852/852 [==============================] - 3s 3ms/step - loss: 0.5124 - accuracy: 0.7586 - val_loss: 0.5053 - val_accuracy: 0.7597
Epoch 8/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.5113 - accuracy: 0.7569
Epoch 8: val_loss did not improve from 0.50360
852/852 [==============================] - 3s 3ms/step - loss: 0.5112 - accuracy: 0.7570 - val_loss: 0.5082 - val_accuracy: 0.7575
Epoch 9/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5128 - accuracy: 0.7535
Epoch 9: val_loss did not improve from 0.50360
852/852 [==============================] - 3s 3ms/step - loss: 0.5129 - accuracy: 0.7531 - val_loss: 0.5063 - val_accuracy: 0.7614
Epoch 10/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.5112 - accuracy: 0.7517
Epoch 10: val_loss did not improve from 0.50360
852/852 [==============================] - 3s 3ms/step - loss: 0.5113 - accuracy: 0.7516 - val_loss: 0.5058 - val_accuracy: 0.7595
Epoch 11/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.5133 - accuracy: 0.7527
Epoch 11: val_loss did not improve from 0.50360
852/852 [==============================] - 3s 3ms/step - loss: 0.5122 - accuracy: 0.7540 - val_loss: 0.5116 - val_accuracy: 0.7529
Epoch 12/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.5115 - accuracy: 0.7546
Epoch 12: val_loss did not improve from 0.50360
852/852 [==============================] - 3s 3ms/step - loss: 0.5122 - accuracy: 0.7547 - val_loss: 0.5073 - val_accuracy: 0.7589
Epoch 13/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.5132 - accuracy: 0.7537
Epoch 13: val_loss did not improve from 0.50360
852/852 [==============================] - 3s 3ms/step - loss: 0.5128 - accuracy: 0.7543 - val_loss: 0.5055 - val_accuracy: 0.7626
Epoch 14/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.5074 - accuracy: 0.7601
Epoch 14: val_loss did not improve from 0.50360
852/852 [==============================] - 3s 3ms/step - loss: 0.5091 - accuracy: 0.7594 - val_loss: 0.5038 - val_accuracy: 0.7612
Epoch 15/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5117 - accuracy: 0.7546
Epoch 15: val_loss did not improve from 0.50360
852/852 [==============================] - 3s 3ms/step - loss: 0.5117 - accuracy: 0.7546 - val_loss: 0.5050 - val_accuracy: 0.7619
Epoch 16/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5119 - accuracy: 0.7551
Epoch 16: val_loss did not improve from 0.50360
852/852 [==============================] - 3s 3ms/step - loss: 0.5118 - accuracy: 0.7552 - val_loss: 0.5070 - val_accuracy: 0.7601
Epoch 17/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.5116 - accuracy: 0.7543
Epoch 17: val_loss did not improve from 0.50360
852/852 [==============================] - 3s 3ms/step - loss: 0.5118 - accuracy: 0.7546 - val_loss: 0.5039 - val_accuracy: 0.7602
Epoch 18/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.5106 - accuracy: 0.7558
Epoch 18: val_loss did not improve from 0.50360
852/852 [==============================] - 3s 3ms/step - loss: 0.5110 - accuracy: 0.7554 - val_loss: 0.5056 - val_accuracy: 0.7607
Epoch 19/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.5118 - accuracy: 0.7547
Epoch 19: val_loss did not improve from 0.50360
852/852 [==============================] - 3s 3ms/step - loss: 0.5118 - accuracy: 0.7545 - val_loss: 0.5066 - val_accuracy: 0.7630
Epoch 20/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.5110 - accuracy: 0.7548
Epoch 20: val_loss improved from 0.50360 to 0.50301, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5106 - accuracy: 0.7552 - val_loss: 0.5030 - val_accuracy: 0.7621
Epoch 21/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.5098 - accuracy: 0.7558
Epoch 21: val_loss improved from 0.50301 to 0.50177, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5096 - accuracy: 0.7561 - val_loss: 0.5018 - val_accuracy: 0.7633
Epoch 22/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.5114 - accuracy: 0.7526
Epoch 22: val_loss did not improve from 0.50177
852/852 [==============================] - 3s 3ms/step - loss: 0.5115 - accuracy: 0.7526 - val_loss: 0.5079 - val_accuracy: 0.7592
Epoch 23/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5097 - accuracy: 0.7572
Epoch 23: val_loss did not improve from 0.50177
852/852 [==============================] - 3s 3ms/step - loss: 0.5100 - accuracy: 0.7568 - val_loss: 0.5023 - val_accuracy: 0.7610
Epoch 24/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.5121 - accuracy: 0.7532
Epoch 24: val_loss improved from 0.50177 to 0.50135, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5111 - accuracy: 0.7546 - val_loss: 0.5013 - val_accuracy: 0.7622
Epoch 25/150
852/852 [==============================] - ETA: 0s - loss: 0.5093 - accuracy: 0.7558
Epoch 25: val_loss did not improve from 0.50135
852/852 [==============================] - 3s 3ms/step - loss: 0.5093 - accuracy: 0.7558 - val_loss: 0.5086 - val_accuracy: 0.7568
Epoch 26/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.5105 - accuracy: 0.7560
Epoch 26: val_loss did not improve from 0.50135
852/852 [==============================] - 3s 3ms/step - loss: 0.5119 - accuracy: 0.7549 - val_loss: 0.5038 - val_accuracy: 0.7606
Epoch 27/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5099 - accuracy: 0.7578
Epoch 27: val_loss did not improve from 0.50135
852/852 [==============================] - 3s 3ms/step - loss: 0.5101 - accuracy: 0.7579 - val_loss: 0.5054 - val_accuracy: 0.7608
Epoch 28/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.5114 - accuracy: 0.7565
Epoch 28: val_loss did not improve from 0.50135
852/852 [==============================] - 3s 3ms/step - loss: 0.5108 - accuracy: 0.7568 - val_loss: 0.5052 - val_accuracy: 0.7615
Epoch 29/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.5115 - accuracy: 0.7554
Epoch 29: val_loss did not improve from 0.50135
852/852 [==============================] - 3s 3ms/step - loss: 0.5106 - accuracy: 0.7559 - val_loss: 0.5038 - val_accuracy: 0.7621
Epoch 30/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5098 - accuracy: 0.7567
Epoch 30: val_loss did not improve from 0.50135
852/852 [==============================] - 3s 3ms/step - loss: 0.5096 - accuracy: 0.7569 - val_loss: 0.5034 - val_accuracy: 0.7614
Epoch 31/150
852/852 [==============================] - ETA: 0s - loss: 0.5098 - accuracy: 0.7567
Epoch 31: val_loss did not improve from 0.50135
852/852 [==============================] - 3s 3ms/step - loss: 0.5098 - accuracy: 0.7567 - val_loss: 0.5073 - val_accuracy: 0.7587
Epoch 32/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.5094 - accuracy: 0.7584
Epoch 32: val_loss improved from 0.50135 to 0.50044, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5098 - accuracy: 0.7580 - val_loss: 0.5004 - val_accuracy: 0.7634
Epoch 33/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.5077 - accuracy: 0.7588
Epoch 33: val_loss did not improve from 0.50044
852/852 [==============================] - 3s 3ms/step - loss: 0.5080 - accuracy: 0.7585 - val_loss: 0.5044 - val_accuracy: 0.7599
Epoch 34/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.5090 - accuracy: 0.7555
Epoch 34: val_loss did not improve from 0.50044
852/852 [==============================] - 3s 3ms/step - loss: 0.5096 - accuracy: 0.7547 - val_loss: 0.5102 - val_accuracy: 0.7543
Epoch 35/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5081 - accuracy: 0.7591
Epoch 35: val_loss did not improve from 0.50044
852/852 [==============================] - 3s 3ms/step - loss: 0.5086 - accuracy: 0.7587 - val_loss: 0.5128 - val_accuracy: 0.7507
Epoch 36/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.5085 - accuracy: 0.7580
Epoch 36: val_loss improved from 0.50044 to 0.49973, saving model to thirty_one_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5092 - accuracy: 0.7574 - val_loss: 0.4997 - val_accuracy: 0.7661
Epoch 37/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.5089 - accuracy: 0.7576
Epoch 37: val_loss did not improve from 0.49973
852/852 [==============================] - 3s 3ms/step - loss: 0.5080 - accuracy: 0.7583 - val_loss: 0.5138 - val_accuracy: 0.7452
Epoch 38/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5085 - accuracy: 0.7580
Epoch 38: val_loss did not improve from 0.49973
852/852 [==============================] - 3s 3ms/step - loss: 0.5091 - accuracy: 0.7579 - val_loss: 0.5002 - val_accuracy: 0.7630
Epoch 39/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5094 - accuracy: 0.7551
Epoch 39: val_loss did not improve from 0.49973
852/852 [==============================] - 3s 3ms/step - loss: 0.5086 - accuracy: 0.7560 - val_loss: 0.5036 - val_accuracy: 0.7621
Epoch 40/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.5091 - accuracy: 0.7591
Epoch 40: val_loss did not improve from 0.49973
852/852 [==============================] - 3s 3ms/step - loss: 0.5089 - accuracy: 0.7592 - val_loss: 0.5017 - val_accuracy: 0.7624
Epoch 41/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.5093 - accuracy: 0.7558
Epoch 41: val_loss did not improve from 0.49973
852/852 [==============================] - 3s 3ms/step - loss: 0.5093 - accuracy: 0.7553 - val_loss: 0.5000 - val_accuracy: 0.7626
Epoch 42/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.5093 - accuracy: 0.7545
Epoch 42: val_loss did not improve from 0.49973
852/852 [==============================] - 3s 3ms/step - loss: 0.5087 - accuracy: 0.7550 - val_loss: 0.5029 - val_accuracy: 0.7627
Epoch 43/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.5068 - accuracy: 0.7603
Epoch 43: val_loss did not improve from 0.49973
852/852 [==============================] - 3s 3ms/step - loss: 0.5072 - accuracy: 0.7596 - val_loss: 0.5048 - val_accuracy: 0.7585
Epoch 44/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.5062 - accuracy: 0.7595
Epoch 44: val_loss did not improve from 0.49973
852/852 [==============================] - 3s 3ms/step - loss: 0.5077 - accuracy: 0.7582 - val_loss: 0.5021 - val_accuracy: 0.7610
Epoch 45/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5066 - accuracy: 0.7606
Epoch 45: val_loss did not improve from 0.49973
852/852 [==============================] - 3s 3ms/step - loss: 0.5069 - accuracy: 0.7602 - val_loss: 0.5034 - val_accuracy: 0.7603
Epoch 46/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.5092 - accuracy: 0.7567
Epoch 46: val_loss did not improve from 0.49973
852/852 [==============================] - 3s 3ms/step - loss: 0.5099 - accuracy: 0.7562 - val_loss: 0.5002 - val_accuracy: 0.7655
Epoch 47/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.5089 - accuracy: 0.7564
Epoch 47: val_loss did not improve from 0.49973
852/852 [==============================] - 3s 3ms/step - loss: 0.5086 - accuracy: 0.7567 - val_loss: 0.5002 - val_accuracy: 0.7630
Epoch 48/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.5062 - accuracy: 0.7566
Epoch 48: val_loss did not improve from 0.49973
852/852 [==============================] - 3s 3ms/step - loss: 0.5063 - accuracy: 0.7566 - val_loss: 0.5026 - val_accuracy: 0.7636
Epoch 49/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.5083 - accuracy: 0.7575
Epoch 49: val_loss did not improve from 0.49973
852/852 [==============================] - 3s 3ms/step - loss: 0.5085 - accuracy: 0.7575 - val_loss: 0.5006 - val_accuracy: 0.7626
Epoch 50/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.5096 - accuracy: 0.7575
Epoch 50: val_loss did not improve from 0.49973
852/852 [==============================] - 3s 3ms/step - loss: 0.5091 - accuracy: 0.7575 - val_loss: 0.5000 - val_accuracy: 0.7636
Epoch 51/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5065 - accuracy: 0.7582
Epoch 51: val_loss did not improve from 0.49973
852/852 [==============================] - 3s 3ms/step - loss: 0.5062 - accuracy: 0.7585 - val_loss: 0.5072 - val_accuracy: 0.7583
Epoch 52/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.5060 - accuracy: 0.7588
Epoch 52: val_loss did not improve from 0.49973
852/852 [==============================] - 3s 3ms/step - loss: 0.5062 - accuracy: 0.7588 - val_loss: 0.5034 - val_accuracy: 0.7576
Epoch 53/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5082 - accuracy: 0.7568
Epoch 53: val_loss did not improve from 0.49973
852/852 [==============================] - 3s 3ms/step - loss: 0.5080 - accuracy: 0.7570 - val_loss: 0.5080 - val_accuracy: 0.7553
Epoch 54/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.5065 - accuracy: 0.7602
Epoch 54: val_loss did not improve from 0.49973
852/852 [==============================] - 3s 3ms/step - loss: 0.5067 - accuracy: 0.7600 - val_loss: 0.5063 - val_accuracy: 0.7593
Epoch 55/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.5087 - accuracy: 0.7568
Epoch 55: val_loss did not improve from 0.49973
852/852 [==============================] - 3s 3ms/step - loss: 0.5078 - accuracy: 0.7572 - val_loss: 0.5009 - val_accuracy: 0.7629
Epoch 56/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.5061 - accuracy: 0.7569
Epoch 56: val_loss did not improve from 0.49973
852/852 [==============================] - 3s 3ms/step - loss: 0.5060 - accuracy: 0.7572 - val_loss: 0.5004 - val_accuracy: 0.7651
Epoch 56: early stopping
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=054765a3-0edd-43df-ac54-ef14324466ca">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [70]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">thirty_one_neuron</span><span class="o">.</span><span class="n">load_weights</span><span class="p">(</span><span class="s1">'thirty_one_model.hdf5'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=6ade23b3-a67b-414b-93f9-5c9e59c48c4a">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [71]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Other Metrics</span>
<span class="n">thirty_one_neuron_preds</span> <span class="o">=</span> <span class="n">thirty_one_neuron</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">thirty_one_neuron_preds</span><span class="o">.</span><span class="n">round</span><span class="p">())</span>
<span class="n">precision</span> <span class="o">=</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">thirty_one_neuron_preds</span><span class="o">.</span><span class="n">round</span><span class="p">())</span>
<span class="n">recall</span> <span class="o">=</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">thirty_one_neuron_preds</span><span class="o">.</span><span class="n">round</span><span class="p">())</span>
<span class="n">f1score</span> <span class="o">=</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">thirty_one_neuron_preds</span><span class="o">.</span><span class="n">round</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Accuracy: </span><span class="si">%.2f</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">accuracy</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Precision: </span><span class="si">%.2f</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">precision</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Recall: </span><span class="si">%.2f</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">recall</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"F1-score: </span><span class="si">%.2f</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">f1score</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>267/267 [==============================] - 0s 962us/step
Accuracy: 0.77
Precision: 0.84
Recall: 0.65
F1-score: 0.74
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=ad0f3932-7359-4a53-82d9-70f17836fca4">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="2.6-Build-a-Model-with-63-Neurons-in-6-Layers">2.6 Build a Model with 63 Neurons in 6 Layers<a class="anchor-link" href="#2.6-Build-a-Model-with-63-Neurons-in-6-Layers">¶</a></h4>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=8a5277ce-18cb-4991-822d-658343238094">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [63]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># constructing a 63 neuron model</span>
<span class="n">sixty_three_neuron</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">sixty_three_neuron</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">input_dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">))</span>
<span class="n">sixty_three_neuron</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">))</span>
<span class="n">sixty_three_neuron</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">))</span>
<span class="n">sixty_three_neuron</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">))</span>
<span class="n">sixty_three_neuron</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">))</span>
<span class="n">sixty_three_neuron</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'sigmoid'</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sixty_three_neuron</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>

<span class="c1">#Check for cycles in the Sequential Model</span>
<span class="n">sixty_three_neuron</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">'binary_crossentropy'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s1">'adam'</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'accuracy'</span><span class="p">])</span>
<span class="n">callback_a</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">filepath</span> <span class="o">=</span> <span class="s1">'sixty_three_model.hdf5'</span><span class="p">,</span> <span class="n">monitor</span><span class="o">=</span><span class="s1">'val_loss'</span><span class="p">,</span> <span class="n">save_best_only</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">save_weights_only</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">verbose</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">callback_b</span> <span class="o">=</span> <span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s1">'val_loss'</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">'min'</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">history_63</span> <span class="o">=</span> <span class="n">sixty_three_neuron</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">callback_a</span><span class="p">,</span> <span class="n">callback_b</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Model: "sequential_15"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense_24 (Dense)            (None, 32)                864       
                                                                 
 dense_25 (Dense)            (None, 16)                528       
                                                                 
 dense_26 (Dense)            (None, 8)                 136       
                                                                 
 dense_27 (Dense)            (None, 4)                 36        
                                                                 
 dense_28 (Dense)            (None, 2)                 10        
                                                                 
 dense_29 (Dense)            (None, 1)                 3         
                                                                 
=================================================================
Total params: 1577 (6.16 KB)
Trainable params: 1577 (6.16 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
None
Epoch 1/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.6428 - accuracy: 0.6744
Epoch 1: val_loss improved from inf to 0.61768, saving model to sixty_three_model.hdf5
852/852 [==============================] - 4s 3ms/step - loss: 0.6429 - accuracy: 0.6740 - val_loss: 0.6177 - val_accuracy: 0.7154
Epoch 2/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.6056 - accuracy: 0.7186
Epoch 2: val_loss improved from 0.61768 to 0.59653, saving model to sixty_three_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.6054 - accuracy: 0.7184 - val_loss: 0.5965 - val_accuracy: 0.7246
Epoch 3/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.5880 - accuracy: 0.7238
Epoch 3: val_loss improved from 0.59653 to 0.58719, saving model to sixty_three_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5885 - accuracy: 0.7231 - val_loss: 0.5872 - val_accuracy: 0.7120
Epoch 4/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.5594 - accuracy: 0.7247
Epoch 4: val_loss improved from 0.58719 to 0.55379, saving model to sixty_three_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5593 - accuracy: 0.7248 - val_loss: 0.5538 - val_accuracy: 0.7310
Epoch 5/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.5549 - accuracy: 0.7254
Epoch 5: val_loss improved from 0.55379 to 0.54993, saving model to sixty_three_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5545 - accuracy: 0.7257 - val_loss: 0.5499 - val_accuracy: 0.7297
Epoch 6/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.5533 - accuracy: 0.7305
Epoch 6: val_loss improved from 0.54993 to 0.54695, saving model to sixty_three_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5530 - accuracy: 0.7311 - val_loss: 0.5470 - val_accuracy: 0.7334
Epoch 7/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.5498 - accuracy: 0.7310
Epoch 7: val_loss improved from 0.54695 to 0.54690, saving model to sixty_three_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5510 - accuracy: 0.7300 - val_loss: 0.5469 - val_accuracy: 0.7316
Epoch 8/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.5484 - accuracy: 0.7340
Epoch 8: val_loss improved from 0.54690 to 0.54461, saving model to sixty_three_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5489 - accuracy: 0.7339 - val_loss: 0.5446 - val_accuracy: 0.7316
Epoch 9/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5475 - accuracy: 0.7322
Epoch 9: val_loss improved from 0.54461 to 0.54225, saving model to sixty_three_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5475 - accuracy: 0.7322 - val_loss: 0.5423 - val_accuracy: 0.7372
Epoch 10/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5451 - accuracy: 0.7343
Epoch 10: val_loss did not improve from 0.54225
852/852 [==============================] - 3s 3ms/step - loss: 0.5448 - accuracy: 0.7344 - val_loss: 0.5424 - val_accuracy: 0.7347
Epoch 11/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.5455 - accuracy: 0.7348
Epoch 11: val_loss improved from 0.54225 to 0.53945, saving model to sixty_three_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5449 - accuracy: 0.7347 - val_loss: 0.5395 - val_accuracy: 0.7372
Epoch 12/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.5431 - accuracy: 0.7349
Epoch 12: val_loss did not improve from 0.53945
852/852 [==============================] - 3s 3ms/step - loss: 0.5435 - accuracy: 0.7344 - val_loss: 0.5395 - val_accuracy: 0.7394
Epoch 13/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.5427 - accuracy: 0.7365
Epoch 13: val_loss improved from 0.53945 to 0.53702, saving model to sixty_three_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5427 - accuracy: 0.7365 - val_loss: 0.5370 - val_accuracy: 0.7412
Epoch 14/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.5395 - accuracy: 0.7378
Epoch 14: val_loss did not improve from 0.53702
852/852 [==============================] - 3s 3ms/step - loss: 0.5409 - accuracy: 0.7367 - val_loss: 0.5372 - val_accuracy: 0.7381
Epoch 15/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.5393 - accuracy: 0.7387
Epoch 15: val_loss improved from 0.53702 to 0.53695, saving model to sixty_three_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5401 - accuracy: 0.7374 - val_loss: 0.5370 - val_accuracy: 0.7413
Epoch 16/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.5393 - accuracy: 0.7373
Epoch 16: val_loss did not improve from 0.53695
852/852 [==============================] - 3s 3ms/step - loss: 0.5391 - accuracy: 0.7377 - val_loss: 0.5370 - val_accuracy: 0.7404
Epoch 17/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.5384 - accuracy: 0.7394
Epoch 17: val_loss improved from 0.53695 to 0.53454, saving model to sixty_three_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5385 - accuracy: 0.7392 - val_loss: 0.5345 - val_accuracy: 0.7425
Epoch 18/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.5369 - accuracy: 0.7390
Epoch 18: val_loss did not improve from 0.53454
852/852 [==============================] - 3s 3ms/step - loss: 0.5373 - accuracy: 0.7386 - val_loss: 0.5362 - val_accuracy: 0.7417
Epoch 19/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.5370 - accuracy: 0.7401
Epoch 19: val_loss improved from 0.53454 to 0.53157, saving model to sixty_three_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5371 - accuracy: 0.7399 - val_loss: 0.5316 - val_accuracy: 0.7398
Epoch 20/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.5353 - accuracy: 0.7414
Epoch 20: val_loss did not improve from 0.53157
852/852 [==============================] - 3s 3ms/step - loss: 0.5356 - accuracy: 0.7403 - val_loss: 0.5323 - val_accuracy: 0.7430
Epoch 21/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5356 - accuracy: 0.7416
Epoch 21: val_loss improved from 0.53157 to 0.52785, saving model to sixty_three_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5349 - accuracy: 0.7419 - val_loss: 0.5279 - val_accuracy: 0.7455
Epoch 22/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.5339 - accuracy: 0.7414
Epoch 22: val_loss did not improve from 0.52785
852/852 [==============================] - 3s 3ms/step - loss: 0.5336 - accuracy: 0.7413 - val_loss: 0.5298 - val_accuracy: 0.7452
Epoch 23/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.5337 - accuracy: 0.7428
Epoch 23: val_loss did not improve from 0.52785
852/852 [==============================] - 3s 3ms/step - loss: 0.5341 - accuracy: 0.7425 - val_loss: 0.5279 - val_accuracy: 0.7459
Epoch 24/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5325 - accuracy: 0.7424
Epoch 24: val_loss did not improve from 0.52785
852/852 [==============================] - 3s 3ms/step - loss: 0.5326 - accuracy: 0.7424 - val_loss: 0.5315 - val_accuracy: 0.7417
Epoch 25/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.5311 - accuracy: 0.7429
Epoch 25: val_loss improved from 0.52785 to 0.52634, saving model to sixty_three_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5314 - accuracy: 0.7426 - val_loss: 0.5263 - val_accuracy: 0.7455
Epoch 26/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.5292 - accuracy: 0.7435
Epoch 26: val_loss improved from 0.52634 to 0.52543, saving model to sixty_three_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5300 - accuracy: 0.7431 - val_loss: 0.5254 - val_accuracy: 0.7493
Epoch 27/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.5304 - accuracy: 0.7426
Epoch 27: val_loss did not improve from 0.52543
852/852 [==============================] - 3s 3ms/step - loss: 0.5305 - accuracy: 0.7430 - val_loss: 0.5256 - val_accuracy: 0.7464
Epoch 28/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.5296 - accuracy: 0.7447
Epoch 28: val_loss improved from 0.52543 to 0.52184, saving model to sixty_three_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5300 - accuracy: 0.7447 - val_loss: 0.5218 - val_accuracy: 0.7488
Epoch 29/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.5299 - accuracy: 0.7444
Epoch 29: val_loss did not improve from 0.52184
852/852 [==============================] - 3s 3ms/step - loss: 0.5298 - accuracy: 0.7444 - val_loss: 0.5250 - val_accuracy: 0.7461
Epoch 30/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.5283 - accuracy: 0.7423
Epoch 30: val_loss did not improve from 0.52184
852/852 [==============================] - 3s 3ms/step - loss: 0.5284 - accuracy: 0.7421 - val_loss: 0.5261 - val_accuracy: 0.7440
Epoch 31/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.5306 - accuracy: 0.7453
Epoch 31: val_loss did not improve from 0.52184
852/852 [==============================] - 3s 3ms/step - loss: 0.5298 - accuracy: 0.7460 - val_loss: 0.5223 - val_accuracy: 0.7466
Epoch 32/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.5265 - accuracy: 0.7455
Epoch 32: val_loss improved from 0.52184 to 0.52017, saving model to sixty_three_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5272 - accuracy: 0.7446 - val_loss: 0.5202 - val_accuracy: 0.7462
Epoch 33/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.5264 - accuracy: 0.7454
Epoch 33: val_loss did not improve from 0.52017
852/852 [==============================] - 3s 3ms/step - loss: 0.5258 - accuracy: 0.7448 - val_loss: 0.5254 - val_accuracy: 0.7480
Epoch 34/150
852/852 [==============================] - ETA: 0s - loss: 0.5272 - accuracy: 0.7430
Epoch 34: val_loss did not improve from 0.52017
852/852 [==============================] - 3s 3ms/step - loss: 0.5272 - accuracy: 0.7430 - val_loss: 0.5208 - val_accuracy: 0.7493
Epoch 35/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.5230 - accuracy: 0.7476
Epoch 35: val_loss did not improve from 0.52017
852/852 [==============================] - 3s 3ms/step - loss: 0.5241 - accuracy: 0.7461 - val_loss: 0.5210 - val_accuracy: 0.7521
Epoch 36/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.5239 - accuracy: 0.7448
Epoch 36: val_loss improved from 0.52017 to 0.51666, saving model to sixty_three_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5242 - accuracy: 0.7445 - val_loss: 0.5167 - val_accuracy: 0.7527
Epoch 37/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.5239 - accuracy: 0.7455
Epoch 37: val_loss did not improve from 0.51666
852/852 [==============================] - 3s 3ms/step - loss: 0.5239 - accuracy: 0.7452 - val_loss: 0.5179 - val_accuracy: 0.7482
Epoch 38/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.5237 - accuracy: 0.7449
Epoch 38: val_loss did not improve from 0.51666
852/852 [==============================] - 3s 3ms/step - loss: 0.5229 - accuracy: 0.7452 - val_loss: 0.5190 - val_accuracy: 0.7511
Epoch 39/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.5209 - accuracy: 0.7465
Epoch 39: val_loss improved from 0.51666 to 0.51637, saving model to sixty_three_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5212 - accuracy: 0.7466 - val_loss: 0.5164 - val_accuracy: 0.7489
Epoch 40/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.5227 - accuracy: 0.7440
Epoch 40: val_loss improved from 0.51637 to 0.51372, saving model to sixty_three_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5223 - accuracy: 0.7446 - val_loss: 0.5137 - val_accuracy: 0.7533
Epoch 41/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.5183 - accuracy: 0.7467
Epoch 41: val_loss did not improve from 0.51372
852/852 [==============================] - 3s 3ms/step - loss: 0.5186 - accuracy: 0.7465 - val_loss: 0.5251 - val_accuracy: 0.7401
Epoch 42/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5192 - accuracy: 0.7476
Epoch 42: val_loss improved from 0.51372 to 0.51349, saving model to sixty_three_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5186 - accuracy: 0.7485 - val_loss: 0.5135 - val_accuracy: 0.7508
Epoch 43/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.5194 - accuracy: 0.7455
Epoch 43: val_loss did not improve from 0.51349
852/852 [==============================] - 3s 3ms/step - loss: 0.5193 - accuracy: 0.7457 - val_loss: 0.5162 - val_accuracy: 0.7472
Epoch 44/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.5177 - accuracy: 0.7480
Epoch 44: val_loss did not improve from 0.51349
852/852 [==============================] - 2s 2ms/step - loss: 0.5175 - accuracy: 0.7480 - val_loss: 0.5230 - val_accuracy: 0.7469
Epoch 45/150
818/852 [===========================&gt;..] - ETA: 0s - loss: 0.5150 - accuracy: 0.7493
Epoch 45: val_loss did not improve from 0.51349
852/852 [==============================] - 2s 2ms/step - loss: 0.5174 - accuracy: 0.7475 - val_loss: 0.5147 - val_accuracy: 0.7495
Epoch 46/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5166 - accuracy: 0.7491
Epoch 46: val_loss did not improve from 0.51349
852/852 [==============================] - 2s 2ms/step - loss: 0.5174 - accuracy: 0.7487 - val_loss: 0.5135 - val_accuracy: 0.7502
Epoch 47/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.5175 - accuracy: 0.7480
Epoch 47: val_loss improved from 0.51349 to 0.51137, saving model to sixty_three_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5170 - accuracy: 0.7482 - val_loss: 0.5114 - val_accuracy: 0.7512
Epoch 48/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.5164 - accuracy: 0.7491
Epoch 48: val_loss improved from 0.51137 to 0.50712, saving model to sixty_three_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5156 - accuracy: 0.7501 - val_loss: 0.5071 - val_accuracy: 0.7542
Epoch 49/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5158 - accuracy: 0.7478
Epoch 49: val_loss improved from 0.50712 to 0.50546, saving model to sixty_three_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5157 - accuracy: 0.7479 - val_loss: 0.5055 - val_accuracy: 0.7542
Epoch 50/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.5131 - accuracy: 0.7491
Epoch 50: val_loss did not improve from 0.50546
852/852 [==============================] - 2s 2ms/step - loss: 0.5142 - accuracy: 0.7481 - val_loss: 0.5076 - val_accuracy: 0.7542
Epoch 51/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.5145 - accuracy: 0.7499
Epoch 51: val_loss did not improve from 0.50546
852/852 [==============================] - 2s 2ms/step - loss: 0.5150 - accuracy: 0.7495 - val_loss: 0.5089 - val_accuracy: 0.7498
Epoch 52/150
811/852 [===========================&gt;..] - ETA: 0s - loss: 0.5126 - accuracy: 0.7508
Epoch 52: val_loss did not improve from 0.50546
852/852 [==============================] - 2s 2ms/step - loss: 0.5147 - accuracy: 0.7495 - val_loss: 0.5093 - val_accuracy: 0.7518
Epoch 53/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.5134 - accuracy: 0.7502
Epoch 53: val_loss improved from 0.50546 to 0.50373, saving model to sixty_three_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5139 - accuracy: 0.7500 - val_loss: 0.5037 - val_accuracy: 0.7554
Epoch 54/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.5123 - accuracy: 0.7486
Epoch 54: val_loss did not improve from 0.50373
852/852 [==============================] - 2s 2ms/step - loss: 0.5128 - accuracy: 0.7481 - val_loss: 0.5056 - val_accuracy: 0.7553
Epoch 55/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.5133 - accuracy: 0.7492
Epoch 55: val_loss improved from 0.50373 to 0.50273, saving model to sixty_three_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5135 - accuracy: 0.7491 - val_loss: 0.5027 - val_accuracy: 0.7540
Epoch 56/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5123 - accuracy: 0.7487
Epoch 56: val_loss did not improve from 0.50273
852/852 [==============================] - 2s 2ms/step - loss: 0.5120 - accuracy: 0.7491 - val_loss: 0.5102 - val_accuracy: 0.7540
Epoch 57/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.5124 - accuracy: 0.7493
Epoch 57: val_loss did not improve from 0.50273
852/852 [==============================] - 2s 2ms/step - loss: 0.5119 - accuracy: 0.7498 - val_loss: 0.5064 - val_accuracy: 0.7542
Epoch 58/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.5102 - accuracy: 0.7531
Epoch 58: val_loss did not improve from 0.50273
852/852 [==============================] - 2s 2ms/step - loss: 0.5108 - accuracy: 0.7527 - val_loss: 0.5096 - val_accuracy: 0.7542
Epoch 59/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.5091 - accuracy: 0.7523
Epoch 59: val_loss did not improve from 0.50273
852/852 [==============================] - 2s 2ms/step - loss: 0.5095 - accuracy: 0.7514 - val_loss: 0.5080 - val_accuracy: 0.7520
Epoch 60/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.5119 - accuracy: 0.7477
Epoch 60: val_loss improved from 0.50273 to 0.50071, saving model to sixty_three_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5110 - accuracy: 0.7481 - val_loss: 0.5007 - val_accuracy: 0.7565
Epoch 61/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5100 - accuracy: 0.7526
Epoch 61: val_loss did not improve from 0.50071
852/852 [==============================] - 2s 2ms/step - loss: 0.5093 - accuracy: 0.7531 - val_loss: 0.5077 - val_accuracy: 0.7481
Epoch 62/150
810/852 [===========================&gt;..] - ETA: 0s - loss: 0.5092 - accuracy: 0.7516
Epoch 62: val_loss did not improve from 0.50071
852/852 [==============================] - 2s 2ms/step - loss: 0.5092 - accuracy: 0.7525 - val_loss: 0.5039 - val_accuracy: 0.7565
Epoch 63/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.5088 - accuracy: 0.7516
Epoch 63: val_loss did not improve from 0.50071
852/852 [==============================] - 2s 2ms/step - loss: 0.5085 - accuracy: 0.7519 - val_loss: 0.5019 - val_accuracy: 0.7553
Epoch 64/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.5069 - accuracy: 0.7524
Epoch 64: val_loss improved from 0.50071 to 0.50057, saving model to sixty_three_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5075 - accuracy: 0.7520 - val_loss: 0.5006 - val_accuracy: 0.7555
Epoch 65/150
819/852 [===========================&gt;..] - ETA: 0s - loss: 0.5073 - accuracy: 0.7512
Epoch 65: val_loss improved from 0.50057 to 0.49634, saving model to sixty_three_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5067 - accuracy: 0.7514 - val_loss: 0.4963 - val_accuracy: 0.7579
Epoch 66/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.5076 - accuracy: 0.7514
Epoch 66: val_loss did not improve from 0.49634
852/852 [==============================] - 2s 2ms/step - loss: 0.5081 - accuracy: 0.7505 - val_loss: 0.5047 - val_accuracy: 0.7506
Epoch 67/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.5076 - accuracy: 0.7521
Epoch 67: val_loss did not improve from 0.49634
852/852 [==============================] - 2s 2ms/step - loss: 0.5072 - accuracy: 0.7526 - val_loss: 0.5002 - val_accuracy: 0.7574
Epoch 68/150
819/852 [===========================&gt;..] - ETA: 0s - loss: 0.5072 - accuracy: 0.7501
Epoch 68: val_loss did not improve from 0.49634
852/852 [==============================] - 2s 2ms/step - loss: 0.5076 - accuracy: 0.7501 - val_loss: 0.5047 - val_accuracy: 0.7531
Epoch 69/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.5046 - accuracy: 0.7534
Epoch 69: val_loss did not improve from 0.49634
852/852 [==============================] - 2s 2ms/step - loss: 0.5046 - accuracy: 0.7540 - val_loss: 0.5027 - val_accuracy: 0.7565
Epoch 70/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.5038 - accuracy: 0.7532
Epoch 70: val_loss did not improve from 0.49634
852/852 [==============================] - 2s 2ms/step - loss: 0.5040 - accuracy: 0.7531 - val_loss: 0.4976 - val_accuracy: 0.7587
Epoch 71/150
814/852 [===========================&gt;..] - ETA: 0s - loss: 0.5031 - accuracy: 0.7531
Epoch 71: val_loss improved from 0.49634 to 0.49369, saving model to sixty_three_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5042 - accuracy: 0.7527 - val_loss: 0.4937 - val_accuracy: 0.7599
Epoch 72/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.5035 - accuracy: 0.7541
Epoch 72: val_loss did not improve from 0.49369
852/852 [==============================] - 2s 2ms/step - loss: 0.5029 - accuracy: 0.7542 - val_loss: 0.5025 - val_accuracy: 0.7507
Epoch 73/150
815/852 [===========================&gt;..] - ETA: 0s - loss: 0.5033 - accuracy: 0.7513
Epoch 73: val_loss did not improve from 0.49369
852/852 [==============================] - 2s 2ms/step - loss: 0.5039 - accuracy: 0.7513 - val_loss: 0.4961 - val_accuracy: 0.7601
Epoch 74/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.5037 - accuracy: 0.7543
Epoch 74: val_loss did not improve from 0.49369
852/852 [==============================] - 2s 2ms/step - loss: 0.5041 - accuracy: 0.7540 - val_loss: 0.4961 - val_accuracy: 0.7553
Epoch 75/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.5020 - accuracy: 0.7554
Epoch 75: val_loss did not improve from 0.49369
852/852 [==============================] - 2s 2ms/step - loss: 0.5021 - accuracy: 0.7553 - val_loss: 0.4997 - val_accuracy: 0.7542
Epoch 76/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.5030 - accuracy: 0.7534
Epoch 76: val_loss did not improve from 0.49369
852/852 [==============================] - 2s 2ms/step - loss: 0.5038 - accuracy: 0.7529 - val_loss: 0.5007 - val_accuracy: 0.7569
Epoch 77/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.5004 - accuracy: 0.7558
Epoch 77: val_loss did not improve from 0.49369
852/852 [==============================] - 2s 2ms/step - loss: 0.5009 - accuracy: 0.7558 - val_loss: 0.4967 - val_accuracy: 0.7558
Epoch 78/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.4999 - accuracy: 0.7552
Epoch 78: val_loss improved from 0.49369 to 0.49156, saving model to sixty_three_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5016 - accuracy: 0.7548 - val_loss: 0.4916 - val_accuracy: 0.7601
Epoch 79/150
809/852 [===========================&gt;..] - ETA: 0s - loss: 0.4985 - accuracy: 0.7546
Epoch 79: val_loss did not improve from 0.49156
852/852 [==============================] - 2s 2ms/step - loss: 0.4990 - accuracy: 0.7547 - val_loss: 0.5012 - val_accuracy: 0.7525
Epoch 80/150
808/852 [===========================&gt;..] - ETA: 0s - loss: 0.5013 - accuracy: 0.7537
Epoch 80: val_loss did not improve from 0.49156
852/852 [==============================] - 2s 2ms/step - loss: 0.5008 - accuracy: 0.7525 - val_loss: 0.4970 - val_accuracy: 0.7563
Epoch 81/150
820/852 [===========================&gt;..] - ETA: 0s - loss: 0.5005 - accuracy: 0.7534
Epoch 81: val_loss improved from 0.49156 to 0.48966, saving model to sixty_three_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5009 - accuracy: 0.7521 - val_loss: 0.4897 - val_accuracy: 0.7592
Epoch 82/150
810/852 [===========================&gt;..] - ETA: 0s - loss: 0.4959 - accuracy: 0.7556
Epoch 82: val_loss did not improve from 0.48966
852/852 [==============================] - 2s 2ms/step - loss: 0.4986 - accuracy: 0.7534 - val_loss: 0.4942 - val_accuracy: 0.7531
Epoch 83/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5011 - accuracy: 0.7562
Epoch 83: val_loss did not improve from 0.48966
852/852 [==============================] - 2s 2ms/step - loss: 0.5020 - accuracy: 0.7555 - val_loss: 0.4910 - val_accuracy: 0.7578
Epoch 84/150
818/852 [===========================&gt;..] - ETA: 0s - loss: 0.4985 - accuracy: 0.7562
Epoch 84: val_loss did not improve from 0.48966
852/852 [==============================] - 2s 2ms/step - loss: 0.4980 - accuracy: 0.7567 - val_loss: 0.4916 - val_accuracy: 0.7580
Epoch 85/150
814/852 [===========================&gt;..] - ETA: 0s - loss: 0.4972 - accuracy: 0.7557
Epoch 85: val_loss did not improve from 0.48966
852/852 [==============================] - 2s 2ms/step - loss: 0.4975 - accuracy: 0.7554 - val_loss: 0.4949 - val_accuracy: 0.7539
Epoch 86/150
815/852 [===========================&gt;..] - ETA: 0s - loss: 0.4993 - accuracy: 0.7553
Epoch 86: val_loss improved from 0.48966 to 0.48687, saving model to sixty_three_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4986 - accuracy: 0.7560 - val_loss: 0.4869 - val_accuracy: 0.7593
Epoch 87/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.4976 - accuracy: 0.7582
Epoch 87: val_loss did not improve from 0.48687
852/852 [==============================] - 2s 2ms/step - loss: 0.4969 - accuracy: 0.7588 - val_loss: 0.4919 - val_accuracy: 0.7555
Epoch 88/150
815/852 [===========================&gt;..] - ETA: 0s - loss: 0.4971 - accuracy: 0.7555
Epoch 88: val_loss improved from 0.48687 to 0.48615, saving model to sixty_three_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4970 - accuracy: 0.7558 - val_loss: 0.4862 - val_accuracy: 0.7588
Epoch 89/150
818/852 [===========================&gt;..] - ETA: 0s - loss: 0.4956 - accuracy: 0.7548
Epoch 89: val_loss did not improve from 0.48615
852/852 [==============================] - 2s 2ms/step - loss: 0.4971 - accuracy: 0.7536 - val_loss: 0.4902 - val_accuracy: 0.7601
Epoch 90/150
818/852 [===========================&gt;..] - ETA: 0s - loss: 0.4969 - accuracy: 0.7555
Epoch 90: val_loss did not improve from 0.48615
852/852 [==============================] - 2s 2ms/step - loss: 0.4980 - accuracy: 0.7545 - val_loss: 0.4878 - val_accuracy: 0.7595
Epoch 91/150
814/852 [===========================&gt;..] - ETA: 0s - loss: 0.4947 - accuracy: 0.7555
Epoch 91: val_loss did not improve from 0.48615
852/852 [==============================] - 2s 2ms/step - loss: 0.4946 - accuracy: 0.7558 - val_loss: 0.4970 - val_accuracy: 0.7568
Epoch 92/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.4965 - accuracy: 0.7564
Epoch 92: val_loss did not improve from 0.48615
852/852 [==============================] - 2s 2ms/step - loss: 0.4962 - accuracy: 0.7566 - val_loss: 0.4902 - val_accuracy: 0.7576
Epoch 93/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.4939 - accuracy: 0.7546
Epoch 93: val_loss did not improve from 0.48615
852/852 [==============================] - 2s 2ms/step - loss: 0.4942 - accuracy: 0.7550 - val_loss: 0.4890 - val_accuracy: 0.7561
Epoch 94/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.4940 - accuracy: 0.7549
Epoch 94: val_loss did not improve from 0.48615
852/852 [==============================] - 2s 2ms/step - loss: 0.4948 - accuracy: 0.7548 - val_loss: 0.4868 - val_accuracy: 0.7614
Epoch 95/150
818/852 [===========================&gt;..] - ETA: 0s - loss: 0.4944 - accuracy: 0.7567
Epoch 95: val_loss did not improve from 0.48615
852/852 [==============================] - 2s 2ms/step - loss: 0.4929 - accuracy: 0.7583 - val_loss: 0.4893 - val_accuracy: 0.7588
Epoch 96/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.4927 - accuracy: 0.7568
Epoch 96: val_loss improved from 0.48615 to 0.48312, saving model to sixty_three_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4933 - accuracy: 0.7568 - val_loss: 0.4831 - val_accuracy: 0.7622
Epoch 97/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.4915 - accuracy: 0.7579
Epoch 97: val_loss did not improve from 0.48312
852/852 [==============================] - 2s 2ms/step - loss: 0.4924 - accuracy: 0.7576 - val_loss: 0.4876 - val_accuracy: 0.7612
Epoch 98/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.4927 - accuracy: 0.7547
Epoch 98: val_loss improved from 0.48312 to 0.48242, saving model to sixty_three_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4918 - accuracy: 0.7555 - val_loss: 0.4824 - val_accuracy: 0.7627
Epoch 99/150
819/852 [===========================&gt;..] - ETA: 0s - loss: 0.4910 - accuracy: 0.7592
Epoch 99: val_loss did not improve from 0.48242
852/852 [==============================] - 2s 2ms/step - loss: 0.4916 - accuracy: 0.7583 - val_loss: 0.4903 - val_accuracy: 0.7578
Epoch 100/150
814/852 [===========================&gt;..] - ETA: 0s - loss: 0.4950 - accuracy: 0.7505
Epoch 100: val_loss did not improve from 0.48242
852/852 [==============================] - 2s 2ms/step - loss: 0.4921 - accuracy: 0.7540 - val_loss: 0.4906 - val_accuracy: 0.7595
Epoch 101/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.4920 - accuracy: 0.7569
Epoch 101: val_loss did not improve from 0.48242
852/852 [==============================] - 2s 2ms/step - loss: 0.4935 - accuracy: 0.7556 - val_loss: 0.4842 - val_accuracy: 0.7607
Epoch 102/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.4904 - accuracy: 0.7583
Epoch 102: val_loss did not improve from 0.48242
852/852 [==============================] - 2s 2ms/step - loss: 0.4917 - accuracy: 0.7586 - val_loss: 0.4883 - val_accuracy: 0.7614
Epoch 103/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.4926 - accuracy: 0.7575
Epoch 103: val_loss did not improve from 0.48242
852/852 [==============================] - 2s 2ms/step - loss: 0.4907 - accuracy: 0.7594 - val_loss: 0.4856 - val_accuracy: 0.7576
Epoch 104/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.4904 - accuracy: 0.7581
Epoch 104: val_loss improved from 0.48242 to 0.47989, saving model to sixty_three_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4902 - accuracy: 0.7579 - val_loss: 0.4799 - val_accuracy: 0.7628
Epoch 105/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.4903 - accuracy: 0.7562
Epoch 105: val_loss did not improve from 0.47989
852/852 [==============================] - 2s 2ms/step - loss: 0.4905 - accuracy: 0.7561 - val_loss: 0.4802 - val_accuracy: 0.7644
Epoch 106/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.4894 - accuracy: 0.7589
Epoch 106: val_loss did not improve from 0.47989
852/852 [==============================] - 2s 2ms/step - loss: 0.4894 - accuracy: 0.7582 - val_loss: 0.4812 - val_accuracy: 0.7637
Epoch 107/150
820/852 [===========================&gt;..] - ETA: 0s - loss: 0.4872 - accuracy: 0.7596
Epoch 107: val_loss improved from 0.47989 to 0.47720, saving model to sixty_three_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4885 - accuracy: 0.7595 - val_loss: 0.4772 - val_accuracy: 0.7627
Epoch 108/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.4860 - accuracy: 0.7584
Epoch 108: val_loss did not improve from 0.47720
852/852 [==============================] - 2s 2ms/step - loss: 0.4878 - accuracy: 0.7575 - val_loss: 0.4787 - val_accuracy: 0.7649
Epoch 109/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.4875 - accuracy: 0.7586
Epoch 109: val_loss did not improve from 0.47720
852/852 [==============================] - 2s 2ms/step - loss: 0.4876 - accuracy: 0.7586 - val_loss: 0.4796 - val_accuracy: 0.7616
Epoch 110/150
813/852 [===========================&gt;..] - ETA: 0s - loss: 0.4891 - accuracy: 0.7584
Epoch 110: val_loss did not improve from 0.47720
852/852 [==============================] - 2s 2ms/step - loss: 0.4890 - accuracy: 0.7592 - val_loss: 0.4777 - val_accuracy: 0.7620
Epoch 111/150
819/852 [===========================&gt;..] - ETA: 0s - loss: 0.4864 - accuracy: 0.7596
Epoch 111: val_loss did not improve from 0.47720
852/852 [==============================] - 2s 2ms/step - loss: 0.4871 - accuracy: 0.7593 - val_loss: 0.4834 - val_accuracy: 0.7594
Epoch 112/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.4879 - accuracy: 0.7616
Epoch 112: val_loss improved from 0.47720 to 0.47432, saving model to sixty_three_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4870 - accuracy: 0.7622 - val_loss: 0.4743 - val_accuracy: 0.7653
Epoch 113/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.4837 - accuracy: 0.7602
Epoch 113: val_loss did not improve from 0.47432
852/852 [==============================] - 2s 2ms/step - loss: 0.4847 - accuracy: 0.7594 - val_loss: 0.4797 - val_accuracy: 0.7650
Epoch 114/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.4861 - accuracy: 0.7594
Epoch 114: val_loss did not improve from 0.47432
852/852 [==============================] - 2s 2ms/step - loss: 0.4856 - accuracy: 0.7603 - val_loss: 0.4812 - val_accuracy: 0.7595
Epoch 115/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.4900 - accuracy: 0.7583
Epoch 115: val_loss did not improve from 0.47432
852/852 [==============================] - 2s 2ms/step - loss: 0.4892 - accuracy: 0.7592 - val_loss: 0.4789 - val_accuracy: 0.7630
Epoch 116/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.4870 - accuracy: 0.7569
Epoch 116: val_loss improved from 0.47432 to 0.47337, saving model to sixty_three_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4871 - accuracy: 0.7572 - val_loss: 0.4734 - val_accuracy: 0.7644
Epoch 117/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.4836 - accuracy: 0.7602
Epoch 117: val_loss did not improve from 0.47337
852/852 [==============================] - 2s 2ms/step - loss: 0.4841 - accuracy: 0.7602 - val_loss: 0.4808 - val_accuracy: 0.7613
Epoch 118/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.4841 - accuracy: 0.7632
Epoch 118: val_loss did not improve from 0.47337
852/852 [==============================] - 2s 2ms/step - loss: 0.4845 - accuracy: 0.7624 - val_loss: 0.4822 - val_accuracy: 0.7594
Epoch 119/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.4859 - accuracy: 0.7595
Epoch 119: val_loss did not improve from 0.47337
852/852 [==============================] - 2s 2ms/step - loss: 0.4859 - accuracy: 0.7599 - val_loss: 0.4932 - val_accuracy: 0.7531
Epoch 120/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.4870 - accuracy: 0.7582
Epoch 120: val_loss did not improve from 0.47337
852/852 [==============================] - 2s 2ms/step - loss: 0.4867 - accuracy: 0.7583 - val_loss: 0.4742 - val_accuracy: 0.7650
Epoch 121/150
819/852 [===========================&gt;..] - ETA: 0s - loss: 0.4816 - accuracy: 0.7598
Epoch 121: val_loss improved from 0.47337 to 0.47309, saving model to sixty_three_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4822 - accuracy: 0.7599 - val_loss: 0.4731 - val_accuracy: 0.7640
Epoch 122/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.4820 - accuracy: 0.7603
Epoch 122: val_loss did not improve from 0.47309
852/852 [==============================] - 2s 2ms/step - loss: 0.4829 - accuracy: 0.7603 - val_loss: 0.4765 - val_accuracy: 0.7653
Epoch 123/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.4855 - accuracy: 0.7592
Epoch 123: val_loss improved from 0.47309 to 0.47239, saving model to sixty_three_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4856 - accuracy: 0.7594 - val_loss: 0.4724 - val_accuracy: 0.7643
Epoch 124/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.4825 - accuracy: 0.7597
Epoch 124: val_loss improved from 0.47239 to 0.46976, saving model to sixty_three_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4816 - accuracy: 0.7601 - val_loss: 0.4698 - val_accuracy: 0.7657
Epoch 125/150
811/852 [===========================&gt;..] - ETA: 0s - loss: 0.4854 - accuracy: 0.7599
Epoch 125: val_loss did not improve from 0.46976
852/852 [==============================] - 2s 2ms/step - loss: 0.4839 - accuracy: 0.7612 - val_loss: 0.4797 - val_accuracy: 0.7668
Epoch 126/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.4834 - accuracy: 0.7630
Epoch 126: val_loss did not improve from 0.46976
852/852 [==============================] - 2s 2ms/step - loss: 0.4827 - accuracy: 0.7630 - val_loss: 0.4701 - val_accuracy: 0.7659
Epoch 127/150
815/852 [===========================&gt;..] - ETA: 0s - loss: 0.4800 - accuracy: 0.7615
Epoch 127: val_loss did not improve from 0.46976
852/852 [==============================] - 2s 2ms/step - loss: 0.4807 - accuracy: 0.7612 - val_loss: 0.4711 - val_accuracy: 0.7693
Epoch 128/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.4812 - accuracy: 0.7613
Epoch 128: val_loss improved from 0.46976 to 0.46827, saving model to sixty_three_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4809 - accuracy: 0.7610 - val_loss: 0.4683 - val_accuracy: 0.7680
Epoch 129/150
809/852 [===========================&gt;..] - ETA: 0s - loss: 0.4785 - accuracy: 0.7642
Epoch 129: val_loss did not improve from 0.46827
852/852 [==============================] - 2s 2ms/step - loss: 0.4797 - accuracy: 0.7627 - val_loss: 0.4806 - val_accuracy: 0.7600
Epoch 130/150
808/852 [===========================&gt;..] - ETA: 0s - loss: 0.4794 - accuracy: 0.7625
Epoch 130: val_loss did not improve from 0.46827
852/852 [==============================] - 2s 2ms/step - loss: 0.4802 - accuracy: 0.7617 - val_loss: 0.4685 - val_accuracy: 0.7648
Epoch 131/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.4821 - accuracy: 0.7616
Epoch 131: val_loss did not improve from 0.46827
852/852 [==============================] - 2s 2ms/step - loss: 0.4811 - accuracy: 0.7622 - val_loss: 0.4701 - val_accuracy: 0.7695
Epoch 132/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.4758 - accuracy: 0.7655
Epoch 132: val_loss did not improve from 0.46827
852/852 [==============================] - 2s 2ms/step - loss: 0.4763 - accuracy: 0.7654 - val_loss: 0.4715 - val_accuracy: 0.7651
Epoch 133/150
820/852 [===========================&gt;..] - ETA: 0s - loss: 0.4841 - accuracy: 0.7599
Epoch 133: val_loss did not improve from 0.46827
852/852 [==============================] - 2s 2ms/step - loss: 0.4817 - accuracy: 0.7619 - val_loss: 0.4733 - val_accuracy: 0.7659
Epoch 134/150
813/852 [===========================&gt;..] - ETA: 0s - loss: 0.4818 - accuracy: 0.7617
Epoch 134: val_loss did not improve from 0.46827
852/852 [==============================] - 2s 2ms/step - loss: 0.4818 - accuracy: 0.7607 - val_loss: 0.4718 - val_accuracy: 0.7677
Epoch 135/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.4773 - accuracy: 0.7619
Epoch 135: val_loss did not improve from 0.46827
852/852 [==============================] - 2s 2ms/step - loss: 0.4777 - accuracy: 0.7617 - val_loss: 0.4728 - val_accuracy: 0.7688
Epoch 136/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.4762 - accuracy: 0.7615
Epoch 136: val_loss did not improve from 0.46827
852/852 [==============================] - 2s 3ms/step - loss: 0.4758 - accuracy: 0.7619 - val_loss: 0.4701 - val_accuracy: 0.7668
Epoch 137/150
808/852 [===========================&gt;..] - ETA: 0s - loss: 0.4764 - accuracy: 0.7650
Epoch 137: val_loss did not improve from 0.46827
852/852 [==============================] - 2s 2ms/step - loss: 0.4782 - accuracy: 0.7639 - val_loss: 0.4727 - val_accuracy: 0.7642
Epoch 138/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.4765 - accuracy: 0.7631
Epoch 138: val_loss improved from 0.46827 to 0.46465, saving model to sixty_three_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4773 - accuracy: 0.7626 - val_loss: 0.4647 - val_accuracy: 0.7680
Epoch 139/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.4770 - accuracy: 0.7624
Epoch 139: val_loss improved from 0.46465 to 0.46122, saving model to sixty_three_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4763 - accuracy: 0.7630 - val_loss: 0.4612 - val_accuracy: 0.7729
Epoch 140/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.4743 - accuracy: 0.7656
Epoch 140: val_loss did not improve from 0.46122
852/852 [==============================] - 2s 2ms/step - loss: 0.4756 - accuracy: 0.7643 - val_loss: 0.4669 - val_accuracy: 0.7687
Epoch 141/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.4744 - accuracy: 0.7624
Epoch 141: val_loss did not improve from 0.46122
852/852 [==============================] - 2s 2ms/step - loss: 0.4742 - accuracy: 0.7624 - val_loss: 0.4724 - val_accuracy: 0.7650
Epoch 142/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.4776 - accuracy: 0.7641
Epoch 142: val_loss did not improve from 0.46122
852/852 [==============================] - 2s 2ms/step - loss: 0.4768 - accuracy: 0.7650 - val_loss: 0.4768 - val_accuracy: 0.7653
Epoch 143/150
818/852 [===========================&gt;..] - ETA: 0s - loss: 0.4757 - accuracy: 0.7638
Epoch 143: val_loss did not improve from 0.46122
852/852 [==============================] - 2s 2ms/step - loss: 0.4766 - accuracy: 0.7635 - val_loss: 0.4696 - val_accuracy: 0.7668
Epoch 144/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.4757 - accuracy: 0.7640
Epoch 144: val_loss did not improve from 0.46122
852/852 [==============================] - 2s 2ms/step - loss: 0.4754 - accuracy: 0.7646 - val_loss: 0.4689 - val_accuracy: 0.7655
Epoch 145/150
815/852 [===========================&gt;..] - ETA: 0s - loss: 0.4767 - accuracy: 0.7643
Epoch 145: val_loss did not improve from 0.46122
852/852 [==============================] - 2s 2ms/step - loss: 0.4738 - accuracy: 0.7661 - val_loss: 0.4740 - val_accuracy: 0.7630
Epoch 146/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.4735 - accuracy: 0.7676
Epoch 146: val_loss did not improve from 0.46122
852/852 [==============================] - 2s 2ms/step - loss: 0.4744 - accuracy: 0.7675 - val_loss: 0.4737 - val_accuracy: 0.7628
Epoch 147/150
810/852 [===========================&gt;..] - ETA: 0s - loss: 0.4745 - accuracy: 0.7656
Epoch 147: val_loss did not improve from 0.46122
852/852 [==============================] - 2s 2ms/step - loss: 0.4737 - accuracy: 0.7661 - val_loss: 0.4681 - val_accuracy: 0.7671
Epoch 148/150
813/852 [===========================&gt;..] - ETA: 0s - loss: 0.4756 - accuracy: 0.7642
Epoch 148: val_loss did not improve from 0.46122
852/852 [==============================] - 2s 2ms/step - loss: 0.4746 - accuracy: 0.7647 - val_loss: 0.4643 - val_accuracy: 0.7695
Epoch 149/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.4734 - accuracy: 0.7649
Epoch 149: val_loss did not improve from 0.46122
852/852 [==============================] - 2s 2ms/step - loss: 0.4740 - accuracy: 0.7643 - val_loss: 0.4688 - val_accuracy: 0.7663
Epoch 150/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.4734 - accuracy: 0.7640
Epoch 150: val_loss did not improve from 0.46122
852/852 [==============================] - 2s 2ms/step - loss: 0.4736 - accuracy: 0.7636 - val_loss: 0.4636 - val_accuracy: 0.7705
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=c1fa36e5-f312-42f9-a123-419482b18f53">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [64]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">history_63</span> <span class="o">=</span> <span class="n">sixty_three_neuron</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">callback_a</span><span class="p">,</span> <span class="n">callback_b</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.4761 - accuracy: 0.7631
Epoch 1: val_loss improved from 0.46122 to 0.46049, saving model to sixty_three_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4754 - accuracy: 0.7640 - val_loss: 0.4605 - val_accuracy: 0.7710
Epoch 2/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.4726 - accuracy: 0.7673
Epoch 2: val_loss did not improve from 0.46049
852/852 [==============================] - 2s 2ms/step - loss: 0.4731 - accuracy: 0.7667 - val_loss: 0.4827 - val_accuracy: 0.7640
Epoch 3/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.4716 - accuracy: 0.7661
Epoch 3: val_loss did not improve from 0.46049
852/852 [==============================] - 2s 2ms/step - loss: 0.4718 - accuracy: 0.7659 - val_loss: 0.4642 - val_accuracy: 0.7709
Epoch 4/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.4712 - accuracy: 0.7680
Epoch 4: val_loss did not improve from 0.46049
852/852 [==============================] - 2s 2ms/step - loss: 0.4714 - accuracy: 0.7667 - val_loss: 0.4665 - val_accuracy: 0.7691
Epoch 5/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.4724 - accuracy: 0.7667
Epoch 5: val_loss did not improve from 0.46049
852/852 [==============================] - 2s 2ms/step - loss: 0.4722 - accuracy: 0.7669 - val_loss: 0.4671 - val_accuracy: 0.7693
Epoch 6/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.4732 - accuracy: 0.7656
Epoch 6: val_loss did not improve from 0.46049
852/852 [==============================] - 2s 2ms/step - loss: 0.4740 - accuracy: 0.7653 - val_loss: 0.4608 - val_accuracy: 0.7727
Epoch 7/150
808/852 [===========================&gt;..] - ETA: 0s - loss: 0.4720 - accuracy: 0.7646
Epoch 7: val_loss improved from 0.46049 to 0.45757, saving model to sixty_three_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4714 - accuracy: 0.7648 - val_loss: 0.4576 - val_accuracy: 0.7725
Epoch 8/150
802/852 [===========================&gt;..] - ETA: 0s - loss: 0.4692 - accuracy: 0.7653
Epoch 8: val_loss did not improve from 0.45757
852/852 [==============================] - 2s 2ms/step - loss: 0.4686 - accuracy: 0.7646 - val_loss: 0.4668 - val_accuracy: 0.7668
Epoch 9/150
804/852 [===========================&gt;..] - ETA: 0s - loss: 0.4705 - accuracy: 0.7677
Epoch 9: val_loss did not improve from 0.45757
852/852 [==============================] - 2s 2ms/step - loss: 0.4712 - accuracy: 0.7671 - val_loss: 0.4623 - val_accuracy: 0.7696
Epoch 10/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.4728 - accuracy: 0.7655
Epoch 10: val_loss did not improve from 0.45757
852/852 [==============================] - 2s 2ms/step - loss: 0.4724 - accuracy: 0.7664 - val_loss: 0.4747 - val_accuracy: 0.7636
Epoch 11/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.4709 - accuracy: 0.7675
Epoch 11: val_loss did not improve from 0.45757
852/852 [==============================] - 2s 2ms/step - loss: 0.4709 - accuracy: 0.7676 - val_loss: 0.4588 - val_accuracy: 0.7709
Epoch 12/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.4694 - accuracy: 0.7689
Epoch 12: val_loss did not improve from 0.45757
852/852 [==============================] - 2s 2ms/step - loss: 0.4691 - accuracy: 0.7696 - val_loss: 0.4685 - val_accuracy: 0.7701
Epoch 13/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.4713 - accuracy: 0.7678
Epoch 13: val_loss improved from 0.45757 to 0.45587, saving model to sixty_three_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4713 - accuracy: 0.7676 - val_loss: 0.4559 - val_accuracy: 0.7743
Epoch 14/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.4709 - accuracy: 0.7659
Epoch 14: val_loss did not improve from 0.45587
852/852 [==============================] - 2s 2ms/step - loss: 0.4707 - accuracy: 0.7662 - val_loss: 0.4646 - val_accuracy: 0.7684
Epoch 15/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.4689 - accuracy: 0.7673
Epoch 15: val_loss did not improve from 0.45587
852/852 [==============================] - 2s 2ms/step - loss: 0.4698 - accuracy: 0.7669 - val_loss: 0.4642 - val_accuracy: 0.7677
Epoch 16/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.4675 - accuracy: 0.7661
Epoch 16: val_loss did not improve from 0.45587
852/852 [==============================] - 2s 2ms/step - loss: 0.4695 - accuracy: 0.7646 - val_loss: 0.4619 - val_accuracy: 0.7728
Epoch 17/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.4694 - accuracy: 0.7698
Epoch 17: val_loss did not improve from 0.45587
852/852 [==============================] - 2s 2ms/step - loss: 0.4694 - accuracy: 0.7700 - val_loss: 0.4575 - val_accuracy: 0.7733
Epoch 18/150
815/852 [===========================&gt;..] - ETA: 0s - loss: 0.4683 - accuracy: 0.7677
Epoch 18: val_loss did not improve from 0.45587
852/852 [==============================] - 2s 2ms/step - loss: 0.4684 - accuracy: 0.7680 - val_loss: 0.4612 - val_accuracy: 0.7703
Epoch 19/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.4708 - accuracy: 0.7667
Epoch 19: val_loss did not improve from 0.45587
852/852 [==============================] - 2s 2ms/step - loss: 0.4701 - accuracy: 0.7670 - val_loss: 0.4600 - val_accuracy: 0.7704
Epoch 20/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.4699 - accuracy: 0.7682
Epoch 20: val_loss did not improve from 0.45587
852/852 [==============================] - 2s 2ms/step - loss: 0.4690 - accuracy: 0.7686 - val_loss: 0.4602 - val_accuracy: 0.7696
Epoch 21/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.4680 - accuracy: 0.7654
Epoch 21: val_loss improved from 0.45587 to 0.45491, saving model to sixty_three_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4687 - accuracy: 0.7655 - val_loss: 0.4549 - val_accuracy: 0.7718
Epoch 22/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.4660 - accuracy: 0.7689
Epoch 22: val_loss improved from 0.45491 to 0.45255, saving model to sixty_three_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4661 - accuracy: 0.7690 - val_loss: 0.4525 - val_accuracy: 0.7757
Epoch 23/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.4657 - accuracy: 0.7658
Epoch 23: val_loss did not improve from 0.45255
852/852 [==============================] - 2s 2ms/step - loss: 0.4644 - accuracy: 0.7673 - val_loss: 0.4597 - val_accuracy: 0.7702
Epoch 24/150
811/852 [===========================&gt;..] - ETA: 0s - loss: 0.4702 - accuracy: 0.7656
Epoch 24: val_loss did not improve from 0.45255
852/852 [==============================] - 2s 2ms/step - loss: 0.4693 - accuracy: 0.7667 - val_loss: 0.4538 - val_accuracy: 0.7728
Epoch 25/150
812/852 [===========================&gt;..] - ETA: 0s - loss: 0.4641 - accuracy: 0.7706
Epoch 25: val_loss did not improve from 0.45255
852/852 [==============================] - 2s 2ms/step - loss: 0.4649 - accuracy: 0.7693 - val_loss: 0.4647 - val_accuracy: 0.7704
Epoch 26/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.4666 - accuracy: 0.7700
Epoch 26: val_loss did not improve from 0.45255
852/852 [==============================] - 2s 2ms/step - loss: 0.4679 - accuracy: 0.7691 - val_loss: 0.4618 - val_accuracy: 0.7748
Epoch 27/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.4643 - accuracy: 0.7707
Epoch 27: val_loss did not improve from 0.45255
852/852 [==============================] - 2s 2ms/step - loss: 0.4655 - accuracy: 0.7695 - val_loss: 0.4593 - val_accuracy: 0.7748
Epoch 28/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.4640 - accuracy: 0.7710
Epoch 28: val_loss did not improve from 0.45255
852/852 [==============================] - 2s 2ms/step - loss: 0.4653 - accuracy: 0.7696 - val_loss: 0.4548 - val_accuracy: 0.7747
Epoch 29/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.4630 - accuracy: 0.7704
Epoch 29: val_loss did not improve from 0.45255
852/852 [==============================] - 2s 2ms/step - loss: 0.4645 - accuracy: 0.7693 - val_loss: 0.4529 - val_accuracy: 0.7763
Epoch 30/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.4632 - accuracy: 0.7689
Epoch 30: val_loss did not improve from 0.45255
852/852 [==============================] - 2s 2ms/step - loss: 0.4637 - accuracy: 0.7689 - val_loss: 0.4623 - val_accuracy: 0.7718
Epoch 31/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.4654 - accuracy: 0.7697
Epoch 31: val_loss did not improve from 0.45255
852/852 [==============================] - 2s 2ms/step - loss: 0.4651 - accuracy: 0.7704 - val_loss: 0.4597 - val_accuracy: 0.7710
Epoch 32/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.4636 - accuracy: 0.7677
Epoch 32: val_loss did not improve from 0.45255
852/852 [==============================] - 2s 2ms/step - loss: 0.4634 - accuracy: 0.7681 - val_loss: 0.4527 - val_accuracy: 0.7747
Epoch 33/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.4632 - accuracy: 0.7693
Epoch 33: val_loss did not improve from 0.45255
852/852 [==============================] - 2s 2ms/step - loss: 0.4619 - accuracy: 0.7713 - val_loss: 0.4606 - val_accuracy: 0.7707
Epoch 34/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.4629 - accuracy: 0.7696
Epoch 34: val_loss did not improve from 0.45255
852/852 [==============================] - 2s 2ms/step - loss: 0.4640 - accuracy: 0.7698 - val_loss: 0.4631 - val_accuracy: 0.7696
Epoch 35/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.4626 - accuracy: 0.7691
Epoch 35: val_loss did not improve from 0.45255
852/852 [==============================] - 2s 2ms/step - loss: 0.4632 - accuracy: 0.7694 - val_loss: 0.4607 - val_accuracy: 0.7754
Epoch 36/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.4650 - accuracy: 0.7713
Epoch 36: val_loss did not improve from 0.45255
852/852 [==============================] - 2s 2ms/step - loss: 0.4653 - accuracy: 0.7708 - val_loss: 0.4575 - val_accuracy: 0.7725
Epoch 37/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.4629 - accuracy: 0.7712
Epoch 37: val_loss did not improve from 0.45255
852/852 [==============================] - 2s 2ms/step - loss: 0.4632 - accuracy: 0.7716 - val_loss: 0.4547 - val_accuracy: 0.7729
Epoch 38/150
815/852 [===========================&gt;..] - ETA: 0s - loss: 0.4636 - accuracy: 0.7698
Epoch 38: val_loss did not improve from 0.45255
852/852 [==============================] - 2s 2ms/step - loss: 0.4638 - accuracy: 0.7705 - val_loss: 0.4531 - val_accuracy: 0.7796
Epoch 39/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.4616 - accuracy: 0.7722
Epoch 39: val_loss improved from 0.45255 to 0.44844, saving model to sixty_three_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4616 - accuracy: 0.7722 - val_loss: 0.4484 - val_accuracy: 0.7765
Epoch 40/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.4598 - accuracy: 0.7724
Epoch 40: val_loss did not improve from 0.44844
852/852 [==============================] - 2s 2ms/step - loss: 0.4605 - accuracy: 0.7723 - val_loss: 0.4571 - val_accuracy: 0.7725
Epoch 41/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.4626 - accuracy: 0.7694
Epoch 41: val_loss did not improve from 0.44844
852/852 [==============================] - 2s 2ms/step - loss: 0.4631 - accuracy: 0.7689 - val_loss: 0.4600 - val_accuracy: 0.7733
Epoch 42/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.4611 - accuracy: 0.7732
Epoch 42: val_loss did not improve from 0.44844
852/852 [==============================] - 2s 2ms/step - loss: 0.4611 - accuracy: 0.7731 - val_loss: 0.4543 - val_accuracy: 0.7769
Epoch 43/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.4645 - accuracy: 0.7697
Epoch 43: val_loss improved from 0.44844 to 0.44810, saving model to sixty_three_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4646 - accuracy: 0.7695 - val_loss: 0.4481 - val_accuracy: 0.7767
Epoch 44/150
810/852 [===========================&gt;..] - ETA: 0s - loss: 0.4606 - accuracy: 0.7722
Epoch 44: val_loss did not improve from 0.44810
852/852 [==============================] - 2s 2ms/step - loss: 0.4602 - accuracy: 0.7731 - val_loss: 0.4540 - val_accuracy: 0.7745
Epoch 45/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.4606 - accuracy: 0.7708
Epoch 45: val_loss did not improve from 0.44810
852/852 [==============================] - 2s 2ms/step - loss: 0.4603 - accuracy: 0.7717 - val_loss: 0.4576 - val_accuracy: 0.7734
Epoch 46/150
811/852 [===========================&gt;..] - ETA: 0s - loss: 0.4626 - accuracy: 0.7711
Epoch 46: val_loss did not improve from 0.44810
852/852 [==============================] - 2s 2ms/step - loss: 0.4609 - accuracy: 0.7718 - val_loss: 0.4486 - val_accuracy: 0.7795
Epoch 47/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.4579 - accuracy: 0.7749
Epoch 47: val_loss did not improve from 0.44810
852/852 [==============================] - 2s 2ms/step - loss: 0.4576 - accuracy: 0.7750 - val_loss: 0.4556 - val_accuracy: 0.7717
Epoch 48/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.4618 - accuracy: 0.7717
Epoch 48: val_loss improved from 0.44810 to 0.44643, saving model to sixty_three_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4617 - accuracy: 0.7716 - val_loss: 0.4464 - val_accuracy: 0.7754
Epoch 49/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.4550 - accuracy: 0.7750
Epoch 49: val_loss did not improve from 0.44643
852/852 [==============================] - 2s 2ms/step - loss: 0.4581 - accuracy: 0.7722 - val_loss: 0.4524 - val_accuracy: 0.7762
Epoch 50/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.4574 - accuracy: 0.7734
Epoch 50: val_loss did not improve from 0.44643
852/852 [==============================] - 2s 2ms/step - loss: 0.4582 - accuracy: 0.7734 - val_loss: 0.4695 - val_accuracy: 0.7646
Epoch 51/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.4614 - accuracy: 0.7720
Epoch 51: val_loss did not improve from 0.44643
852/852 [==============================] - 2s 2ms/step - loss: 0.4617 - accuracy: 0.7714 - val_loss: 0.4519 - val_accuracy: 0.7750
Epoch 52/150
809/852 [===========================&gt;..] - ETA: 0s - loss: 0.4571 - accuracy: 0.7727
Epoch 52: val_loss did not improve from 0.44643
852/852 [==============================] - 2s 2ms/step - loss: 0.4587 - accuracy: 0.7718 - val_loss: 0.4495 - val_accuracy: 0.7798
Epoch 53/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.4567 - accuracy: 0.7735
Epoch 53: val_loss did not improve from 0.44643
852/852 [==============================] - 2s 2ms/step - loss: 0.4556 - accuracy: 0.7748 - val_loss: 0.4498 - val_accuracy: 0.7765
Epoch 54/150
814/852 [===========================&gt;..] - ETA: 0s - loss: 0.4596 - accuracy: 0.7715
Epoch 54: val_loss did not improve from 0.44643
852/852 [==============================] - 2s 2ms/step - loss: 0.4597 - accuracy: 0.7723 - val_loss: 0.4527 - val_accuracy: 0.7738
Epoch 55/150
818/852 [===========================&gt;..] - ETA: 0s - loss: 0.4592 - accuracy: 0.7715
Epoch 55: val_loss did not improve from 0.44643
852/852 [==============================] - 2s 2ms/step - loss: 0.4604 - accuracy: 0.7701 - val_loss: 0.4498 - val_accuracy: 0.7771
Epoch 56/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.4555 - accuracy: 0.7740
Epoch 56: val_loss did not improve from 0.44643
852/852 [==============================] - 2s 2ms/step - loss: 0.4561 - accuracy: 0.7735 - val_loss: 0.4514 - val_accuracy: 0.7758
Epoch 57/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.4553 - accuracy: 0.7723
Epoch 57: val_loss did not improve from 0.44643
852/852 [==============================] - 2s 2ms/step - loss: 0.4562 - accuracy: 0.7727 - val_loss: 0.4480 - val_accuracy: 0.7779
Epoch 58/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.4571 - accuracy: 0.7743
Epoch 58: val_loss did not improve from 0.44643
852/852 [==============================] - 2s 2ms/step - loss: 0.4570 - accuracy: 0.7738 - val_loss: 0.4560 - val_accuracy: 0.7758
Epoch 59/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.4563 - accuracy: 0.7739
Epoch 59: val_loss did not improve from 0.44643
852/852 [==============================] - 2s 2ms/step - loss: 0.4564 - accuracy: 0.7736 - val_loss: 0.4504 - val_accuracy: 0.7727
Epoch 60/150
813/852 [===========================&gt;..] - ETA: 0s - loss: 0.4536 - accuracy: 0.7776
Epoch 60: val_loss improved from 0.44643 to 0.44229, saving model to sixty_three_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4543 - accuracy: 0.7772 - val_loss: 0.4423 - val_accuracy: 0.7811
Epoch 61/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.4560 - accuracy: 0.7770
Epoch 61: val_loss did not improve from 0.44229
852/852 [==============================] - 2s 2ms/step - loss: 0.4548 - accuracy: 0.7781 - val_loss: 0.4495 - val_accuracy: 0.7771
Epoch 62/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.4555 - accuracy: 0.7764
Epoch 62: val_loss did not improve from 0.44229
852/852 [==============================] - 2s 2ms/step - loss: 0.4568 - accuracy: 0.7745 - val_loss: 0.4491 - val_accuracy: 0.7761
Epoch 63/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.4556 - accuracy: 0.7743
Epoch 63: val_loss did not improve from 0.44229
852/852 [==============================] - 2s 2ms/step - loss: 0.4556 - accuracy: 0.7742 - val_loss: 0.4464 - val_accuracy: 0.7816
Epoch 64/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.4572 - accuracy: 0.7736
Epoch 64: val_loss did not improve from 0.44229
852/852 [==============================] - 2s 2ms/step - loss: 0.4562 - accuracy: 0.7742 - val_loss: 0.4553 - val_accuracy: 0.7727
Epoch 65/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.4566 - accuracy: 0.7706
Epoch 65: val_loss did not improve from 0.44229
852/852 [==============================] - 2s 2ms/step - loss: 0.4562 - accuracy: 0.7721 - val_loss: 0.4495 - val_accuracy: 0.7792
Epoch 66/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.4551 - accuracy: 0.7725
Epoch 66: val_loss did not improve from 0.44229
852/852 [==============================] - 2s 2ms/step - loss: 0.4553 - accuracy: 0.7728 - val_loss: 0.4459 - val_accuracy: 0.7830
Epoch 67/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.4580 - accuracy: 0.7740
Epoch 67: val_loss did not improve from 0.44229
852/852 [==============================] - 2s 2ms/step - loss: 0.4580 - accuracy: 0.7740 - val_loss: 0.4532 - val_accuracy: 0.7713
Epoch 68/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.4548 - accuracy: 0.7741
Epoch 68: val_loss did not improve from 0.44229
852/852 [==============================] - 2s 2ms/step - loss: 0.4550 - accuracy: 0.7741 - val_loss: 0.4473 - val_accuracy: 0.7802
Epoch 69/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.4561 - accuracy: 0.7748
Epoch 69: val_loss did not improve from 0.44229
852/852 [==============================] - 2s 2ms/step - loss: 0.4555 - accuracy: 0.7750 - val_loss: 0.4464 - val_accuracy: 0.7809
Epoch 70/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.4531 - accuracy: 0.7767
Epoch 70: val_loss did not improve from 0.44229
852/852 [==============================] - 2s 2ms/step - loss: 0.4542 - accuracy: 0.7757 - val_loss: 0.4471 - val_accuracy: 0.7791
Epoch 71/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.4576 - accuracy: 0.7752
Epoch 71: val_loss did not improve from 0.44229
852/852 [==============================] - 2s 2ms/step - loss: 0.4592 - accuracy: 0.7741 - val_loss: 0.4515 - val_accuracy: 0.7770
Epoch 72/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.4555 - accuracy: 0.7755
Epoch 72: val_loss did not improve from 0.44229
852/852 [==============================] - 2s 2ms/step - loss: 0.4554 - accuracy: 0.7754 - val_loss: 0.4462 - val_accuracy: 0.7799
Epoch 73/150
813/852 [===========================&gt;..] - ETA: 0s - loss: 0.4511 - accuracy: 0.7763
Epoch 73: val_loss did not improve from 0.44229
852/852 [==============================] - 2s 2ms/step - loss: 0.4520 - accuracy: 0.7761 - val_loss: 0.4466 - val_accuracy: 0.7762
Epoch 74/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.4514 - accuracy: 0.7777
Epoch 74: val_loss improved from 0.44229 to 0.44197, saving model to sixty_three_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4513 - accuracy: 0.7777 - val_loss: 0.4420 - val_accuracy: 0.7805
Epoch 75/150
812/852 [===========================&gt;..] - ETA: 0s - loss: 0.4547 - accuracy: 0.7777
Epoch 75: val_loss improved from 0.44197 to 0.44072, saving model to sixty_three_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4554 - accuracy: 0.7767 - val_loss: 0.4407 - val_accuracy: 0.7841
Epoch 76/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.4573 - accuracy: 0.7723
Epoch 76: val_loss did not improve from 0.44072
852/852 [==============================] - 2s 2ms/step - loss: 0.4570 - accuracy: 0.7725 - val_loss: 0.4497 - val_accuracy: 0.7775
Epoch 77/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.4529 - accuracy: 0.7757
Epoch 77: val_loss did not improve from 0.44072
852/852 [==============================] - 2s 2ms/step - loss: 0.4530 - accuracy: 0.7757 - val_loss: 0.4482 - val_accuracy: 0.7775
Epoch 78/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.4508 - accuracy: 0.7751
Epoch 78: val_loss improved from 0.44072 to 0.43985, saving model to sixty_three_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4507 - accuracy: 0.7751 - val_loss: 0.4398 - val_accuracy: 0.7838
Epoch 79/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.4538 - accuracy: 0.7736
Epoch 79: val_loss did not improve from 0.43985
852/852 [==============================] - 2s 2ms/step - loss: 0.4541 - accuracy: 0.7734 - val_loss: 0.4457 - val_accuracy: 0.7791
Epoch 80/150
818/852 [===========================&gt;..] - ETA: 0s - loss: 0.4483 - accuracy: 0.7775
Epoch 80: val_loss did not improve from 0.43985
852/852 [==============================] - 2s 2ms/step - loss: 0.4499 - accuracy: 0.7758 - val_loss: 0.4431 - val_accuracy: 0.7794
Epoch 81/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.4553 - accuracy: 0.7738
Epoch 81: val_loss did not improve from 0.43985
852/852 [==============================] - 2s 2ms/step - loss: 0.4554 - accuracy: 0.7736 - val_loss: 0.4441 - val_accuracy: 0.7801
Epoch 82/150
815/852 [===========================&gt;..] - ETA: 0s - loss: 0.4510 - accuracy: 0.7775
Epoch 82: val_loss did not improve from 0.43985
852/852 [==============================] - 2s 2ms/step - loss: 0.4511 - accuracy: 0.7764 - val_loss: 0.4476 - val_accuracy: 0.7802
Epoch 83/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.4529 - accuracy: 0.7774
Epoch 83: val_loss did not improve from 0.43985
852/852 [==============================] - 2s 2ms/step - loss: 0.4551 - accuracy: 0.7767 - val_loss: 0.4430 - val_accuracy: 0.7798
Epoch 84/150
811/852 [===========================&gt;..] - ETA: 0s - loss: 0.4519 - accuracy: 0.7783
Epoch 84: val_loss did not improve from 0.43985
852/852 [==============================] - 2s 2ms/step - loss: 0.4532 - accuracy: 0.7778 - val_loss: 0.4456 - val_accuracy: 0.7787
Epoch 85/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.4477 - accuracy: 0.7806
Epoch 85: val_loss improved from 0.43985 to 0.43671, saving model to sixty_three_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4476 - accuracy: 0.7805 - val_loss: 0.4367 - val_accuracy: 0.7849
Epoch 86/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.4514 - accuracy: 0.7771
Epoch 86: val_loss did not improve from 0.43671
852/852 [==============================] - 2s 2ms/step - loss: 0.4510 - accuracy: 0.7775 - val_loss: 0.4392 - val_accuracy: 0.7836
Epoch 87/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.4525 - accuracy: 0.7753
Epoch 87: val_loss did not improve from 0.43671
852/852 [==============================] - 2s 2ms/step - loss: 0.4537 - accuracy: 0.7745 - val_loss: 0.4481 - val_accuracy: 0.7797
Epoch 88/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.4490 - accuracy: 0.7763
Epoch 88: val_loss did not improve from 0.43671
852/852 [==============================] - 2s 2ms/step - loss: 0.4479 - accuracy: 0.7774 - val_loss: 0.4385 - val_accuracy: 0.7852
Epoch 89/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.4506 - accuracy: 0.7774
Epoch 89: val_loss did not improve from 0.43671
852/852 [==============================] - 2s 2ms/step - loss: 0.4505 - accuracy: 0.7775 - val_loss: 0.4420 - val_accuracy: 0.7817
Epoch 90/150
812/852 [===========================&gt;..] - ETA: 0s - loss: 0.4491 - accuracy: 0.7766
Epoch 90: val_loss did not improve from 0.43671
852/852 [==============================] - 2s 2ms/step - loss: 0.4494 - accuracy: 0.7768 - val_loss: 0.4371 - val_accuracy: 0.7830
Epoch 91/150
852/852 [==============================] - ETA: 0s - loss: 0.4488 - accuracy: 0.7768
Epoch 91: val_loss did not improve from 0.43671
852/852 [==============================] - 2s 2ms/step - loss: 0.4488 - accuracy: 0.7768 - val_loss: 0.4507 - val_accuracy: 0.7795
Epoch 92/150
814/852 [===========================&gt;..] - ETA: 0s - loss: 0.4492 - accuracy: 0.7774
Epoch 92: val_loss did not improve from 0.43671
852/852 [==============================] - 2s 2ms/step - loss: 0.4504 - accuracy: 0.7763 - val_loss: 0.4449 - val_accuracy: 0.7783
Epoch 93/150
807/852 [===========================&gt;..] - ETA: 0s - loss: 0.4486 - accuracy: 0.7809
Epoch 93: val_loss did not improve from 0.43671
852/852 [==============================] - 2s 2ms/step - loss: 0.4487 - accuracy: 0.7815 - val_loss: 0.4393 - val_accuracy: 0.7829
Epoch 94/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.4482 - accuracy: 0.7770
Epoch 94: val_loss did not improve from 0.43671
852/852 [==============================] - 2s 2ms/step - loss: 0.4479 - accuracy: 0.7776 - val_loss: 0.4488 - val_accuracy: 0.7745
Epoch 95/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.4466 - accuracy: 0.7810
Epoch 95: val_loss did not improve from 0.43671
852/852 [==============================] - 2s 2ms/step - loss: 0.4496 - accuracy: 0.7790 - val_loss: 0.4460 - val_accuracy: 0.7833
Epoch 96/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.4511 - accuracy: 0.7792
Epoch 96: val_loss did not improve from 0.43671
852/852 [==============================] - 2s 2ms/step - loss: 0.4516 - accuracy: 0.7783 - val_loss: 0.4394 - val_accuracy: 0.7808
Epoch 97/150
814/852 [===========================&gt;..] - ETA: 0s - loss: 0.4500 - accuracy: 0.7800
Epoch 97: val_loss did not improve from 0.43671
852/852 [==============================] - 2s 2ms/step - loss: 0.4499 - accuracy: 0.7803 - val_loss: 0.4406 - val_accuracy: 0.7805
Epoch 98/150
813/852 [===========================&gt;..] - ETA: 0s - loss: 0.4492 - accuracy: 0.7790
Epoch 98: val_loss did not improve from 0.43671
852/852 [==============================] - 2s 2ms/step - loss: 0.4496 - accuracy: 0.7783 - val_loss: 0.4415 - val_accuracy: 0.7788
Epoch 99/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.4486 - accuracy: 0.7764
Epoch 99: val_loss did not improve from 0.43671
852/852 [==============================] - 2s 2ms/step - loss: 0.4483 - accuracy: 0.7768 - val_loss: 0.4462 - val_accuracy: 0.7784
Epoch 100/150
806/852 [===========================&gt;..] - ETA: 0s - loss: 0.4452 - accuracy: 0.7790
Epoch 100: val_loss did not improve from 0.43671
852/852 [==============================] - 2s 2ms/step - loss: 0.4464 - accuracy: 0.7777 - val_loss: 0.4469 - val_accuracy: 0.7774
Epoch 101/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.4524 - accuracy: 0.7768
Epoch 101: val_loss improved from 0.43671 to 0.43663, saving model to sixty_three_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4535 - accuracy: 0.7760 - val_loss: 0.4366 - val_accuracy: 0.7853
Epoch 102/150
814/852 [===========================&gt;..] - ETA: 0s - loss: 0.4499 - accuracy: 0.7763
Epoch 102: val_loss did not improve from 0.43663
852/852 [==============================] - 2s 2ms/step - loss: 0.4499 - accuracy: 0.7770 - val_loss: 0.4452 - val_accuracy: 0.7802
Epoch 103/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.4489 - accuracy: 0.7787
Epoch 103: val_loss improved from 0.43663 to 0.43590, saving model to sixty_three_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4480 - accuracy: 0.7791 - val_loss: 0.4359 - val_accuracy: 0.7828
Epoch 104/150
820/852 [===========================&gt;..] - ETA: 0s - loss: 0.4471 - accuracy: 0.7795
Epoch 104: val_loss improved from 0.43590 to 0.43567, saving model to sixty_three_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4464 - accuracy: 0.7801 - val_loss: 0.4357 - val_accuracy: 0.7832
Epoch 105/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.4490 - accuracy: 0.7780
Epoch 105: val_loss did not improve from 0.43567
852/852 [==============================] - 2s 2ms/step - loss: 0.4482 - accuracy: 0.7790 - val_loss: 0.4359 - val_accuracy: 0.7818
Epoch 106/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.4455 - accuracy: 0.7794
Epoch 106: val_loss did not improve from 0.43567
852/852 [==============================] - 2s 2ms/step - loss: 0.4456 - accuracy: 0.7794 - val_loss: 0.4365 - val_accuracy: 0.7833
Epoch 107/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.4519 - accuracy: 0.7787
Epoch 107: val_loss did not improve from 0.43567
852/852 [==============================] - 2s 2ms/step - loss: 0.4518 - accuracy: 0.7787 - val_loss: 0.4421 - val_accuracy: 0.7825
Epoch 108/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.4477 - accuracy: 0.7792
Epoch 108: val_loss improved from 0.43567 to 0.43509, saving model to sixty_three_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4484 - accuracy: 0.7783 - val_loss: 0.4351 - val_accuracy: 0.7851
Epoch 109/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.4423 - accuracy: 0.7820
Epoch 109: val_loss did not improve from 0.43509
852/852 [==============================] - 2s 2ms/step - loss: 0.4428 - accuracy: 0.7816 - val_loss: 0.4371 - val_accuracy: 0.7846
Epoch 110/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.4473 - accuracy: 0.7794
Epoch 110: val_loss did not improve from 0.43509
852/852 [==============================] - 2s 2ms/step - loss: 0.4470 - accuracy: 0.7797 - val_loss: 0.4519 - val_accuracy: 0.7789
Epoch 111/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.4440 - accuracy: 0.7792
Epoch 111: val_loss did not improve from 0.43509
852/852 [==============================] - 2s 2ms/step - loss: 0.4448 - accuracy: 0.7778 - val_loss: 0.4456 - val_accuracy: 0.7797
Epoch 112/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.4465 - accuracy: 0.7778
Epoch 112: val_loss did not improve from 0.43509
852/852 [==============================] - 2s 2ms/step - loss: 0.4451 - accuracy: 0.7789 - val_loss: 0.4358 - val_accuracy: 0.7803
Epoch 113/150
810/852 [===========================&gt;..] - ETA: 0s - loss: 0.4470 - accuracy: 0.7774
Epoch 113: val_loss did not improve from 0.43509
852/852 [==============================] - 2s 2ms/step - loss: 0.4455 - accuracy: 0.7792 - val_loss: 0.4428 - val_accuracy: 0.7815
Epoch 114/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.4471 - accuracy: 0.7788
Epoch 114: val_loss improved from 0.43509 to 0.43322, saving model to sixty_three_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4470 - accuracy: 0.7787 - val_loss: 0.4332 - val_accuracy: 0.7835
Epoch 115/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.4452 - accuracy: 0.7800
Epoch 115: val_loss did not improve from 0.43322
852/852 [==============================] - 2s 2ms/step - loss: 0.4449 - accuracy: 0.7808 - val_loss: 0.4494 - val_accuracy: 0.7778
Epoch 116/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.4487 - accuracy: 0.7778
Epoch 116: val_loss did not improve from 0.43322
852/852 [==============================] - 2s 2ms/step - loss: 0.4485 - accuracy: 0.7782 - val_loss: 0.4358 - val_accuracy: 0.7856
Epoch 117/150
818/852 [===========================&gt;..] - ETA: 0s - loss: 0.4456 - accuracy: 0.7804
Epoch 117: val_loss improved from 0.43322 to 0.42975, saving model to sixty_three_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4455 - accuracy: 0.7802 - val_loss: 0.4297 - val_accuracy: 0.7899
Epoch 118/150
818/852 [===========================&gt;..] - ETA: 0s - loss: 0.4440 - accuracy: 0.7791
Epoch 118: val_loss did not improve from 0.42975
852/852 [==============================] - 2s 2ms/step - loss: 0.4437 - accuracy: 0.7792 - val_loss: 0.4368 - val_accuracy: 0.7856
Epoch 119/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.4402 - accuracy: 0.7835
Epoch 119: val_loss did not improve from 0.42975
852/852 [==============================] - 2s 2ms/step - loss: 0.4407 - accuracy: 0.7831 - val_loss: 0.4394 - val_accuracy: 0.7804
Epoch 120/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.4488 - accuracy: 0.7782
Epoch 120: val_loss did not improve from 0.42975
852/852 [==============================] - 2s 2ms/step - loss: 0.4485 - accuracy: 0.7787 - val_loss: 0.4402 - val_accuracy: 0.7837
Epoch 121/150
813/852 [===========================&gt;..] - ETA: 0s - loss: 0.4436 - accuracy: 0.7811
Epoch 121: val_loss did not improve from 0.42975
852/852 [==============================] - 2s 2ms/step - loss: 0.4456 - accuracy: 0.7794 - val_loss: 0.4454 - val_accuracy: 0.7806
Epoch 122/150
811/852 [===========================&gt;..] - ETA: 0s - loss: 0.4479 - accuracy: 0.7785
Epoch 122: val_loss did not improve from 0.42975
852/852 [==============================] - 2s 2ms/step - loss: 0.4499 - accuracy: 0.7772 - val_loss: 0.4399 - val_accuracy: 0.7829
Epoch 123/150
818/852 [===========================&gt;..] - ETA: 0s - loss: 0.4462 - accuracy: 0.7778
Epoch 123: val_loss did not improve from 0.42975
852/852 [==============================] - 2s 2ms/step - loss: 0.4464 - accuracy: 0.7782 - val_loss: 0.4325 - val_accuracy: 0.7826
Epoch 124/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.4432 - accuracy: 0.7811
Epoch 124: val_loss improved from 0.42975 to 0.42904, saving model to sixty_three_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4436 - accuracy: 0.7804 - val_loss: 0.4290 - val_accuracy: 0.7865
Epoch 125/150
812/852 [===========================&gt;..] - ETA: 0s - loss: 0.4454 - accuracy: 0.7788
Epoch 125: val_loss did not improve from 0.42904
852/852 [==============================] - 2s 2ms/step - loss: 0.4458 - accuracy: 0.7795 - val_loss: 0.4383 - val_accuracy: 0.7822
Epoch 126/150
806/852 [===========================&gt;..] - ETA: 0s - loss: 0.4381 - accuracy: 0.7823
Epoch 126: val_loss did not improve from 0.42904
852/852 [==============================] - 2s 2ms/step - loss: 0.4409 - accuracy: 0.7806 - val_loss: 0.4407 - val_accuracy: 0.7828
Epoch 127/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.4437 - accuracy: 0.7794
Epoch 127: val_loss improved from 0.42904 to 0.42760, saving model to sixty_three_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4431 - accuracy: 0.7804 - val_loss: 0.4276 - val_accuracy: 0.7853
Epoch 128/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.4413 - accuracy: 0.7788
Epoch 128: val_loss did not improve from 0.42760
852/852 [==============================] - 2s 2ms/step - loss: 0.4426 - accuracy: 0.7781 - val_loss: 0.4333 - val_accuracy: 0.7848
Epoch 129/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.4426 - accuracy: 0.7793
Epoch 129: val_loss did not improve from 0.42760
852/852 [==============================] - 2s 2ms/step - loss: 0.4426 - accuracy: 0.7796 - val_loss: 0.4388 - val_accuracy: 0.7816
Epoch 130/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.4443 - accuracy: 0.7815
Epoch 130: val_loss did not improve from 0.42760
852/852 [==============================] - 2s 2ms/step - loss: 0.4444 - accuracy: 0.7814 - val_loss: 0.4335 - val_accuracy: 0.7866
Epoch 131/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.4423 - accuracy: 0.7794
Epoch 131: val_loss did not improve from 0.42760
852/852 [==============================] - 2s 2ms/step - loss: 0.4424 - accuracy: 0.7794 - val_loss: 0.4393 - val_accuracy: 0.7799
Epoch 132/150
812/852 [===========================&gt;..] - ETA: 0s - loss: 0.4452 - accuracy: 0.7764
Epoch 132: val_loss did not improve from 0.42760
852/852 [==============================] - 2s 2ms/step - loss: 0.4427 - accuracy: 0.7783 - val_loss: 0.4374 - val_accuracy: 0.7819
Epoch 133/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.4459 - accuracy: 0.7786
Epoch 133: val_loss improved from 0.42760 to 0.42618, saving model to sixty_three_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4458 - accuracy: 0.7787 - val_loss: 0.4262 - val_accuracy: 0.7873
Epoch 134/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.4420 - accuracy: 0.7803
Epoch 134: val_loss did not improve from 0.42618
852/852 [==============================] - 2s 2ms/step - loss: 0.4432 - accuracy: 0.7791 - val_loss: 0.4365 - val_accuracy: 0.7832
Epoch 135/150
810/852 [===========================&gt;..] - ETA: 0s - loss: 0.4422 - accuracy: 0.7806
Epoch 135: val_loss did not improve from 0.42618
852/852 [==============================] - 2s 2ms/step - loss: 0.4441 - accuracy: 0.7801 - val_loss: 0.4407 - val_accuracy: 0.7866
Epoch 136/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.4450 - accuracy: 0.7824
Epoch 136: val_loss did not improve from 0.42618
852/852 [==============================] - 2s 2ms/step - loss: 0.4452 - accuracy: 0.7825 - val_loss: 0.4351 - val_accuracy: 0.7889
Epoch 137/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.4434 - accuracy: 0.7808
Epoch 137: val_loss improved from 0.42618 to 0.42561, saving model to sixty_three_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4424 - accuracy: 0.7811 - val_loss: 0.4256 - val_accuracy: 0.7875
Epoch 138/150
813/852 [===========================&gt;..] - ETA: 0s - loss: 0.4412 - accuracy: 0.7812
Epoch 138: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.4402 - accuracy: 0.7816 - val_loss: 0.4284 - val_accuracy: 0.7868
Epoch 139/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.4461 - accuracy: 0.7825
Epoch 139: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.4462 - accuracy: 0.7825 - val_loss: 0.4629 - val_accuracy: 0.7762
Epoch 140/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.4511 - accuracy: 0.7799
Epoch 140: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.4496 - accuracy: 0.7806 - val_loss: 0.4328 - val_accuracy: 0.7873
Epoch 141/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.4424 - accuracy: 0.7837
Epoch 141: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.4420 - accuracy: 0.7832 - val_loss: 0.4356 - val_accuracy: 0.7864
Epoch 142/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.4427 - accuracy: 0.7854
Epoch 142: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.4425 - accuracy: 0.7853 - val_loss: 0.4272 - val_accuracy: 0.7895
Epoch 143/150
815/852 [===========================&gt;..] - ETA: 0s - loss: 0.4399 - accuracy: 0.7823
Epoch 143: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.4413 - accuracy: 0.7814 - val_loss: 0.4313 - val_accuracy: 0.7877
Epoch 144/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.4384 - accuracy: 0.7848
Epoch 144: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.4396 - accuracy: 0.7837 - val_loss: 0.4278 - val_accuracy: 0.7886
Epoch 145/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.4445 - accuracy: 0.7786
Epoch 145: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.4440 - accuracy: 0.7792 - val_loss: 0.4291 - val_accuracy: 0.7859
Epoch 146/150
818/852 [===========================&gt;..] - ETA: 0s - loss: 0.4408 - accuracy: 0.7824
Epoch 146: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.4414 - accuracy: 0.7822 - val_loss: 0.4374 - val_accuracy: 0.7845
Epoch 147/150
813/852 [===========================&gt;..] - ETA: 0s - loss: 0.4423 - accuracy: 0.7823
Epoch 147: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.4412 - accuracy: 0.7825 - val_loss: 0.4306 - val_accuracy: 0.7869
Epoch 148/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.4453 - accuracy: 0.7786
Epoch 148: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.4444 - accuracy: 0.7791 - val_loss: 0.4377 - val_accuracy: 0.7831
Epoch 149/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.4426 - accuracy: 0.7818
Epoch 149: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.4418 - accuracy: 0.7819 - val_loss: 0.4403 - val_accuracy: 0.7808
Epoch 150/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.4429 - accuracy: 0.7810
Epoch 150: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.4431 - accuracy: 0.7808 - val_loss: 0.4354 - val_accuracy: 0.7882
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=9f9fa647-8afc-4e11-b8eb-64d9e2b09058">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [85]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">sixty_three_neuron</span><span class="o">.</span><span class="n">load_weights</span><span class="p">(</span><span class="s1">'sixty_three_model.hdf5'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=00867f11-6e89-4ea3-a555-651ff5d135e0">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [86]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Other Metrics</span>
<span class="n">sixty_three_neuron_preds</span> <span class="o">=</span> <span class="n">sixty_three_neuron</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">sixty_three_neuron_preds</span><span class="o">.</span><span class="n">round</span><span class="p">())</span>
<span class="n">precision</span> <span class="o">=</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">sixty_three_neuron_preds</span><span class="o">.</span><span class="n">round</span><span class="p">())</span>
<span class="n">recall</span> <span class="o">=</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">sixty_three_neuron_preds</span><span class="o">.</span><span class="n">round</span><span class="p">())</span>
<span class="n">f1score</span> <span class="o">=</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">sixty_three_neuron_preds</span><span class="o">.</span><span class="n">round</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Accuracy: </span><span class="si">%.2f</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">accuracy</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Precision: </span><span class="si">%.2f</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">precision</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Recall: </span><span class="si">%.2f</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">recall</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"F1-score: </span><span class="si">%.2f</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">f1score</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>267/267 [==============================] - 0s 991us/step
Accuracy: 0.79
Precision: 0.84
Recall: 0.71
F1-score: 0.77
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=f08ae58f-0843-4779-a826-4d1e10e6e237">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="2.7-Build-a-Model-with-125-Neurons-in-6-Layers">2.7 Build a Model with 125 Neurons in 6 Layers<a class="anchor-link" href="#2.7-Build-a-Model-with-125-Neurons-in-6-Layers">¶</a></h4>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=8326a1b7-6f48-4e83-91db-ffe5abdc8797">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [73]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># constructing a 125 neuron model</span>
<span class="n">one_twentyfive_neuron</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">one_twentyfive_neuron</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">input_dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">))</span>
<span class="n">one_twentyfive_neuron</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">))</span>
<span class="n">one_twentyfive_neuron</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">))</span>
<span class="n">one_twentyfive_neuron</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">))</span>
<span class="n">one_twentyfive_neuron</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">))</span>
<span class="n">one_twentyfive_neuron</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'sigmoid'</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">one_twentyfive_neuron</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>

<span class="c1">#Check for cycles in the Sequential Model</span>
<span class="n">one_twentyfive_neuron</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">'binary_crossentropy'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s1">'adam'</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'accuracy'</span><span class="p">])</span>
<span class="n">callback_a</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">filepath</span> <span class="o">=</span> <span class="s1">'one_twentyfive_model.hdf5'</span><span class="p">,</span> <span class="n">monitor</span><span class="o">=</span><span class="s1">'val_loss'</span><span class="p">,</span> <span class="n">save_best_only</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">save_weights_only</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">verbose</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">callback_b</span> <span class="o">=</span> <span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s1">'val_loss'</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">'min'</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">history_125</span> <span class="o">=</span> <span class="n">one_twentyfive_neuron</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">callback_a</span><span class="p">,</span> <span class="n">callback_b</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Model: "sequential_17"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense_36 (Dense)            (None, 64)                1728      
                                                                 
 dense_37 (Dense)            (None, 32)                2080      
                                                                 
 dense_38 (Dense)            (None, 16)                528       
                                                                 
 dense_39 (Dense)            (None, 8)                 136       
                                                                 
 dense_40 (Dense)            (None, 4)                 36        
                                                                 
 dense_41 (Dense)            (None, 1)                 5         
                                                                 
=================================================================
Total params: 4513 (17.63 KB)
Trainable params: 4513 (17.63 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
None
Epoch 1/150
809/852 [===========================&gt;..] - ETA: 0s - loss: 0.6035 - accuracy: 0.6979
Epoch 1: val_loss improved from inf to 0.57345, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 3s 2ms/step - loss: 0.6025 - accuracy: 0.7000 - val_loss: 0.5735 - val_accuracy: 0.7249
Epoch 2/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.5670 - accuracy: 0.7226
Epoch 2: val_loss improved from 0.57345 to 0.56794, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5685 - accuracy: 0.7199 - val_loss: 0.5679 - val_accuracy: 0.7210
Epoch 3/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.5593 - accuracy: 0.7222
Epoch 3: val_loss improved from 0.56794 to 0.55304, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5606 - accuracy: 0.7205 - val_loss: 0.5530 - val_accuracy: 0.7305
Epoch 4/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.5558 - accuracy: 0.7248
Epoch 4: val_loss did not improve from 0.55304
852/852 [==============================] - 2s 2ms/step - loss: 0.5557 - accuracy: 0.7250 - val_loss: 0.5552 - val_accuracy: 0.7329
Epoch 5/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5527 - accuracy: 0.7282
Epoch 5: val_loss improved from 0.55304 to 0.54930, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5528 - accuracy: 0.7282 - val_loss: 0.5493 - val_accuracy: 0.7252
Epoch 6/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.5498 - accuracy: 0.7291
Epoch 6: val_loss improved from 0.54930 to 0.54489, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5504 - accuracy: 0.7278 - val_loss: 0.5449 - val_accuracy: 0.7320
Epoch 7/150
811/852 [===========================&gt;..] - ETA: 0s - loss: 0.5482 - accuracy: 0.7311
Epoch 7: val_loss improved from 0.54489 to 0.54294, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5487 - accuracy: 0.7307 - val_loss: 0.5429 - val_accuracy: 0.7359
Epoch 8/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.5470 - accuracy: 0.7305
Epoch 8: val_loss did not improve from 0.54294
852/852 [==============================] - 2s 2ms/step - loss: 0.5474 - accuracy: 0.7302 - val_loss: 0.5500 - val_accuracy: 0.7352
Epoch 9/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.5482 - accuracy: 0.7322
Epoch 9: val_loss improved from 0.54294 to 0.54062, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5477 - accuracy: 0.7326 - val_loss: 0.5406 - val_accuracy: 0.7370
Epoch 10/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.5458 - accuracy: 0.7334
Epoch 10: val_loss improved from 0.54062 to 0.53839, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5456 - accuracy: 0.7336 - val_loss: 0.5384 - val_accuracy: 0.7380
Epoch 11/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.5440 - accuracy: 0.7337
Epoch 11: val_loss did not improve from 0.53839
852/852 [==============================] - 2s 2ms/step - loss: 0.5436 - accuracy: 0.7345 - val_loss: 0.5399 - val_accuracy: 0.7363
Epoch 12/150
812/852 [===========================&gt;..] - ETA: 0s - loss: 0.5421 - accuracy: 0.7360
Epoch 12: val_loss improved from 0.53839 to 0.53450, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5430 - accuracy: 0.7358 - val_loss: 0.5345 - val_accuracy: 0.7385
Epoch 13/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.5411 - accuracy: 0.7345
Epoch 13: val_loss improved from 0.53450 to 0.53311, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5417 - accuracy: 0.7334 - val_loss: 0.5331 - val_accuracy: 0.7400
Epoch 14/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.5389 - accuracy: 0.7338
Epoch 14: val_loss did not improve from 0.53311
852/852 [==============================] - 2s 2ms/step - loss: 0.5388 - accuracy: 0.7340 - val_loss: 0.5343 - val_accuracy: 0.7425
Epoch 15/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5366 - accuracy: 0.7408
Epoch 15: val_loss improved from 0.53311 to 0.53048, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5366 - accuracy: 0.7407 - val_loss: 0.5305 - val_accuracy: 0.7437
Epoch 16/150
811/852 [===========================&gt;..] - ETA: 0s - loss: 0.5355 - accuracy: 0.7397
Epoch 16: val_loss improved from 0.53048 to 0.52825, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5366 - accuracy: 0.7391 - val_loss: 0.5283 - val_accuracy: 0.7447
Epoch 17/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.5348 - accuracy: 0.7391
Epoch 17: val_loss improved from 0.52825 to 0.52700, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5348 - accuracy: 0.7390 - val_loss: 0.5270 - val_accuracy: 0.7461
Epoch 18/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.5317 - accuracy: 0.7425
Epoch 18: val_loss improved from 0.52700 to 0.52595, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5317 - accuracy: 0.7424 - val_loss: 0.5259 - val_accuracy: 0.7450
Epoch 19/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.5313 - accuracy: 0.7413
Epoch 19: val_loss improved from 0.52595 to 0.52490, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5315 - accuracy: 0.7412 - val_loss: 0.5249 - val_accuracy: 0.7438
Epoch 20/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.5296 - accuracy: 0.7434
Epoch 20: val_loss improved from 0.52490 to 0.52065, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5286 - accuracy: 0.7444 - val_loss: 0.5207 - val_accuracy: 0.7475
Epoch 21/150
852/852 [==============================] - ETA: 0s - loss: 0.5284 - accuracy: 0.7427
Epoch 21: val_loss did not improve from 0.52065
852/852 [==============================] - 2s 2ms/step - loss: 0.5284 - accuracy: 0.7427 - val_loss: 0.5209 - val_accuracy: 0.7491
Epoch 22/150
809/852 [===========================&gt;..] - ETA: 0s - loss: 0.5271 - accuracy: 0.7465
Epoch 22: val_loss improved from 0.52065 to 0.51627, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5253 - accuracy: 0.7480 - val_loss: 0.5163 - val_accuracy: 0.7519
Epoch 23/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.5248 - accuracy: 0.7424
Epoch 23: val_loss did not improve from 0.51627
852/852 [==============================] - 2s 2ms/step - loss: 0.5249 - accuracy: 0.7424 - val_loss: 0.5182 - val_accuracy: 0.7526
Epoch 24/150
810/852 [===========================&gt;..] - ETA: 0s - loss: 0.5219 - accuracy: 0.7472
Epoch 24: val_loss improved from 0.51627 to 0.51380, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5230 - accuracy: 0.7460 - val_loss: 0.5138 - val_accuracy: 0.7559
Epoch 25/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.5218 - accuracy: 0.7479
Epoch 25: val_loss did not improve from 0.51380
852/852 [==============================] - 2s 2ms/step - loss: 0.5213 - accuracy: 0.7477 - val_loss: 0.5191 - val_accuracy: 0.7495
Epoch 26/150
819/852 [===========================&gt;..] - ETA: 0s - loss: 0.5179 - accuracy: 0.7521
Epoch 26: val_loss improved from 0.51380 to 0.51013, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5199 - accuracy: 0.7508 - val_loss: 0.5101 - val_accuracy: 0.7567
Epoch 27/150
809/852 [===========================&gt;..] - ETA: 0s - loss: 0.5181 - accuracy: 0.7486
Epoch 27: val_loss improved from 0.51013 to 0.50855, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5191 - accuracy: 0.7481 - val_loss: 0.5085 - val_accuracy: 0.7588
Epoch 28/150
818/852 [===========================&gt;..] - ETA: 0s - loss: 0.5174 - accuracy: 0.7529
Epoch 28: val_loss did not improve from 0.50855
852/852 [==============================] - 2s 2ms/step - loss: 0.5177 - accuracy: 0.7535 - val_loss: 0.5093 - val_accuracy: 0.7555
Epoch 29/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.5162 - accuracy: 0.7519
Epoch 29: val_loss improved from 0.50855 to 0.50488, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5169 - accuracy: 0.7508 - val_loss: 0.5049 - val_accuracy: 0.7594
Epoch 30/150
811/852 [===========================&gt;..] - ETA: 0s - loss: 0.5159 - accuracy: 0.7510
Epoch 30: val_loss did not improve from 0.50488
852/852 [==============================] - 2s 2ms/step - loss: 0.5148 - accuracy: 0.7519 - val_loss: 0.5076 - val_accuracy: 0.7563
Epoch 31/150
812/852 [===========================&gt;..] - ETA: 0s - loss: 0.5144 - accuracy: 0.7512
Epoch 31: val_loss improved from 0.50488 to 0.50181, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5139 - accuracy: 0.7516 - val_loss: 0.5018 - val_accuracy: 0.7595
Epoch 32/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.5139 - accuracy: 0.7514
Epoch 32: val_loss improved from 0.50181 to 0.49925, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5137 - accuracy: 0.7514 - val_loss: 0.4993 - val_accuracy: 0.7662
Epoch 33/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.5109 - accuracy: 0.7536
Epoch 33: val_loss did not improve from 0.49925
852/852 [==============================] - 2s 2ms/step - loss: 0.5113 - accuracy: 0.7532 - val_loss: 0.5015 - val_accuracy: 0.7622
Epoch 34/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.5095 - accuracy: 0.7583
Epoch 34: val_loss did not improve from 0.49925
852/852 [==============================] - 2s 2ms/step - loss: 0.5100 - accuracy: 0.7579 - val_loss: 0.5112 - val_accuracy: 0.7565
Epoch 35/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.5081 - accuracy: 0.7551
Epoch 35: val_loss improved from 0.49925 to 0.49733, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5089 - accuracy: 0.7546 - val_loss: 0.4973 - val_accuracy: 0.7634
Epoch 36/150
809/852 [===========================&gt;..] - ETA: 0s - loss: 0.5065 - accuracy: 0.7556
Epoch 36: val_loss improved from 0.49733 to 0.49591, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5066 - accuracy: 0.7554 - val_loss: 0.4959 - val_accuracy: 0.7643
Epoch 37/150
813/852 [===========================&gt;..] - ETA: 0s - loss: 0.5081 - accuracy: 0.7561
Epoch 37: val_loss did not improve from 0.49591
852/852 [==============================] - 2s 2ms/step - loss: 0.5081 - accuracy: 0.7558 - val_loss: 0.4969 - val_accuracy: 0.7628
Epoch 38/150
815/852 [===========================&gt;..] - ETA: 0s - loss: 0.5067 - accuracy: 0.7553
Epoch 38: val_loss improved from 0.49591 to 0.49422, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5055 - accuracy: 0.7566 - val_loss: 0.4942 - val_accuracy: 0.7636
Epoch 39/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.5022 - accuracy: 0.7610
Epoch 39: val_loss did not improve from 0.49422
852/852 [==============================] - 2s 2ms/step - loss: 0.5028 - accuracy: 0.7612 - val_loss: 0.4981 - val_accuracy: 0.7626
Epoch 40/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.5031 - accuracy: 0.7555
Epoch 40: val_loss improved from 0.49422 to 0.49009, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5026 - accuracy: 0.7560 - val_loss: 0.4901 - val_accuracy: 0.7694
Epoch 41/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.5016 - accuracy: 0.7577
Epoch 41: val_loss did not improve from 0.49009
852/852 [==============================] - 2s 2ms/step - loss: 0.5016 - accuracy: 0.7578 - val_loss: 0.4928 - val_accuracy: 0.7641
Epoch 42/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.5028 - accuracy: 0.7576
Epoch 42: val_loss did not improve from 0.49009
852/852 [==============================] - 2s 2ms/step - loss: 0.5019 - accuracy: 0.7588 - val_loss: 0.4908 - val_accuracy: 0.7670
Epoch 43/150
852/852 [==============================] - ETA: 0s - loss: 0.4983 - accuracy: 0.7630
Epoch 43: val_loss improved from 0.49009 to 0.48666, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4983 - accuracy: 0.7630 - val_loss: 0.4867 - val_accuracy: 0.7707
Epoch 44/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.4989 - accuracy: 0.7604
Epoch 44: val_loss improved from 0.48666 to 0.48597, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4982 - accuracy: 0.7610 - val_loss: 0.4860 - val_accuracy: 0.7702
Epoch 45/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.4958 - accuracy: 0.7641
Epoch 45: val_loss did not improve from 0.48597
852/852 [==============================] - 2s 2ms/step - loss: 0.4963 - accuracy: 0.7634 - val_loss: 0.4869 - val_accuracy: 0.7669
Epoch 46/150
809/852 [===========================&gt;..] - ETA: 0s - loss: 0.4952 - accuracy: 0.7635
Epoch 46: val_loss did not improve from 0.48597
852/852 [==============================] - 2s 2ms/step - loss: 0.4952 - accuracy: 0.7636 - val_loss: 0.4883 - val_accuracy: 0.7669
Epoch 47/150
810/852 [===========================&gt;..] - ETA: 0s - loss: 0.4927 - accuracy: 0.7658
Epoch 47: val_loss did not improve from 0.48597
852/852 [==============================] - 2s 2ms/step - loss: 0.4941 - accuracy: 0.7650 - val_loss: 0.4894 - val_accuracy: 0.7666
Epoch 48/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.4947 - accuracy: 0.7629
Epoch 48: val_loss improved from 0.48597 to 0.48018, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4945 - accuracy: 0.7629 - val_loss: 0.4802 - val_accuracy: 0.7755
Epoch 49/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.4921 - accuracy: 0.7650
Epoch 49: val_loss improved from 0.48018 to 0.47799, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4929 - accuracy: 0.7644 - val_loss: 0.4780 - val_accuracy: 0.7758
Epoch 50/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.4910 - accuracy: 0.7684
Epoch 50: val_loss did not improve from 0.47799
852/852 [==============================] - 2s 2ms/step - loss: 0.4920 - accuracy: 0.7677 - val_loss: 0.4833 - val_accuracy: 0.7729
Epoch 51/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.4899 - accuracy: 0.7675
Epoch 51: val_loss improved from 0.47799 to 0.47621, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4899 - accuracy: 0.7670 - val_loss: 0.4762 - val_accuracy: 0.7745
Epoch 52/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.4889 - accuracy: 0.7679
Epoch 52: val_loss did not improve from 0.47621
852/852 [==============================] - 2s 2ms/step - loss: 0.4882 - accuracy: 0.7681 - val_loss: 0.4778 - val_accuracy: 0.7734
Epoch 53/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.4857 - accuracy: 0.7691
Epoch 53: val_loss did not improve from 0.47621
852/852 [==============================] - 2s 2ms/step - loss: 0.4861 - accuracy: 0.7688 - val_loss: 0.4839 - val_accuracy: 0.7696
Epoch 54/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.4879 - accuracy: 0.7667
Epoch 54: val_loss improved from 0.47621 to 0.47389, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4879 - accuracy: 0.7668 - val_loss: 0.4739 - val_accuracy: 0.7762
Epoch 55/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.4842 - accuracy: 0.7686
Epoch 55: val_loss improved from 0.47389 to 0.47212, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4848 - accuracy: 0.7681 - val_loss: 0.4721 - val_accuracy: 0.7779
Epoch 56/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.4864 - accuracy: 0.7681
Epoch 56: val_loss did not improve from 0.47212
852/852 [==============================] - 2s 2ms/step - loss: 0.4861 - accuracy: 0.7682 - val_loss: 0.4758 - val_accuracy: 0.7749
Epoch 57/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.4822 - accuracy: 0.7703
Epoch 57: val_loss did not improve from 0.47212
852/852 [==============================] - 2s 2ms/step - loss: 0.4830 - accuracy: 0.7696 - val_loss: 0.4764 - val_accuracy: 0.7716
Epoch 58/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.4825 - accuracy: 0.7719
Epoch 58: val_loss improved from 0.47212 to 0.46883, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4824 - accuracy: 0.7717 - val_loss: 0.4688 - val_accuracy: 0.7810
Epoch 59/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.4824 - accuracy: 0.7751
Epoch 59: val_loss improved from 0.46883 to 0.46830, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4839 - accuracy: 0.7736 - val_loss: 0.4683 - val_accuracy: 0.7775
Epoch 60/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.4800 - accuracy: 0.7704
Epoch 60: val_loss did not improve from 0.46830
852/852 [==============================] - 2s 2ms/step - loss: 0.4801 - accuracy: 0.7704 - val_loss: 0.4691 - val_accuracy: 0.7798
Epoch 61/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.4813 - accuracy: 0.7710
Epoch 61: val_loss did not improve from 0.46830
852/852 [==============================] - 2s 2ms/step - loss: 0.4811 - accuracy: 0.7710 - val_loss: 0.4700 - val_accuracy: 0.7826
Epoch 62/150
852/852 [==============================] - ETA: 0s - loss: 0.4776 - accuracy: 0.7731
Epoch 62: val_loss improved from 0.46830 to 0.46549, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4776 - accuracy: 0.7731 - val_loss: 0.4655 - val_accuracy: 0.7809
Epoch 63/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.4769 - accuracy: 0.7727
Epoch 63: val_loss did not improve from 0.46549
852/852 [==============================] - 2s 2ms/step - loss: 0.4771 - accuracy: 0.7722 - val_loss: 0.4750 - val_accuracy: 0.7757
Epoch 64/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.4756 - accuracy: 0.7728
Epoch 64: val_loss improved from 0.46549 to 0.45887, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.4769 - accuracy: 0.7717 - val_loss: 0.4589 - val_accuracy: 0.7831
Epoch 65/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.4765 - accuracy: 0.7748
Epoch 65: val_loss did not improve from 0.45887
852/852 [==============================] - 2s 2ms/step - loss: 0.4760 - accuracy: 0.7754 - val_loss: 0.4632 - val_accuracy: 0.7810
Epoch 66/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.4764 - accuracy: 0.7718
Epoch 66: val_loss did not improve from 0.45887
852/852 [==============================] - 2s 2ms/step - loss: 0.4762 - accuracy: 0.7720 - val_loss: 0.4591 - val_accuracy: 0.7871
Epoch 67/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.4746 - accuracy: 0.7740
Epoch 67: val_loss did not improve from 0.45887
852/852 [==============================] - 2s 2ms/step - loss: 0.4748 - accuracy: 0.7737 - val_loss: 0.4693 - val_accuracy: 0.7775
Epoch 68/150
814/852 [===========================&gt;..] - ETA: 0s - loss: 0.4690 - accuracy: 0.7771
Epoch 68: val_loss did not improve from 0.45887
852/852 [==============================] - 2s 2ms/step - loss: 0.4686 - accuracy: 0.7771 - val_loss: 0.4689 - val_accuracy: 0.7797
Epoch 69/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.4710 - accuracy: 0.7756
Epoch 69: val_loss improved from 0.45887 to 0.45371, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4713 - accuracy: 0.7750 - val_loss: 0.4537 - val_accuracy: 0.7895
Epoch 70/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.4671 - accuracy: 0.7808
Epoch 70: val_loss did not improve from 0.45371
852/852 [==============================] - 2s 2ms/step - loss: 0.4693 - accuracy: 0.7789 - val_loss: 0.4623 - val_accuracy: 0.7790
Epoch 71/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.4692 - accuracy: 0.7764
Epoch 71: val_loss did not improve from 0.45371
852/852 [==============================] - 2s 2ms/step - loss: 0.4693 - accuracy: 0.7762 - val_loss: 0.4594 - val_accuracy: 0.7839
Epoch 72/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.4683 - accuracy: 0.7780
Epoch 72: val_loss did not improve from 0.45371
852/852 [==============================] - 2s 2ms/step - loss: 0.4684 - accuracy: 0.7784 - val_loss: 0.4640 - val_accuracy: 0.7833
Epoch 73/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.4680 - accuracy: 0.7791
Epoch 73: val_loss did not improve from 0.45371
852/852 [==============================] - 2s 2ms/step - loss: 0.4682 - accuracy: 0.7783 - val_loss: 0.4583 - val_accuracy: 0.7832
Epoch 74/150
820/852 [===========================&gt;..] - ETA: 0s - loss: 0.4655 - accuracy: 0.7790
Epoch 74: val_loss did not improve from 0.45371
852/852 [==============================] - 2s 2ms/step - loss: 0.4668 - accuracy: 0.7782 - val_loss: 0.4621 - val_accuracy: 0.7791
Epoch 75/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.4635 - accuracy: 0.7809
Epoch 75: val_loss improved from 0.45371 to 0.45115, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4638 - accuracy: 0.7808 - val_loss: 0.4511 - val_accuracy: 0.7891
Epoch 76/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.4644 - accuracy: 0.7796
Epoch 76: val_loss improved from 0.45115 to 0.45077, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4645 - accuracy: 0.7797 - val_loss: 0.4508 - val_accuracy: 0.7890
Epoch 77/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.4631 - accuracy: 0.7823
Epoch 77: val_loss did not improve from 0.45077
852/852 [==============================] - 2s 2ms/step - loss: 0.4630 - accuracy: 0.7821 - val_loss: 0.4549 - val_accuracy: 0.7825
Epoch 78/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.4627 - accuracy: 0.7803
Epoch 78: val_loss improved from 0.45077 to 0.44887, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4635 - accuracy: 0.7795 - val_loss: 0.4489 - val_accuracy: 0.7891
Epoch 79/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.4628 - accuracy: 0.7801
Epoch 79: val_loss did not improve from 0.44887
852/852 [==============================] - 2s 2ms/step - loss: 0.4628 - accuracy: 0.7803 - val_loss: 0.4576 - val_accuracy: 0.7855
Epoch 80/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.4607 - accuracy: 0.7804
Epoch 80: val_loss improved from 0.44887 to 0.44400, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4610 - accuracy: 0.7801 - val_loss: 0.4440 - val_accuracy: 0.7958
Epoch 81/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.4606 - accuracy: 0.7842
Epoch 81: val_loss did not improve from 0.44400
852/852 [==============================] - 2s 2ms/step - loss: 0.4618 - accuracy: 0.7830 - val_loss: 0.4460 - val_accuracy: 0.7925
Epoch 82/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.4591 - accuracy: 0.7820
Epoch 82: val_loss did not improve from 0.44400
852/852 [==============================] - 2s 2ms/step - loss: 0.4589 - accuracy: 0.7821 - val_loss: 0.4471 - val_accuracy: 0.7905
Epoch 83/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.4587 - accuracy: 0.7826
Epoch 83: val_loss improved from 0.44400 to 0.44039, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4588 - accuracy: 0.7825 - val_loss: 0.4404 - val_accuracy: 0.7964
Epoch 84/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.4564 - accuracy: 0.7822
Epoch 84: val_loss did not improve from 0.44039
852/852 [==============================] - 2s 2ms/step - loss: 0.4586 - accuracy: 0.7810 - val_loss: 0.4423 - val_accuracy: 0.7940
Epoch 85/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.4555 - accuracy: 0.7854
Epoch 85: val_loss did not improve from 0.44039
852/852 [==============================] - 2s 2ms/step - loss: 0.4556 - accuracy: 0.7853 - val_loss: 0.4464 - val_accuracy: 0.7922
Epoch 86/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.4541 - accuracy: 0.7834
Epoch 86: val_loss did not improve from 0.44039
852/852 [==============================] - 2s 2ms/step - loss: 0.4541 - accuracy: 0.7832 - val_loss: 0.4429 - val_accuracy: 0.7915
Epoch 87/150
813/852 [===========================&gt;..] - ETA: 0s - loss: 0.4519 - accuracy: 0.7861
Epoch 87: val_loss improved from 0.44039 to 0.43929, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4521 - accuracy: 0.7864 - val_loss: 0.4393 - val_accuracy: 0.7929
Epoch 88/150
809/852 [===========================&gt;..] - ETA: 0s - loss: 0.4501 - accuracy: 0.7853
Epoch 88: val_loss improved from 0.43929 to 0.43464, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4515 - accuracy: 0.7853 - val_loss: 0.4346 - val_accuracy: 0.7974
Epoch 89/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.4528 - accuracy: 0.7854
Epoch 89: val_loss did not improve from 0.43464
852/852 [==============================] - 2s 2ms/step - loss: 0.4536 - accuracy: 0.7848 - val_loss: 0.4449 - val_accuracy: 0.7916
Epoch 90/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.4527 - accuracy: 0.7861
Epoch 90: val_loss did not improve from 0.43464
852/852 [==============================] - 2s 2ms/step - loss: 0.4500 - accuracy: 0.7885 - val_loss: 0.4425 - val_accuracy: 0.7927
Epoch 91/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.4480 - accuracy: 0.7899
Epoch 91: val_loss improved from 0.43464 to 0.43173, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4482 - accuracy: 0.7899 - val_loss: 0.4317 - val_accuracy: 0.8013
Epoch 92/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.4484 - accuracy: 0.7893
Epoch 92: val_loss did not improve from 0.43173
852/852 [==============================] - 2s 2ms/step - loss: 0.4497 - accuracy: 0.7882 - val_loss: 0.4409 - val_accuracy: 0.7923
Epoch 93/150
814/852 [===========================&gt;..] - ETA: 0s - loss: 0.4476 - accuracy: 0.7928
Epoch 93: val_loss did not improve from 0.43173
852/852 [==============================] - 2s 2ms/step - loss: 0.4489 - accuracy: 0.7916 - val_loss: 0.4398 - val_accuracy: 0.7944
Epoch 94/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.4492 - accuracy: 0.7907
Epoch 94: val_loss did not improve from 0.43173
852/852 [==============================] - 2s 2ms/step - loss: 0.4491 - accuracy: 0.7906 - val_loss: 0.4393 - val_accuracy: 0.7923
Epoch 95/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.4508 - accuracy: 0.7874
Epoch 95: val_loss did not improve from 0.43173
852/852 [==============================] - 2s 2ms/step - loss: 0.4503 - accuracy: 0.7876 - val_loss: 0.4440 - val_accuracy: 0.7932
Epoch 96/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.4472 - accuracy: 0.7874
Epoch 96: val_loss improved from 0.43173 to 0.42899, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4473 - accuracy: 0.7873 - val_loss: 0.4290 - val_accuracy: 0.8026
Epoch 97/150
811/852 [===========================&gt;..] - ETA: 0s - loss: 0.4435 - accuracy: 0.7938
Epoch 97: val_loss did not improve from 0.42899
852/852 [==============================] - 2s 2ms/step - loss: 0.4441 - accuracy: 0.7932 - val_loss: 0.4345 - val_accuracy: 0.7970
Epoch 98/150
814/852 [===========================&gt;..] - ETA: 0s - loss: 0.4450 - accuracy: 0.7903
Epoch 98: val_loss did not improve from 0.42899
852/852 [==============================] - 2s 2ms/step - loss: 0.4451 - accuracy: 0.7897 - val_loss: 0.4330 - val_accuracy: 0.7996
Epoch 99/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.4464 - accuracy: 0.7871
Epoch 99: val_loss did not improve from 0.42899
852/852 [==============================] - 2s 2ms/step - loss: 0.4455 - accuracy: 0.7878 - val_loss: 0.4387 - val_accuracy: 0.7944
Epoch 100/150
852/852 [==============================] - ETA: 0s - loss: 0.4424 - accuracy: 0.7909
Epoch 100: val_loss did not improve from 0.42899
852/852 [==============================] - 2s 2ms/step - loss: 0.4424 - accuracy: 0.7909 - val_loss: 0.4308 - val_accuracy: 0.8035
Epoch 101/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.4441 - accuracy: 0.7901
Epoch 101: val_loss did not improve from 0.42899
852/852 [==============================] - 2s 2ms/step - loss: 0.4441 - accuracy: 0.7900 - val_loss: 0.4298 - val_accuracy: 0.8006
Epoch 102/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.4442 - accuracy: 0.7937
Epoch 102: val_loss did not improve from 0.42899
852/852 [==============================] - 2s 2ms/step - loss: 0.4445 - accuracy: 0.7937 - val_loss: 0.4370 - val_accuracy: 0.7991
Epoch 103/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.4419 - accuracy: 0.7929
Epoch 103: val_loss did not improve from 0.42899
852/852 [==============================] - 2s 2ms/step - loss: 0.4422 - accuracy: 0.7927 - val_loss: 0.4343 - val_accuracy: 0.7943
Epoch 104/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.4441 - accuracy: 0.7884
Epoch 104: val_loss improved from 0.42899 to 0.42610, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4435 - accuracy: 0.7888 - val_loss: 0.4261 - val_accuracy: 0.8012
Epoch 105/150
818/852 [===========================&gt;..] - ETA: 0s - loss: 0.4427 - accuracy: 0.7927
Epoch 105: val_loss improved from 0.42610 to 0.42509, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4441 - accuracy: 0.7915 - val_loss: 0.4251 - val_accuracy: 0.8054
Epoch 106/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.4390 - accuracy: 0.7929
Epoch 106: val_loss improved from 0.42509 to 0.42105, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4388 - accuracy: 0.7930 - val_loss: 0.4210 - val_accuracy: 0.8065
Epoch 107/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.4391 - accuracy: 0.7923
Epoch 107: val_loss did not improve from 0.42105
852/852 [==============================] - 2s 2ms/step - loss: 0.4393 - accuracy: 0.7930 - val_loss: 0.4300 - val_accuracy: 0.7999
Epoch 108/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.4347 - accuracy: 0.7945
Epoch 108: val_loss did not improve from 0.42105
852/852 [==============================] - 2s 2ms/step - loss: 0.4356 - accuracy: 0.7943 - val_loss: 0.4300 - val_accuracy: 0.8001
Epoch 109/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.4431 - accuracy: 0.7946
Epoch 109: val_loss improved from 0.42105 to 0.41899, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4430 - accuracy: 0.7946 - val_loss: 0.4190 - val_accuracy: 0.8053
Epoch 110/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.4357 - accuracy: 0.7960
Epoch 110: val_loss did not improve from 0.41899
852/852 [==============================] - 2s 2ms/step - loss: 0.4356 - accuracy: 0.7964 - val_loss: 0.4200 - val_accuracy: 0.8054
Epoch 111/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.4369 - accuracy: 0.7931
Epoch 111: val_loss did not improve from 0.41899
852/852 [==============================] - 2s 2ms/step - loss: 0.4375 - accuracy: 0.7927 - val_loss: 0.4239 - val_accuracy: 0.8001
Epoch 112/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.4380 - accuracy: 0.7934
Epoch 112: val_loss did not improve from 0.41899
852/852 [==============================] - 2s 2ms/step - loss: 0.4382 - accuracy: 0.7930 - val_loss: 0.4202 - val_accuracy: 0.8087
Epoch 113/150
815/852 [===========================&gt;..] - ETA: 0s - loss: 0.4383 - accuracy: 0.7935
Epoch 113: val_loss did not improve from 0.41899
852/852 [==============================] - 2s 2ms/step - loss: 0.4359 - accuracy: 0.7946 - val_loss: 0.4238 - val_accuracy: 0.8008
Epoch 114/150
852/852 [==============================] - ETA: 0s - loss: 0.4296 - accuracy: 0.7969
Epoch 114: val_loss improved from 0.41899 to 0.41243, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4296 - accuracy: 0.7969 - val_loss: 0.4124 - val_accuracy: 0.8088
Epoch 115/150
852/852 [==============================] - ETA: 0s - loss: 0.4303 - accuracy: 0.7986
Epoch 115: val_loss did not improve from 0.41243
852/852 [==============================] - 2s 2ms/step - loss: 0.4303 - accuracy: 0.7986 - val_loss: 0.4149 - val_accuracy: 0.8097
Epoch 116/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.4331 - accuracy: 0.7964
Epoch 116: val_loss did not improve from 0.41243
852/852 [==============================] - 2s 2ms/step - loss: 0.4333 - accuracy: 0.7964 - val_loss: 0.4197 - val_accuracy: 0.8043
Epoch 117/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.4306 - accuracy: 0.7989
Epoch 117: val_loss did not improve from 0.41243
852/852 [==============================] - 2s 2ms/step - loss: 0.4307 - accuracy: 0.7994 - val_loss: 0.4128 - val_accuracy: 0.8087
Epoch 118/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.4273 - accuracy: 0.8008
Epoch 118: val_loss did not improve from 0.41243
852/852 [==============================] - 2s 2ms/step - loss: 0.4276 - accuracy: 0.8006 - val_loss: 0.4188 - val_accuracy: 0.8033
Epoch 119/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.4279 - accuracy: 0.8012
Epoch 119: val_loss did not improve from 0.41243
852/852 [==============================] - 2s 2ms/step - loss: 0.4277 - accuracy: 0.8013 - val_loss: 0.4151 - val_accuracy: 0.8065
Epoch 120/150
815/852 [===========================&gt;..] - ETA: 0s - loss: 0.4266 - accuracy: 0.8004
Epoch 120: val_loss did not improve from 0.41243
852/852 [==============================] - 2s 2ms/step - loss: 0.4265 - accuracy: 0.8007 - val_loss: 0.4161 - val_accuracy: 0.8059
Epoch 121/150
819/852 [===========================&gt;..] - ETA: 0s - loss: 0.4282 - accuracy: 0.7968
Epoch 121: val_loss improved from 0.41243 to 0.41222, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4286 - accuracy: 0.7965 - val_loss: 0.4122 - val_accuracy: 0.8109
Epoch 122/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.4251 - accuracy: 0.7975
Epoch 122: val_loss improved from 0.41222 to 0.40626, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4250 - accuracy: 0.7978 - val_loss: 0.4063 - val_accuracy: 0.8071
Epoch 123/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.4263 - accuracy: 0.7983
Epoch 123: val_loss did not improve from 0.40626
852/852 [==============================] - 2s 2ms/step - loss: 0.4260 - accuracy: 0.7984 - val_loss: 0.4071 - val_accuracy: 0.8116
Epoch 124/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.4281 - accuracy: 0.7976
Epoch 124: val_loss did not improve from 0.40626
852/852 [==============================] - 2s 2ms/step - loss: 0.4281 - accuracy: 0.7977 - val_loss: 0.4151 - val_accuracy: 0.8041
Epoch 125/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.4252 - accuracy: 0.7992
Epoch 125: val_loss did not improve from 0.40626
852/852 [==============================] - 2s 2ms/step - loss: 0.4257 - accuracy: 0.7987 - val_loss: 0.4071 - val_accuracy: 0.8081
Epoch 126/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.4231 - accuracy: 0.8011
Epoch 126: val_loss did not improve from 0.40626
852/852 [==============================] - 2s 2ms/step - loss: 0.4233 - accuracy: 0.8012 - val_loss: 0.4063 - val_accuracy: 0.8097
Epoch 127/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.4178 - accuracy: 0.8027
Epoch 127: val_loss did not improve from 0.40626
852/852 [==============================] - 2s 2ms/step - loss: 0.4181 - accuracy: 0.8028 - val_loss: 0.4177 - val_accuracy: 0.8025
Epoch 128/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.4244 - accuracy: 0.8001
Epoch 128: val_loss improved from 0.40626 to 0.40562, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4247 - accuracy: 0.8000 - val_loss: 0.4056 - val_accuracy: 0.8094
Epoch 129/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.4206 - accuracy: 0.8011
Epoch 129: val_loss did not improve from 0.40562
852/852 [==============================] - 2s 2ms/step - loss: 0.4209 - accuracy: 0.8010 - val_loss: 0.4147 - val_accuracy: 0.8084
Epoch 130/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.4160 - accuracy: 0.8046
Epoch 130: val_loss did not improve from 0.40562
852/852 [==============================] - 2s 2ms/step - loss: 0.4169 - accuracy: 0.8038 - val_loss: 0.4138 - val_accuracy: 0.8026
Epoch 131/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.4191 - accuracy: 0.8030
Epoch 131: val_loss improved from 0.40562 to 0.40105, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4185 - accuracy: 0.8035 - val_loss: 0.4011 - val_accuracy: 0.8115
Epoch 132/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.4198 - accuracy: 0.8024
Epoch 132: val_loss improved from 0.40105 to 0.39705, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4194 - accuracy: 0.8028 - val_loss: 0.3970 - val_accuracy: 0.8146
Epoch 133/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.4152 - accuracy: 0.8016
Epoch 133: val_loss did not improve from 0.39705
852/852 [==============================] - 2s 2ms/step - loss: 0.4145 - accuracy: 0.8025 - val_loss: 0.4113 - val_accuracy: 0.8058
Epoch 134/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.4135 - accuracy: 0.8048
Epoch 134: val_loss did not improve from 0.39705
852/852 [==============================] - 2s 2ms/step - loss: 0.4133 - accuracy: 0.8050 - val_loss: 0.4349 - val_accuracy: 0.7949
Epoch 135/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.4200 - accuracy: 0.8034
Epoch 135: val_loss did not improve from 0.39705
852/852 [==============================] - 2s 2ms/step - loss: 0.4199 - accuracy: 0.8032 - val_loss: 0.3979 - val_accuracy: 0.8166
Epoch 136/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.4089 - accuracy: 0.8067
Epoch 136: val_loss did not improve from 0.39705
852/852 [==============================] - 2s 2ms/step - loss: 0.4089 - accuracy: 0.8067 - val_loss: 0.4077 - val_accuracy: 0.8078
Epoch 137/150
818/852 [===========================&gt;..] - ETA: 0s - loss: 0.4113 - accuracy: 0.8007
Epoch 137: val_loss did not improve from 0.39705
852/852 [==============================] - 2s 2ms/step - loss: 0.4130 - accuracy: 0.7998 - val_loss: 0.3973 - val_accuracy: 0.8119
Epoch 138/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.4123 - accuracy: 0.8036
Epoch 138: val_loss did not improve from 0.39705
852/852 [==============================] - 2s 2ms/step - loss: 0.4122 - accuracy: 0.8037 - val_loss: 0.3998 - val_accuracy: 0.8119
Epoch 139/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.4145 - accuracy: 0.8018
Epoch 139: val_loss improved from 0.39705 to 0.39546, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4139 - accuracy: 0.8020 - val_loss: 0.3955 - val_accuracy: 0.8134
Epoch 140/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.4113 - accuracy: 0.8083
Epoch 140: val_loss improved from 0.39546 to 0.39437, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4104 - accuracy: 0.8087 - val_loss: 0.3944 - val_accuracy: 0.8174
Epoch 141/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.4082 - accuracy: 0.8090
Epoch 141: val_loss did not improve from 0.39437
852/852 [==============================] - 2s 2ms/step - loss: 0.4100 - accuracy: 0.8080 - val_loss: 0.4057 - val_accuracy: 0.8085
Epoch 142/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.4070 - accuracy: 0.8075
Epoch 142: val_loss did not improve from 0.39437
852/852 [==============================] - 2s 2ms/step - loss: 0.4067 - accuracy: 0.8075 - val_loss: 0.3999 - val_accuracy: 0.8121
Epoch 143/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.4080 - accuracy: 0.8061
Epoch 143: val_loss did not improve from 0.39437
852/852 [==============================] - 2s 2ms/step - loss: 0.4079 - accuracy: 0.8070 - val_loss: 0.4095 - val_accuracy: 0.8054
Epoch 144/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.4074 - accuracy: 0.8081
Epoch 144: val_loss did not improve from 0.39437
852/852 [==============================] - 2s 2ms/step - loss: 0.4079 - accuracy: 0.8080 - val_loss: 0.3968 - val_accuracy: 0.8119
Epoch 145/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.4037 - accuracy: 0.8070
Epoch 145: val_loss improved from 0.39437 to 0.38957, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4031 - accuracy: 0.8066 - val_loss: 0.3896 - val_accuracy: 0.8180
Epoch 146/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.4031 - accuracy: 0.8098
Epoch 146: val_loss improved from 0.38957 to 0.38837, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4027 - accuracy: 0.8099 - val_loss: 0.3884 - val_accuracy: 0.8189
Epoch 147/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.4021 - accuracy: 0.8090
Epoch 147: val_loss did not improve from 0.38837
852/852 [==============================] - 2s 2ms/step - loss: 0.4027 - accuracy: 0.8088 - val_loss: 0.4077 - val_accuracy: 0.8044
Epoch 148/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.4012 - accuracy: 0.8098
Epoch 148: val_loss improved from 0.38837 to 0.38444, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4016 - accuracy: 0.8102 - val_loss: 0.3844 - val_accuracy: 0.8206
Epoch 149/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.4029 - accuracy: 0.8076
Epoch 149: val_loss did not improve from 0.38444
852/852 [==============================] - 2s 2ms/step - loss: 0.4027 - accuracy: 0.8075 - val_loss: 0.3975 - val_accuracy: 0.8124
Epoch 150/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.4031 - accuracy: 0.8081
Epoch 150: val_loss did not improve from 0.38444
852/852 [==============================] - 2s 2ms/step - loss: 0.4018 - accuracy: 0.8087 - val_loss: 0.3924 - val_accuracy: 0.8145
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=645b0695-03eb-4d1c-a821-f6caae33a5c2">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [74]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">history_125</span> <span class="o">=</span> <span class="n">one_twentyfive_neuron</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">callback_a</span><span class="p">,</span> <span class="n">callback_b</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.4005 - accuracy: 0.8086
Epoch 1: val_loss did not improve from 0.38444
852/852 [==============================] - 2s 2ms/step - loss: 0.4032 - accuracy: 0.8066 - val_loss: 0.4075 - val_accuracy: 0.8084
Epoch 2/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.4045 - accuracy: 0.8099
Epoch 2: val_loss did not improve from 0.38444
852/852 [==============================] - 2s 2ms/step - loss: 0.4034 - accuracy: 0.8101 - val_loss: 0.3890 - val_accuracy: 0.8158
Epoch 3/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.4004 - accuracy: 0.8116
Epoch 3: val_loss did not improve from 0.38444
852/852 [==============================] - 2s 2ms/step - loss: 0.3994 - accuracy: 0.8121 - val_loss: 0.3869 - val_accuracy: 0.8173
Epoch 4/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.3996 - accuracy: 0.8128
Epoch 4: val_loss did not improve from 0.38444
852/852 [==============================] - 2s 2ms/step - loss: 0.3993 - accuracy: 0.8128 - val_loss: 0.3867 - val_accuracy: 0.8200
Epoch 5/150
852/852 [==============================] - ETA: 0s - loss: 0.3959 - accuracy: 0.8124
Epoch 5: val_loss improved from 0.38444 to 0.38338, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3959 - accuracy: 0.8124 - val_loss: 0.3834 - val_accuracy: 0.8207
Epoch 6/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.3953 - accuracy: 0.8145
Epoch 6: val_loss improved from 0.38338 to 0.37954, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3962 - accuracy: 0.8134 - val_loss: 0.3795 - val_accuracy: 0.8223
Epoch 7/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.4004 - accuracy: 0.8137
Epoch 7: val_loss did not improve from 0.37954
852/852 [==============================] - 2s 2ms/step - loss: 0.4012 - accuracy: 0.8132 - val_loss: 0.4018 - val_accuracy: 0.8116
Epoch 8/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.3967 - accuracy: 0.8135
Epoch 8: val_loss did not improve from 0.37954
852/852 [==============================] - 2s 2ms/step - loss: 0.3968 - accuracy: 0.8135 - val_loss: 0.3849 - val_accuracy: 0.8198
Epoch 9/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.3962 - accuracy: 0.8103
Epoch 9: val_loss did not improve from 0.37954
852/852 [==============================] - 2s 2ms/step - loss: 0.3970 - accuracy: 0.8097 - val_loss: 0.3949 - val_accuracy: 0.8147
Epoch 10/150
811/852 [===========================&gt;..] - ETA: 0s - loss: 0.3951 - accuracy: 0.8139
Epoch 10: val_loss did not improve from 0.37954
852/852 [==============================] - 2s 2ms/step - loss: 0.3974 - accuracy: 0.8129 - val_loss: 0.3817 - val_accuracy: 0.8235
Epoch 11/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.3932 - accuracy: 0.8129
Epoch 11: val_loss did not improve from 0.37954
852/852 [==============================] - 2s 2ms/step - loss: 0.3933 - accuracy: 0.8127 - val_loss: 0.3804 - val_accuracy: 0.8194
Epoch 12/150
813/852 [===========================&gt;..] - ETA: 0s - loss: 0.3974 - accuracy: 0.8132
Epoch 12: val_loss improved from 0.37954 to 0.37593, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3964 - accuracy: 0.8143 - val_loss: 0.3759 - val_accuracy: 0.8246
Epoch 13/150
815/852 [===========================&gt;..] - ETA: 0s - loss: 0.3900 - accuracy: 0.8168
Epoch 13: val_loss improved from 0.37593 to 0.37356, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3932 - accuracy: 0.8148 - val_loss: 0.3736 - val_accuracy: 0.8261
Epoch 14/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.3913 - accuracy: 0.8128
Epoch 14: val_loss did not improve from 0.37356
852/852 [==============================] - 2s 2ms/step - loss: 0.3911 - accuracy: 0.8127 - val_loss: 0.3761 - val_accuracy: 0.8236
Epoch 15/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.3915 - accuracy: 0.8134
Epoch 15: val_loss did not improve from 0.37356
852/852 [==============================] - 2s 2ms/step - loss: 0.3926 - accuracy: 0.8132 - val_loss: 0.3961 - val_accuracy: 0.8149
Epoch 16/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.3881 - accuracy: 0.8187
Epoch 16: val_loss improved from 0.37356 to 0.37265, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3895 - accuracy: 0.8181 - val_loss: 0.3726 - val_accuracy: 0.8249
Epoch 17/150
852/852 [==============================] - ETA: 0s - loss: 0.3826 - accuracy: 0.8167
Epoch 17: val_loss did not improve from 0.37265
852/852 [==============================] - 2s 2ms/step - loss: 0.3826 - accuracy: 0.8167 - val_loss: 0.3744 - val_accuracy: 0.8266
Epoch 18/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.3884 - accuracy: 0.8174
Epoch 18: val_loss did not improve from 0.37265
852/852 [==============================] - 2s 2ms/step - loss: 0.3881 - accuracy: 0.8174 - val_loss: 0.3773 - val_accuracy: 0.8240
Epoch 19/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.3895 - accuracy: 0.8160
Epoch 19: val_loss did not improve from 0.37265
852/852 [==============================] - 2s 2ms/step - loss: 0.3892 - accuracy: 0.8159 - val_loss: 0.3804 - val_accuracy: 0.8262
Epoch 20/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.3895 - accuracy: 0.8164
Epoch 20: val_loss did not improve from 0.37265
852/852 [==============================] - 2s 2ms/step - loss: 0.3909 - accuracy: 0.8155 - val_loss: 0.3746 - val_accuracy: 0.8260
Epoch 21/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.3889 - accuracy: 0.8192
Epoch 21: val_loss improved from 0.37265 to 0.36187, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3895 - accuracy: 0.8190 - val_loss: 0.3619 - val_accuracy: 0.8324
Epoch 22/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.3864 - accuracy: 0.8201
Epoch 22: val_loss did not improve from 0.36187
852/852 [==============================] - 2s 2ms/step - loss: 0.3864 - accuracy: 0.8202 - val_loss: 0.3833 - val_accuracy: 0.8175
Epoch 23/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.3839 - accuracy: 0.8182
Epoch 23: val_loss did not improve from 0.36187
852/852 [==============================] - 2s 2ms/step - loss: 0.3858 - accuracy: 0.8173 - val_loss: 0.3808 - val_accuracy: 0.8220
Epoch 24/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.3890 - accuracy: 0.8165
Epoch 24: val_loss did not improve from 0.36187
852/852 [==============================] - 2s 2ms/step - loss: 0.3894 - accuracy: 0.8159 - val_loss: 0.3748 - val_accuracy: 0.8217
Epoch 25/150
812/852 [===========================&gt;..] - ETA: 0s - loss: 0.3866 - accuracy: 0.8203
Epoch 25: val_loss did not improve from 0.36187
852/852 [==============================] - 2s 2ms/step - loss: 0.3881 - accuracy: 0.8194 - val_loss: 0.3869 - val_accuracy: 0.8206
Epoch 26/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.3828 - accuracy: 0.8162
Epoch 26: val_loss did not improve from 0.36187
852/852 [==============================] - 2s 2ms/step - loss: 0.3828 - accuracy: 0.8162 - val_loss: 0.3714 - val_accuracy: 0.8243
Epoch 27/150
815/852 [===========================&gt;..] - ETA: 0s - loss: 0.3874 - accuracy: 0.8180
Epoch 27: val_loss did not improve from 0.36187
852/852 [==============================] - 2s 2ms/step - loss: 0.3901 - accuracy: 0.8167 - val_loss: 0.3737 - val_accuracy: 0.8244
Epoch 28/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.3868 - accuracy: 0.8170
Epoch 28: val_loss did not improve from 0.36187
852/852 [==============================] - 2s 2ms/step - loss: 0.3877 - accuracy: 0.8168 - val_loss: 0.3653 - val_accuracy: 0.8300
Epoch 29/150
820/852 [===========================&gt;..] - ETA: 0s - loss: 0.3815 - accuracy: 0.8193
Epoch 29: val_loss did not improve from 0.36187
852/852 [==============================] - 2s 2ms/step - loss: 0.3822 - accuracy: 0.8179 - val_loss: 0.3833 - val_accuracy: 0.8225
Epoch 30/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.3818 - accuracy: 0.8196
Epoch 30: val_loss did not improve from 0.36187
852/852 [==============================] - 2s 2ms/step - loss: 0.3813 - accuracy: 0.8205 - val_loss: 0.3713 - val_accuracy: 0.8246
Epoch 31/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.3813 - accuracy: 0.8202
Epoch 31: val_loss did not improve from 0.36187
852/852 [==============================] - 2s 2ms/step - loss: 0.3820 - accuracy: 0.8196 - val_loss: 0.3678 - val_accuracy: 0.8242
Epoch 32/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.3827 - accuracy: 0.8217
Epoch 32: val_loss did not improve from 0.36187
852/852 [==============================] - 2s 2ms/step - loss: 0.3816 - accuracy: 0.8234 - val_loss: 0.3680 - val_accuracy: 0.8290
Epoch 33/150
852/852 [==============================] - ETA: 0s - loss: 0.3788 - accuracy: 0.8230
Epoch 33: val_loss did not improve from 0.36187
852/852 [==============================] - 2s 2ms/step - loss: 0.3788 - accuracy: 0.8230 - val_loss: 0.3802 - val_accuracy: 0.8260
Epoch 34/150
809/852 [===========================&gt;..] - ETA: 0s - loss: 0.3820 - accuracy: 0.8188
Epoch 34: val_loss did not improve from 0.36187
852/852 [==============================] - 2s 2ms/step - loss: 0.3805 - accuracy: 0.8201 - val_loss: 0.3735 - val_accuracy: 0.8257
Epoch 35/150
811/852 [===========================&gt;..] - ETA: 0s - loss: 0.3849 - accuracy: 0.8191
Epoch 35: val_loss did not improve from 0.36187
852/852 [==============================] - 2s 2ms/step - loss: 0.3854 - accuracy: 0.8187 - val_loss: 0.3694 - val_accuracy: 0.8282
Epoch 36/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.3807 - accuracy: 0.8218
Epoch 36: val_loss did not improve from 0.36187
852/852 [==============================] - 2s 2ms/step - loss: 0.3814 - accuracy: 0.8214 - val_loss: 0.3784 - val_accuracy: 0.8257
Epoch 37/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.3795 - accuracy: 0.8192
Epoch 37: val_loss improved from 0.36187 to 0.35598, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3792 - accuracy: 0.8195 - val_loss: 0.3560 - val_accuracy: 0.8342
Epoch 38/150
818/852 [===========================&gt;..] - ETA: 0s - loss: 0.3748 - accuracy: 0.8246
Epoch 38: val_loss did not improve from 0.35598
852/852 [==============================] - 2s 2ms/step - loss: 0.3737 - accuracy: 0.8259 - val_loss: 0.3682 - val_accuracy: 0.8283
Epoch 39/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.3799 - accuracy: 0.8206
Epoch 39: val_loss did not improve from 0.35598
852/852 [==============================] - 2s 2ms/step - loss: 0.3772 - accuracy: 0.8220 - val_loss: 0.3618 - val_accuracy: 0.8307
Epoch 40/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.3697 - accuracy: 0.8282
Epoch 40: val_loss did not improve from 0.35598
852/852 [==============================] - 2s 2ms/step - loss: 0.3703 - accuracy: 0.8275 - val_loss: 0.3596 - val_accuracy: 0.8367
Epoch 41/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.3776 - accuracy: 0.8225
Epoch 41: val_loss did not improve from 0.35598
852/852 [==============================] - 2s 2ms/step - loss: 0.3770 - accuracy: 0.8229 - val_loss: 0.3597 - val_accuracy: 0.8343
Epoch 42/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.3775 - accuracy: 0.8239
Epoch 42: val_loss improved from 0.35598 to 0.35309, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3771 - accuracy: 0.8239 - val_loss: 0.3531 - val_accuracy: 0.8360
Epoch 43/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.3701 - accuracy: 0.8259
Epoch 43: val_loss did not improve from 0.35309
852/852 [==============================] - 2s 2ms/step - loss: 0.3716 - accuracy: 0.8252 - val_loss: 0.3535 - val_accuracy: 0.8367
Epoch 44/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.3766 - accuracy: 0.8228
Epoch 44: val_loss did not improve from 0.35309
852/852 [==============================] - 2s 2ms/step - loss: 0.3767 - accuracy: 0.8228 - val_loss: 0.3664 - val_accuracy: 0.8282
Epoch 45/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.3765 - accuracy: 0.8233
Epoch 45: val_loss did not improve from 0.35309
852/852 [==============================] - 2s 2ms/step - loss: 0.3781 - accuracy: 0.8226 - val_loss: 0.3704 - val_accuracy: 0.8266
Epoch 46/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.3733 - accuracy: 0.8229
Epoch 46: val_loss did not improve from 0.35309
852/852 [==============================] - 2s 2ms/step - loss: 0.3731 - accuracy: 0.8228 - val_loss: 0.3637 - val_accuracy: 0.8335
Epoch 47/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.3716 - accuracy: 0.8262
Epoch 47: val_loss did not improve from 0.35309
852/852 [==============================] - 2s 2ms/step - loss: 0.3729 - accuracy: 0.8259 - val_loss: 0.3654 - val_accuracy: 0.8288
Epoch 48/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.3784 - accuracy: 0.8214
Epoch 48: val_loss did not improve from 0.35309
852/852 [==============================] - 2s 2ms/step - loss: 0.3797 - accuracy: 0.8206 - val_loss: 0.3621 - val_accuracy: 0.8337
Epoch 49/150
852/852 [==============================] - ETA: 0s - loss: 0.3724 - accuracy: 0.8269
Epoch 49: val_loss did not improve from 0.35309
852/852 [==============================] - 2s 2ms/step - loss: 0.3724 - accuracy: 0.8269 - val_loss: 0.3544 - val_accuracy: 0.8388
Epoch 50/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.3690 - accuracy: 0.8322
Epoch 50: val_loss did not improve from 0.35309
852/852 [==============================] - 2s 2ms/step - loss: 0.3702 - accuracy: 0.8315 - val_loss: 0.3708 - val_accuracy: 0.8257
Epoch 51/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.3736 - accuracy: 0.8244
Epoch 51: val_loss did not improve from 0.35309
852/852 [==============================] - 2s 2ms/step - loss: 0.3740 - accuracy: 0.8242 - val_loss: 0.3677 - val_accuracy: 0.8321
Epoch 52/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.3784 - accuracy: 0.8213
Epoch 52: val_loss did not improve from 0.35309
852/852 [==============================] - 2s 2ms/step - loss: 0.3789 - accuracy: 0.8217 - val_loss: 0.3576 - val_accuracy: 0.8331
Epoch 53/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.3717 - accuracy: 0.8269
Epoch 53: val_loss improved from 0.35309 to 0.35067, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3720 - accuracy: 0.8264 - val_loss: 0.3507 - val_accuracy: 0.8387
Epoch 54/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.3672 - accuracy: 0.8274
Epoch 54: val_loss improved from 0.35067 to 0.34683, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3673 - accuracy: 0.8279 - val_loss: 0.3468 - val_accuracy: 0.8369
Epoch 55/150
852/852 [==============================] - ETA: 0s - loss: 0.3760 - accuracy: 0.8237
Epoch 55: val_loss did not improve from 0.34683
852/852 [==============================] - 2s 2ms/step - loss: 0.3760 - accuracy: 0.8237 - val_loss: 0.3549 - val_accuracy: 0.8351
Epoch 56/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.3728 - accuracy: 0.8230
Epoch 56: val_loss improved from 0.34683 to 0.34007, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3718 - accuracy: 0.8235 - val_loss: 0.3401 - val_accuracy: 0.8443
Epoch 57/150
814/852 [===========================&gt;..] - ETA: 0s - loss: 0.3685 - accuracy: 0.8260
Epoch 57: val_loss did not improve from 0.34007
852/852 [==============================] - 2s 2ms/step - loss: 0.3677 - accuracy: 0.8271 - val_loss: 0.3568 - val_accuracy: 0.8358
Epoch 58/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.3674 - accuracy: 0.8272
Epoch 58: val_loss did not improve from 0.34007
852/852 [==============================] - 2s 2ms/step - loss: 0.3671 - accuracy: 0.8275 - val_loss: 0.3580 - val_accuracy: 0.8322
Epoch 59/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.3700 - accuracy: 0.8254
Epoch 59: val_loss did not improve from 0.34007
852/852 [==============================] - 2s 2ms/step - loss: 0.3698 - accuracy: 0.8255 - val_loss: 0.3674 - val_accuracy: 0.8297
Epoch 60/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.3717 - accuracy: 0.8273
Epoch 60: val_loss did not improve from 0.34007
852/852 [==============================] - 2s 2ms/step - loss: 0.3702 - accuracy: 0.8284 - val_loss: 0.3477 - val_accuracy: 0.8397
Epoch 61/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.3648 - accuracy: 0.8304
Epoch 61: val_loss did not improve from 0.34007
852/852 [==============================] - 2s 2ms/step - loss: 0.3662 - accuracy: 0.8293 - val_loss: 0.3593 - val_accuracy: 0.8315
Epoch 62/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.3688 - accuracy: 0.8254
Epoch 62: val_loss did not improve from 0.34007
852/852 [==============================] - 2s 2ms/step - loss: 0.3700 - accuracy: 0.8249 - val_loss: 0.3614 - val_accuracy: 0.8276
Epoch 63/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.3642 - accuracy: 0.8269
Epoch 63: val_loss did not improve from 0.34007
852/852 [==============================] - 2s 2ms/step - loss: 0.3647 - accuracy: 0.8269 - val_loss: 0.3508 - val_accuracy: 0.8402
Epoch 64/150
811/852 [===========================&gt;..] - ETA: 0s - loss: 0.3670 - accuracy: 0.8233
Epoch 64: val_loss did not improve from 0.34007
852/852 [==============================] - 2s 2ms/step - loss: 0.3684 - accuracy: 0.8233 - val_loss: 0.3741 - val_accuracy: 0.8271
Epoch 65/150
819/852 [===========================&gt;..] - ETA: 0s - loss: 0.3679 - accuracy: 0.8288
Epoch 65: val_loss did not improve from 0.34007
852/852 [==============================] - 2s 2ms/step - loss: 0.3662 - accuracy: 0.8297 - val_loss: 0.3492 - val_accuracy: 0.8411
Epoch 66/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.3649 - accuracy: 0.8284
Epoch 66: val_loss did not improve from 0.34007
852/852 [==============================] - 2s 2ms/step - loss: 0.3657 - accuracy: 0.8280 - val_loss: 0.3523 - val_accuracy: 0.8404
Epoch 67/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.3657 - accuracy: 0.8308
Epoch 67: val_loss did not improve from 0.34007
852/852 [==============================] - 2s 2ms/step - loss: 0.3662 - accuracy: 0.8310 - val_loss: 0.3523 - val_accuracy: 0.8368
Epoch 68/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.3648 - accuracy: 0.8294
Epoch 68: val_loss did not improve from 0.34007
852/852 [==============================] - 2s 2ms/step - loss: 0.3653 - accuracy: 0.8288 - val_loss: 0.3747 - val_accuracy: 0.8270
Epoch 69/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.3658 - accuracy: 0.8285
Epoch 69: val_loss did not improve from 0.34007
852/852 [==============================] - 2s 2ms/step - loss: 0.3664 - accuracy: 0.8277 - val_loss: 0.3415 - val_accuracy: 0.8423
Epoch 70/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.3654 - accuracy: 0.8278
Epoch 70: val_loss did not improve from 0.34007
852/852 [==============================] - 2s 2ms/step - loss: 0.3657 - accuracy: 0.8279 - val_loss: 0.3495 - val_accuracy: 0.8404
Epoch 71/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.3583 - accuracy: 0.8299
Epoch 71: val_loss did not improve from 0.34007
852/852 [==============================] - 2s 2ms/step - loss: 0.3599 - accuracy: 0.8291 - val_loss: 0.3628 - val_accuracy: 0.8344
Epoch 72/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.3593 - accuracy: 0.8332
Epoch 72: val_loss improved from 0.34007 to 0.33962, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.3595 - accuracy: 0.8333 - val_loss: 0.3396 - val_accuracy: 0.8424
Epoch 73/150
820/852 [===========================&gt;..] - ETA: 0s - loss: 0.3602 - accuracy: 0.8305
Epoch 73: val_loss did not improve from 0.33962
852/852 [==============================] - 2s 2ms/step - loss: 0.3615 - accuracy: 0.8300 - val_loss: 0.3479 - val_accuracy: 0.8394
Epoch 74/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.3619 - accuracy: 0.8309
Epoch 74: val_loss did not improve from 0.33962
852/852 [==============================] - 2s 2ms/step - loss: 0.3636 - accuracy: 0.8304 - val_loss: 0.3468 - val_accuracy: 0.8399
Epoch 75/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.3631 - accuracy: 0.8285
Epoch 75: val_loss did not improve from 0.33962
852/852 [==============================] - 2s 2ms/step - loss: 0.3632 - accuracy: 0.8286 - val_loss: 0.3429 - val_accuracy: 0.8439
Epoch 76/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.3565 - accuracy: 0.8338
Epoch 76: val_loss did not improve from 0.33962
852/852 [==============================] - 2s 2ms/step - loss: 0.3567 - accuracy: 0.8334 - val_loss: 0.3489 - val_accuracy: 0.8390
Epoch 77/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.3624 - accuracy: 0.8301
Epoch 77: val_loss improved from 0.33962 to 0.33729, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3623 - accuracy: 0.8303 - val_loss: 0.3373 - val_accuracy: 0.8439
Epoch 78/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.3609 - accuracy: 0.8309
Epoch 78: val_loss did not improve from 0.33729
852/852 [==============================] - 2s 2ms/step - loss: 0.3609 - accuracy: 0.8309 - val_loss: 0.3664 - val_accuracy: 0.8277
Epoch 79/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.3593 - accuracy: 0.8324
Epoch 79: val_loss did not improve from 0.33729
852/852 [==============================] - 2s 2ms/step - loss: 0.3593 - accuracy: 0.8326 - val_loss: 0.3513 - val_accuracy: 0.8375
Epoch 80/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.3583 - accuracy: 0.8331
Epoch 80: val_loss did not improve from 0.33729
852/852 [==============================] - 2s 2ms/step - loss: 0.3583 - accuracy: 0.8330 - val_loss: 0.3424 - val_accuracy: 0.8411
Epoch 81/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.3582 - accuracy: 0.8324
Epoch 81: val_loss did not improve from 0.33729
852/852 [==============================] - 2s 2ms/step - loss: 0.3575 - accuracy: 0.8328 - val_loss: 0.3379 - val_accuracy: 0.8416
Epoch 82/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.3553 - accuracy: 0.8350
Epoch 82: val_loss improved from 0.33729 to 0.32966, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3561 - accuracy: 0.8342 - val_loss: 0.3297 - val_accuracy: 0.8485
Epoch 83/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.3551 - accuracy: 0.8358
Epoch 83: val_loss did not improve from 0.32966
852/852 [==============================] - 2s 2ms/step - loss: 0.3557 - accuracy: 0.8353 - val_loss: 0.3464 - val_accuracy: 0.8403
Epoch 84/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.3576 - accuracy: 0.8331
Epoch 84: val_loss did not improve from 0.32966
852/852 [==============================] - 2s 2ms/step - loss: 0.3568 - accuracy: 0.8334 - val_loss: 0.3398 - val_accuracy: 0.8408
Epoch 85/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.3548 - accuracy: 0.8381
Epoch 85: val_loss did not improve from 0.32966
852/852 [==============================] - 2s 2ms/step - loss: 0.3544 - accuracy: 0.8381 - val_loss: 0.3434 - val_accuracy: 0.8415
Epoch 86/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.3574 - accuracy: 0.8369
Epoch 86: val_loss did not improve from 0.32966
852/852 [==============================] - 2s 2ms/step - loss: 0.3567 - accuracy: 0.8368 - val_loss: 0.3372 - val_accuracy: 0.8472
Epoch 87/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.3579 - accuracy: 0.8321
Epoch 87: val_loss did not improve from 0.32966
852/852 [==============================] - 2s 2ms/step - loss: 0.3569 - accuracy: 0.8323 - val_loss: 0.3370 - val_accuracy: 0.8439
Epoch 88/150
820/852 [===========================&gt;..] - ETA: 0s - loss: 0.3605 - accuracy: 0.8351
Epoch 88: val_loss did not improve from 0.32966
852/852 [==============================] - 2s 2ms/step - loss: 0.3608 - accuracy: 0.8345 - val_loss: 0.3365 - val_accuracy: 0.8470
Epoch 89/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.3535 - accuracy: 0.8315
Epoch 89: val_loss did not improve from 0.32966
852/852 [==============================] - 2s 2ms/step - loss: 0.3544 - accuracy: 0.8315 - val_loss: 0.3366 - val_accuracy: 0.8450
Epoch 90/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.3534 - accuracy: 0.8357
Epoch 90: val_loss did not improve from 0.32966
852/852 [==============================] - 2s 2ms/step - loss: 0.3547 - accuracy: 0.8342 - val_loss: 0.3382 - val_accuracy: 0.8453
Epoch 91/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.3552 - accuracy: 0.8355
Epoch 91: val_loss did not improve from 0.32966
852/852 [==============================] - 2s 2ms/step - loss: 0.3540 - accuracy: 0.8356 - val_loss: 0.3389 - val_accuracy: 0.8441
Epoch 92/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.3565 - accuracy: 0.8350
Epoch 92: val_loss did not improve from 0.32966
852/852 [==============================] - 2s 2ms/step - loss: 0.3558 - accuracy: 0.8355 - val_loss: 0.3377 - val_accuracy: 0.8438
Epoch 93/150
818/852 [===========================&gt;..] - ETA: 0s - loss: 0.3507 - accuracy: 0.8401
Epoch 93: val_loss did not improve from 0.32966
852/852 [==============================] - 2s 2ms/step - loss: 0.3518 - accuracy: 0.8389 - val_loss: 0.3344 - val_accuracy: 0.8449
Epoch 94/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.3506 - accuracy: 0.8364
Epoch 94: val_loss did not improve from 0.32966
852/852 [==============================] - 2s 2ms/step - loss: 0.3513 - accuracy: 0.8364 - val_loss: 0.3427 - val_accuracy: 0.8411
Epoch 95/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.3539 - accuracy: 0.8341
Epoch 95: val_loss improved from 0.32966 to 0.32462, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3529 - accuracy: 0.8344 - val_loss: 0.3246 - val_accuracy: 0.8485
Epoch 96/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.3504 - accuracy: 0.8365
Epoch 96: val_loss did not improve from 0.32462
852/852 [==============================] - 2s 2ms/step - loss: 0.3500 - accuracy: 0.8368 - val_loss: 0.3493 - val_accuracy: 0.8414
Epoch 97/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.3448 - accuracy: 0.8404
Epoch 97: val_loss did not improve from 0.32462
852/852 [==============================] - 2s 2ms/step - loss: 0.3455 - accuracy: 0.8391 - val_loss: 0.3361 - val_accuracy: 0.8424
Epoch 98/150
820/852 [===========================&gt;..] - ETA: 0s - loss: 0.3522 - accuracy: 0.8334
Epoch 98: val_loss did not improve from 0.32462
852/852 [==============================] - 2s 2ms/step - loss: 0.3536 - accuracy: 0.8326 - val_loss: 0.3363 - val_accuracy: 0.8448
Epoch 99/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.3518 - accuracy: 0.8367
Epoch 99: val_loss did not improve from 0.32462
852/852 [==============================] - 2s 2ms/step - loss: 0.3505 - accuracy: 0.8369 - val_loss: 0.3270 - val_accuracy: 0.8485
Epoch 100/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.3479 - accuracy: 0.8372
Epoch 100: val_loss did not improve from 0.32462
852/852 [==============================] - 2s 2ms/step - loss: 0.3488 - accuracy: 0.8368 - val_loss: 0.3349 - val_accuracy: 0.8478
Epoch 101/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.3546 - accuracy: 0.8349
Epoch 101: val_loss did not improve from 0.32462
852/852 [==============================] - 2s 2ms/step - loss: 0.3556 - accuracy: 0.8344 - val_loss: 0.3247 - val_accuracy: 0.8519
Epoch 102/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.3519 - accuracy: 0.8366
Epoch 102: val_loss did not improve from 0.32462
852/852 [==============================] - 2s 3ms/step - loss: 0.3517 - accuracy: 0.8364 - val_loss: 0.3301 - val_accuracy: 0.8468
Epoch 103/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.3464 - accuracy: 0.8362
Epoch 103: val_loss did not improve from 0.32462
852/852 [==============================] - 2s 2ms/step - loss: 0.3466 - accuracy: 0.8368 - val_loss: 0.3322 - val_accuracy: 0.8463
Epoch 104/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.3471 - accuracy: 0.8363
Epoch 104: val_loss did not improve from 0.32462
852/852 [==============================] - 2s 2ms/step - loss: 0.3476 - accuracy: 0.8358 - val_loss: 0.3377 - val_accuracy: 0.8439
Epoch 105/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.3469 - accuracy: 0.8401
Epoch 105: val_loss did not improve from 0.32462
852/852 [==============================] - 2s 2ms/step - loss: 0.3481 - accuracy: 0.8397 - val_loss: 0.3267 - val_accuracy: 0.8486
Epoch 106/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.3504 - accuracy: 0.8340
Epoch 106: val_loss improved from 0.32462 to 0.31743, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3504 - accuracy: 0.8345 - val_loss: 0.3174 - val_accuracy: 0.8553
Epoch 107/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.3417 - accuracy: 0.8397
Epoch 107: val_loss did not improve from 0.31743
852/852 [==============================] - 2s 2ms/step - loss: 0.3423 - accuracy: 0.8397 - val_loss: 0.3271 - val_accuracy: 0.8493
Epoch 108/150
815/852 [===========================&gt;..] - ETA: 0s - loss: 0.3386 - accuracy: 0.8412
Epoch 108: val_loss did not improve from 0.31743
852/852 [==============================] - 2s 2ms/step - loss: 0.3395 - accuracy: 0.8409 - val_loss: 0.3344 - val_accuracy: 0.8438
Epoch 109/150
852/852 [==============================] - ETA: 0s - loss: 0.3462 - accuracy: 0.8370
Epoch 109: val_loss did not improve from 0.31743
852/852 [==============================] - 2s 2ms/step - loss: 0.3462 - accuracy: 0.8370 - val_loss: 0.3298 - val_accuracy: 0.8505
Epoch 110/150
852/852 [==============================] - ETA: 0s - loss: 0.3446 - accuracy: 0.8380
Epoch 110: val_loss did not improve from 0.31743
852/852 [==============================] - 2s 2ms/step - loss: 0.3446 - accuracy: 0.8380 - val_loss: 0.3358 - val_accuracy: 0.8435
Epoch 111/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.3446 - accuracy: 0.8388
Epoch 111: val_loss did not improve from 0.31743
852/852 [==============================] - 2s 2ms/step - loss: 0.3451 - accuracy: 0.8380 - val_loss: 0.3306 - val_accuracy: 0.8484
Epoch 112/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.3373 - accuracy: 0.8436
Epoch 112: val_loss did not improve from 0.31743
852/852 [==============================] - 2s 2ms/step - loss: 0.3374 - accuracy: 0.8436 - val_loss: 0.3207 - val_accuracy: 0.8505
Epoch 113/150
813/852 [===========================&gt;..] - ETA: 0s - loss: 0.3431 - accuracy: 0.8397
Epoch 113: val_loss did not improve from 0.31743
852/852 [==============================] - 2s 2ms/step - loss: 0.3436 - accuracy: 0.8390 - val_loss: 0.3219 - val_accuracy: 0.8530
Epoch 114/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.3442 - accuracy: 0.8423
Epoch 114: val_loss did not improve from 0.31743
852/852 [==============================] - 2s 2ms/step - loss: 0.3447 - accuracy: 0.8422 - val_loss: 0.3414 - val_accuracy: 0.8429
Epoch 115/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.3451 - accuracy: 0.8384
Epoch 115: val_loss did not improve from 0.31743
852/852 [==============================] - 2s 2ms/step - loss: 0.3456 - accuracy: 0.8382 - val_loss: 0.3235 - val_accuracy: 0.8508
Epoch 116/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.3414 - accuracy: 0.8418
Epoch 116: val_loss did not improve from 0.31743
852/852 [==============================] - 2s 2ms/step - loss: 0.3419 - accuracy: 0.8415 - val_loss: 0.3300 - val_accuracy: 0.8491
Epoch 117/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.3465 - accuracy: 0.8373
Epoch 117: val_loss did not improve from 0.31743
852/852 [==============================] - 2s 2ms/step - loss: 0.3461 - accuracy: 0.8381 - val_loss: 0.3379 - val_accuracy: 0.8441
Epoch 118/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.3370 - accuracy: 0.8439
Epoch 118: val_loss did not improve from 0.31743
852/852 [==============================] - 2s 2ms/step - loss: 0.3375 - accuracy: 0.8442 - val_loss: 0.3219 - val_accuracy: 0.8533
Epoch 119/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.3383 - accuracy: 0.8442
Epoch 119: val_loss did not improve from 0.31743
852/852 [==============================] - 2s 2ms/step - loss: 0.3397 - accuracy: 0.8435 - val_loss: 0.3280 - val_accuracy: 0.8479
Epoch 120/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.3421 - accuracy: 0.8395
Epoch 120: val_loss did not improve from 0.31743
852/852 [==============================] - 2s 2ms/step - loss: 0.3416 - accuracy: 0.8397 - val_loss: 0.3240 - val_accuracy: 0.8554
Epoch 121/150
820/852 [===========================&gt;..] - ETA: 0s - loss: 0.3343 - accuracy: 0.8454
Epoch 121: val_loss did not improve from 0.31743
852/852 [==============================] - 2s 2ms/step - loss: 0.3353 - accuracy: 0.8456 - val_loss: 0.3382 - val_accuracy: 0.8476
Epoch 122/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.3472 - accuracy: 0.8382
Epoch 122: val_loss did not improve from 0.31743
852/852 [==============================] - 2s 2ms/step - loss: 0.3469 - accuracy: 0.8384 - val_loss: 0.3312 - val_accuracy: 0.8464
Epoch 123/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.3349 - accuracy: 0.8417
Epoch 123: val_loss did not improve from 0.31743
852/852 [==============================] - 2s 2ms/step - loss: 0.3365 - accuracy: 0.8409 - val_loss: 0.3313 - val_accuracy: 0.8477
Epoch 124/150
818/852 [===========================&gt;..] - ETA: 0s - loss: 0.3381 - accuracy: 0.8402
Epoch 124: val_loss did not improve from 0.31743
852/852 [==============================] - 2s 2ms/step - loss: 0.3395 - accuracy: 0.8401 - val_loss: 0.3465 - val_accuracy: 0.8395
Epoch 125/150
812/852 [===========================&gt;..] - ETA: 0s - loss: 0.3378 - accuracy: 0.8459
Epoch 125: val_loss did not improve from 0.31743
852/852 [==============================] - 2s 2ms/step - loss: 0.3397 - accuracy: 0.8449 - val_loss: 0.3177 - val_accuracy: 0.8538
Epoch 126/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.3368 - accuracy: 0.8432
Epoch 126: val_loss did not improve from 0.31743
852/852 [==============================] - 2s 2ms/step - loss: 0.3369 - accuracy: 0.8431 - val_loss: 0.3498 - val_accuracy: 0.8382
Epoch 126: early stopping
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=91a41d90-dd37-4a6c-849a-390df355ca26">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [75]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">history_125</span> <span class="o">=</span> <span class="n">one_twentyfive_neuron</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">callback_a</span><span class="p">,</span> <span class="n">callback_b</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.3373 - accuracy: 0.8438
Epoch 1: val_loss did not improve from 0.31743
852/852 [==============================] - 2s 2ms/step - loss: 0.3374 - accuracy: 0.8441 - val_loss: 0.3227 - val_accuracy: 0.8505
Epoch 2/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.3480 - accuracy: 0.8360
Epoch 2: val_loss did not improve from 0.31743
852/852 [==============================] - 2s 2ms/step - loss: 0.3458 - accuracy: 0.8371 - val_loss: 0.3207 - val_accuracy: 0.8506
Epoch 3/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.3369 - accuracy: 0.8397
Epoch 3: val_loss did not improve from 0.31743
852/852 [==============================] - 2s 2ms/step - loss: 0.3360 - accuracy: 0.8403 - val_loss: 0.3235 - val_accuracy: 0.8463
Epoch 4/150
818/852 [===========================&gt;..] - ETA: 0s - loss: 0.3372 - accuracy: 0.8439
Epoch 4: val_loss improved from 0.31743 to 0.31557, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3384 - accuracy: 0.8429 - val_loss: 0.3156 - val_accuracy: 0.8543
Epoch 5/150
814/852 [===========================&gt;..] - ETA: 0s - loss: 0.3395 - accuracy: 0.8403
Epoch 5: val_loss did not improve from 0.31557
852/852 [==============================] - 2s 2ms/step - loss: 0.3396 - accuracy: 0.8407 - val_loss: 0.3305 - val_accuracy: 0.8456
Epoch 6/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.3292 - accuracy: 0.8447
Epoch 6: val_loss did not improve from 0.31557
852/852 [==============================] - 2s 2ms/step - loss: 0.3286 - accuracy: 0.8453 - val_loss: 0.3235 - val_accuracy: 0.8533
Epoch 7/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.3376 - accuracy: 0.8456
Epoch 7: val_loss did not improve from 0.31557
852/852 [==============================] - 2s 2ms/step - loss: 0.3390 - accuracy: 0.8444 - val_loss: 0.3346 - val_accuracy: 0.8452
Epoch 8/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.3399 - accuracy: 0.8427
Epoch 8: val_loss did not improve from 0.31557
852/852 [==============================] - 2s 2ms/step - loss: 0.3387 - accuracy: 0.8431 - val_loss: 0.3231 - val_accuracy: 0.8512
Epoch 9/150
812/852 [===========================&gt;..] - ETA: 0s - loss: 0.3291 - accuracy: 0.8450
Epoch 9: val_loss did not improve from 0.31557
852/852 [==============================] - 2s 2ms/step - loss: 0.3311 - accuracy: 0.8436 - val_loss: 0.3317 - val_accuracy: 0.8476
Epoch 10/150
852/852 [==============================] - ETA: 0s - loss: 0.3374 - accuracy: 0.8405
Epoch 10: val_loss did not improve from 0.31557
852/852 [==============================] - 2s 2ms/step - loss: 0.3374 - accuracy: 0.8405 - val_loss: 0.3214 - val_accuracy: 0.8532
Epoch 11/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.3349 - accuracy: 0.8459
Epoch 11: val_loss did not improve from 0.31557
852/852 [==============================] - 2s 2ms/step - loss: 0.3351 - accuracy: 0.8453 - val_loss: 0.3161 - val_accuracy: 0.8536
Epoch 12/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.3329 - accuracy: 0.8472
Epoch 12: val_loss did not improve from 0.31557
852/852 [==============================] - 2s 2ms/step - loss: 0.3317 - accuracy: 0.8476 - val_loss: 0.3357 - val_accuracy: 0.8423
Epoch 13/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.3330 - accuracy: 0.8411
Epoch 13: val_loss improved from 0.31557 to 0.30764, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3342 - accuracy: 0.8408 - val_loss: 0.3076 - val_accuracy: 0.8611
Epoch 14/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.3282 - accuracy: 0.8458
Epoch 14: val_loss did not improve from 0.30764
852/852 [==============================] - 2s 2ms/step - loss: 0.3289 - accuracy: 0.8453 - val_loss: 0.3162 - val_accuracy: 0.8552
Epoch 15/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.3310 - accuracy: 0.8461
Epoch 15: val_loss did not improve from 0.30764
852/852 [==============================] - 2s 2ms/step - loss: 0.3304 - accuracy: 0.8469 - val_loss: 0.3246 - val_accuracy: 0.8506
Epoch 16/150
820/852 [===========================&gt;..] - ETA: 0s - loss: 0.3320 - accuracy: 0.8471
Epoch 16: val_loss did not improve from 0.30764
852/852 [==============================] - 2s 2ms/step - loss: 0.3335 - accuracy: 0.8456 - val_loss: 0.3280 - val_accuracy: 0.8464
Epoch 17/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.3299 - accuracy: 0.8445
Epoch 17: val_loss did not improve from 0.30764
852/852 [==============================] - 2s 2ms/step - loss: 0.3282 - accuracy: 0.8465 - val_loss: 0.3143 - val_accuracy: 0.8512
Epoch 18/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.3324 - accuracy: 0.8447
Epoch 18: val_loss did not improve from 0.30764
852/852 [==============================] - 2s 2ms/step - loss: 0.3329 - accuracy: 0.8438 - val_loss: 0.3251 - val_accuracy: 0.8504
Epoch 19/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.3248 - accuracy: 0.8476
Epoch 19: val_loss did not improve from 0.30764
852/852 [==============================] - 2s 2ms/step - loss: 0.3239 - accuracy: 0.8478 - val_loss: 0.3161 - val_accuracy: 0.8547
Epoch 20/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.3307 - accuracy: 0.8440
Epoch 20: val_loss did not improve from 0.30764
852/852 [==============================] - 2s 2ms/step - loss: 0.3334 - accuracy: 0.8422 - val_loss: 0.3442 - val_accuracy: 0.8419
Epoch 21/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.3314 - accuracy: 0.8476
Epoch 21: val_loss did not improve from 0.30764
852/852 [==============================] - 2s 2ms/step - loss: 0.3314 - accuracy: 0.8477 - val_loss: 0.3088 - val_accuracy: 0.8605
Epoch 22/150
812/852 [===========================&gt;..] - ETA: 0s - loss: 0.3346 - accuracy: 0.8416
Epoch 22: val_loss did not improve from 0.30764
852/852 [==============================] - 2s 2ms/step - loss: 0.3347 - accuracy: 0.8425 - val_loss: 0.3093 - val_accuracy: 0.8590
Epoch 23/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.3222 - accuracy: 0.8474
Epoch 23: val_loss did not improve from 0.30764
852/852 [==============================] - 2s 2ms/step - loss: 0.3228 - accuracy: 0.8475 - val_loss: 0.3192 - val_accuracy: 0.8539
Epoch 24/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.3224 - accuracy: 0.8524
Epoch 24: val_loss did not improve from 0.30764
852/852 [==============================] - 2s 2ms/step - loss: 0.3235 - accuracy: 0.8516 - val_loss: 0.3086 - val_accuracy: 0.8590
Epoch 25/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.3330 - accuracy: 0.8415
Epoch 25: val_loss did not improve from 0.30764
852/852 [==============================] - 2s 2ms/step - loss: 0.3332 - accuracy: 0.8422 - val_loss: 0.3243 - val_accuracy: 0.8516
Epoch 26/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.3375 - accuracy: 0.8452
Epoch 26: val_loss did not improve from 0.30764
852/852 [==============================] - 2s 2ms/step - loss: 0.3366 - accuracy: 0.8450 - val_loss: 0.3201 - val_accuracy: 0.8508
Epoch 27/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.3262 - accuracy: 0.8510
Epoch 27: val_loss improved from 0.30764 to 0.29863, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3270 - accuracy: 0.8499 - val_loss: 0.2986 - val_accuracy: 0.8645
Epoch 28/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.3311 - accuracy: 0.8451
Epoch 28: val_loss did not improve from 0.29863
852/852 [==============================] - 2s 2ms/step - loss: 0.3320 - accuracy: 0.8445 - val_loss: 0.3084 - val_accuracy: 0.8579
Epoch 29/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.3233 - accuracy: 0.8528
Epoch 29: val_loss did not improve from 0.29863
852/852 [==============================] - 2s 2ms/step - loss: 0.3249 - accuracy: 0.8516 - val_loss: 0.3110 - val_accuracy: 0.8605
Epoch 30/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.3268 - accuracy: 0.8467
Epoch 30: val_loss did not improve from 0.29863
852/852 [==============================] - 2s 2ms/step - loss: 0.3260 - accuracy: 0.8466 - val_loss: 0.3115 - val_accuracy: 0.8596
Epoch 31/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.3203 - accuracy: 0.8511
Epoch 31: val_loss did not improve from 0.29863
852/852 [==============================] - 2s 2ms/step - loss: 0.3199 - accuracy: 0.8516 - val_loss: 0.3204 - val_accuracy: 0.8523
Epoch 32/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.3330 - accuracy: 0.8464
Epoch 32: val_loss did not improve from 0.29863
852/852 [==============================] - 2s 2ms/step - loss: 0.3319 - accuracy: 0.8472 - val_loss: 0.2999 - val_accuracy: 0.8636
Epoch 33/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.3210 - accuracy: 0.8477
Epoch 33: val_loss did not improve from 0.29863
852/852 [==============================] - 2s 2ms/step - loss: 0.3205 - accuracy: 0.8482 - val_loss: 0.3057 - val_accuracy: 0.8584
Epoch 34/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.3309 - accuracy: 0.8455
Epoch 34: val_loss did not improve from 0.29863
852/852 [==============================] - 2s 2ms/step - loss: 0.3302 - accuracy: 0.8458 - val_loss: 0.3054 - val_accuracy: 0.8596
Epoch 35/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.3273 - accuracy: 0.8492
Epoch 35: val_loss did not improve from 0.29863
852/852 [==============================] - 2s 2ms/step - loss: 0.3271 - accuracy: 0.8493 - val_loss: 0.3122 - val_accuracy: 0.8565
Epoch 36/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.3194 - accuracy: 0.8519
Epoch 36: val_loss did not improve from 0.29863
852/852 [==============================] - 2s 2ms/step - loss: 0.3209 - accuracy: 0.8515 - val_loss: 0.3093 - val_accuracy: 0.8579
Epoch 37/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.3197 - accuracy: 0.8518
Epoch 37: val_loss did not improve from 0.29863
852/852 [==============================] - 2s 2ms/step - loss: 0.3204 - accuracy: 0.8515 - val_loss: 0.3210 - val_accuracy: 0.8499
Epoch 38/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.3253 - accuracy: 0.8459
Epoch 38: val_loss did not improve from 0.29863
852/852 [==============================] - 2s 2ms/step - loss: 0.3250 - accuracy: 0.8463 - val_loss: 0.3078 - val_accuracy: 0.8594
Epoch 39/150
814/852 [===========================&gt;..] - ETA: 0s - loss: 0.3294 - accuracy: 0.8462
Epoch 39: val_loss did not improve from 0.29863
852/852 [==============================] - 2s 2ms/step - loss: 0.3296 - accuracy: 0.8466 - val_loss: 0.3087 - val_accuracy: 0.8618
Epoch 40/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.3240 - accuracy: 0.8483
Epoch 40: val_loss did not improve from 0.29863
852/852 [==============================] - 2s 2ms/step - loss: 0.3237 - accuracy: 0.8485 - val_loss: 0.3108 - val_accuracy: 0.8563
Epoch 41/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.3200 - accuracy: 0.8526
Epoch 41: val_loss did not improve from 0.29863
852/852 [==============================] - 2s 2ms/step - loss: 0.3208 - accuracy: 0.8515 - val_loss: 0.3293 - val_accuracy: 0.8491
Epoch 42/150
814/852 [===========================&gt;..] - ETA: 0s - loss: 0.3223 - accuracy: 0.8498
Epoch 42: val_loss did not improve from 0.29863
852/852 [==============================] - 2s 2ms/step - loss: 0.3222 - accuracy: 0.8493 - val_loss: 0.3070 - val_accuracy: 0.8612
Epoch 43/150
813/852 [===========================&gt;..] - ETA: 0s - loss: 0.3276 - accuracy: 0.8485
Epoch 43: val_loss improved from 0.29863 to 0.29827, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3267 - accuracy: 0.8486 - val_loss: 0.2983 - val_accuracy: 0.8646
Epoch 44/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.3197 - accuracy: 0.8526
Epoch 44: val_loss did not improve from 0.29827
852/852 [==============================] - 2s 2ms/step - loss: 0.3202 - accuracy: 0.8524 - val_loss: 0.3051 - val_accuracy: 0.8596
Epoch 45/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.3198 - accuracy: 0.8520
Epoch 45: val_loss improved from 0.29827 to 0.29821, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3192 - accuracy: 0.8524 - val_loss: 0.2982 - val_accuracy: 0.8639
Epoch 46/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.3202 - accuracy: 0.8540
Epoch 46: val_loss improved from 0.29821 to 0.29444, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3198 - accuracy: 0.8546 - val_loss: 0.2944 - val_accuracy: 0.8667
Epoch 47/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.3172 - accuracy: 0.8536
Epoch 47: val_loss did not improve from 0.29444
852/852 [==============================] - 2s 2ms/step - loss: 0.3180 - accuracy: 0.8527 - val_loss: 0.3229 - val_accuracy: 0.8496
Epoch 48/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.3192 - accuracy: 0.8529
Epoch 48: val_loss did not improve from 0.29444
852/852 [==============================] - 2s 2ms/step - loss: 0.3205 - accuracy: 0.8517 - val_loss: 0.3086 - val_accuracy: 0.8587
Epoch 49/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.3244 - accuracy: 0.8462
Epoch 49: val_loss did not improve from 0.29444
852/852 [==============================] - 2s 2ms/step - loss: 0.3239 - accuracy: 0.8464 - val_loss: 0.2998 - val_accuracy: 0.8637
Epoch 50/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.3232 - accuracy: 0.8508
Epoch 50: val_loss did not improve from 0.29444
852/852 [==============================] - 2s 2ms/step - loss: 0.3240 - accuracy: 0.8502 - val_loss: 0.3198 - val_accuracy: 0.8525
Epoch 51/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.3240 - accuracy: 0.8508
Epoch 51: val_loss did not improve from 0.29444
852/852 [==============================] - 2s 2ms/step - loss: 0.3244 - accuracy: 0.8504 - val_loss: 0.3098 - val_accuracy: 0.8591
Epoch 52/150
811/852 [===========================&gt;..] - ETA: 0s - loss: 0.3175 - accuracy: 0.8514
Epoch 52: val_loss did not improve from 0.29444
852/852 [==============================] - 2s 2ms/step - loss: 0.3171 - accuracy: 0.8518 - val_loss: 0.3059 - val_accuracy: 0.8598
Epoch 53/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.3202 - accuracy: 0.8527
Epoch 53: val_loss did not improve from 0.29444
852/852 [==============================] - 2s 2ms/step - loss: 0.3205 - accuracy: 0.8524 - val_loss: 0.3121 - val_accuracy: 0.8552
Epoch 54/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.3261 - accuracy: 0.8486
Epoch 54: val_loss did not improve from 0.29444
852/852 [==============================] - 2s 2ms/step - loss: 0.3251 - accuracy: 0.8489 - val_loss: 0.3036 - val_accuracy: 0.8596
Epoch 55/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.3211 - accuracy: 0.8499
Epoch 55: val_loss did not improve from 0.29444
852/852 [==============================] - 2s 2ms/step - loss: 0.3211 - accuracy: 0.8500 - val_loss: 0.3093 - val_accuracy: 0.8578
Epoch 56/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.3171 - accuracy: 0.8496
Epoch 56: val_loss did not improve from 0.29444
852/852 [==============================] - 2s 2ms/step - loss: 0.3169 - accuracy: 0.8496 - val_loss: 0.3059 - val_accuracy: 0.8587
Epoch 57/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.3133 - accuracy: 0.8558
Epoch 57: val_loss improved from 0.29444 to 0.28866, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3151 - accuracy: 0.8552 - val_loss: 0.2887 - val_accuracy: 0.8679
Epoch 58/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.3083 - accuracy: 0.8583
Epoch 58: val_loss did not improve from 0.28866
852/852 [==============================] - 2s 2ms/step - loss: 0.3105 - accuracy: 0.8573 - val_loss: 0.3058 - val_accuracy: 0.8609
Epoch 59/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.3242 - accuracy: 0.8486
Epoch 59: val_loss did not improve from 0.28866
852/852 [==============================] - 2s 2ms/step - loss: 0.3239 - accuracy: 0.8488 - val_loss: 0.3035 - val_accuracy: 0.8601
Epoch 60/150
813/852 [===========================&gt;..] - ETA: 0s - loss: 0.3189 - accuracy: 0.8540
Epoch 60: val_loss did not improve from 0.28866
852/852 [==============================] - 2s 2ms/step - loss: 0.3207 - accuracy: 0.8530 - val_loss: 0.3032 - val_accuracy: 0.8614
Epoch 61/150
808/852 [===========================&gt;..] - ETA: 0s - loss: 0.3199 - accuracy: 0.8512
Epoch 61: val_loss did not improve from 0.28866
852/852 [==============================] - 2s 2ms/step - loss: 0.3231 - accuracy: 0.8500 - val_loss: 0.3169 - val_accuracy: 0.8520
Epoch 62/150
814/852 [===========================&gt;..] - ETA: 0s - loss: 0.3148 - accuracy: 0.8549
Epoch 62: val_loss did not improve from 0.28866
852/852 [==============================] - 2s 2ms/step - loss: 0.3171 - accuracy: 0.8532 - val_loss: 0.3201 - val_accuracy: 0.8489
Epoch 63/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.3131 - accuracy: 0.8515
Epoch 63: val_loss did not improve from 0.28866
852/852 [==============================] - 2s 2ms/step - loss: 0.3142 - accuracy: 0.8515 - val_loss: 0.2921 - val_accuracy: 0.8679
Epoch 64/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.3160 - accuracy: 0.8528
Epoch 64: val_loss did not improve from 0.28866
852/852 [==============================] - 2s 2ms/step - loss: 0.3156 - accuracy: 0.8527 - val_loss: 0.3113 - val_accuracy: 0.8606
Epoch 65/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.3191 - accuracy: 0.8529
Epoch 65: val_loss did not improve from 0.28866
852/852 [==============================] - 2s 2ms/step - loss: 0.3202 - accuracy: 0.8523 - val_loss: 0.3007 - val_accuracy: 0.8624
Epoch 66/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.3170 - accuracy: 0.8502
Epoch 66: val_loss did not improve from 0.28866
852/852 [==============================] - 2s 2ms/step - loss: 0.3169 - accuracy: 0.8496 - val_loss: 0.2959 - val_accuracy: 0.8626
Epoch 67/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.3186 - accuracy: 0.8531
Epoch 67: val_loss did not improve from 0.28866
852/852 [==============================] - 2s 2ms/step - loss: 0.3191 - accuracy: 0.8525 - val_loss: 0.2966 - val_accuracy: 0.8632
Epoch 68/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.3214 - accuracy: 0.8517
Epoch 68: val_loss did not improve from 0.28866
852/852 [==============================] - 2s 2ms/step - loss: 0.3211 - accuracy: 0.8519 - val_loss: 0.2966 - val_accuracy: 0.8652
Epoch 69/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.3120 - accuracy: 0.8544
Epoch 69: val_loss did not improve from 0.28866
852/852 [==============================] - 2s 2ms/step - loss: 0.3135 - accuracy: 0.8537 - val_loss: 0.3094 - val_accuracy: 0.8597
Epoch 70/150
852/852 [==============================] - ETA: 0s - loss: 0.3147 - accuracy: 0.8529
Epoch 70: val_loss did not improve from 0.28866
852/852 [==============================] - 2s 2ms/step - loss: 0.3147 - accuracy: 0.8529 - val_loss: 0.3037 - val_accuracy: 0.8630
Epoch 71/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.3128 - accuracy: 0.8564
Epoch 71: val_loss improved from 0.28866 to 0.28657, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3138 - accuracy: 0.8554 - val_loss: 0.2866 - val_accuracy: 0.8672
Epoch 72/150
852/852 [==============================] - ETA: 0s - loss: 0.3138 - accuracy: 0.8510
Epoch 72: val_loss did not improve from 0.28657
852/852 [==============================] - 2s 2ms/step - loss: 0.3138 - accuracy: 0.8510 - val_loss: 0.2928 - val_accuracy: 0.8675
Epoch 73/150
820/852 [===========================&gt;..] - ETA: 0s - loss: 0.3124 - accuracy: 0.8555
Epoch 73: val_loss did not improve from 0.28657
852/852 [==============================] - 2s 2ms/step - loss: 0.3143 - accuracy: 0.8552 - val_loss: 0.3100 - val_accuracy: 0.8569
Epoch 74/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.3183 - accuracy: 0.8537
Epoch 74: val_loss did not improve from 0.28657
852/852 [==============================] - 2s 2ms/step - loss: 0.3187 - accuracy: 0.8530 - val_loss: 0.2964 - val_accuracy: 0.8638
Epoch 75/150
810/852 [===========================&gt;..] - ETA: 0s - loss: 0.3087 - accuracy: 0.8583
Epoch 75: val_loss did not improve from 0.28657
852/852 [==============================] - 2s 2ms/step - loss: 0.3068 - accuracy: 0.8592 - val_loss: 0.2990 - val_accuracy: 0.8627
Epoch 76/150
818/852 [===========================&gt;..] - ETA: 0s - loss: 0.3132 - accuracy: 0.8527
Epoch 76: val_loss did not improve from 0.28657
852/852 [==============================] - 2s 2ms/step - loss: 0.3122 - accuracy: 0.8535 - val_loss: 0.2907 - val_accuracy: 0.8701
Epoch 77/150
814/852 [===========================&gt;..] - ETA: 0s - loss: 0.3069 - accuracy: 0.8581
Epoch 77: val_loss did not improve from 0.28657
852/852 [==============================] - 2s 2ms/step - loss: 0.3091 - accuracy: 0.8567 - val_loss: 0.3202 - val_accuracy: 0.8539
Epoch 78/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.3150 - accuracy: 0.8532
Epoch 78: val_loss did not improve from 0.28657
852/852 [==============================] - 2s 2ms/step - loss: 0.3155 - accuracy: 0.8531 - val_loss: 0.2941 - val_accuracy: 0.8677
Epoch 79/150
814/852 [===========================&gt;..] - ETA: 0s - loss: 0.3086 - accuracy: 0.8597
Epoch 79: val_loss did not improve from 0.28657
852/852 [==============================] - 2s 2ms/step - loss: 0.3095 - accuracy: 0.8591 - val_loss: 0.3007 - val_accuracy: 0.8623
Epoch 80/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.3213 - accuracy: 0.8514
Epoch 80: val_loss did not improve from 0.28657
852/852 [==============================] - 2s 2ms/step - loss: 0.3215 - accuracy: 0.8512 - val_loss: 0.3220 - val_accuracy: 0.8493
Epoch 81/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.3088 - accuracy: 0.8586
Epoch 81: val_loss did not improve from 0.28657
852/852 [==============================] - 2s 2ms/step - loss: 0.3089 - accuracy: 0.8586 - val_loss: 0.2898 - val_accuracy: 0.8698
Epoch 82/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.3144 - accuracy: 0.8525
Epoch 82: val_loss did not improve from 0.28657
852/852 [==============================] - 2s 2ms/step - loss: 0.3131 - accuracy: 0.8530 - val_loss: 0.3042 - val_accuracy: 0.8597
Epoch 83/150
811/852 [===========================&gt;..] - ETA: 0s - loss: 0.3082 - accuracy: 0.8586
Epoch 83: val_loss did not improve from 0.28657
852/852 [==============================] - 2s 2ms/step - loss: 0.3087 - accuracy: 0.8581 - val_loss: 0.2962 - val_accuracy: 0.8647
Epoch 84/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.3070 - accuracy: 0.8565
Epoch 84: val_loss did not improve from 0.28657
852/852 [==============================] - 2s 2ms/step - loss: 0.3071 - accuracy: 0.8565 - val_loss: 0.2894 - val_accuracy: 0.8665
Epoch 85/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.3127 - accuracy: 0.8559
Epoch 85: val_loss did not improve from 0.28657
852/852 [==============================] - 2s 2ms/step - loss: 0.3120 - accuracy: 0.8560 - val_loss: 0.3042 - val_accuracy: 0.8601
Epoch 86/150
818/852 [===========================&gt;..] - ETA: 0s - loss: 0.3147 - accuracy: 0.8517
Epoch 86: val_loss did not improve from 0.28657
852/852 [==============================] - 2s 2ms/step - loss: 0.3122 - accuracy: 0.8533 - val_loss: 0.2920 - val_accuracy: 0.8658
Epoch 87/150
812/852 [===========================&gt;..] - ETA: 0s - loss: 0.3073 - accuracy: 0.8573
Epoch 87: val_loss did not improve from 0.28657
852/852 [==============================] - 2s 2ms/step - loss: 0.3086 - accuracy: 0.8565 - val_loss: 0.2938 - val_accuracy: 0.8702
Epoch 88/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.3123 - accuracy: 0.8573
Epoch 88: val_loss did not improve from 0.28657
852/852 [==============================] - 2s 2ms/step - loss: 0.3117 - accuracy: 0.8577 - val_loss: 0.3101 - val_accuracy: 0.8590
Epoch 89/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.3076 - accuracy: 0.8594
Epoch 89: val_loss did not improve from 0.28657
852/852 [==============================] - 2s 2ms/step - loss: 0.3070 - accuracy: 0.8598 - val_loss: 0.3131 - val_accuracy: 0.8591
Epoch 90/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.3150 - accuracy: 0.8558
Epoch 90: val_loss did not improve from 0.28657
852/852 [==============================] - 2s 2ms/step - loss: 0.3140 - accuracy: 0.8565 - val_loss: 0.3133 - val_accuracy: 0.8559
Epoch 91/150
852/852 [==============================] - ETA: 0s - loss: 0.3151 - accuracy: 0.8517
Epoch 91: val_loss did not improve from 0.28657
852/852 [==============================] - 2s 2ms/step - loss: 0.3151 - accuracy: 0.8517 - val_loss: 0.2925 - val_accuracy: 0.8665
Epoch 91: early stopping
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=9a116e6b-7bb3-4275-a8e4-91862f2ae431">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [76]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">history_125</span> <span class="o">=</span> <span class="n">one_twentyfive_neuron</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">callback_a</span><span class="p">,</span> <span class="n">callback_b</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.3205 - accuracy: 0.8502
Epoch 1: val_loss did not improve from 0.28657
852/852 [==============================] - 2s 2ms/step - loss: 0.3203 - accuracy: 0.8505 - val_loss: 0.2881 - val_accuracy: 0.8685
Epoch 2/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.3026 - accuracy: 0.8584
Epoch 2: val_loss did not improve from 0.28657
852/852 [==============================] - 2s 2ms/step - loss: 0.3051 - accuracy: 0.8573 - val_loss: 0.2946 - val_accuracy: 0.8664
Epoch 3/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.3105 - accuracy: 0.8600
Epoch 3: val_loss improved from 0.28657 to 0.28574, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3094 - accuracy: 0.8605 - val_loss: 0.2857 - val_accuracy: 0.8727
Epoch 4/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.3092 - accuracy: 0.8554
Epoch 4: val_loss did not improve from 0.28574
852/852 [==============================] - 2s 2ms/step - loss: 0.3091 - accuracy: 0.8553 - val_loss: 0.2882 - val_accuracy: 0.8725
Epoch 5/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.3055 - accuracy: 0.8602
Epoch 5: val_loss did not improve from 0.28574
852/852 [==============================] - 2s 2ms/step - loss: 0.3035 - accuracy: 0.8616 - val_loss: 0.3020 - val_accuracy: 0.8609
Epoch 6/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.3093 - accuracy: 0.8553
Epoch 6: val_loss did not improve from 0.28574
852/852 [==============================] - 2s 2ms/step - loss: 0.3105 - accuracy: 0.8556 - val_loss: 0.3294 - val_accuracy: 0.8503
Epoch 7/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.3100 - accuracy: 0.8583
Epoch 7: val_loss did not improve from 0.28574
852/852 [==============================] - 2s 2ms/step - loss: 0.3106 - accuracy: 0.8578 - val_loss: 0.2929 - val_accuracy: 0.8679
Epoch 8/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.3126 - accuracy: 0.8571
Epoch 8: val_loss did not improve from 0.28574
852/852 [==============================] - 2s 2ms/step - loss: 0.3113 - accuracy: 0.8584 - val_loss: 0.2905 - val_accuracy: 0.8639
Epoch 9/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.3092 - accuracy: 0.8575
Epoch 9: val_loss did not improve from 0.28574
852/852 [==============================] - 2s 2ms/step - loss: 0.3096 - accuracy: 0.8570 - val_loss: 0.2933 - val_accuracy: 0.8661
Epoch 10/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.3058 - accuracy: 0.8610
Epoch 10: val_loss improved from 0.28574 to 0.28511, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3060 - accuracy: 0.8609 - val_loss: 0.2851 - val_accuracy: 0.8700
Epoch 11/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.3082 - accuracy: 0.8545
Epoch 11: val_loss did not improve from 0.28511
852/852 [==============================] - 2s 2ms/step - loss: 0.3082 - accuracy: 0.8545 - val_loss: 0.2855 - val_accuracy: 0.8691
Epoch 12/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.3070 - accuracy: 0.8566
Epoch 12: val_loss did not improve from 0.28511
852/852 [==============================] - 2s 2ms/step - loss: 0.3072 - accuracy: 0.8559 - val_loss: 0.2978 - val_accuracy: 0.8652
Epoch 13/150
814/852 [===========================&gt;..] - ETA: 0s - loss: 0.3084 - accuracy: 0.8588
Epoch 13: val_loss did not improve from 0.28511
852/852 [==============================] - 2s 3ms/step - loss: 0.3072 - accuracy: 0.8596 - val_loss: 0.2889 - val_accuracy: 0.8670
Epoch 14/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.3046 - accuracy: 0.8603
Epoch 14: val_loss did not improve from 0.28511
852/852 [==============================] - 3s 4ms/step - loss: 0.3050 - accuracy: 0.8600 - val_loss: 0.2943 - val_accuracy: 0.8657
Epoch 15/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.3053 - accuracy: 0.8599
Epoch 15: val_loss did not improve from 0.28511
852/852 [==============================] - 2s 3ms/step - loss: 0.3058 - accuracy: 0.8593 - val_loss: 0.2933 - val_accuracy: 0.8660
Epoch 16/150
809/852 [===========================&gt;..] - ETA: 0s - loss: 0.3153 - accuracy: 0.8550
Epoch 16: val_loss did not improve from 0.28511
852/852 [==============================] - 2s 2ms/step - loss: 0.3141 - accuracy: 0.8562 - val_loss: 0.2894 - val_accuracy: 0.8708
Epoch 17/150
815/852 [===========================&gt;..] - ETA: 0s - loss: 0.3115 - accuracy: 0.8577
Epoch 17: val_loss did not improve from 0.28511
852/852 [==============================] - 2s 2ms/step - loss: 0.3118 - accuracy: 0.8574 - val_loss: 0.2923 - val_accuracy: 0.8674
Epoch 18/150
811/852 [===========================&gt;..] - ETA: 0s - loss: 0.3029 - accuracy: 0.8584
Epoch 18: val_loss did not improve from 0.28511
852/852 [==============================] - 2s 2ms/step - loss: 0.3044 - accuracy: 0.8572 - val_loss: 0.2912 - val_accuracy: 0.8673
Epoch 19/150
818/852 [===========================&gt;..] - ETA: 0s - loss: 0.3027 - accuracy: 0.8597
Epoch 19: val_loss did not improve from 0.28511
852/852 [==============================] - 2s 2ms/step - loss: 0.3075 - accuracy: 0.8566 - val_loss: 0.3075 - val_accuracy: 0.8633
Epoch 20/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.2957 - accuracy: 0.8624
Epoch 20: val_loss did not improve from 0.28511
852/852 [==============================] - 2s 2ms/step - loss: 0.2966 - accuracy: 0.8619 - val_loss: 0.2887 - val_accuracy: 0.8709
Epoch 21/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.3068 - accuracy: 0.8564
Epoch 21: val_loss did not improve from 0.28511
852/852 [==============================] - 2s 2ms/step - loss: 0.3069 - accuracy: 0.8567 - val_loss: 0.3095 - val_accuracy: 0.8578
Epoch 22/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.3027 - accuracy: 0.8599
Epoch 22: val_loss did not improve from 0.28511
852/852 [==============================] - 2s 2ms/step - loss: 0.3027 - accuracy: 0.8599 - val_loss: 0.2858 - val_accuracy: 0.8708
Epoch 23/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.3031 - accuracy: 0.8596
Epoch 23: val_loss did not improve from 0.28511
852/852 [==============================] - 2s 2ms/step - loss: 0.3049 - accuracy: 0.8585 - val_loss: 0.3019 - val_accuracy: 0.8655
Epoch 24/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.3126 - accuracy: 0.8539
Epoch 24: val_loss did not improve from 0.28511
852/852 [==============================] - 2s 2ms/step - loss: 0.3125 - accuracy: 0.8544 - val_loss: 0.3188 - val_accuracy: 0.8553
Epoch 25/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.3002 - accuracy: 0.8674
Epoch 25: val_loss did not improve from 0.28511
852/852 [==============================] - 2s 2ms/step - loss: 0.3028 - accuracy: 0.8650 - val_loss: 0.2891 - val_accuracy: 0.8670
Epoch 26/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.3065 - accuracy: 0.8597
Epoch 26: val_loss improved from 0.28511 to 0.27680, saving model to one_twentyfive_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3067 - accuracy: 0.8596 - val_loss: 0.2768 - val_accuracy: 0.8762
Epoch 27/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.3070 - accuracy: 0.8612
Epoch 27: val_loss did not improve from 0.27680
852/852 [==============================] - 2s 2ms/step - loss: 0.3068 - accuracy: 0.8612 - val_loss: 0.2901 - val_accuracy: 0.8691
Epoch 28/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.3042 - accuracy: 0.8618
Epoch 28: val_loss did not improve from 0.27680
852/852 [==============================] - 2s 2ms/step - loss: 0.3032 - accuracy: 0.8624 - val_loss: 0.2869 - val_accuracy: 0.8715
Epoch 29/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.2980 - accuracy: 0.8624
Epoch 29: val_loss did not improve from 0.27680
852/852 [==============================] - 2s 2ms/step - loss: 0.2985 - accuracy: 0.8620 - val_loss: 0.2864 - val_accuracy: 0.8700
Epoch 30/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.3060 - accuracy: 0.8603
Epoch 30: val_loss did not improve from 0.27680
852/852 [==============================] - 2s 2ms/step - loss: 0.3062 - accuracy: 0.8604 - val_loss: 0.2778 - val_accuracy: 0.8729
Epoch 31/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.2971 - accuracy: 0.8628
Epoch 31: val_loss did not improve from 0.27680
852/852 [==============================] - 2s 2ms/step - loss: 0.2961 - accuracy: 0.8632 - val_loss: 0.2817 - val_accuracy: 0.8728
Epoch 32/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.2996 - accuracy: 0.8629
Epoch 32: val_loss did not improve from 0.27680
852/852 [==============================] - 2s 2ms/step - loss: 0.2998 - accuracy: 0.8628 - val_loss: 0.2800 - val_accuracy: 0.8694
Epoch 33/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.3042 - accuracy: 0.8601
Epoch 33: val_loss did not improve from 0.27680
852/852 [==============================] - 2s 2ms/step - loss: 0.3040 - accuracy: 0.8600 - val_loss: 0.2948 - val_accuracy: 0.8659
Epoch 34/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.3116 - accuracy: 0.8576
Epoch 34: val_loss did not improve from 0.27680
852/852 [==============================] - 2s 2ms/step - loss: 0.3117 - accuracy: 0.8578 - val_loss: 0.2862 - val_accuracy: 0.8705
Epoch 35/150
818/852 [===========================&gt;..] - ETA: 0s - loss: 0.2995 - accuracy: 0.8630
Epoch 35: val_loss did not improve from 0.27680
852/852 [==============================] - 2s 2ms/step - loss: 0.2999 - accuracy: 0.8626 - val_loss: 0.2804 - val_accuracy: 0.8727
Epoch 36/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.3002 - accuracy: 0.8620
Epoch 36: val_loss did not improve from 0.27680
852/852 [==============================] - 2s 2ms/step - loss: 0.3017 - accuracy: 0.8616 - val_loss: 0.2932 - val_accuracy: 0.8671
Epoch 37/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.3003 - accuracy: 0.8620
Epoch 37: val_loss did not improve from 0.27680
852/852 [==============================] - 2s 2ms/step - loss: 0.3013 - accuracy: 0.8614 - val_loss: 0.2977 - val_accuracy: 0.8665
Epoch 38/150
811/852 [===========================&gt;..] - ETA: 0s - loss: 0.3083 - accuracy: 0.8589
Epoch 38: val_loss did not improve from 0.27680
852/852 [==============================] - 2s 2ms/step - loss: 0.3074 - accuracy: 0.8600 - val_loss: 0.2794 - val_accuracy: 0.8728
Epoch 39/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.2974 - accuracy: 0.8610
Epoch 39: val_loss did not improve from 0.27680
852/852 [==============================] - 2s 2ms/step - loss: 0.2990 - accuracy: 0.8601 - val_loss: 0.2790 - val_accuracy: 0.8717
Epoch 40/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.2978 - accuracy: 0.8624
Epoch 40: val_loss did not improve from 0.27680
852/852 [==============================] - 2s 2ms/step - loss: 0.2970 - accuracy: 0.8628 - val_loss: 0.2942 - val_accuracy: 0.8675
Epoch 41/150
820/852 [===========================&gt;..] - ETA: 0s - loss: 0.3044 - accuracy: 0.8559
Epoch 41: val_loss did not improve from 0.27680
852/852 [==============================] - 2s 2ms/step - loss: 0.3055 - accuracy: 0.8554 - val_loss: 0.2884 - val_accuracy: 0.8693
Epoch 42/150
813/852 [===========================&gt;..] - ETA: 0s - loss: 0.3054 - accuracy: 0.8599
Epoch 42: val_loss did not improve from 0.27680
852/852 [==============================] - 2s 2ms/step - loss: 0.3047 - accuracy: 0.8605 - val_loss: 0.3036 - val_accuracy: 0.8638
Epoch 43/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.2980 - accuracy: 0.8623
Epoch 43: val_loss did not improve from 0.27680
852/852 [==============================] - 2s 2ms/step - loss: 0.2975 - accuracy: 0.8626 - val_loss: 0.2829 - val_accuracy: 0.8701
Epoch 44/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.3040 - accuracy: 0.8600
Epoch 44: val_loss did not improve from 0.27680
852/852 [==============================] - 2s 2ms/step - loss: 0.3040 - accuracy: 0.8599 - val_loss: 0.2847 - val_accuracy: 0.8728
Epoch 45/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.2969 - accuracy: 0.8625
Epoch 45: val_loss did not improve from 0.27680
852/852 [==============================] - 2s 2ms/step - loss: 0.2967 - accuracy: 0.8626 - val_loss: 0.2866 - val_accuracy: 0.8717
Epoch 46/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.2954 - accuracy: 0.8650
Epoch 46: val_loss did not improve from 0.27680
852/852 [==============================] - 2s 2ms/step - loss: 0.2956 - accuracy: 0.8647 - val_loss: 0.2864 - val_accuracy: 0.8680
Epoch 46: early stopping
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=f3acfcc1-1401-42a3-add3-a990f450299d">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [87]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">one_twentyfive_neuron</span><span class="o">.</span><span class="n">load_weights</span><span class="p">(</span><span class="s1">'one_twentyfive_model.hdf5'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=cb47d787-8a24-4b90-8d4e-2269d09b709f">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [88]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Other Metrics</span>
<span class="n">one_twentyfive_neuron_preds</span> <span class="o">=</span> <span class="n">one_twentyfive_neuron</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">one_twentyfive_neuron_preds</span><span class="o">.</span><span class="n">round</span><span class="p">())</span>
<span class="n">precision</span> <span class="o">=</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">one_twentyfive_neuron_preds</span><span class="o">.</span><span class="n">round</span><span class="p">())</span>
<span class="n">recall</span> <span class="o">=</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">one_twentyfive_neuron_preds</span><span class="o">.</span><span class="n">round</span><span class="p">())</span>
<span class="n">f1score</span> <span class="o">=</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">one_twentyfive_neuron_preds</span><span class="o">.</span><span class="n">round</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Accuracy: </span><span class="si">%.2f</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">accuracy</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Precision: </span><span class="si">%.2f</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">precision</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Recall: </span><span class="si">%.2f</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">recall</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"F1-score: </span><span class="si">%.2f</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">f1score</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>267/267 [==============================] - 0s 1ms/step
Accuracy: 0.88
Precision: 0.94
Recall: 0.80
F1-score: 0.87
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=a9f84414-5337-4a43-8a37-b76a3a4f0063">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="2.8-Build-a-Model-with-249-Neurons-in-6-Layers">2.8 Build a Model with 249 Neurons in 6 Layers<a class="anchor-link" href="#2.8-Build-a-Model-with-249-Neurons-in-6-Layers">¶</a></h4>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=5cd0eb86-273b-4c39-b902-9ed965d8d5ca">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [77]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># constructing a 249 neuron model</span>
<span class="n">two_fortynine_neuron</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">two_fortynine_neuron</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">input_dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">))</span>
<span class="n">two_fortynine_neuron</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">))</span>
<span class="n">two_fortynine_neuron</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">))</span>
<span class="n">two_fortynine_neuron</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">))</span>
<span class="n">two_fortynine_neuron</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">))</span>
<span class="n">two_fortynine_neuron</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'sigmoid'</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">two_fortynine_neuron</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>

<span class="c1">#Check for cycles in the Sequential Model</span>
<span class="n">two_fortynine_neuron</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">'binary_crossentropy'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s1">'adam'</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'accuracy'</span><span class="p">])</span>
<span class="n">callback_a</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">filepath</span> <span class="o">=</span> <span class="s1">'two_fortynine_model.hdf5'</span><span class="p">,</span> <span class="n">monitor</span><span class="o">=</span><span class="s1">'val_loss'</span><span class="p">,</span> <span class="n">save_best_only</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">save_weights_only</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">verbose</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">callback_b</span> <span class="o">=</span> <span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s1">'val_loss'</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">'min'</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">history_249</span> <span class="o">=</span> <span class="n">two_fortynine_neuron</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">callback_a</span><span class="p">,</span> <span class="n">callback_b</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Model: "sequential_18"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense_42 (Dense)            (None, 128)               3456      
                                                                 
 dense_43 (Dense)            (None, 64)                8256      
                                                                 
 dense_44 (Dense)            (None, 32)                2080      
                                                                 
 dense_45 (Dense)            (None, 16)                528       
                                                                 
 dense_46 (Dense)            (None, 8)                 136       
                                                                 
 dense_47 (Dense)            (None, 1)                 9         
                                                                 
=================================================================
Total params: 14465 (56.50 KB)
Trainable params: 14465 (56.50 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
None
Epoch 1/150
814/852 [===========================&gt;..] - ETA: 0s - loss: 0.5749 - accuracy: 0.7107
Epoch 1: val_loss improved from inf to 0.56009, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 3s 2ms/step - loss: 0.5730 - accuracy: 0.7131 - val_loss: 0.5601 - val_accuracy: 0.7265
Epoch 2/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.5609 - accuracy: 0.7214
Epoch 2: val_loss improved from 0.56009 to 0.55740, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5614 - accuracy: 0.7213 - val_loss: 0.5574 - val_accuracy: 0.7256
Epoch 3/150
806/852 [===========================&gt;..] - ETA: 0s - loss: 0.5569 - accuracy: 0.7242
Epoch 3: val_loss improved from 0.55740 to 0.54836, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5561 - accuracy: 0.7257 - val_loss: 0.5484 - val_accuracy: 0.7318
Epoch 4/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.5532 - accuracy: 0.7290
Epoch 4: val_loss improved from 0.54836 to 0.54643, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5527 - accuracy: 0.7295 - val_loss: 0.5464 - val_accuracy: 0.7329
Epoch 5/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.5505 - accuracy: 0.7301
Epoch 5: val_loss improved from 0.54643 to 0.54558, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5506 - accuracy: 0.7300 - val_loss: 0.5456 - val_accuracy: 0.7324
Epoch 6/150
807/852 [===========================&gt;..] - ETA: 0s - loss: 0.5514 - accuracy: 0.7299
Epoch 6: val_loss improved from 0.54558 to 0.54152, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5503 - accuracy: 0.7303 - val_loss: 0.5415 - val_accuracy: 0.7347
Epoch 7/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.5482 - accuracy: 0.7313
Epoch 7: val_loss did not improve from 0.54152
852/852 [==============================] - 2s 2ms/step - loss: 0.5475 - accuracy: 0.7316 - val_loss: 0.5449 - val_accuracy: 0.7385
Epoch 8/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.5460 - accuracy: 0.7337
Epoch 8: val_loss improved from 0.54152 to 0.53908, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5463 - accuracy: 0.7331 - val_loss: 0.5391 - val_accuracy: 0.7395
Epoch 9/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.5451 - accuracy: 0.7343
Epoch 9: val_loss improved from 0.53908 to 0.53516, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5446 - accuracy: 0.7344 - val_loss: 0.5352 - val_accuracy: 0.7420
Epoch 10/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.5429 - accuracy: 0.7345
Epoch 10: val_loss improved from 0.53516 to 0.53366, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5430 - accuracy: 0.7344 - val_loss: 0.5337 - val_accuracy: 0.7406
Epoch 11/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.5403 - accuracy: 0.7383
Epoch 11: val_loss improved from 0.53366 to 0.53267, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5407 - accuracy: 0.7380 - val_loss: 0.5327 - val_accuracy: 0.7373
Epoch 12/150
811/852 [===========================&gt;..] - ETA: 0s - loss: 0.5391 - accuracy: 0.7376
Epoch 12: val_loss did not improve from 0.53267
852/852 [==============================] - 2s 2ms/step - loss: 0.5384 - accuracy: 0.7376 - val_loss: 0.5340 - val_accuracy: 0.7446
Epoch 13/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.5366 - accuracy: 0.7391
Epoch 13: val_loss improved from 0.53267 to 0.52827, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5365 - accuracy: 0.7395 - val_loss: 0.5283 - val_accuracy: 0.7428
Epoch 14/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.5336 - accuracy: 0.7395
Epoch 14: val_loss improved from 0.52827 to 0.52361, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5331 - accuracy: 0.7398 - val_loss: 0.5236 - val_accuracy: 0.7472
Epoch 15/150
818/852 [===========================&gt;..] - ETA: 0s - loss: 0.5314 - accuracy: 0.7449
Epoch 15: val_loss did not improve from 0.52361
852/852 [==============================] - 2s 2ms/step - loss: 0.5315 - accuracy: 0.7446 - val_loss: 0.5286 - val_accuracy: 0.7413
Epoch 16/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5299 - accuracy: 0.7426
Epoch 16: val_loss improved from 0.52361 to 0.51855, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5307 - accuracy: 0.7427 - val_loss: 0.5186 - val_accuracy: 0.7465
Epoch 17/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.5293 - accuracy: 0.7424
Epoch 17: val_loss did not improve from 0.51855
852/852 [==============================] - 2s 2ms/step - loss: 0.5294 - accuracy: 0.7426 - val_loss: 0.5241 - val_accuracy: 0.7465
Epoch 18/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.5264 - accuracy: 0.7423
Epoch 18: val_loss improved from 0.51855 to 0.51536, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5267 - accuracy: 0.7421 - val_loss: 0.5154 - val_accuracy: 0.7454
Epoch 19/150
820/852 [===========================&gt;..] - ETA: 0s - loss: 0.5222 - accuracy: 0.7472
Epoch 19: val_loss did not improve from 0.51536
852/852 [==============================] - 2s 2ms/step - loss: 0.5233 - accuracy: 0.7459 - val_loss: 0.5156 - val_accuracy: 0.7488
Epoch 20/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.5210 - accuracy: 0.7458
Epoch 20: val_loss improved from 0.51536 to 0.51117, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5206 - accuracy: 0.7459 - val_loss: 0.5112 - val_accuracy: 0.7513
Epoch 21/150
807/852 [===========================&gt;..] - ETA: 0s - loss: 0.5161 - accuracy: 0.7499
Epoch 21: val_loss improved from 0.51117 to 0.50851, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5175 - accuracy: 0.7487 - val_loss: 0.5085 - val_accuracy: 0.7489
Epoch 22/150
813/852 [===========================&gt;..] - ETA: 0s - loss: 0.5163 - accuracy: 0.7466
Epoch 22: val_loss improved from 0.50851 to 0.50090, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5160 - accuracy: 0.7466 - val_loss: 0.5009 - val_accuracy: 0.7562
Epoch 23/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.5132 - accuracy: 0.7489
Epoch 23: val_loss did not improve from 0.50090
852/852 [==============================] - 2s 2ms/step - loss: 0.5141 - accuracy: 0.7478 - val_loss: 0.5014 - val_accuracy: 0.7574
Epoch 24/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.5117 - accuracy: 0.7489
Epoch 24: val_loss improved from 0.50090 to 0.49466, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5112 - accuracy: 0.7492 - val_loss: 0.4947 - val_accuracy: 0.7575
Epoch 25/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.5076 - accuracy: 0.7483
Epoch 25: val_loss did not improve from 0.49466
852/852 [==============================] - 2s 2ms/step - loss: 0.5089 - accuracy: 0.7468 - val_loss: 0.5022 - val_accuracy: 0.7562
Epoch 26/150
807/852 [===========================&gt;..] - ETA: 0s - loss: 0.5075 - accuracy: 0.7513
Epoch 26: val_loss did not improve from 0.49466
852/852 [==============================] - 2s 2ms/step - loss: 0.5083 - accuracy: 0.7508 - val_loss: 0.4970 - val_accuracy: 0.7533
Epoch 27/150
852/852 [==============================] - ETA: 0s - loss: 0.5039 - accuracy: 0.7512
Epoch 27: val_loss improved from 0.49466 to 0.49371, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5039 - accuracy: 0.7512 - val_loss: 0.4937 - val_accuracy: 0.7561
Epoch 28/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.5030 - accuracy: 0.7501
Epoch 28: val_loss improved from 0.49371 to 0.49284, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5029 - accuracy: 0.7502 - val_loss: 0.4928 - val_accuracy: 0.7543
Epoch 29/150
820/852 [===========================&gt;..] - ETA: 0s - loss: 0.5009 - accuracy: 0.7502
Epoch 29: val_loss improved from 0.49284 to 0.48646, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5003 - accuracy: 0.7513 - val_loss: 0.4865 - val_accuracy: 0.7617
Epoch 30/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.4946 - accuracy: 0.7566
Epoch 30: val_loss improved from 0.48646 to 0.48134, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4956 - accuracy: 0.7558 - val_loss: 0.4813 - val_accuracy: 0.7627
Epoch 31/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.4944 - accuracy: 0.7565
Epoch 31: val_loss did not improve from 0.48134
852/852 [==============================] - 2s 2ms/step - loss: 0.4946 - accuracy: 0.7565 - val_loss: 0.4817 - val_accuracy: 0.7608
Epoch 32/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.4924 - accuracy: 0.7563
Epoch 32: val_loss did not improve from 0.48134
852/852 [==============================] - 2s 2ms/step - loss: 0.4925 - accuracy: 0.7559 - val_loss: 0.4844 - val_accuracy: 0.7639
Epoch 33/150
852/852 [==============================] - ETA: 0s - loss: 0.4915 - accuracy: 0.7567
Epoch 33: val_loss did not improve from 0.48134
852/852 [==============================] - 2s 2ms/step - loss: 0.4915 - accuracy: 0.7567 - val_loss: 0.4822 - val_accuracy: 0.7644
Epoch 34/150
819/852 [===========================&gt;..] - ETA: 0s - loss: 0.4891 - accuracy: 0.7587
Epoch 34: val_loss improved from 0.48134 to 0.47930, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4888 - accuracy: 0.7587 - val_loss: 0.4793 - val_accuracy: 0.7600
Epoch 35/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.4876 - accuracy: 0.7574
Epoch 35: val_loss improved from 0.47930 to 0.47117, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4882 - accuracy: 0.7572 - val_loss: 0.4712 - val_accuracy: 0.7646
Epoch 36/150
820/852 [===========================&gt;..] - ETA: 0s - loss: 0.4858 - accuracy: 0.7572
Epoch 36: val_loss improved from 0.47117 to 0.47004, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4865 - accuracy: 0.7572 - val_loss: 0.4700 - val_accuracy: 0.7667
Epoch 37/150
815/852 [===========================&gt;..] - ETA: 0s - loss: 0.4808 - accuracy: 0.7590
Epoch 37: val_loss improved from 0.47004 to 0.46740, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4792 - accuracy: 0.7603 - val_loss: 0.4674 - val_accuracy: 0.7649
Epoch 38/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.4793 - accuracy: 0.7636
Epoch 38: val_loss did not improve from 0.46740
852/852 [==============================] - 2s 2ms/step - loss: 0.4797 - accuracy: 0.7637 - val_loss: 0.4699 - val_accuracy: 0.7656
Epoch 39/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.4768 - accuracy: 0.7625
Epoch 39: val_loss improved from 0.46740 to 0.46234, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4763 - accuracy: 0.7627 - val_loss: 0.4623 - val_accuracy: 0.7702
Epoch 40/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.4750 - accuracy: 0.7617
Epoch 40: val_loss did not improve from 0.46234
852/852 [==============================] - 2s 2ms/step - loss: 0.4748 - accuracy: 0.7627 - val_loss: 0.4677 - val_accuracy: 0.7689
Epoch 41/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.4767 - accuracy: 0.7623
Epoch 41: val_loss did not improve from 0.46234
852/852 [==============================] - 2s 2ms/step - loss: 0.4764 - accuracy: 0.7627 - val_loss: 0.4697 - val_accuracy: 0.7709
Epoch 42/150
813/852 [===========================&gt;..] - ETA: 0s - loss: 0.4724 - accuracy: 0.7645
Epoch 42: val_loss improved from 0.46234 to 0.45757, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4742 - accuracy: 0.7635 - val_loss: 0.4576 - val_accuracy: 0.7748
Epoch 43/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.4695 - accuracy: 0.7686
Epoch 43: val_loss improved from 0.45757 to 0.45655, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4688 - accuracy: 0.7682 - val_loss: 0.4565 - val_accuracy: 0.7715
Epoch 44/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.4645 - accuracy: 0.7697
Epoch 44: val_loss improved from 0.45655 to 0.44992, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4664 - accuracy: 0.7688 - val_loss: 0.4499 - val_accuracy: 0.7824
Epoch 45/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.4601 - accuracy: 0.7705
Epoch 45: val_loss improved from 0.44992 to 0.44612, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4632 - accuracy: 0.7680 - val_loss: 0.4461 - val_accuracy: 0.7789
Epoch 46/150
820/852 [===========================&gt;..] - ETA: 0s - loss: 0.4591 - accuracy: 0.7730
Epoch 46: val_loss improved from 0.44612 to 0.44524, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4614 - accuracy: 0.7707 - val_loss: 0.4452 - val_accuracy: 0.7809
Epoch 47/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.4580 - accuracy: 0.7705
Epoch 47: val_loss did not improve from 0.44524
852/852 [==============================] - 2s 2ms/step - loss: 0.4584 - accuracy: 0.7698 - val_loss: 0.4470 - val_accuracy: 0.7788
Epoch 48/150
815/852 [===========================&gt;..] - ETA: 0s - loss: 0.4578 - accuracy: 0.7683
Epoch 48: val_loss improved from 0.44524 to 0.43692, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4576 - accuracy: 0.7684 - val_loss: 0.4369 - val_accuracy: 0.7825
Epoch 49/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.4542 - accuracy: 0.7744
Epoch 49: val_loss did not improve from 0.43692
852/852 [==============================] - 2s 2ms/step - loss: 0.4540 - accuracy: 0.7750 - val_loss: 0.4393 - val_accuracy: 0.7851
Epoch 50/150
819/852 [===========================&gt;..] - ETA: 0s - loss: 0.4481 - accuracy: 0.7770
Epoch 50: val_loss improved from 0.43692 to 0.43148, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4509 - accuracy: 0.7763 - val_loss: 0.4315 - val_accuracy: 0.7885
Epoch 51/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.4483 - accuracy: 0.7762
Epoch 51: val_loss did not improve from 0.43148
852/852 [==============================] - 2s 2ms/step - loss: 0.4481 - accuracy: 0.7762 - val_loss: 0.4433 - val_accuracy: 0.7812
Epoch 52/150
815/852 [===========================&gt;..] - ETA: 0s - loss: 0.4490 - accuracy: 0.7775
Epoch 52: val_loss improved from 0.43148 to 0.42561, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4494 - accuracy: 0.7764 - val_loss: 0.4256 - val_accuracy: 0.7895
Epoch 53/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.4435 - accuracy: 0.7793
Epoch 53: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.4440 - accuracy: 0.7784 - val_loss: 0.4272 - val_accuracy: 0.7910
Epoch 54/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.4456 - accuracy: 0.7766
Epoch 54: val_loss did not improve from 0.42561
852/852 [==============================] - 2s 2ms/step - loss: 0.4473 - accuracy: 0.7748 - val_loss: 0.4335 - val_accuracy: 0.7895
Epoch 55/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.4401 - accuracy: 0.7807
Epoch 55: val_loss improved from 0.42561 to 0.42516, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4411 - accuracy: 0.7794 - val_loss: 0.4252 - val_accuracy: 0.7953
Epoch 56/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.4405 - accuracy: 0.7812
Epoch 56: val_loss did not improve from 0.42516
852/852 [==============================] - 2s 2ms/step - loss: 0.4394 - accuracy: 0.7814 - val_loss: 0.4271 - val_accuracy: 0.7905
Epoch 57/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.4462 - accuracy: 0.7819
Epoch 57: val_loss improved from 0.42516 to 0.41935, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4460 - accuracy: 0.7822 - val_loss: 0.4193 - val_accuracy: 0.7949
Epoch 58/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.4344 - accuracy: 0.7880
Epoch 58: val_loss did not improve from 0.41935
852/852 [==============================] - 2s 2ms/step - loss: 0.4349 - accuracy: 0.7875 - val_loss: 0.4221 - val_accuracy: 0.7942
Epoch 59/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.4345 - accuracy: 0.7836
Epoch 59: val_loss improved from 0.41935 to 0.41098, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4339 - accuracy: 0.7844 - val_loss: 0.4110 - val_accuracy: 0.7988
Epoch 60/150
852/852 [==============================] - ETA: 0s - loss: 0.4327 - accuracy: 0.7804
Epoch 60: val_loss improved from 0.41098 to 0.40580, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4327 - accuracy: 0.7804 - val_loss: 0.4058 - val_accuracy: 0.7974
Epoch 61/150
852/852 [==============================] - ETA: 0s - loss: 0.4282 - accuracy: 0.7864
Epoch 61: val_loss did not improve from 0.40580
852/852 [==============================] - 2s 3ms/step - loss: 0.4282 - accuracy: 0.7864 - val_loss: 0.4139 - val_accuracy: 0.8019
Epoch 62/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.4229 - accuracy: 0.7929
Epoch 62: val_loss did not improve from 0.40580
852/852 [==============================] - 2s 2ms/step - loss: 0.4231 - accuracy: 0.7918 - val_loss: 0.4151 - val_accuracy: 0.8001
Epoch 63/150
818/852 [===========================&gt;..] - ETA: 0s - loss: 0.4242 - accuracy: 0.7924
Epoch 63: val_loss did not improve from 0.40580
852/852 [==============================] - 2s 2ms/step - loss: 0.4246 - accuracy: 0.7925 - val_loss: 0.4131 - val_accuracy: 0.7939
Epoch 64/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.4211 - accuracy: 0.7895
Epoch 64: val_loss did not improve from 0.40580
852/852 [==============================] - 2s 2ms/step - loss: 0.4219 - accuracy: 0.7895 - val_loss: 0.4069 - val_accuracy: 0.8064
Epoch 65/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.4183 - accuracy: 0.7948
Epoch 65: val_loss improved from 0.40580 to 0.40138, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4184 - accuracy: 0.7944 - val_loss: 0.4014 - val_accuracy: 0.8050
Epoch 66/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.4179 - accuracy: 0.7916
Epoch 66: val_loss improved from 0.40138 to 0.39976, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4186 - accuracy: 0.7916 - val_loss: 0.3998 - val_accuracy: 0.8059
Epoch 67/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.4149 - accuracy: 0.7970
Epoch 67: val_loss improved from 0.39976 to 0.39690, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4150 - accuracy: 0.7980 - val_loss: 0.3969 - val_accuracy: 0.8040
Epoch 68/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.4141 - accuracy: 0.7938
Epoch 68: val_loss did not improve from 0.39690
852/852 [==============================] - 2s 2ms/step - loss: 0.4157 - accuracy: 0.7920 - val_loss: 0.4023 - val_accuracy: 0.8088
Epoch 69/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.4133 - accuracy: 0.7954
Epoch 69: val_loss did not improve from 0.39690
852/852 [==============================] - 2s 2ms/step - loss: 0.4135 - accuracy: 0.7953 - val_loss: 0.4019 - val_accuracy: 0.8052
Epoch 70/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.4058 - accuracy: 0.8022
Epoch 70: val_loss improved from 0.39690 to 0.38997, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4071 - accuracy: 0.8012 - val_loss: 0.3900 - val_accuracy: 0.8115
Epoch 71/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.4073 - accuracy: 0.8001
Epoch 71: val_loss did not improve from 0.38997
852/852 [==============================] - 2s 2ms/step - loss: 0.4060 - accuracy: 0.8016 - val_loss: 0.4033 - val_accuracy: 0.8000
Epoch 72/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.4075 - accuracy: 0.7978
Epoch 72: val_loss did not improve from 0.38997
852/852 [==============================] - 2s 2ms/step - loss: 0.4062 - accuracy: 0.7990 - val_loss: 0.3940 - val_accuracy: 0.8034
Epoch 73/150
819/852 [===========================&gt;..] - ETA: 0s - loss: 0.4029 - accuracy: 0.8002
Epoch 73: val_loss improved from 0.38997 to 0.37781, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4012 - accuracy: 0.8014 - val_loss: 0.3778 - val_accuracy: 0.8131
Epoch 74/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.3999 - accuracy: 0.7998
Epoch 74: val_loss improved from 0.37781 to 0.37326, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3997 - accuracy: 0.7999 - val_loss: 0.3733 - val_accuracy: 0.8181
Epoch 75/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.4002 - accuracy: 0.8072
Epoch 75: val_loss did not improve from 0.37326
852/852 [==============================] - 2s 2ms/step - loss: 0.4004 - accuracy: 0.8068 - val_loss: 0.3736 - val_accuracy: 0.8229
Epoch 76/150
812/852 [===========================&gt;..] - ETA: 0s - loss: 0.3988 - accuracy: 0.8042
Epoch 76: val_loss did not improve from 0.37326
852/852 [==============================] - 2s 2ms/step - loss: 0.3984 - accuracy: 0.8050 - val_loss: 0.3991 - val_accuracy: 0.8012
Epoch 77/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.3967 - accuracy: 0.8046
Epoch 77: val_loss improved from 0.37326 to 0.37158, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3956 - accuracy: 0.8052 - val_loss: 0.3716 - val_accuracy: 0.8183
Epoch 78/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.3932 - accuracy: 0.8042
Epoch 78: val_loss did not improve from 0.37158
852/852 [==============================] - 2s 2ms/step - loss: 0.3935 - accuracy: 0.8040 - val_loss: 0.3770 - val_accuracy: 0.8154
Epoch 79/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.3915 - accuracy: 0.8079
Epoch 79: val_loss did not improve from 0.37158
852/852 [==============================] - 2s 2ms/step - loss: 0.3935 - accuracy: 0.8059 - val_loss: 0.3773 - val_accuracy: 0.8163
Epoch 80/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.3887 - accuracy: 0.8080
Epoch 80: val_loss improved from 0.37158 to 0.37028, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3888 - accuracy: 0.8079 - val_loss: 0.3703 - val_accuracy: 0.8162
Epoch 81/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.3849 - accuracy: 0.8105
Epoch 81: val_loss did not improve from 0.37028
852/852 [==============================] - 2s 2ms/step - loss: 0.3866 - accuracy: 0.8095 - val_loss: 0.4009 - val_accuracy: 0.8088
Epoch 82/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.3879 - accuracy: 0.8085
Epoch 82: val_loss improved from 0.37028 to 0.36839, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3875 - accuracy: 0.8085 - val_loss: 0.3684 - val_accuracy: 0.8232
Epoch 83/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.3829 - accuracy: 0.8116
Epoch 83: val_loss improved from 0.36839 to 0.35666, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3827 - accuracy: 0.8114 - val_loss: 0.3567 - val_accuracy: 0.8288
Epoch 84/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.3794 - accuracy: 0.8133
Epoch 84: val_loss did not improve from 0.35666
852/852 [==============================] - 2s 2ms/step - loss: 0.3801 - accuracy: 0.8128 - val_loss: 0.3659 - val_accuracy: 0.8229
Epoch 85/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.3806 - accuracy: 0.8129
Epoch 85: val_loss did not improve from 0.35666
852/852 [==============================] - 2s 2ms/step - loss: 0.3803 - accuracy: 0.8132 - val_loss: 0.3615 - val_accuracy: 0.8243
Epoch 86/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.3849 - accuracy: 0.8116
Epoch 86: val_loss did not improve from 0.35666
852/852 [==============================] - 2s 2ms/step - loss: 0.3851 - accuracy: 0.8113 - val_loss: 0.3670 - val_accuracy: 0.8220
Epoch 87/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.3737 - accuracy: 0.8226
Epoch 87: val_loss did not improve from 0.35666
852/852 [==============================] - 2s 2ms/step - loss: 0.3744 - accuracy: 0.8217 - val_loss: 0.3627 - val_accuracy: 0.8219
Epoch 88/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.3755 - accuracy: 0.8156
Epoch 88: val_loss did not improve from 0.35666
852/852 [==============================] - 2s 2ms/step - loss: 0.3752 - accuracy: 0.8153 - val_loss: 0.3618 - val_accuracy: 0.8266
Epoch 89/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.3745 - accuracy: 0.8164
Epoch 89: val_loss improved from 0.35666 to 0.35279, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3761 - accuracy: 0.8154 - val_loss: 0.3528 - val_accuracy: 0.8321
Epoch 90/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.3731 - accuracy: 0.8181
Epoch 90: val_loss did not improve from 0.35279
852/852 [==============================] - 2s 2ms/step - loss: 0.3728 - accuracy: 0.8183 - val_loss: 0.3697 - val_accuracy: 0.8220
Epoch 91/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.3694 - accuracy: 0.8205
Epoch 91: val_loss improved from 0.35279 to 0.34810, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3703 - accuracy: 0.8194 - val_loss: 0.3481 - val_accuracy: 0.8295
Epoch 92/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.3718 - accuracy: 0.8202
Epoch 92: val_loss improved from 0.34810 to 0.34739, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3714 - accuracy: 0.8205 - val_loss: 0.3474 - val_accuracy: 0.8356
Epoch 93/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.3635 - accuracy: 0.8189
Epoch 93: val_loss did not improve from 0.34739
852/852 [==============================] - 2s 2ms/step - loss: 0.3633 - accuracy: 0.8192 - val_loss: 0.3499 - val_accuracy: 0.8262
Epoch 94/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.3629 - accuracy: 0.8231
Epoch 94: val_loss improved from 0.34739 to 0.33823, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3635 - accuracy: 0.8222 - val_loss: 0.3382 - val_accuracy: 0.8384
Epoch 95/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.3576 - accuracy: 0.8227
Epoch 95: val_loss did not improve from 0.33823
852/852 [==============================] - 2s 2ms/step - loss: 0.3573 - accuracy: 0.8232 - val_loss: 0.3468 - val_accuracy: 0.8296
Epoch 96/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.3615 - accuracy: 0.8227
Epoch 96: val_loss did not improve from 0.33823
852/852 [==============================] - 2s 2ms/step - loss: 0.3619 - accuracy: 0.8222 - val_loss: 0.3472 - val_accuracy: 0.8297
Epoch 97/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.3622 - accuracy: 0.8210
Epoch 97: val_loss improved from 0.33823 to 0.33457, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3625 - accuracy: 0.8208 - val_loss: 0.3346 - val_accuracy: 0.8341
Epoch 98/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.3656 - accuracy: 0.8228
Epoch 98: val_loss did not improve from 0.33457
852/852 [==============================] - 2s 2ms/step - loss: 0.3645 - accuracy: 0.8234 - val_loss: 0.3439 - val_accuracy: 0.8297
Epoch 99/150
812/852 [===========================&gt;..] - ETA: 0s - loss: 0.3596 - accuracy: 0.8225
Epoch 99: val_loss did not improve from 0.33457
852/852 [==============================] - 2s 2ms/step - loss: 0.3592 - accuracy: 0.8232 - val_loss: 0.3430 - val_accuracy: 0.8344
Epoch 100/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.3554 - accuracy: 0.8252
Epoch 100: val_loss improved from 0.33457 to 0.32833, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3555 - accuracy: 0.8253 - val_loss: 0.3283 - val_accuracy: 0.8395
Epoch 101/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.3587 - accuracy: 0.8254
Epoch 101: val_loss did not improve from 0.32833
852/852 [==============================] - 2s 2ms/step - loss: 0.3590 - accuracy: 0.8254 - val_loss: 0.3312 - val_accuracy: 0.8399
Epoch 102/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.3472 - accuracy: 0.8227
Epoch 102: val_loss did not improve from 0.32833
852/852 [==============================] - 2s 2ms/step - loss: 0.3473 - accuracy: 0.8235 - val_loss: 0.3303 - val_accuracy: 0.8385
Epoch 103/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.3533 - accuracy: 0.8251
Epoch 103: val_loss improved from 0.32833 to 0.32349, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3549 - accuracy: 0.8239 - val_loss: 0.3235 - val_accuracy: 0.8421
Epoch 104/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.3430 - accuracy: 0.8317
Epoch 104: val_loss did not improve from 0.32349
852/852 [==============================] - 2s 2ms/step - loss: 0.3420 - accuracy: 0.8324 - val_loss: 0.3268 - val_accuracy: 0.8376
Epoch 105/150
812/852 [===========================&gt;..] - ETA: 0s - loss: 0.3460 - accuracy: 0.8289
Epoch 105: val_loss did not improve from 0.32349
852/852 [==============================] - 2s 2ms/step - loss: 0.3482 - accuracy: 0.8279 - val_loss: 0.3284 - val_accuracy: 0.8429
Epoch 106/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.3508 - accuracy: 0.8297
Epoch 106: val_loss did not improve from 0.32349
852/852 [==============================] - 2s 2ms/step - loss: 0.3495 - accuracy: 0.8310 - val_loss: 0.3494 - val_accuracy: 0.8256
Epoch 107/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.3565 - accuracy: 0.8277
Epoch 107: val_loss did not improve from 0.32349
852/852 [==============================] - 2s 2ms/step - loss: 0.3575 - accuracy: 0.8263 - val_loss: 0.3279 - val_accuracy: 0.8428
Epoch 108/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.3450 - accuracy: 0.8305
Epoch 108: val_loss did not improve from 0.32349
852/852 [==============================] - 2s 2ms/step - loss: 0.3448 - accuracy: 0.8306 - val_loss: 0.3327 - val_accuracy: 0.8358
Epoch 109/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.3357 - accuracy: 0.8326
Epoch 109: val_loss improved from 0.32349 to 0.31425, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3351 - accuracy: 0.8327 - val_loss: 0.3143 - val_accuracy: 0.8450
Epoch 110/150
809/852 [===========================&gt;..] - ETA: 0s - loss: 0.3408 - accuracy: 0.8329
Epoch 110: val_loss did not improve from 0.31425
852/852 [==============================] - 2s 2ms/step - loss: 0.3399 - accuracy: 0.8331 - val_loss: 0.3210 - val_accuracy: 0.8408
Epoch 111/150
820/852 [===========================&gt;..] - ETA: 0s - loss: 0.3439 - accuracy: 0.8313
Epoch 111: val_loss did not improve from 0.31425
852/852 [==============================] - 2s 2ms/step - loss: 0.3434 - accuracy: 0.8320 - val_loss: 0.3315 - val_accuracy: 0.8349
Epoch 112/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.3408 - accuracy: 0.8333
Epoch 112: val_loss did not improve from 0.31425
852/852 [==============================] - 2s 2ms/step - loss: 0.3420 - accuracy: 0.8322 - val_loss: 0.3247 - val_accuracy: 0.8398
Epoch 113/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.3335 - accuracy: 0.8361
Epoch 113: val_loss improved from 0.31425 to 0.30550, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3340 - accuracy: 0.8367 - val_loss: 0.3055 - val_accuracy: 0.8519
Epoch 114/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.3351 - accuracy: 0.8335
Epoch 114: val_loss did not improve from 0.30550
852/852 [==============================] - 2s 2ms/step - loss: 0.3349 - accuracy: 0.8335 - val_loss: 0.3182 - val_accuracy: 0.8462
Epoch 115/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.3367 - accuracy: 0.8379
Epoch 115: val_loss did not improve from 0.30550
852/852 [==============================] - 2s 2ms/step - loss: 0.3367 - accuracy: 0.8381 - val_loss: 0.3062 - val_accuracy: 0.8509
Epoch 116/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.3353 - accuracy: 0.8331
Epoch 116: val_loss did not improve from 0.30550
852/852 [==============================] - 2s 2ms/step - loss: 0.3367 - accuracy: 0.8329 - val_loss: 0.3122 - val_accuracy: 0.8493
Epoch 117/150
818/852 [===========================&gt;..] - ETA: 0s - loss: 0.3304 - accuracy: 0.8366
Epoch 117: val_loss did not improve from 0.30550
852/852 [==============================] - 2s 2ms/step - loss: 0.3311 - accuracy: 0.8358 - val_loss: 0.3079 - val_accuracy: 0.8525
Epoch 118/150
809/852 [===========================&gt;..] - ETA: 0s - loss: 0.3301 - accuracy: 0.8371
Epoch 118: val_loss did not improve from 0.30550
852/852 [==============================] - 2s 2ms/step - loss: 0.3313 - accuracy: 0.8368 - val_loss: 0.3151 - val_accuracy: 0.8476
Epoch 119/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.3231 - accuracy: 0.8376
Epoch 119: val_loss did not improve from 0.30550
852/852 [==============================] - 2s 2ms/step - loss: 0.3237 - accuracy: 0.8375 - val_loss: 0.3065 - val_accuracy: 0.8517
Epoch 120/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.3295 - accuracy: 0.8376
Epoch 120: val_loss improved from 0.30550 to 0.30446, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3300 - accuracy: 0.8370 - val_loss: 0.3045 - val_accuracy: 0.8525
Epoch 121/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.3198 - accuracy: 0.8446
Epoch 121: val_loss improved from 0.30446 to 0.30091, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3200 - accuracy: 0.8444 - val_loss: 0.3009 - val_accuracy: 0.8505
Epoch 122/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.3265 - accuracy: 0.8379
Epoch 122: val_loss did not improve from 0.30091
852/852 [==============================] - 2s 2ms/step - loss: 0.3258 - accuracy: 0.8383 - val_loss: 0.3044 - val_accuracy: 0.8512
Epoch 123/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.3225 - accuracy: 0.8403
Epoch 123: val_loss improved from 0.30091 to 0.29810, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3219 - accuracy: 0.8407 - val_loss: 0.2981 - val_accuracy: 0.8485
Epoch 124/150
852/852 [==============================] - ETA: 0s - loss: 0.3238 - accuracy: 0.8417
Epoch 124: val_loss improved from 0.29810 to 0.29422, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3238 - accuracy: 0.8417 - val_loss: 0.2942 - val_accuracy: 0.8607
Epoch 125/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.3219 - accuracy: 0.8421
Epoch 125: val_loss did not improve from 0.29422
852/852 [==============================] - 2s 2ms/step - loss: 0.3227 - accuracy: 0.8414 - val_loss: 0.3215 - val_accuracy: 0.8444
Epoch 126/150
819/852 [===========================&gt;..] - ETA: 0s - loss: 0.3351 - accuracy: 0.8349
Epoch 126: val_loss did not improve from 0.29422
852/852 [==============================] - 2s 2ms/step - loss: 0.3356 - accuracy: 0.8342 - val_loss: 0.3013 - val_accuracy: 0.8553
Epoch 127/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.3228 - accuracy: 0.8414
Epoch 127: val_loss did not improve from 0.29422
852/852 [==============================] - 2s 2ms/step - loss: 0.3221 - accuracy: 0.8418 - val_loss: 0.3046 - val_accuracy: 0.8538
Epoch 128/150
815/852 [===========================&gt;..] - ETA: 0s - loss: 0.3170 - accuracy: 0.8437
Epoch 128: val_loss did not improve from 0.29422
852/852 [==============================] - 2s 2ms/step - loss: 0.3178 - accuracy: 0.8435 - val_loss: 0.2989 - val_accuracy: 0.8589
Epoch 129/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.3185 - accuracy: 0.8439
Epoch 129: val_loss did not improve from 0.29422
852/852 [==============================] - 2s 2ms/step - loss: 0.3184 - accuracy: 0.8438 - val_loss: 0.2967 - val_accuracy: 0.8576
Epoch 130/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.3154 - accuracy: 0.8443
Epoch 130: val_loss did not improve from 0.29422
852/852 [==============================] - 2s 2ms/step - loss: 0.3162 - accuracy: 0.8442 - val_loss: 0.2989 - val_accuracy: 0.8525
Epoch 131/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.3205 - accuracy: 0.8419
Epoch 131: val_loss did not improve from 0.29422
852/852 [==============================] - 2s 2ms/step - loss: 0.3195 - accuracy: 0.8426 - val_loss: 0.3095 - val_accuracy: 0.8505
Epoch 132/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.3170 - accuracy: 0.8442
Epoch 132: val_loss improved from 0.29422 to 0.29308, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3167 - accuracy: 0.8444 - val_loss: 0.2931 - val_accuracy: 0.8572
Epoch 133/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.3115 - accuracy: 0.8475
Epoch 133: val_loss did not improve from 0.29308
852/852 [==============================] - 2s 2ms/step - loss: 0.3110 - accuracy: 0.8477 - val_loss: 0.3230 - val_accuracy: 0.8423
Epoch 134/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.3214 - accuracy: 0.8398
Epoch 134: val_loss did not improve from 0.29308
852/852 [==============================] - 2s 2ms/step - loss: 0.3220 - accuracy: 0.8399 - val_loss: 0.3214 - val_accuracy: 0.8426
Epoch 135/150
815/852 [===========================&gt;..] - ETA: 0s - loss: 0.3135 - accuracy: 0.8458
Epoch 135: val_loss did not improve from 0.29308
852/852 [==============================] - 2s 2ms/step - loss: 0.3150 - accuracy: 0.8455 - val_loss: 0.2980 - val_accuracy: 0.8545
Epoch 136/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.3095 - accuracy: 0.8478
Epoch 136: val_loss improved from 0.29308 to 0.29066, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3112 - accuracy: 0.8461 - val_loss: 0.2907 - val_accuracy: 0.8611
Epoch 137/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.3142 - accuracy: 0.8475
Epoch 137: val_loss improved from 0.29066 to 0.28600, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3139 - accuracy: 0.8468 - val_loss: 0.2860 - val_accuracy: 0.8578
Epoch 138/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.3035 - accuracy: 0.8479
Epoch 138: val_loss did not improve from 0.28600
852/852 [==============================] - 2s 2ms/step - loss: 0.3034 - accuracy: 0.8481 - val_loss: 0.3016 - val_accuracy: 0.8531
Epoch 139/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.3100 - accuracy: 0.8468
Epoch 139: val_loss did not improve from 0.28600
852/852 [==============================] - 2s 2ms/step - loss: 0.3100 - accuracy: 0.8470 - val_loss: 0.3143 - val_accuracy: 0.8497
Epoch 140/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.3062 - accuracy: 0.8470
Epoch 140: val_loss improved from 0.28600 to 0.28167, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3062 - accuracy: 0.8469 - val_loss: 0.2817 - val_accuracy: 0.8616
Epoch 141/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.3044 - accuracy: 0.8536
Epoch 141: val_loss did not improve from 0.28167
852/852 [==============================] - 2s 2ms/step - loss: 0.3034 - accuracy: 0.8544 - val_loss: 0.2879 - val_accuracy: 0.8605
Epoch 142/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.3103 - accuracy: 0.8454
Epoch 142: val_loss did not improve from 0.28167
852/852 [==============================] - 2s 2ms/step - loss: 0.3092 - accuracy: 0.8463 - val_loss: 0.3014 - val_accuracy: 0.8504
Epoch 143/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.3128 - accuracy: 0.8462
Epoch 143: val_loss did not improve from 0.28167
852/852 [==============================] - 2s 2ms/step - loss: 0.3116 - accuracy: 0.8473 - val_loss: 0.2896 - val_accuracy: 0.8590
Epoch 144/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.3055 - accuracy: 0.8512
Epoch 144: val_loss did not improve from 0.28167
852/852 [==============================] - 2s 2ms/step - loss: 0.3075 - accuracy: 0.8503 - val_loss: 0.3102 - val_accuracy: 0.8499
Epoch 145/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.3088 - accuracy: 0.8471
Epoch 145: val_loss did not improve from 0.28167
852/852 [==============================] - 2s 2ms/step - loss: 0.3102 - accuracy: 0.8457 - val_loss: 0.2865 - val_accuracy: 0.8641
Epoch 146/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.3074 - accuracy: 0.8500
Epoch 146: val_loss did not improve from 0.28167
852/852 [==============================] - 2s 2ms/step - loss: 0.3068 - accuracy: 0.8503 - val_loss: 0.3061 - val_accuracy: 0.8531
Epoch 147/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.3064 - accuracy: 0.8512
Epoch 147: val_loss did not improve from 0.28167
852/852 [==============================] - 2s 2ms/step - loss: 0.3049 - accuracy: 0.8525 - val_loss: 0.2869 - val_accuracy: 0.8598
Epoch 148/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.2948 - accuracy: 0.8516
Epoch 148: val_loss improved from 0.28167 to 0.27824, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.2935 - accuracy: 0.8523 - val_loss: 0.2782 - val_accuracy: 0.8628
Epoch 149/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.3041 - accuracy: 0.8482
Epoch 149: val_loss did not improve from 0.27824
852/852 [==============================] - 2s 2ms/step - loss: 0.3035 - accuracy: 0.8488 - val_loss: 0.2849 - val_accuracy: 0.8624
Epoch 150/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.3019 - accuracy: 0.8507
Epoch 150: val_loss did not improve from 0.27824
852/852 [==============================] - 2s 2ms/step - loss: 0.3014 - accuracy: 0.8512 - val_loss: 0.2880 - val_accuracy: 0.8581
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=9148d211-8138-4960-8983-e938c839e02d">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [78]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">history_249</span> <span class="o">=</span> <span class="n">two_fortynine_neuron</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">callback_a</span><span class="p">,</span> <span class="n">callback_b</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.2999 - accuracy: 0.8508
Epoch 1: val_loss did not improve from 0.27824
852/852 [==============================] - 2s 2ms/step - loss: 0.3004 - accuracy: 0.8502 - val_loss: 0.2989 - val_accuracy: 0.8570
Epoch 2/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.3136 - accuracy: 0.8513
Epoch 2: val_loss did not improve from 0.27824
852/852 [==============================] - 2s 2ms/step - loss: 0.3141 - accuracy: 0.8506 - val_loss: 0.2823 - val_accuracy: 0.8672
Epoch 3/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.2951 - accuracy: 0.8556
Epoch 3: val_loss did not improve from 0.27824
852/852 [==============================] - 2s 2ms/step - loss: 0.2955 - accuracy: 0.8552 - val_loss: 0.2939 - val_accuracy: 0.8571
Epoch 4/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.2938 - accuracy: 0.8515
Epoch 4: val_loss did not improve from 0.27824
852/852 [==============================] - 2s 2ms/step - loss: 0.2937 - accuracy: 0.8519 - val_loss: 0.2856 - val_accuracy: 0.8630
Epoch 5/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.2889 - accuracy: 0.8582
Epoch 5: val_loss did not improve from 0.27824
852/852 [==============================] - 2s 2ms/step - loss: 0.2920 - accuracy: 0.8567 - val_loss: 0.2815 - val_accuracy: 0.8643
Epoch 6/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.2927 - accuracy: 0.8544
Epoch 6: val_loss improved from 0.27824 to 0.27495, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.2929 - accuracy: 0.8549 - val_loss: 0.2750 - val_accuracy: 0.8640
Epoch 7/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.2977 - accuracy: 0.8496
Epoch 7: val_loss did not improve from 0.27495
852/852 [==============================] - 2s 2ms/step - loss: 0.2973 - accuracy: 0.8504 - val_loss: 0.2797 - val_accuracy: 0.8668
Epoch 8/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.2986 - accuracy: 0.8547
Epoch 8: val_loss did not improve from 0.27495
852/852 [==============================] - 2s 2ms/step - loss: 0.2992 - accuracy: 0.8539 - val_loss: 0.2815 - val_accuracy: 0.8632
Epoch 9/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.2966 - accuracy: 0.8516
Epoch 9: val_loss did not improve from 0.27495
852/852 [==============================] - 2s 2ms/step - loss: 0.2969 - accuracy: 0.8517 - val_loss: 0.2773 - val_accuracy: 0.8610
Epoch 10/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.2940 - accuracy: 0.8599
Epoch 10: val_loss improved from 0.27495 to 0.27409, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.2950 - accuracy: 0.8589 - val_loss: 0.2741 - val_accuracy: 0.8699
Epoch 11/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.2938 - accuracy: 0.8571
Epoch 11: val_loss improved from 0.27409 to 0.25726, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.2938 - accuracy: 0.8572 - val_loss: 0.2573 - val_accuracy: 0.8760
Epoch 12/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.2911 - accuracy: 0.8576
Epoch 12: val_loss did not improve from 0.25726
852/852 [==============================] - 2s 2ms/step - loss: 0.2917 - accuracy: 0.8569 - val_loss: 0.2719 - val_accuracy: 0.8677
Epoch 13/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.2870 - accuracy: 0.8576
Epoch 13: val_loss did not improve from 0.25726
852/852 [==============================] - 2s 2ms/step - loss: 0.2869 - accuracy: 0.8574 - val_loss: 0.2619 - val_accuracy: 0.8755
Epoch 14/150
819/852 [===========================&gt;..] - ETA: 0s - loss: 0.2866 - accuracy: 0.8600
Epoch 14: val_loss did not improve from 0.25726
852/852 [==============================] - 2s 2ms/step - loss: 0.2901 - accuracy: 0.8592 - val_loss: 0.2970 - val_accuracy: 0.8586
Epoch 15/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.2856 - accuracy: 0.8589
Epoch 15: val_loss did not improve from 0.25726
852/852 [==============================] - 2s 2ms/step - loss: 0.2860 - accuracy: 0.8586 - val_loss: 0.2671 - val_accuracy: 0.8711
Epoch 16/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.2933 - accuracy: 0.8558
Epoch 16: val_loss did not improve from 0.25726
852/852 [==============================] - 2s 2ms/step - loss: 0.2956 - accuracy: 0.8550 - val_loss: 0.2811 - val_accuracy: 0.8638
Epoch 17/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.2781 - accuracy: 0.8634
Epoch 17: val_loss improved from 0.25726 to 0.25138, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.2803 - accuracy: 0.8623 - val_loss: 0.2514 - val_accuracy: 0.8771
Epoch 18/150
852/852 [==============================] - ETA: 0s - loss: 0.2854 - accuracy: 0.8620
Epoch 18: val_loss did not improve from 0.25138
852/852 [==============================] - 2s 2ms/step - loss: 0.2854 - accuracy: 0.8620 - val_loss: 0.2574 - val_accuracy: 0.8768
Epoch 19/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.2821 - accuracy: 0.8619
Epoch 19: val_loss did not improve from 0.25138
852/852 [==============================] - 2s 2ms/step - loss: 0.2820 - accuracy: 0.8618 - val_loss: 0.2616 - val_accuracy: 0.8731
Epoch 20/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.2806 - accuracy: 0.8619
Epoch 20: val_loss did not improve from 0.25138
852/852 [==============================] - 2s 2ms/step - loss: 0.2805 - accuracy: 0.8620 - val_loss: 0.2882 - val_accuracy: 0.8650
Epoch 21/150
820/852 [===========================&gt;..] - ETA: 0s - loss: 0.2839 - accuracy: 0.8630
Epoch 21: val_loss did not improve from 0.25138
852/852 [==============================] - 2s 2ms/step - loss: 0.2854 - accuracy: 0.8624 - val_loss: 0.2544 - val_accuracy: 0.8772
Epoch 22/150
815/852 [===========================&gt;..] - ETA: 0s - loss: 0.2767 - accuracy: 0.8652
Epoch 22: val_loss did not improve from 0.25138
852/852 [==============================] - 2s 2ms/step - loss: 0.2785 - accuracy: 0.8638 - val_loss: 0.2793 - val_accuracy: 0.8660
Epoch 23/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.2814 - accuracy: 0.8637
Epoch 23: val_loss did not improve from 0.25138
852/852 [==============================] - 2s 2ms/step - loss: 0.2830 - accuracy: 0.8621 - val_loss: 0.2589 - val_accuracy: 0.8719
Epoch 24/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.2773 - accuracy: 0.8599
Epoch 24: val_loss did not improve from 0.25138
852/852 [==============================] - 2s 2ms/step - loss: 0.2779 - accuracy: 0.8604 - val_loss: 0.2693 - val_accuracy: 0.8695
Epoch 25/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.2780 - accuracy: 0.8645
Epoch 25: val_loss did not improve from 0.25138
852/852 [==============================] - 2s 2ms/step - loss: 0.2788 - accuracy: 0.8634 - val_loss: 0.2528 - val_accuracy: 0.8765
Epoch 26/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.2740 - accuracy: 0.8668
Epoch 26: val_loss did not improve from 0.25138
852/852 [==============================] - 2s 2ms/step - loss: 0.2761 - accuracy: 0.8654 - val_loss: 0.2630 - val_accuracy: 0.8720
Epoch 27/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.2738 - accuracy: 0.8631
Epoch 27: val_loss improved from 0.25138 to 0.24344, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.2746 - accuracy: 0.8627 - val_loss: 0.2434 - val_accuracy: 0.8842
Epoch 28/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.2838 - accuracy: 0.8623
Epoch 28: val_loss did not improve from 0.24344
852/852 [==============================] - 2s 2ms/step - loss: 0.2838 - accuracy: 0.8623 - val_loss: 0.2692 - val_accuracy: 0.8694
Epoch 29/150
819/852 [===========================&gt;..] - ETA: 0s - loss: 0.2815 - accuracy: 0.8643
Epoch 29: val_loss did not improve from 0.24344
852/852 [==============================] - 2s 2ms/step - loss: 0.2815 - accuracy: 0.8639 - val_loss: 0.2544 - val_accuracy: 0.8775
Epoch 30/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.2700 - accuracy: 0.8688
Epoch 30: val_loss did not improve from 0.24344
852/852 [==============================] - 2s 2ms/step - loss: 0.2700 - accuracy: 0.8688 - val_loss: 0.2630 - val_accuracy: 0.8726
Epoch 31/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.2732 - accuracy: 0.8680
Epoch 31: val_loss did not improve from 0.24344
852/852 [==============================] - 2s 2ms/step - loss: 0.2724 - accuracy: 0.8682 - val_loss: 0.2613 - val_accuracy: 0.8760
Epoch 32/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.2785 - accuracy: 0.8622
Epoch 32: val_loss did not improve from 0.24344
852/852 [==============================] - 2s 2ms/step - loss: 0.2787 - accuracy: 0.8620 - val_loss: 0.2652 - val_accuracy: 0.8698
Epoch 33/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.2811 - accuracy: 0.8693
Epoch 33: val_loss did not improve from 0.24344
852/852 [==============================] - 2s 2ms/step - loss: 0.2804 - accuracy: 0.8699 - val_loss: 0.2463 - val_accuracy: 0.8798
Epoch 34/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.2709 - accuracy: 0.8663
Epoch 34: val_loss did not improve from 0.24344
852/852 [==============================] - 2s 2ms/step - loss: 0.2718 - accuracy: 0.8654 - val_loss: 0.2680 - val_accuracy: 0.8733
Epoch 35/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.2752 - accuracy: 0.8657
Epoch 35: val_loss did not improve from 0.24344
852/852 [==============================] - 2s 2ms/step - loss: 0.2740 - accuracy: 0.8660 - val_loss: 0.2512 - val_accuracy: 0.8787
Epoch 36/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.2635 - accuracy: 0.8711
Epoch 36: val_loss did not improve from 0.24344
852/852 [==============================] - 2s 2ms/step - loss: 0.2627 - accuracy: 0.8719 - val_loss: 0.2475 - val_accuracy: 0.8798
Epoch 37/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.2736 - accuracy: 0.8632
Epoch 37: val_loss did not improve from 0.24344
852/852 [==============================] - 2s 2ms/step - loss: 0.2737 - accuracy: 0.8633 - val_loss: 0.2451 - val_accuracy: 0.8836
Epoch 38/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.2725 - accuracy: 0.8672
Epoch 38: val_loss did not improve from 0.24344
852/852 [==============================] - 2s 2ms/step - loss: 0.2720 - accuracy: 0.8675 - val_loss: 0.2572 - val_accuracy: 0.8775
Epoch 39/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.2697 - accuracy: 0.8684
Epoch 39: val_loss improved from 0.24344 to 0.24185, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.2698 - accuracy: 0.8680 - val_loss: 0.2418 - val_accuracy: 0.8863
Epoch 40/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.2711 - accuracy: 0.8675
Epoch 40: val_loss did not improve from 0.24185
852/852 [==============================] - 2s 2ms/step - loss: 0.2731 - accuracy: 0.8675 - val_loss: 0.2464 - val_accuracy: 0.8823
Epoch 41/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.2609 - accuracy: 0.8726
Epoch 41: val_loss did not improve from 0.24185
852/852 [==============================] - 2s 2ms/step - loss: 0.2623 - accuracy: 0.8719 - val_loss: 0.2477 - val_accuracy: 0.8800
Epoch 42/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.2634 - accuracy: 0.8703
Epoch 42: val_loss did not improve from 0.24185
852/852 [==============================] - 2s 2ms/step - loss: 0.2641 - accuracy: 0.8704 - val_loss: 0.2586 - val_accuracy: 0.8755
Epoch 43/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.2668 - accuracy: 0.8732
Epoch 43: val_loss did not improve from 0.24185
852/852 [==============================] - 2s 2ms/step - loss: 0.2670 - accuracy: 0.8729 - val_loss: 0.2598 - val_accuracy: 0.8776
Epoch 44/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.2617 - accuracy: 0.8707
Epoch 44: val_loss did not improve from 0.24185
852/852 [==============================] - 2s 2ms/step - loss: 0.2617 - accuracy: 0.8700 - val_loss: 0.2578 - val_accuracy: 0.8742
Epoch 45/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.2659 - accuracy: 0.8721
Epoch 45: val_loss did not improve from 0.24185
852/852 [==============================] - 2s 2ms/step - loss: 0.2654 - accuracy: 0.8719 - val_loss: 0.2553 - val_accuracy: 0.8779
Epoch 46/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.2691 - accuracy: 0.8719
Epoch 46: val_loss did not improve from 0.24185
852/852 [==============================] - 2s 2ms/step - loss: 0.2694 - accuracy: 0.8715 - val_loss: 0.2508 - val_accuracy: 0.8818
Epoch 47/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.2676 - accuracy: 0.8719
Epoch 47: val_loss improved from 0.24185 to 0.23665, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.2672 - accuracy: 0.8719 - val_loss: 0.2367 - val_accuracy: 0.8924
Epoch 48/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.2579 - accuracy: 0.8754
Epoch 48: val_loss improved from 0.23665 to 0.23082, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.2578 - accuracy: 0.8752 - val_loss: 0.2308 - val_accuracy: 0.8884
Epoch 49/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.2626 - accuracy: 0.8717
Epoch 49: val_loss did not improve from 0.23082
852/852 [==============================] - 2s 2ms/step - loss: 0.2634 - accuracy: 0.8715 - val_loss: 0.2555 - val_accuracy: 0.8802
Epoch 50/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.2551 - accuracy: 0.8744
Epoch 50: val_loss did not improve from 0.23082
852/852 [==============================] - 2s 2ms/step - loss: 0.2555 - accuracy: 0.8745 - val_loss: 0.2474 - val_accuracy: 0.8801
Epoch 51/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.2626 - accuracy: 0.8748
Epoch 51: val_loss improved from 0.23082 to 0.23013, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.2630 - accuracy: 0.8745 - val_loss: 0.2301 - val_accuracy: 0.8914
Epoch 52/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.2487 - accuracy: 0.8768
Epoch 52: val_loss did not improve from 0.23013
852/852 [==============================] - 2s 2ms/step - loss: 0.2493 - accuracy: 0.8772 - val_loss: 0.2425 - val_accuracy: 0.8841
Epoch 53/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.2574 - accuracy: 0.8752
Epoch 53: val_loss did not improve from 0.23013
852/852 [==============================] - 2s 2ms/step - loss: 0.2583 - accuracy: 0.8753 - val_loss: 0.2349 - val_accuracy: 0.8859
Epoch 54/150
811/852 [===========================&gt;..] - ETA: 0s - loss: 0.2604 - accuracy: 0.8709
Epoch 54: val_loss did not improve from 0.23013
852/852 [==============================] - 2s 2ms/step - loss: 0.2593 - accuracy: 0.8707 - val_loss: 0.2321 - val_accuracy: 0.8891
Epoch 55/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.2549 - accuracy: 0.8753
Epoch 55: val_loss did not improve from 0.23013
852/852 [==============================] - 2s 2ms/step - loss: 0.2555 - accuracy: 0.8751 - val_loss: 0.2376 - val_accuracy: 0.8850
Epoch 56/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.2606 - accuracy: 0.8725
Epoch 56: val_loss did not improve from 0.23013
852/852 [==============================] - 2s 2ms/step - loss: 0.2623 - accuracy: 0.8718 - val_loss: 0.2492 - val_accuracy: 0.8791
Epoch 57/150
820/852 [===========================&gt;..] - ETA: 0s - loss: 0.2602 - accuracy: 0.8691
Epoch 57: val_loss did not improve from 0.23013
852/852 [==============================] - 2s 2ms/step - loss: 0.2604 - accuracy: 0.8693 - val_loss: 0.2356 - val_accuracy: 0.8857
Epoch 58/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.2591 - accuracy: 0.8745
Epoch 58: val_loss did not improve from 0.23013
852/852 [==============================] - 2s 2ms/step - loss: 0.2594 - accuracy: 0.8742 - val_loss: 0.2491 - val_accuracy: 0.8793
Epoch 59/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.2526 - accuracy: 0.8785
Epoch 59: val_loss did not improve from 0.23013
852/852 [==============================] - 2s 2ms/step - loss: 0.2534 - accuracy: 0.8783 - val_loss: 0.2390 - val_accuracy: 0.8830
Epoch 60/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.2467 - accuracy: 0.8793
Epoch 60: val_loss did not improve from 0.23013
852/852 [==============================] - 2s 2ms/step - loss: 0.2477 - accuracy: 0.8787 - val_loss: 0.2368 - val_accuracy: 0.8873
Epoch 61/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.2510 - accuracy: 0.8794
Epoch 61: val_loss did not improve from 0.23013
852/852 [==============================] - 2s 3ms/step - loss: 0.2524 - accuracy: 0.8780 - val_loss: 0.2507 - val_accuracy: 0.8772
Epoch 62/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.2544 - accuracy: 0.8725
Epoch 62: val_loss did not improve from 0.23013
852/852 [==============================] - 2s 2ms/step - loss: 0.2554 - accuracy: 0.8717 - val_loss: 0.2381 - val_accuracy: 0.8840
Epoch 63/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.2438 - accuracy: 0.8823
Epoch 63: val_loss did not improve from 0.23013
852/852 [==============================] - 2s 2ms/step - loss: 0.2455 - accuracy: 0.8814 - val_loss: 0.2384 - val_accuracy: 0.8860
Epoch 64/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.2600 - accuracy: 0.8719
Epoch 64: val_loss improved from 0.23013 to 0.22839, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.2589 - accuracy: 0.8725 - val_loss: 0.2284 - val_accuracy: 0.8911
Epoch 65/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.2600 - accuracy: 0.8762
Epoch 65: val_loss did not improve from 0.22839
852/852 [==============================] - 2s 2ms/step - loss: 0.2601 - accuracy: 0.8765 - val_loss: 0.2405 - val_accuracy: 0.8876
Epoch 66/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.2487 - accuracy: 0.8833
Epoch 66: val_loss improved from 0.22839 to 0.21700, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.2488 - accuracy: 0.8829 - val_loss: 0.2170 - val_accuracy: 0.8957
Epoch 67/150
810/852 [===========================&gt;..] - ETA: 0s - loss: 0.2467 - accuracy: 0.8809
Epoch 67: val_loss did not improve from 0.21700
852/852 [==============================] - 2s 2ms/step - loss: 0.2461 - accuracy: 0.8810 - val_loss: 0.2214 - val_accuracy: 0.8930
Epoch 68/150
810/852 [===========================&gt;..] - ETA: 0s - loss: 0.2443 - accuracy: 0.8814
Epoch 68: val_loss did not improve from 0.21700
852/852 [==============================] - 2s 2ms/step - loss: 0.2449 - accuracy: 0.8813 - val_loss: 0.2444 - val_accuracy: 0.8841
Epoch 69/150
820/852 [===========================&gt;..] - ETA: 0s - loss: 0.2584 - accuracy: 0.8776
Epoch 69: val_loss did not improve from 0.21700
852/852 [==============================] - 2s 2ms/step - loss: 0.2579 - accuracy: 0.8779 - val_loss: 0.2284 - val_accuracy: 0.8901
Epoch 70/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.2460 - accuracy: 0.8812
Epoch 70: val_loss did not improve from 0.21700
852/852 [==============================] - 2s 2ms/step - loss: 0.2461 - accuracy: 0.8812 - val_loss: 0.2251 - val_accuracy: 0.8903
Epoch 71/150
811/852 [===========================&gt;..] - ETA: 0s - loss: 0.2574 - accuracy: 0.8787
Epoch 71: val_loss did not improve from 0.21700
852/852 [==============================] - 2s 2ms/step - loss: 0.2589 - accuracy: 0.8772 - val_loss: 0.2346 - val_accuracy: 0.8868
Epoch 72/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.2451 - accuracy: 0.8789
Epoch 72: val_loss did not improve from 0.21700
852/852 [==============================] - 2s 2ms/step - loss: 0.2451 - accuracy: 0.8787 - val_loss: 0.2314 - val_accuracy: 0.8859
Epoch 73/150
820/852 [===========================&gt;..] - ETA: 0s - loss: 0.2448 - accuracy: 0.8820
Epoch 73: val_loss did not improve from 0.21700
852/852 [==============================] - 2s 3ms/step - loss: 0.2453 - accuracy: 0.8813 - val_loss: 0.2462 - val_accuracy: 0.8794
Epoch 74/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.2518 - accuracy: 0.8793
Epoch 74: val_loss did not improve from 0.21700
852/852 [==============================] - 2s 2ms/step - loss: 0.2513 - accuracy: 0.8799 - val_loss: 0.2375 - val_accuracy: 0.8863
Epoch 75/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.2505 - accuracy: 0.8808
Epoch 75: val_loss did not improve from 0.21700
852/852 [==============================] - 2s 3ms/step - loss: 0.2505 - accuracy: 0.8809 - val_loss: 0.2363 - val_accuracy: 0.8869
Epoch 76/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.2509 - accuracy: 0.8766
Epoch 76: val_loss did not improve from 0.21700
852/852 [==============================] - 2s 2ms/step - loss: 0.2516 - accuracy: 0.8761 - val_loss: 0.2194 - val_accuracy: 0.8988
Epoch 77/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.2374 - accuracy: 0.8838
Epoch 77: val_loss did not improve from 0.21700
852/852 [==============================] - 2s 2ms/step - loss: 0.2377 - accuracy: 0.8835 - val_loss: 0.2218 - val_accuracy: 0.8943
Epoch 78/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.2425 - accuracy: 0.8796
Epoch 78: val_loss did not improve from 0.21700
852/852 [==============================] - 2s 2ms/step - loss: 0.2461 - accuracy: 0.8781 - val_loss: 0.2715 - val_accuracy: 0.8815
Epoch 79/150
813/852 [===========================&gt;..] - ETA: 0s - loss: 0.2641 - accuracy: 0.8774
Epoch 79: val_loss did not improve from 0.21700
852/852 [==============================] - 2s 2ms/step - loss: 0.2635 - accuracy: 0.8771 - val_loss: 0.2245 - val_accuracy: 0.8927
Epoch 80/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.2419 - accuracy: 0.8878
Epoch 80: val_loss did not improve from 0.21700
852/852 [==============================] - 2s 2ms/step - loss: 0.2435 - accuracy: 0.8867 - val_loss: 0.2436 - val_accuracy: 0.8837
Epoch 81/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.2314 - accuracy: 0.8872
Epoch 81: val_loss improved from 0.21700 to 0.21335, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.2324 - accuracy: 0.8862 - val_loss: 0.2134 - val_accuracy: 0.9011
Epoch 82/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.2434 - accuracy: 0.8809
Epoch 82: val_loss improved from 0.21335 to 0.20869, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.2436 - accuracy: 0.8807 - val_loss: 0.2087 - val_accuracy: 0.9045
Epoch 83/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.2427 - accuracy: 0.8831
Epoch 83: val_loss did not improve from 0.20869
852/852 [==============================] - 2s 2ms/step - loss: 0.2426 - accuracy: 0.8833 - val_loss: 0.2343 - val_accuracy: 0.8880
Epoch 84/150
814/852 [===========================&gt;..] - ETA: 0s - loss: 0.2369 - accuracy: 0.8829
Epoch 84: val_loss did not improve from 0.20869
852/852 [==============================] - 2s 2ms/step - loss: 0.2374 - accuracy: 0.8822 - val_loss: 0.2248 - val_accuracy: 0.8911
Epoch 85/150
820/852 [===========================&gt;..] - ETA: 0s - loss: 0.2399 - accuracy: 0.8822
Epoch 85: val_loss did not improve from 0.20869
852/852 [==============================] - 2s 2ms/step - loss: 0.2403 - accuracy: 0.8822 - val_loss: 0.2188 - val_accuracy: 0.8965
Epoch 86/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.2406 - accuracy: 0.8870
Epoch 86: val_loss did not improve from 0.20869
852/852 [==============================] - 2s 2ms/step - loss: 0.2414 - accuracy: 0.8864 - val_loss: 0.2256 - val_accuracy: 0.8917
Epoch 87/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.2374 - accuracy: 0.8850
Epoch 87: val_loss did not improve from 0.20869
852/852 [==============================] - 2s 2ms/step - loss: 0.2376 - accuracy: 0.8848 - val_loss: 0.2114 - val_accuracy: 0.8977
Epoch 88/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.2391 - accuracy: 0.8824
Epoch 88: val_loss did not improve from 0.20869
852/852 [==============================] - 2s 2ms/step - loss: 0.2393 - accuracy: 0.8825 - val_loss: 0.2335 - val_accuracy: 0.8906
Epoch 89/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.2364 - accuracy: 0.8836
Epoch 89: val_loss did not improve from 0.20869
852/852 [==============================] - 2s 2ms/step - loss: 0.2368 - accuracy: 0.8828 - val_loss: 0.2261 - val_accuracy: 0.8923
Epoch 90/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.2359 - accuracy: 0.8865
Epoch 90: val_loss did not improve from 0.20869
852/852 [==============================] - 2s 2ms/step - loss: 0.2360 - accuracy: 0.8863 - val_loss: 0.2268 - val_accuracy: 0.8913
Epoch 91/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.2373 - accuracy: 0.8861
Epoch 91: val_loss did not improve from 0.20869
852/852 [==============================] - 2s 2ms/step - loss: 0.2375 - accuracy: 0.8861 - val_loss: 0.2165 - val_accuracy: 0.8996
Epoch 92/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.2350 - accuracy: 0.8864
Epoch 92: val_loss improved from 0.20869 to 0.20649, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.2347 - accuracy: 0.8864 - val_loss: 0.2065 - val_accuracy: 0.9008
Epoch 93/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.2371 - accuracy: 0.8824
Epoch 93: val_loss did not improve from 0.20649
852/852 [==============================] - 2s 2ms/step - loss: 0.2371 - accuracy: 0.8825 - val_loss: 0.2208 - val_accuracy: 0.8923
Epoch 94/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.2342 - accuracy: 0.8867
Epoch 94: val_loss did not improve from 0.20649
852/852 [==============================] - 2s 2ms/step - loss: 0.2338 - accuracy: 0.8867 - val_loss: 0.2133 - val_accuracy: 0.9004
Epoch 95/150
852/852 [==============================] - ETA: 0s - loss: 0.2336 - accuracy: 0.8869
Epoch 95: val_loss did not improve from 0.20649
852/852 [==============================] - 2s 2ms/step - loss: 0.2336 - accuracy: 0.8869 - val_loss: 0.2251 - val_accuracy: 0.8917
Epoch 96/150
815/852 [===========================&gt;..] - ETA: 0s - loss: 0.2250 - accuracy: 0.8914
Epoch 96: val_loss did not improve from 0.20649
852/852 [==============================] - 2s 2ms/step - loss: 0.2247 - accuracy: 0.8915 - val_loss: 0.2158 - val_accuracy: 0.8941
Epoch 97/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.2397 - accuracy: 0.8827
Epoch 97: val_loss did not improve from 0.20649
852/852 [==============================] - 2s 2ms/step - loss: 0.2386 - accuracy: 0.8839 - val_loss: 0.2236 - val_accuracy: 0.8957
Epoch 98/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.2318 - accuracy: 0.8870
Epoch 98: val_loss did not improve from 0.20649
852/852 [==============================] - 2s 2ms/step - loss: 0.2319 - accuracy: 0.8863 - val_loss: 0.2121 - val_accuracy: 0.8978
Epoch 99/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.2343 - accuracy: 0.8877
Epoch 99: val_loss did not improve from 0.20649
852/852 [==============================] - 2s 2ms/step - loss: 0.2341 - accuracy: 0.8887 - val_loss: 0.2240 - val_accuracy: 0.8935
Epoch 100/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.2279 - accuracy: 0.8894
Epoch 100: val_loss did not improve from 0.20649
852/852 [==============================] - 2s 2ms/step - loss: 0.2281 - accuracy: 0.8893 - val_loss: 0.2247 - val_accuracy: 0.8938
Epoch 101/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.2352 - accuracy: 0.8855
Epoch 101: val_loss did not improve from 0.20649
852/852 [==============================] - 2s 2ms/step - loss: 0.2334 - accuracy: 0.8861 - val_loss: 0.2231 - val_accuracy: 0.8942
Epoch 102/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.2479 - accuracy: 0.8867
Epoch 102: val_loss did not improve from 0.20649
852/852 [==============================] - 2s 2ms/step - loss: 0.2476 - accuracy: 0.8869 - val_loss: 0.2112 - val_accuracy: 0.9028
Epoch 103/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.2211 - accuracy: 0.8925
Epoch 103: val_loss improved from 0.20649 to 0.19766, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.2222 - accuracy: 0.8920 - val_loss: 0.1977 - val_accuracy: 0.9036
Epoch 104/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.2330 - accuracy: 0.8845
Epoch 104: val_loss did not improve from 0.19766
852/852 [==============================] - 2s 2ms/step - loss: 0.2335 - accuracy: 0.8841 - val_loss: 0.2182 - val_accuracy: 0.8971
Epoch 105/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.2292 - accuracy: 0.8883
Epoch 105: val_loss did not improve from 0.19766
852/852 [==============================] - 2s 2ms/step - loss: 0.2298 - accuracy: 0.8873 - val_loss: 0.2197 - val_accuracy: 0.8950
Epoch 106/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.2320 - accuracy: 0.8882
Epoch 106: val_loss did not improve from 0.19766
852/852 [==============================] - 2s 2ms/step - loss: 0.2321 - accuracy: 0.8881 - val_loss: 0.2281 - val_accuracy: 0.8924
Epoch 107/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.2316 - accuracy: 0.8871
Epoch 107: val_loss did not improve from 0.19766
852/852 [==============================] - 2s 2ms/step - loss: 0.2319 - accuracy: 0.8869 - val_loss: 0.2080 - val_accuracy: 0.9039
Epoch 108/150
819/852 [===========================&gt;..] - ETA: 0s - loss: 0.2205 - accuracy: 0.8929
Epoch 108: val_loss improved from 0.19766 to 0.19585, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.2200 - accuracy: 0.8930 - val_loss: 0.1958 - val_accuracy: 0.9050
Epoch 109/150
812/852 [===========================&gt;..] - ETA: 0s - loss: 0.2233 - accuracy: 0.8937
Epoch 109: val_loss did not improve from 0.19585
852/852 [==============================] - 2s 2ms/step - loss: 0.2229 - accuracy: 0.8936 - val_loss: 0.2056 - val_accuracy: 0.8988
Epoch 110/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.2389 - accuracy: 0.8881
Epoch 110: val_loss did not improve from 0.19585
852/852 [==============================] - 2s 2ms/step - loss: 0.2393 - accuracy: 0.8875 - val_loss: 0.2218 - val_accuracy: 0.8950
Epoch 111/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.2233 - accuracy: 0.8905
Epoch 111: val_loss did not improve from 0.19585
852/852 [==============================] - 2s 2ms/step - loss: 0.2229 - accuracy: 0.8909 - val_loss: 0.1971 - val_accuracy: 0.9064
Epoch 112/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.2314 - accuracy: 0.8873
Epoch 112: val_loss did not improve from 0.19585
852/852 [==============================] - 2s 2ms/step - loss: 0.2307 - accuracy: 0.8879 - val_loss: 0.2055 - val_accuracy: 0.9045
Epoch 113/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.2246 - accuracy: 0.8907
Epoch 113: val_loss did not improve from 0.19585
852/852 [==============================] - 2s 2ms/step - loss: 0.2245 - accuracy: 0.8904 - val_loss: 0.1997 - val_accuracy: 0.9065
Epoch 114/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.2306 - accuracy: 0.8916
Epoch 114: val_loss did not improve from 0.19585
852/852 [==============================] - 2s 2ms/step - loss: 0.2314 - accuracy: 0.8910 - val_loss: 0.1967 - val_accuracy: 0.9088
Epoch 115/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.2185 - accuracy: 0.8966
Epoch 115: val_loss did not improve from 0.19585
852/852 [==============================] - 2s 2ms/step - loss: 0.2183 - accuracy: 0.8967 - val_loss: 0.2017 - val_accuracy: 0.9057
Epoch 116/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.2223 - accuracy: 0.8904
Epoch 116: val_loss did not improve from 0.19585
852/852 [==============================] - 2s 2ms/step - loss: 0.2236 - accuracy: 0.8900 - val_loss: 0.2290 - val_accuracy: 0.8949
Epoch 117/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.2405 - accuracy: 0.8917
Epoch 117: val_loss did not improve from 0.19585
852/852 [==============================] - 2s 2ms/step - loss: 0.2392 - accuracy: 0.8922 - val_loss: 0.2014 - val_accuracy: 0.9063
Epoch 118/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.2305 - accuracy: 0.8896
Epoch 118: val_loss improved from 0.19585 to 0.19099, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.2286 - accuracy: 0.8907 - val_loss: 0.1910 - val_accuracy: 0.9075
Epoch 119/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.2129 - accuracy: 0.8976
Epoch 119: val_loss did not improve from 0.19099
852/852 [==============================] - 2s 3ms/step - loss: 0.2154 - accuracy: 0.8961 - val_loss: 0.2357 - val_accuracy: 0.8891
Epoch 120/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.2216 - accuracy: 0.8948
Epoch 120: val_loss did not improve from 0.19099
852/852 [==============================] - 2s 2ms/step - loss: 0.2239 - accuracy: 0.8935 - val_loss: 0.2258 - val_accuracy: 0.8901
Epoch 121/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.2283 - accuracy: 0.8907
Epoch 121: val_loss did not improve from 0.19099
852/852 [==============================] - 2s 2ms/step - loss: 0.2279 - accuracy: 0.8914 - val_loss: 0.1966 - val_accuracy: 0.9106
Epoch 122/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.2195 - accuracy: 0.8951
Epoch 122: val_loss did not improve from 0.19099
852/852 [==============================] - 2s 2ms/step - loss: 0.2195 - accuracy: 0.8951 - val_loss: 0.1914 - val_accuracy: 0.9119
Epoch 123/150
812/852 [===========================&gt;..] - ETA: 0s - loss: 0.2334 - accuracy: 0.8903
Epoch 123: val_loss did not improve from 0.19099
852/852 [==============================] - 2s 2ms/step - loss: 0.2333 - accuracy: 0.8904 - val_loss: 0.2232 - val_accuracy: 0.8976
Epoch 124/150
814/852 [===========================&gt;..] - ETA: 0s - loss: 0.2233 - accuracy: 0.8928
Epoch 124: val_loss did not improve from 0.19099
852/852 [==============================] - 2s 2ms/step - loss: 0.2226 - accuracy: 0.8926 - val_loss: 0.1998 - val_accuracy: 0.9077
Epoch 125/150
814/852 [===========================&gt;..] - ETA: 0s - loss: 0.2172 - accuracy: 0.8943
Epoch 125: val_loss did not improve from 0.19099
852/852 [==============================] - 2s 2ms/step - loss: 0.2164 - accuracy: 0.8946 - val_loss: 0.2021 - val_accuracy: 0.9062
Epoch 126/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.2233 - accuracy: 0.8939
Epoch 126: val_loss did not improve from 0.19099
852/852 [==============================] - 2s 2ms/step - loss: 0.2254 - accuracy: 0.8931 - val_loss: 0.2076 - val_accuracy: 0.9016
Epoch 127/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.2271 - accuracy: 0.8933
Epoch 127: val_loss did not improve from 0.19099
852/852 [==============================] - 2s 3ms/step - loss: 0.2267 - accuracy: 0.8935 - val_loss: 0.2236 - val_accuracy: 0.8943
Epoch 128/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.2179 - accuracy: 0.8948
Epoch 128: val_loss did not improve from 0.19099
852/852 [==============================] - 2s 2ms/step - loss: 0.2185 - accuracy: 0.8942 - val_loss: 0.2110 - val_accuracy: 0.8991
Epoch 129/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.2184 - accuracy: 0.8946
Epoch 129: val_loss improved from 0.19099 to 0.18932, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.2178 - accuracy: 0.8950 - val_loss: 0.1893 - val_accuracy: 0.9103
Epoch 130/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.2151 - accuracy: 0.8954
Epoch 130: val_loss did not improve from 0.18932
852/852 [==============================] - 2s 3ms/step - loss: 0.2153 - accuracy: 0.8953 - val_loss: 0.2106 - val_accuracy: 0.9038
Epoch 131/150
818/852 [===========================&gt;..] - ETA: 0s - loss: 0.2247 - accuracy: 0.8912
Epoch 131: val_loss did not improve from 0.18932
852/852 [==============================] - 2s 2ms/step - loss: 0.2236 - accuracy: 0.8917 - val_loss: 0.2002 - val_accuracy: 0.9052
Epoch 132/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.2235 - accuracy: 0.8949
Epoch 132: val_loss did not improve from 0.18932
852/852 [==============================] - 2s 2ms/step - loss: 0.2241 - accuracy: 0.8942 - val_loss: 0.2304 - val_accuracy: 0.8943
Epoch 133/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.2258 - accuracy: 0.8931
Epoch 133: val_loss improved from 0.18932 to 0.18768, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.2264 - accuracy: 0.8927 - val_loss: 0.1877 - val_accuracy: 0.9115
Epoch 134/150
815/852 [===========================&gt;..] - ETA: 0s - loss: 0.2089 - accuracy: 0.9027
Epoch 134: val_loss did not improve from 0.18768
852/852 [==============================] - 2s 2ms/step - loss: 0.2097 - accuracy: 0.9016 - val_loss: 0.2063 - val_accuracy: 0.9051
Epoch 135/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.2129 - accuracy: 0.8977
Epoch 135: val_loss did not improve from 0.18768
852/852 [==============================] - 2s 3ms/step - loss: 0.2132 - accuracy: 0.8977 - val_loss: 0.1910 - val_accuracy: 0.9112
Epoch 136/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.2188 - accuracy: 0.8956
Epoch 136: val_loss did not improve from 0.18768
852/852 [==============================] - 2s 2ms/step - loss: 0.2191 - accuracy: 0.8958 - val_loss: 0.1980 - val_accuracy: 0.9062
Epoch 137/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.2148 - accuracy: 0.8993
Epoch 137: val_loss improved from 0.18768 to 0.18649, saving model to two_fortynine_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.2157 - accuracy: 0.8991 - val_loss: 0.1865 - val_accuracy: 0.9129
Epoch 138/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.2225 - accuracy: 0.8946
Epoch 138: val_loss did not improve from 0.18649
852/852 [==============================] - 2s 2ms/step - loss: 0.2236 - accuracy: 0.8931 - val_loss: 0.1942 - val_accuracy: 0.9115
Epoch 139/150
811/852 [===========================&gt;..] - ETA: 0s - loss: 0.2149 - accuracy: 0.8982
Epoch 139: val_loss did not improve from 0.18649
852/852 [==============================] - 2s 2ms/step - loss: 0.2156 - accuracy: 0.8976 - val_loss: 0.2263 - val_accuracy: 0.8913
Epoch 140/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.2186 - accuracy: 0.8998
Epoch 140: val_loss did not improve from 0.18649
852/852 [==============================] - 2s 2ms/step - loss: 0.2189 - accuracy: 0.8995 - val_loss: 0.1921 - val_accuracy: 0.9119
Epoch 141/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.2124 - accuracy: 0.8983
Epoch 141: val_loss did not improve from 0.18649
852/852 [==============================] - 2s 2ms/step - loss: 0.2116 - accuracy: 0.8989 - val_loss: 0.2011 - val_accuracy: 0.9055
Epoch 142/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.2198 - accuracy: 0.8975
Epoch 142: val_loss did not improve from 0.18649
852/852 [==============================] - 2s 2ms/step - loss: 0.2187 - accuracy: 0.8974 - val_loss: 0.1869 - val_accuracy: 0.9122
Epoch 143/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.2135 - accuracy: 0.8992
Epoch 143: val_loss did not improve from 0.18649
852/852 [==============================] - 2s 2ms/step - loss: 0.2152 - accuracy: 0.8981 - val_loss: 0.2025 - val_accuracy: 0.9031
Epoch 144/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.2222 - accuracy: 0.8951
Epoch 144: val_loss did not improve from 0.18649
852/852 [==============================] - 2s 2ms/step - loss: 0.2221 - accuracy: 0.8951 - val_loss: 0.2233 - val_accuracy: 0.8943
Epoch 145/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.2084 - accuracy: 0.9015
Epoch 145: val_loss did not improve from 0.18649
852/852 [==============================] - 2s 2ms/step - loss: 0.2080 - accuracy: 0.9016 - val_loss: 0.2018 - val_accuracy: 0.9034
Epoch 146/150
852/852 [==============================] - ETA: 0s - loss: 0.2072 - accuracy: 0.9012
Epoch 146: val_loss did not improve from 0.18649
852/852 [==============================] - 2s 2ms/step - loss: 0.2072 - accuracy: 0.9012 - val_loss: 0.1946 - val_accuracy: 0.9091
Epoch 147/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.2111 - accuracy: 0.8972
Epoch 147: val_loss did not improve from 0.18649
852/852 [==============================] - 2s 2ms/step - loss: 0.2105 - accuracy: 0.8976 - val_loss: 0.2012 - val_accuracy: 0.9048
Epoch 148/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.2089 - accuracy: 0.8995
Epoch 148: val_loss did not improve from 0.18649
852/852 [==============================] - 2s 2ms/step - loss: 0.2086 - accuracy: 0.8995 - val_loss: 0.1925 - val_accuracy: 0.9090
Epoch 149/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.1996 - accuracy: 0.9062
Epoch 149: val_loss did not improve from 0.18649
852/852 [==============================] - 2s 2ms/step - loss: 0.1992 - accuracy: 0.9065 - val_loss: 0.2037 - val_accuracy: 0.9048
Epoch 150/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.2238 - accuracy: 0.8954
Epoch 150: val_loss did not improve from 0.18649
852/852 [==============================] - 2s 2ms/step - loss: 0.2245 - accuracy: 0.8951 - val_loss: 0.1990 - val_accuracy: 0.9055
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=7acfd0c7-f38f-498e-a1ca-45f87480e58c">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [80]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">history_249</span> <span class="o">=</span> <span class="n">two_fortynine_neuron</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">callback_a</span><span class="p">,</span> <span class="n">callback_b</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.1963 - accuracy: 0.9058
Epoch 1: val_loss improved from 0.23781 to 0.17656, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.1962 - accuracy: 0.9062 - val_loss: 0.1766 - val_accuracy: 0.9165
Epoch 2/150
813/852 [===========================&gt;..] - ETA: 0s - loss: 0.2112 - accuracy: 0.8995
Epoch 2: val_loss did not improve from 0.17656
852/852 [==============================] - 2s 2ms/step - loss: 0.2106 - accuracy: 0.9002 - val_loss: 0.1948 - val_accuracy: 0.9068
Epoch 3/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.2011 - accuracy: 0.9041
Epoch 3: val_loss did not improve from 0.17656
852/852 [==============================] - 2s 2ms/step - loss: 0.2030 - accuracy: 0.9027 - val_loss: 0.2155 - val_accuracy: 0.9003
Epoch 4/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.2139 - accuracy: 0.9013
Epoch 4: val_loss did not improve from 0.17656
852/852 [==============================] - 2s 2ms/step - loss: 0.2142 - accuracy: 0.9009 - val_loss: 0.1839 - val_accuracy: 0.9177
Epoch 5/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.2080 - accuracy: 0.9009
Epoch 5: val_loss did not improve from 0.17656
852/852 [==============================] - 2s 2ms/step - loss: 0.2086 - accuracy: 0.9005 - val_loss: 0.1981 - val_accuracy: 0.9140
Epoch 6/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.2084 - accuracy: 0.9006
Epoch 6: val_loss did not improve from 0.17656
852/852 [==============================] - 2s 2ms/step - loss: 0.2083 - accuracy: 0.9004 - val_loss: 0.1831 - val_accuracy: 0.9172
Epoch 7/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.2120 - accuracy: 0.8970
Epoch 7: val_loss did not improve from 0.17656
852/852 [==============================] - 2s 2ms/step - loss: 0.2115 - accuracy: 0.8976 - val_loss: 0.2149 - val_accuracy: 0.9037
Epoch 8/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.2131 - accuracy: 0.8992
Epoch 8: val_loss did not improve from 0.17656
852/852 [==============================] - 2s 2ms/step - loss: 0.2136 - accuracy: 0.8987 - val_loss: 0.1794 - val_accuracy: 0.9171
Epoch 9/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.1971 - accuracy: 0.9024
Epoch 9: val_loss did not improve from 0.17656
852/852 [==============================] - 2s 2ms/step - loss: 0.1998 - accuracy: 0.9012 - val_loss: 0.2135 - val_accuracy: 0.8975
Epoch 10/150
852/852 [==============================] - ETA: 0s - loss: 0.2091 - accuracy: 0.8977
Epoch 10: val_loss did not improve from 0.17656
852/852 [==============================] - 2s 2ms/step - loss: 0.2091 - accuracy: 0.8977 - val_loss: 0.2305 - val_accuracy: 0.8961
Epoch 11/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.2086 - accuracy: 0.9036
Epoch 11: val_loss did not improve from 0.17656
852/852 [==============================] - 2s 2ms/step - loss: 0.2105 - accuracy: 0.9021 - val_loss: 0.2047 - val_accuracy: 0.9043
Epoch 12/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.2146 - accuracy: 0.8961
Epoch 12: val_loss did not improve from 0.17656
852/852 [==============================] - 2s 2ms/step - loss: 0.2170 - accuracy: 0.8948 - val_loss: 0.1942 - val_accuracy: 0.9105
Epoch 13/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.2100 - accuracy: 0.8987
Epoch 13: val_loss did not improve from 0.17656
852/852 [==============================] - 2s 2ms/step - loss: 0.2097 - accuracy: 0.8989 - val_loss: 0.1990 - val_accuracy: 0.9083
Epoch 14/150
852/852 [==============================] - ETA: 0s - loss: 0.2155 - accuracy: 0.8998
Epoch 14: val_loss did not improve from 0.17656
852/852 [==============================] - 2s 3ms/step - loss: 0.2155 - accuracy: 0.8998 - val_loss: 0.1910 - val_accuracy: 0.9112
Epoch 15/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.2028 - accuracy: 0.9074
Epoch 15: val_loss did not improve from 0.17656
852/852 [==============================] - 2s 2ms/step - loss: 0.2020 - accuracy: 0.9078 - val_loss: 0.1779 - val_accuracy: 0.9186
Epoch 16/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.1935 - accuracy: 0.9056
Epoch 16: val_loss did not improve from 0.17656
852/852 [==============================] - 2s 2ms/step - loss: 0.1933 - accuracy: 0.9055 - val_loss: 0.1902 - val_accuracy: 0.9097
Epoch 17/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.2100 - accuracy: 0.8977
Epoch 17: val_loss did not improve from 0.17656
852/852 [==============================] - 2s 2ms/step - loss: 0.2090 - accuracy: 0.8984 - val_loss: 0.1930 - val_accuracy: 0.9109
Epoch 18/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.2151 - accuracy: 0.8967
Epoch 18: val_loss improved from 0.17656 to 0.17074, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.2129 - accuracy: 0.8978 - val_loss: 0.1707 - val_accuracy: 0.9216
Epoch 19/150
819/852 [===========================&gt;..] - ETA: 0s - loss: 0.1991 - accuracy: 0.9045
Epoch 19: val_loss did not improve from 0.17074
852/852 [==============================] - 2s 2ms/step - loss: 0.1985 - accuracy: 0.9049 - val_loss: 0.1835 - val_accuracy: 0.9156
Epoch 20/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.2057 - accuracy: 0.9035
Epoch 20: val_loss did not improve from 0.17074
852/852 [==============================] - 2s 2ms/step - loss: 0.2059 - accuracy: 0.9029 - val_loss: 0.1726 - val_accuracy: 0.9191
Epoch 21/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.1950 - accuracy: 0.9056
Epoch 21: val_loss did not improve from 0.17074
852/852 [==============================] - 2s 2ms/step - loss: 0.1949 - accuracy: 0.9057 - val_loss: 0.1718 - val_accuracy: 0.9209
Epoch 22/150
810/852 [===========================&gt;..] - ETA: 0s - loss: 0.2069 - accuracy: 0.8993
Epoch 22: val_loss did not improve from 0.17074
852/852 [==============================] - 2s 2ms/step - loss: 0.2075 - accuracy: 0.8991 - val_loss: 0.1820 - val_accuracy: 0.9156
Epoch 23/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.1959 - accuracy: 0.9080
Epoch 23: val_loss did not improve from 0.17074
852/852 [==============================] - 2s 2ms/step - loss: 0.1972 - accuracy: 0.9075 - val_loss: 0.1787 - val_accuracy: 0.9194
Epoch 24/150
812/852 [===========================&gt;..] - ETA: 0s - loss: 0.2019 - accuracy: 0.9021
Epoch 24: val_loss did not improve from 0.17074
852/852 [==============================] - 2s 2ms/step - loss: 0.2013 - accuracy: 0.9025 - val_loss: 0.1777 - val_accuracy: 0.9186
Epoch 25/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.1990 - accuracy: 0.9085
Epoch 25: val_loss did not improve from 0.17074
852/852 [==============================] - 2s 2ms/step - loss: 0.2006 - accuracy: 0.9072 - val_loss: 0.1801 - val_accuracy: 0.9164
Epoch 26/150
819/852 [===========================&gt;..] - ETA: 0s - loss: 0.1935 - accuracy: 0.9044
Epoch 26: val_loss did not improve from 0.17074
852/852 [==============================] - 2s 2ms/step - loss: 0.1936 - accuracy: 0.9050 - val_loss: 0.1874 - val_accuracy: 0.9136
Epoch 27/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.2057 - accuracy: 0.9029
Epoch 27: val_loss did not improve from 0.17074
852/852 [==============================] - 2s 2ms/step - loss: 0.2045 - accuracy: 0.9038 - val_loss: 0.1774 - val_accuracy: 0.9151
Epoch 28/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.2048 - accuracy: 0.9037
Epoch 28: val_loss did not improve from 0.17074
852/852 [==============================] - 2s 2ms/step - loss: 0.2044 - accuracy: 0.9038 - val_loss: 0.1834 - val_accuracy: 0.9135
Epoch 29/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.1901 - accuracy: 0.9116
Epoch 29: val_loss did not improve from 0.17074
852/852 [==============================] - 2s 2ms/step - loss: 0.1903 - accuracy: 0.9116 - val_loss: 0.1948 - val_accuracy: 0.9089
Epoch 30/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.1987 - accuracy: 0.9051
Epoch 30: val_loss did not improve from 0.17074
852/852 [==============================] - 2s 2ms/step - loss: 0.1979 - accuracy: 0.9055 - val_loss: 0.1780 - val_accuracy: 0.9177
Epoch 31/150
810/852 [===========================&gt;..] - ETA: 0s - loss: 0.2091 - accuracy: 0.9032
Epoch 31: val_loss did not improve from 0.17074
852/852 [==============================] - 2s 2ms/step - loss: 0.2086 - accuracy: 0.9031 - val_loss: 0.1918 - val_accuracy: 0.9099
Epoch 32/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.1938 - accuracy: 0.9089
Epoch 32: val_loss did not improve from 0.17074
852/852 [==============================] - 2s 2ms/step - loss: 0.1938 - accuracy: 0.9089 - val_loss: 0.1953 - val_accuracy: 0.9110
Epoch 33/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.2100 - accuracy: 0.9008
Epoch 33: val_loss did not improve from 0.17074
852/852 [==============================] - 2s 2ms/step - loss: 0.2093 - accuracy: 0.9015 - val_loss: 0.1767 - val_accuracy: 0.9179
Epoch 34/150
813/852 [===========================&gt;..] - ETA: 0s - loss: 0.1889 - accuracy: 0.9095
Epoch 34: val_loss did not improve from 0.17074
852/852 [==============================] - 2s 2ms/step - loss: 0.1908 - accuracy: 0.9095 - val_loss: 0.1778 - val_accuracy: 0.9174
Epoch 35/150
810/852 [===========================&gt;..] - ETA: 0s - loss: 0.1932 - accuracy: 0.9054
Epoch 35: val_loss did not improve from 0.17074
852/852 [==============================] - 2s 2ms/step - loss: 0.1928 - accuracy: 0.9055 - val_loss: 0.1812 - val_accuracy: 0.9151
Epoch 36/150
818/852 [===========================&gt;..] - ETA: 0s - loss: 0.2064 - accuracy: 0.9039
Epoch 36: val_loss did not improve from 0.17074
852/852 [==============================] - 2s 2ms/step - loss: 0.2076 - accuracy: 0.9042 - val_loss: 0.1935 - val_accuracy: 0.9111
Epoch 37/150
811/852 [===========================&gt;..] - ETA: 0s - loss: 0.1971 - accuracy: 0.9085
Epoch 37: val_loss improved from 0.17074 to 0.17055, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.1971 - accuracy: 0.9088 - val_loss: 0.1705 - val_accuracy: 0.9179
Epoch 38/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.1881 - accuracy: 0.9094
Epoch 38: val_loss did not improve from 0.17055
852/852 [==============================] - 3s 4ms/step - loss: 0.1904 - accuracy: 0.9086 - val_loss: 0.1821 - val_accuracy: 0.9160
Epoch 39/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.1866 - accuracy: 0.9103
Epoch 39: val_loss did not improve from 0.17055
852/852 [==============================] - 2s 2ms/step - loss: 0.1868 - accuracy: 0.9103 - val_loss: 0.1833 - val_accuracy: 0.9130
Epoch 40/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.1988 - accuracy: 0.9048
Epoch 40: val_loss did not improve from 0.17055
852/852 [==============================] - 2s 2ms/step - loss: 0.1986 - accuracy: 0.9044 - val_loss: 0.1851 - val_accuracy: 0.9158
Epoch 41/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.2049 - accuracy: 0.9061
Epoch 41: val_loss did not improve from 0.17055
852/852 [==============================] - 2s 2ms/step - loss: 0.2062 - accuracy: 0.9050 - val_loss: 0.1941 - val_accuracy: 0.9128
Epoch 42/150
814/852 [===========================&gt;..] - ETA: 0s - loss: 0.2014 - accuracy: 0.9039
Epoch 42: val_loss did not improve from 0.17055
852/852 [==============================] - 2s 2ms/step - loss: 0.2011 - accuracy: 0.9041 - val_loss: 0.1883 - val_accuracy: 0.9138
Epoch 43/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.1965 - accuracy: 0.9054
Epoch 43: val_loss did not improve from 0.17055
852/852 [==============================] - 2s 2ms/step - loss: 0.1962 - accuracy: 0.9056 - val_loss: 0.1806 - val_accuracy: 0.9184
Epoch 44/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.1955 - accuracy: 0.9079
Epoch 44: val_loss did not improve from 0.17055
852/852 [==============================] - 2s 2ms/step - loss: 0.1954 - accuracy: 0.9081 - val_loss: 0.1709 - val_accuracy: 0.9200
Epoch 45/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.1956 - accuracy: 0.9057
Epoch 45: val_loss did not improve from 0.17055
852/852 [==============================] - 2s 2ms/step - loss: 0.1964 - accuracy: 0.9054 - val_loss: 0.2214 - val_accuracy: 0.8950
Epoch 46/150
815/852 [===========================&gt;..] - ETA: 0s - loss: 0.1917 - accuracy: 0.9080
Epoch 46: val_loss improved from 0.17055 to 0.16662, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.1937 - accuracy: 0.9075 - val_loss: 0.1666 - val_accuracy: 0.9193
Epoch 47/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.2074 - accuracy: 0.9039
Epoch 47: val_loss did not improve from 0.16662
852/852 [==============================] - 2s 2ms/step - loss: 0.2081 - accuracy: 0.9027 - val_loss: 0.1782 - val_accuracy: 0.9221
Epoch 48/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.1955 - accuracy: 0.9045
Epoch 48: val_loss did not improve from 0.16662
852/852 [==============================] - 2s 2ms/step - loss: 0.1954 - accuracy: 0.9045 - val_loss: 0.1787 - val_accuracy: 0.9196
Epoch 49/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.1935 - accuracy: 0.9081
Epoch 49: val_loss did not improve from 0.16662
852/852 [==============================] - 2s 2ms/step - loss: 0.1938 - accuracy: 0.9078 - val_loss: 0.1911 - val_accuracy: 0.9089
Epoch 50/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.1912 - accuracy: 0.9059
Epoch 50: val_loss did not improve from 0.16662
852/852 [==============================] - 2s 2ms/step - loss: 0.1915 - accuracy: 0.9061 - val_loss: 0.1693 - val_accuracy: 0.9221
Epoch 51/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.1900 - accuracy: 0.9102
Epoch 51: val_loss did not improve from 0.16662
852/852 [==============================] - 2s 2ms/step - loss: 0.1895 - accuracy: 0.9102 - val_loss: 0.1856 - val_accuracy: 0.9183
Epoch 52/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.1954 - accuracy: 0.9071
Epoch 52: val_loss did not improve from 0.16662
852/852 [==============================] - 2s 2ms/step - loss: 0.1957 - accuracy: 0.9062 - val_loss: 0.1692 - val_accuracy: 0.9197
Epoch 53/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.1881 - accuracy: 0.9106
Epoch 53: val_loss did not improve from 0.16662
852/852 [==============================] - 2s 2ms/step - loss: 0.1879 - accuracy: 0.9105 - val_loss: 0.1825 - val_accuracy: 0.9172
Epoch 54/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.1980 - accuracy: 0.9068
Epoch 54: val_loss did not improve from 0.16662
852/852 [==============================] - 2s 2ms/step - loss: 0.1980 - accuracy: 0.9066 - val_loss: 0.1814 - val_accuracy: 0.9167
Epoch 55/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.1996 - accuracy: 0.9085
Epoch 55: val_loss did not improve from 0.16662
852/852 [==============================] - 2s 2ms/step - loss: 0.1992 - accuracy: 0.9086 - val_loss: 0.1699 - val_accuracy: 0.9217
Epoch 56/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.1921 - accuracy: 0.9095
Epoch 56: val_loss improved from 0.16662 to 0.16230, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.1915 - accuracy: 0.9099 - val_loss: 0.1623 - val_accuracy: 0.9252
Epoch 57/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.1849 - accuracy: 0.9100
Epoch 57: val_loss did not improve from 0.16230
852/852 [==============================] - 2s 2ms/step - loss: 0.1851 - accuracy: 0.9098 - val_loss: 0.1798 - val_accuracy: 0.9183
Epoch 58/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.1911 - accuracy: 0.9097
Epoch 58: val_loss improved from 0.16230 to 0.16176, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.1908 - accuracy: 0.9096 - val_loss: 0.1618 - val_accuracy: 0.9252
Epoch 59/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.1876 - accuracy: 0.9082
Epoch 59: val_loss did not improve from 0.16176
852/852 [==============================] - 2s 2ms/step - loss: 0.1876 - accuracy: 0.9086 - val_loss: 0.1697 - val_accuracy: 0.9203
Epoch 60/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.1882 - accuracy: 0.9105
Epoch 60: val_loss did not improve from 0.16176
852/852 [==============================] - 2s 2ms/step - loss: 0.1892 - accuracy: 0.9103 - val_loss: 0.1742 - val_accuracy: 0.9236
Epoch 61/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.1900 - accuracy: 0.9069
Epoch 61: val_loss did not improve from 0.16176
852/852 [==============================] - 2s 2ms/step - loss: 0.1918 - accuracy: 0.9061 - val_loss: 0.1785 - val_accuracy: 0.9147
Epoch 62/150
812/852 [===========================&gt;..] - ETA: 0s - loss: 0.1910 - accuracy: 0.9067
Epoch 62: val_loss improved from 0.16176 to 0.16175, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.1903 - accuracy: 0.9072 - val_loss: 0.1618 - val_accuracy: 0.9257
Epoch 63/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.1861 - accuracy: 0.9095
Epoch 63: val_loss did not improve from 0.16175
852/852 [==============================] - 2s 2ms/step - loss: 0.1861 - accuracy: 0.9096 - val_loss: 0.1786 - val_accuracy: 0.9162
Epoch 64/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.1989 - accuracy: 0.9089
Epoch 64: val_loss did not improve from 0.16175
852/852 [==============================] - 2s 2ms/step - loss: 0.1987 - accuracy: 0.9091 - val_loss: 0.1736 - val_accuracy: 0.9200
Epoch 65/150
811/852 [===========================&gt;..] - ETA: 0s - loss: 0.1854 - accuracy: 0.9094
Epoch 65: val_loss did not improve from 0.16175
852/852 [==============================] - 2s 2ms/step - loss: 0.1849 - accuracy: 0.9104 - val_loss: 0.1738 - val_accuracy: 0.9210
Epoch 66/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.1815 - accuracy: 0.9143
Epoch 66: val_loss did not improve from 0.16175
852/852 [==============================] - 2s 2ms/step - loss: 0.1814 - accuracy: 0.9143 - val_loss: 0.1708 - val_accuracy: 0.9204
Epoch 67/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.2074 - accuracy: 0.9024
Epoch 67: val_loss did not improve from 0.16175
852/852 [==============================] - 2s 2ms/step - loss: 0.2080 - accuracy: 0.9022 - val_loss: 0.1875 - val_accuracy: 0.9133
Epoch 68/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.1843 - accuracy: 0.9115
Epoch 68: val_loss improved from 0.16175 to 0.15809, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.1844 - accuracy: 0.9112 - val_loss: 0.1581 - val_accuracy: 0.9304
Epoch 69/150
852/852 [==============================] - ETA: 0s - loss: 0.1908 - accuracy: 0.9111
Epoch 69: val_loss did not improve from 0.15809
852/852 [==============================] - 2s 2ms/step - loss: 0.1908 - accuracy: 0.9111 - val_loss: 0.1814 - val_accuracy: 0.9136
Epoch 70/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.1838 - accuracy: 0.9121
Epoch 70: val_loss did not improve from 0.15809
852/852 [==============================] - 2s 2ms/step - loss: 0.1840 - accuracy: 0.9116 - val_loss: 0.1700 - val_accuracy: 0.9212
Epoch 71/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.1906 - accuracy: 0.9110
Epoch 71: val_loss did not improve from 0.15809
852/852 [==============================] - 2s 2ms/step - loss: 0.1913 - accuracy: 0.9110 - val_loss: 0.1883 - val_accuracy: 0.9130
Epoch 72/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.1820 - accuracy: 0.9144
Epoch 72: val_loss did not improve from 0.15809
852/852 [==============================] - 2s 2ms/step - loss: 0.1822 - accuracy: 0.9143 - val_loss: 0.1605 - val_accuracy: 0.9264
Epoch 73/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.1853 - accuracy: 0.9103
Epoch 73: val_loss did not improve from 0.15809
852/852 [==============================] - 2s 2ms/step - loss: 0.1846 - accuracy: 0.9105 - val_loss: 0.1832 - val_accuracy: 0.9194
Epoch 74/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.2015 - accuracy: 0.9099
Epoch 74: val_loss did not improve from 0.15809
852/852 [==============================] - 2s 2ms/step - loss: 0.1998 - accuracy: 0.9105 - val_loss: 0.1696 - val_accuracy: 0.9229
Epoch 75/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.1744 - accuracy: 0.9189
Epoch 75: val_loss did not improve from 0.15809
852/852 [==============================] - 2s 2ms/step - loss: 0.1750 - accuracy: 0.9189 - val_loss: 0.1897 - val_accuracy: 0.9178
Epoch 76/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.1808 - accuracy: 0.9154
Epoch 76: val_loss improved from 0.15809 to 0.15291, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.1803 - accuracy: 0.9152 - val_loss: 0.1529 - val_accuracy: 0.9306
Epoch 77/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.1745 - accuracy: 0.9156
Epoch 77: val_loss did not improve from 0.15291
852/852 [==============================] - 2s 2ms/step - loss: 0.1744 - accuracy: 0.9160 - val_loss: 0.1608 - val_accuracy: 0.9277
Epoch 78/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.1857 - accuracy: 0.9136
Epoch 78: val_loss did not improve from 0.15291
852/852 [==============================] - 2s 2ms/step - loss: 0.1856 - accuracy: 0.9137 - val_loss: 0.1945 - val_accuracy: 0.9119
Epoch 79/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.2025 - accuracy: 0.9097
Epoch 79: val_loss did not improve from 0.15291
852/852 [==============================] - 2s 2ms/step - loss: 0.2046 - accuracy: 0.9088 - val_loss: 0.2033 - val_accuracy: 0.9075
Epoch 80/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.1842 - accuracy: 0.9124
Epoch 80: val_loss did not improve from 0.15291
852/852 [==============================] - 2s 2ms/step - loss: 0.1865 - accuracy: 0.9117 - val_loss: 0.1666 - val_accuracy: 0.9230
Epoch 81/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.1820 - accuracy: 0.9139
Epoch 81: val_loss did not improve from 0.15291
852/852 [==============================] - 2s 2ms/step - loss: 0.1821 - accuracy: 0.9139 - val_loss: 0.1565 - val_accuracy: 0.9294
Epoch 82/150
813/852 [===========================&gt;..] - ETA: 0s - loss: 0.1809 - accuracy: 0.9130
Epoch 82: val_loss did not improve from 0.15291
852/852 [==============================] - 2s 2ms/step - loss: 0.1846 - accuracy: 0.9113 - val_loss: 0.1994 - val_accuracy: 0.9059
Epoch 83/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.1829 - accuracy: 0.9145
Epoch 83: val_loss did not improve from 0.15291
852/852 [==============================] - 2s 3ms/step - loss: 0.1836 - accuracy: 0.9146 - val_loss: 0.1556 - val_accuracy: 0.9300
Epoch 84/150
813/852 [===========================&gt;..] - ETA: 0s - loss: 0.1796 - accuracy: 0.9144
Epoch 84: val_loss did not improve from 0.15291
852/852 [==============================] - 2s 2ms/step - loss: 0.1815 - accuracy: 0.9142 - val_loss: 0.1713 - val_accuracy: 0.9204
Epoch 85/150
813/852 [===========================&gt;..] - ETA: 0s - loss: 0.1770 - accuracy: 0.9141
Epoch 85: val_loss did not improve from 0.15291
852/852 [==============================] - 2s 2ms/step - loss: 0.1763 - accuracy: 0.9139 - val_loss: 0.1708 - val_accuracy: 0.9212
Epoch 86/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.1978 - accuracy: 0.9091
Epoch 86: val_loss did not improve from 0.15291
852/852 [==============================] - 2s 2ms/step - loss: 0.1996 - accuracy: 0.9090 - val_loss: 0.1996 - val_accuracy: 0.9091
Epoch 87/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.1804 - accuracy: 0.9141
Epoch 87: val_loss improved from 0.15291 to 0.15116, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.1800 - accuracy: 0.9142 - val_loss: 0.1512 - val_accuracy: 0.9293
Epoch 88/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.1762 - accuracy: 0.9170
Epoch 88: val_loss did not improve from 0.15116
852/852 [==============================] - 2s 2ms/step - loss: 0.1765 - accuracy: 0.9170 - val_loss: 0.1843 - val_accuracy: 0.9250
Epoch 89/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.1903 - accuracy: 0.9122
Epoch 89: val_loss did not improve from 0.15116
852/852 [==============================] - 2s 2ms/step - loss: 0.1901 - accuracy: 0.9126 - val_loss: 0.1674 - val_accuracy: 0.9223
Epoch 90/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.1773 - accuracy: 0.9178
Epoch 90: val_loss improved from 0.15116 to 0.14931, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.1774 - accuracy: 0.9174 - val_loss: 0.1493 - val_accuracy: 0.9318
Epoch 91/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.1858 - accuracy: 0.9096
Epoch 91: val_loss did not improve from 0.14931
852/852 [==============================] - 2s 2ms/step - loss: 0.1847 - accuracy: 0.9099 - val_loss: 0.1518 - val_accuracy: 0.9291
Epoch 92/150
819/852 [===========================&gt;..] - ETA: 0s - loss: 0.1735 - accuracy: 0.9194
Epoch 92: val_loss did not improve from 0.14931
852/852 [==============================] - 2s 2ms/step - loss: 0.1742 - accuracy: 0.9191 - val_loss: 0.1532 - val_accuracy: 0.9278
Epoch 93/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.1769 - accuracy: 0.9157
Epoch 93: val_loss did not improve from 0.14931
852/852 [==============================] - 2s 2ms/step - loss: 0.1772 - accuracy: 0.9149 - val_loss: 0.1606 - val_accuracy: 0.9246
Epoch 94/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.1881 - accuracy: 0.9108
Epoch 94: val_loss did not improve from 0.14931
852/852 [==============================] - 2s 2ms/step - loss: 0.1873 - accuracy: 0.9113 - val_loss: 0.1702 - val_accuracy: 0.9250
Epoch 95/150
818/852 [===========================&gt;..] - ETA: 0s - loss: 0.1742 - accuracy: 0.9177
Epoch 95: val_loss did not improve from 0.14931
852/852 [==============================] - 2s 2ms/step - loss: 0.1765 - accuracy: 0.9164 - val_loss: 0.1687 - val_accuracy: 0.9238
Epoch 96/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.1869 - accuracy: 0.9123
Epoch 96: val_loss did not improve from 0.14931
852/852 [==============================] - 2s 2ms/step - loss: 0.1867 - accuracy: 0.9124 - val_loss: 0.1616 - val_accuracy: 0.9291
Epoch 97/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.1757 - accuracy: 0.9165
Epoch 97: val_loss did not improve from 0.14931
852/852 [==============================] - 2s 2ms/step - loss: 0.1759 - accuracy: 0.9164 - val_loss: 0.1777 - val_accuracy: 0.9170
Epoch 98/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.1906 - accuracy: 0.9092
Epoch 98: val_loss did not improve from 0.14931
852/852 [==============================] - 2s 2ms/step - loss: 0.1902 - accuracy: 0.9099 - val_loss: 0.1796 - val_accuracy: 0.9220
Epoch 99/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.1815 - accuracy: 0.9178
Epoch 99: val_loss did not improve from 0.14931
852/852 [==============================] - 2s 2ms/step - loss: 0.1807 - accuracy: 0.9178 - val_loss: 0.1546 - val_accuracy: 0.9299
Epoch 100/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.1792 - accuracy: 0.9169
Epoch 100: val_loss did not improve from 0.14931
852/852 [==============================] - 2s 2ms/step - loss: 0.1792 - accuracy: 0.9167 - val_loss: 0.1545 - val_accuracy: 0.9290
Epoch 101/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.1812 - accuracy: 0.9129
Epoch 101: val_loss did not improve from 0.14931
852/852 [==============================] - 2s 2ms/step - loss: 0.1804 - accuracy: 0.9136 - val_loss: 0.1707 - val_accuracy: 0.9232
Epoch 102/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.1747 - accuracy: 0.9148
Epoch 102: val_loss did not improve from 0.14931
852/852 [==============================] - 2s 2ms/step - loss: 0.1747 - accuracy: 0.9152 - val_loss: 0.1548 - val_accuracy: 0.9280
Epoch 103/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.1815 - accuracy: 0.9159
Epoch 103: val_loss did not improve from 0.14931
852/852 [==============================] - 2s 2ms/step - loss: 0.1815 - accuracy: 0.9162 - val_loss: 0.1764 - val_accuracy: 0.9202
Epoch 104/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.1855 - accuracy: 0.9125
Epoch 104: val_loss did not improve from 0.14931
852/852 [==============================] - 2s 2ms/step - loss: 0.1857 - accuracy: 0.9123 - val_loss: 0.1749 - val_accuracy: 0.9213
Epoch 105/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.1798 - accuracy: 0.9137
Epoch 105: val_loss did not improve from 0.14931
852/852 [==============================] - 2s 2ms/step - loss: 0.1799 - accuracy: 0.9133 - val_loss: 0.1632 - val_accuracy: 0.9263
Epoch 106/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.1735 - accuracy: 0.9171
Epoch 106: val_loss did not improve from 0.14931
852/852 [==============================] - 2s 2ms/step - loss: 0.1740 - accuracy: 0.9170 - val_loss: 0.1524 - val_accuracy: 0.9290
Epoch 107/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.1718 - accuracy: 0.9203
Epoch 107: val_loss did not improve from 0.14931
852/852 [==============================] - 2s 2ms/step - loss: 0.1737 - accuracy: 0.9189 - val_loss: 0.1801 - val_accuracy: 0.9177
Epoch 108/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.1831 - accuracy: 0.9173
Epoch 108: val_loss did not improve from 0.14931
852/852 [==============================] - 2s 2ms/step - loss: 0.1830 - accuracy: 0.9171 - val_loss: 0.1750 - val_accuracy: 0.9218
Epoch 109/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.1786 - accuracy: 0.9169
Epoch 109: val_loss improved from 0.14931 to 0.14641, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.1775 - accuracy: 0.9172 - val_loss: 0.1464 - val_accuracy: 0.9314
Epoch 110/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.1642 - accuracy: 0.9219
Epoch 110: val_loss did not improve from 0.14641
852/852 [==============================] - 2s 2ms/step - loss: 0.1651 - accuracy: 0.9218 - val_loss: 0.1822 - val_accuracy: 0.9158
Epoch 111/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.1810 - accuracy: 0.9158
Epoch 111: val_loss did not improve from 0.14641
852/852 [==============================] - 2s 2ms/step - loss: 0.1813 - accuracy: 0.9158 - val_loss: 0.1615 - val_accuracy: 0.9264
Epoch 112/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.1792 - accuracy: 0.9174
Epoch 112: val_loss did not improve from 0.14641
852/852 [==============================] - 2s 2ms/step - loss: 0.1778 - accuracy: 0.9180 - val_loss: 0.1730 - val_accuracy: 0.9226
Epoch 113/150
813/852 [===========================&gt;..] - ETA: 0s - loss: 0.1802 - accuracy: 0.9186
Epoch 113: val_loss did not improve from 0.14641
852/852 [==============================] - 2s 2ms/step - loss: 0.1807 - accuracy: 0.9180 - val_loss: 0.1650 - val_accuracy: 0.9258
Epoch 114/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.1844 - accuracy: 0.9129
Epoch 114: val_loss did not improve from 0.14641
852/852 [==============================] - 2s 2ms/step - loss: 0.1834 - accuracy: 0.9133 - val_loss: 0.1600 - val_accuracy: 0.9268
Epoch 115/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.1819 - accuracy: 0.9145
Epoch 115: val_loss did not improve from 0.14641
852/852 [==============================] - 2s 2ms/step - loss: 0.1809 - accuracy: 0.9147 - val_loss: 0.1540 - val_accuracy: 0.9293
Epoch 116/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.1710 - accuracy: 0.9224
Epoch 116: val_loss did not improve from 0.14641
852/852 [==============================] - 2s 2ms/step - loss: 0.1717 - accuracy: 0.9218 - val_loss: 0.1493 - val_accuracy: 0.9308
Epoch 117/150
812/852 [===========================&gt;..] - ETA: 0s - loss: 0.1737 - accuracy: 0.9167
Epoch 117: val_loss did not improve from 0.14641
852/852 [==============================] - 2s 2ms/step - loss: 0.1741 - accuracy: 0.9165 - val_loss: 0.1701 - val_accuracy: 0.9209
Epoch 118/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.1747 - accuracy: 0.9226
Epoch 118: val_loss did not improve from 0.14641
852/852 [==============================] - 2s 2ms/step - loss: 0.1746 - accuracy: 0.9227 - val_loss: 0.1550 - val_accuracy: 0.9297
Epoch 119/150
820/852 [===========================&gt;..] - ETA: 0s - loss: 0.1768 - accuracy: 0.9183
Epoch 119: val_loss did not improve from 0.14641
852/852 [==============================] - 2s 2ms/step - loss: 0.1773 - accuracy: 0.9182 - val_loss: 0.1649 - val_accuracy: 0.9251
Epoch 120/150
852/852 [==============================] - ETA: 0s - loss: 0.1763 - accuracy: 0.9156
Epoch 120: val_loss did not improve from 0.14641
852/852 [==============================] - 2s 2ms/step - loss: 0.1763 - accuracy: 0.9156 - val_loss: 0.1553 - val_accuracy: 0.9256
Epoch 121/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.1737 - accuracy: 0.9208
Epoch 121: val_loss did not improve from 0.14641
852/852 [==============================] - 2s 2ms/step - loss: 0.1734 - accuracy: 0.9214 - val_loss: 0.1498 - val_accuracy: 0.9308
Epoch 122/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.1761 - accuracy: 0.9186
Epoch 122: val_loss improved from 0.14641 to 0.14626, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.1760 - accuracy: 0.9184 - val_loss: 0.1463 - val_accuracy: 0.9338
Epoch 123/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.1727 - accuracy: 0.9162
Epoch 123: val_loss did not improve from 0.14626
852/852 [==============================] - 2s 2ms/step - loss: 0.1742 - accuracy: 0.9153 - val_loss: 0.1775 - val_accuracy: 0.9207
Epoch 124/150
852/852 [==============================] - ETA: 0s - loss: 0.1841 - accuracy: 0.9170
Epoch 124: val_loss did not improve from 0.14626
852/852 [==============================] - 2s 3ms/step - loss: 0.1841 - accuracy: 0.9170 - val_loss: 0.1502 - val_accuracy: 0.9295
Epoch 125/150
811/852 [===========================&gt;..] - ETA: 0s - loss: 0.1789 - accuracy: 0.9153
Epoch 125: val_loss did not improve from 0.14626
852/852 [==============================] - 2s 2ms/step - loss: 0.1779 - accuracy: 0.9157 - val_loss: 0.1564 - val_accuracy: 0.9319
Epoch 126/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.1651 - accuracy: 0.9227
Epoch 126: val_loss improved from 0.14626 to 0.14396, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.1652 - accuracy: 0.9224 - val_loss: 0.1440 - val_accuracy: 0.9339
Epoch 127/150
813/852 [===========================&gt;..] - ETA: 0s - loss: 0.1751 - accuracy: 0.9207
Epoch 127: val_loss did not improve from 0.14396
852/852 [==============================] - 2s 2ms/step - loss: 0.1758 - accuracy: 0.9202 - val_loss: 0.1805 - val_accuracy: 0.9243
Epoch 128/150
852/852 [==============================] - ETA: 0s - loss: 0.1807 - accuracy: 0.9187
Epoch 128: val_loss did not improve from 0.14396
852/852 [==============================] - 2s 2ms/step - loss: 0.1807 - accuracy: 0.9187 - val_loss: 0.1602 - val_accuracy: 0.9297
Epoch 129/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.1663 - accuracy: 0.9185
Epoch 129: val_loss did not improve from 0.14396
852/852 [==============================] - 2s 2ms/step - loss: 0.1662 - accuracy: 0.9190 - val_loss: 0.1452 - val_accuracy: 0.9320
Epoch 130/150
813/852 [===========================&gt;..] - ETA: 0s - loss: 0.1648 - accuracy: 0.9218
Epoch 130: val_loss did not improve from 0.14396
852/852 [==============================] - 2s 2ms/step - loss: 0.1646 - accuracy: 0.9216 - val_loss: 0.1448 - val_accuracy: 0.9317
Epoch 131/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.1686 - accuracy: 0.9219
Epoch 131: val_loss did not improve from 0.14396
852/852 [==============================] - 2s 2ms/step - loss: 0.1688 - accuracy: 0.9221 - val_loss: 0.1756 - val_accuracy: 0.9180
Epoch 132/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.1672 - accuracy: 0.9196
Epoch 132: val_loss did not improve from 0.14396
852/852 [==============================] - 2s 2ms/step - loss: 0.1670 - accuracy: 0.9198 - val_loss: 0.1489 - val_accuracy: 0.9314
Epoch 133/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.1759 - accuracy: 0.9183
Epoch 133: val_loss did not improve from 0.14396
852/852 [==============================] - 2s 2ms/step - loss: 0.1762 - accuracy: 0.9187 - val_loss: 0.1515 - val_accuracy: 0.9302
Epoch 134/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.1714 - accuracy: 0.9195
Epoch 134: val_loss did not improve from 0.14396
852/852 [==============================] - 2s 2ms/step - loss: 0.1715 - accuracy: 0.9194 - val_loss: 0.1524 - val_accuracy: 0.9297
Epoch 135/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.1630 - accuracy: 0.9232
Epoch 135: val_loss did not improve from 0.14396
852/852 [==============================] - 2s 2ms/step - loss: 0.1658 - accuracy: 0.9219 - val_loss: 0.1632 - val_accuracy: 0.9251
Epoch 136/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.1793 - accuracy: 0.9152
Epoch 136: val_loss did not improve from 0.14396
852/852 [==============================] - 2s 2ms/step - loss: 0.1784 - accuracy: 0.9153 - val_loss: 0.1529 - val_accuracy: 0.9294
Epoch 137/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.1675 - accuracy: 0.9193
Epoch 137: val_loss did not improve from 0.14396
852/852 [==============================] - 2s 2ms/step - loss: 0.1681 - accuracy: 0.9191 - val_loss: 0.1534 - val_accuracy: 0.9308
Epoch 138/150
811/852 [===========================&gt;..] - ETA: 0s - loss: 0.1737 - accuracy: 0.9186
Epoch 138: val_loss did not improve from 0.14396
852/852 [==============================] - 2s 2ms/step - loss: 0.1742 - accuracy: 0.9191 - val_loss: 0.1477 - val_accuracy: 0.9322
Epoch 139/150
852/852 [==============================] - ETA: 0s - loss: 0.1720 - accuracy: 0.9205
Epoch 139: val_loss did not improve from 0.14396
852/852 [==============================] - 2s 2ms/step - loss: 0.1720 - accuracy: 0.9205 - val_loss: 0.1447 - val_accuracy: 0.9319
Epoch 140/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.1801 - accuracy: 0.9162
Epoch 140: val_loss did not improve from 0.14396
852/852 [==============================] - 2s 2ms/step - loss: 0.1807 - accuracy: 0.9165 - val_loss: 0.1621 - val_accuracy: 0.9298
Epoch 141/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.1685 - accuracy: 0.9204
Epoch 141: val_loss did not improve from 0.14396
852/852 [==============================] - 2s 2ms/step - loss: 0.1680 - accuracy: 0.9207 - val_loss: 0.1635 - val_accuracy: 0.9274
Epoch 142/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.1621 - accuracy: 0.9263
Epoch 142: val_loss did not improve from 0.14396
852/852 [==============================] - 2s 2ms/step - loss: 0.1620 - accuracy: 0.9265 - val_loss: 0.1560 - val_accuracy: 0.9251
Epoch 143/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.1734 - accuracy: 0.9213
Epoch 143: val_loss did not improve from 0.14396
852/852 [==============================] - 2s 2ms/step - loss: 0.1723 - accuracy: 0.9218 - val_loss: 0.1511 - val_accuracy: 0.9321
Epoch 144/150
852/852 [==============================] - ETA: 0s - loss: 0.1661 - accuracy: 0.9219
Epoch 144: val_loss did not improve from 0.14396
852/852 [==============================] - 2s 2ms/step - loss: 0.1661 - accuracy: 0.9219 - val_loss: 0.1495 - val_accuracy: 0.9313
Epoch 145/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.1633 - accuracy: 0.9215
Epoch 145: val_loss did not improve from 0.14396
852/852 [==============================] - 2s 2ms/step - loss: 0.1647 - accuracy: 0.9202 - val_loss: 0.1565 - val_accuracy: 0.9315
Epoch 146/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.1595 - accuracy: 0.9252
Epoch 146: val_loss did not improve from 0.14396
852/852 [==============================] - 2s 2ms/step - loss: 0.1598 - accuracy: 0.9256 - val_loss: 0.1626 - val_accuracy: 0.9284
Epoch 146: early stopping
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=03dc16d9-8b75-4834-baff-265c68f99896">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [91]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">history_249</span> <span class="o">=</span> <span class="n">two_fortynine_neuron</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">callback_a</span><span class="p">,</span> <span class="n">callback_b</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.2184 - accuracy: 0.8975
Epoch 1: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.2190 - accuracy: 0.8973 - val_loss: 0.2181 - val_accuracy: 0.8976
Epoch 2/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.2267 - accuracy: 0.8945
Epoch 2: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.2256 - accuracy: 0.8949 - val_loss: 0.1876 - val_accuracy: 0.9124
Epoch 3/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.2122 - accuracy: 0.9010
Epoch 3: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.2121 - accuracy: 0.9007 - val_loss: 0.1957 - val_accuracy: 0.9074
Epoch 4/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.2243 - accuracy: 0.8964
Epoch 4: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.2243 - accuracy: 0.8962 - val_loss: 0.2347 - val_accuracy: 0.8897
Epoch 5/150
815/852 [===========================&gt;..] - ETA: 0s - loss: 0.2274 - accuracy: 0.8919
Epoch 5: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.2291 - accuracy: 0.8904 - val_loss: 0.2097 - val_accuracy: 0.9019
Epoch 6/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.2002 - accuracy: 0.9053
Epoch 6: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.2013 - accuracy: 0.9048 - val_loss: 0.2109 - val_accuracy: 0.9000
Epoch 7/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.2110 - accuracy: 0.9002
Epoch 7: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.2115 - accuracy: 0.9000 - val_loss: 0.1833 - val_accuracy: 0.9146
Epoch 8/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.2010 - accuracy: 0.9039
Epoch 8: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.2024 - accuracy: 0.9032 - val_loss: 0.2057 - val_accuracy: 0.8994
Epoch 9/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.2243 - accuracy: 0.8939
Epoch 9: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.2238 - accuracy: 0.8941 - val_loss: 0.2041 - val_accuracy: 0.9039
Epoch 10/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.2175 - accuracy: 0.8963
Epoch 10: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.2179 - accuracy: 0.8962 - val_loss: 0.2063 - val_accuracy: 0.9045
Epoch 11/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.2100 - accuracy: 0.8961
Epoch 11: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.2092 - accuracy: 0.8968 - val_loss: 0.1959 - val_accuracy: 0.9077
Epoch 12/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.2163 - accuracy: 0.8943
Epoch 12: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.2162 - accuracy: 0.8944 - val_loss: 0.1873 - val_accuracy: 0.9139
Epoch 13/150
852/852 [==============================] - ETA: 0s - loss: 0.2097 - accuracy: 0.9003
Epoch 13: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.2097 - accuracy: 0.9003 - val_loss: 0.2089 - val_accuracy: 0.9041
Epoch 14/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.2092 - accuracy: 0.9013
Epoch 14: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.2093 - accuracy: 0.9017 - val_loss: 0.1895 - val_accuracy: 0.9118
Epoch 15/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.2172 - accuracy: 0.8946
Epoch 15: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.2175 - accuracy: 0.8947 - val_loss: 0.1857 - val_accuracy: 0.9118
Epoch 16/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.2113 - accuracy: 0.9005
Epoch 16: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.2107 - accuracy: 0.9010 - val_loss: 0.2027 - val_accuracy: 0.9070
Epoch 17/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.2118 - accuracy: 0.8983
Epoch 17: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.2113 - accuracy: 0.8987 - val_loss: 0.1758 - val_accuracy: 0.9186
Epoch 18/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.2037 - accuracy: 0.9023
Epoch 18: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.2027 - accuracy: 0.9028 - val_loss: 0.1884 - val_accuracy: 0.9124
Epoch 19/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.1970 - accuracy: 0.9061
Epoch 19: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.1976 - accuracy: 0.9058 - val_loss: 0.1965 - val_accuracy: 0.9045
Epoch 20/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.2014 - accuracy: 0.9055
Epoch 20: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.2023 - accuracy: 0.9050 - val_loss: 0.1956 - val_accuracy: 0.9036
Epoch 21/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.2002 - accuracy: 0.9035
Epoch 21: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.1996 - accuracy: 0.9034 - val_loss: 0.1984 - val_accuracy: 0.9078
Epoch 22/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.1979 - accuracy: 0.9022
Epoch 22: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.1982 - accuracy: 0.9022 - val_loss: 0.1995 - val_accuracy: 0.9082
Epoch 23/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.2142 - accuracy: 0.9005
Epoch 23: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.2128 - accuracy: 0.9016 - val_loss: 0.1907 - val_accuracy: 0.9096
Epoch 24/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.2055 - accuracy: 0.9007
Epoch 24: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.2051 - accuracy: 0.9004 - val_loss: 0.1931 - val_accuracy: 0.9102
Epoch 25/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.2007 - accuracy: 0.9025
Epoch 25: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.2019 - accuracy: 0.9016 - val_loss: 0.1889 - val_accuracy: 0.9115
Epoch 26/150
819/852 [===========================&gt;..] - ETA: 0s - loss: 0.2093 - accuracy: 0.9009
Epoch 26: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.2097 - accuracy: 0.9010 - val_loss: 0.2142 - val_accuracy: 0.9007
Epoch 27/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.1988 - accuracy: 0.9048
Epoch 27: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.1981 - accuracy: 0.9058 - val_loss: 0.1972 - val_accuracy: 0.9044
Epoch 28/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.2111 - accuracy: 0.8985
Epoch 28: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.2120 - accuracy: 0.8980 - val_loss: 0.1815 - val_accuracy: 0.9140
Epoch 29/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.2095 - accuracy: 0.9025
Epoch 29: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.2104 - accuracy: 0.9015 - val_loss: 0.1804 - val_accuracy: 0.9177
Epoch 30/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.1972 - accuracy: 0.9040
Epoch 30: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.1970 - accuracy: 0.9042 - val_loss: 0.1858 - val_accuracy: 0.9145
Epoch 31/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.2077 - accuracy: 0.9052
Epoch 31: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.2077 - accuracy: 0.9052 - val_loss: 0.1829 - val_accuracy: 0.9138
Epoch 32/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.2060 - accuracy: 0.9012
Epoch 32: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.2060 - accuracy: 0.9016 - val_loss: 0.1938 - val_accuracy: 0.9093
Epoch 33/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.2056 - accuracy: 0.9027
Epoch 33: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.2061 - accuracy: 0.9018 - val_loss: 0.1893 - val_accuracy: 0.9116
Epoch 34/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.2107 - accuracy: 0.9018
Epoch 34: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.2117 - accuracy: 0.9014 - val_loss: 0.1925 - val_accuracy: 0.9116
Epoch 35/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.2007 - accuracy: 0.9053
Epoch 35: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.1996 - accuracy: 0.9058 - val_loss: 0.1749 - val_accuracy: 0.9165
Epoch 36/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.1917 - accuracy: 0.9078
Epoch 36: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.1930 - accuracy: 0.9070 - val_loss: 0.1816 - val_accuracy: 0.9130
Epoch 37/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.2033 - accuracy: 0.9071
Epoch 37: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.2035 - accuracy: 0.9069 - val_loss: 0.2153 - val_accuracy: 0.9010
Epoch 38/150
819/852 [===========================&gt;..] - ETA: 0s - loss: 0.2083 - accuracy: 0.9022
Epoch 38: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.2089 - accuracy: 0.9018 - val_loss: 0.2104 - val_accuracy: 0.9007
Epoch 39/150
820/852 [===========================&gt;..] - ETA: 0s - loss: 0.2180 - accuracy: 0.8972
Epoch 39: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.2175 - accuracy: 0.8970 - val_loss: 0.1724 - val_accuracy: 0.9218
Epoch 40/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.2007 - accuracy: 0.9053
Epoch 40: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.2018 - accuracy: 0.9039 - val_loss: 0.1796 - val_accuracy: 0.9211
Epoch 41/150
811/852 [===========================&gt;..] - ETA: 0s - loss: 0.1901 - accuracy: 0.9116
Epoch 41: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.1922 - accuracy: 0.9104 - val_loss: 0.1844 - val_accuracy: 0.9140
Epoch 42/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.1987 - accuracy: 0.9063
Epoch 42: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.2003 - accuracy: 0.9056 - val_loss: 0.1888 - val_accuracy: 0.9143
Epoch 43/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.2070 - accuracy: 0.9002
Epoch 43: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 3ms/step - loss: 0.2064 - accuracy: 0.9008 - val_loss: 0.1865 - val_accuracy: 0.9150
Epoch 44/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.1996 - accuracy: 0.9043
Epoch 44: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.2007 - accuracy: 0.9035 - val_loss: 0.2165 - val_accuracy: 0.9031
Epoch 45/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.2044 - accuracy: 0.9005
Epoch 45: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.2042 - accuracy: 0.9007 - val_loss: 0.1947 - val_accuracy: 0.9095
Epoch 46/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.1960 - accuracy: 0.9084
Epoch 46: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.1958 - accuracy: 0.9085 - val_loss: 0.2083 - val_accuracy: 0.9077
Epoch 47/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.2043 - accuracy: 0.9045
Epoch 47: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.2045 - accuracy: 0.9043 - val_loss: 0.1724 - val_accuracy: 0.9209
Epoch 48/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.1844 - accuracy: 0.9116
Epoch 48: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.1850 - accuracy: 0.9112 - val_loss: 0.1671 - val_accuracy: 0.9247
Epoch 49/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.2003 - accuracy: 0.9055
Epoch 49: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.2010 - accuracy: 0.9058 - val_loss: 0.1985 - val_accuracy: 0.9076
Epoch 50/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.1961 - accuracy: 0.9071
Epoch 50: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.1960 - accuracy: 0.9071 - val_loss: 0.1901 - val_accuracy: 0.9130
Epoch 51/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.1990 - accuracy: 0.9076
Epoch 51: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.1993 - accuracy: 0.9070 - val_loss: 0.1658 - val_accuracy: 0.9225
Epoch 52/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.1944 - accuracy: 0.9079
Epoch 52: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.1944 - accuracy: 0.9079 - val_loss: 0.1819 - val_accuracy: 0.9145
Epoch 53/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.2014 - accuracy: 0.9082
Epoch 53: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.2014 - accuracy: 0.9084 - val_loss: 0.1908 - val_accuracy: 0.9083
Epoch 54/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.1988 - accuracy: 0.9065
Epoch 54: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.1987 - accuracy: 0.9069 - val_loss: 0.1661 - val_accuracy: 0.9190
Epoch 55/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.1891 - accuracy: 0.9095
Epoch 55: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.1892 - accuracy: 0.9093 - val_loss: 0.1927 - val_accuracy: 0.9150
Epoch 56/150
807/852 [===========================&gt;..] - ETA: 0s - loss: 0.1974 - accuracy: 0.9068
Epoch 56: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.1992 - accuracy: 0.9058 - val_loss: 0.1808 - val_accuracy: 0.9150
Epoch 57/150
814/852 [===========================&gt;..] - ETA: 0s - loss: 0.1867 - accuracy: 0.9125
Epoch 57: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.1892 - accuracy: 0.9106 - val_loss: 0.1952 - val_accuracy: 0.9085
Epoch 58/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.2004 - accuracy: 0.9055
Epoch 58: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.1998 - accuracy: 0.9059 - val_loss: 0.1863 - val_accuracy: 0.9135
Epoch 59/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.1983 - accuracy: 0.9070
Epoch 59: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.1986 - accuracy: 0.9072 - val_loss: 0.1671 - val_accuracy: 0.9232
Epoch 60/150
815/852 [===========================&gt;..] - ETA: 0s - loss: 0.2070 - accuracy: 0.9103
Epoch 60: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.2074 - accuracy: 0.9098 - val_loss: 0.1735 - val_accuracy: 0.9210
Epoch 61/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.1931 - accuracy: 0.9093
Epoch 61: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.1932 - accuracy: 0.9093 - val_loss: 0.1671 - val_accuracy: 0.9226
Epoch 62/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.1969 - accuracy: 0.9095
Epoch 62: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.1975 - accuracy: 0.9096 - val_loss: 0.1694 - val_accuracy: 0.9202
Epoch 63/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.1898 - accuracy: 0.9113
Epoch 63: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.1884 - accuracy: 0.9118 - val_loss: 0.1831 - val_accuracy: 0.9133
Epoch 64/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.1945 - accuracy: 0.9090
Epoch 64: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.1947 - accuracy: 0.9090 - val_loss: 0.1895 - val_accuracy: 0.9132
Epoch 65/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.1987 - accuracy: 0.9054
Epoch 65: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.1983 - accuracy: 0.9059 - val_loss: 0.1783 - val_accuracy: 0.9163
Epoch 66/150
819/852 [===========================&gt;..] - ETA: 0s - loss: 0.1870 - accuracy: 0.9120
Epoch 66: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.1874 - accuracy: 0.9112 - val_loss: 0.1667 - val_accuracy: 0.9229
Epoch 67/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.1831 - accuracy: 0.9131
Epoch 67: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.1834 - accuracy: 0.9123 - val_loss: 0.1767 - val_accuracy: 0.9180
Epoch 68/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.1921 - accuracy: 0.9121
Epoch 68: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.1933 - accuracy: 0.9115 - val_loss: 0.1701 - val_accuracy: 0.9247
Epoch 69/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.1914 - accuracy: 0.9119
Epoch 69: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.1915 - accuracy: 0.9125 - val_loss: 0.1962 - val_accuracy: 0.9069
Epoch 70/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.1948 - accuracy: 0.9029
Epoch 70: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.1946 - accuracy: 0.9031 - val_loss: 0.1838 - val_accuracy: 0.9165
Epoch 71/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.1896 - accuracy: 0.9090
Epoch 71: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.1897 - accuracy: 0.9089 - val_loss: 0.2138 - val_accuracy: 0.9108
Epoch 71: early stopping
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=519dcee4-e3a0-49c2-82a5-f0ff1f8ab8d0">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [95]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">two_fortynine_neuron</span><span class="o">.</span><span class="n">load_weights</span><span class="p">(</span><span class="s1">'two_fortynine_model.hdf5'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=d6e23fa4-e789-4226-bda7-4af7b12e75fd">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [96]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Other Metrics</span>
<span class="n">two_fortynine_neuron_preds</span> <span class="o">=</span> <span class="n">two_fortynine_neuron</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">two_fortynine_neuron_preds</span><span class="o">.</span><span class="n">round</span><span class="p">())</span>
<span class="n">precision</span> <span class="o">=</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">two_fortynine_neuron_preds</span><span class="o">.</span><span class="n">round</span><span class="p">())</span>
<span class="n">recall</span> <span class="o">=</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">two_fortynine_neuron_preds</span><span class="o">.</span><span class="n">round</span><span class="p">())</span>
<span class="n">f1score</span> <span class="o">=</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">two_fortynine_neuron_preds</span><span class="o">.</span><span class="n">round</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Accuracy: </span><span class="si">%.2f</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">accuracy</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Precision: </span><span class="si">%.2f</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">precision</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Recall: </span><span class="si">%.2f</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">recall</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"F1-score: </span><span class="si">%.2f</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">f1score</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>267/267 [==============================] - 0s 978us/step
Accuracy: 0.91
Precision: 0.92
Recall: 0.91
F1-score: 0.91
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=b01ae594-4485-4131-aab1-19ded522aa4f">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="2.9-Build-a-Model-with-497-Neurons-in-6-Layers">2.9 Build a Model with 497 Neurons in 6 Layers<a class="anchor-link" href="#2.9-Build-a-Model-with-497-Neurons-in-6-Layers">¶</a></h4>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=0787b033-4454-4988-a9ac-0de5a031309b">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [81]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># constructing a 497 neuron model</span>
<span class="n">four_ninetyseven_neuron</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">four_ninetyseven_neuron</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">input_dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">))</span>
<span class="n">four_ninetyseven_neuron</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">))</span>
<span class="n">four_ninetyseven_neuron</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">))</span>
<span class="n">four_ninetyseven_neuron</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">))</span>
<span class="n">four_ninetyseven_neuron</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">))</span>
<span class="n">four_ninetyseven_neuron</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'sigmoid'</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">four_ninetyseven_neuron</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>

<span class="c1">#Check for cycles in the Sequential Model</span>
<span class="n">four_ninetyseven_neuron</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">'binary_crossentropy'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s1">'adam'</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'accuracy'</span><span class="p">])</span>
<span class="n">callback_a</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">filepath</span> <span class="o">=</span> <span class="s1">'four_ninetyseven_model.hdf5'</span><span class="p">,</span> <span class="n">monitor</span><span class="o">=</span><span class="s1">'val_loss'</span><span class="p">,</span> <span class="n">save_best_only</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">save_weights_only</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">verbose</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">callback_b</span> <span class="o">=</span> <span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s1">'val_loss'</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">'min'</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">history_497</span> <span class="o">=</span> <span class="n">four_ninetyseven_neuron</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">callback_a</span><span class="p">,</span> <span class="n">callback_b</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Model: "sequential_20"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense_54 (Dense)            (None, 256)               6912      
                                                                 
 dense_55 (Dense)            (None, 128)               32896     
                                                                 
 dense_56 (Dense)            (None, 64)                8256      
                                                                 
 dense_57 (Dense)            (None, 32)                2080      
                                                                 
 dense_58 (Dense)            (None, 16)                528       
                                                                 
 dense_59 (Dense)            (None, 1)                 17        
                                                                 
=================================================================
Total params: 50689 (198.00 KB)
Trainable params: 50689 (198.00 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
None
Epoch 1/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.5788 - accuracy: 0.7094
Epoch 1: val_loss improved from inf to 0.56072, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.5788 - accuracy: 0.7096 - val_loss: 0.5607 - val_accuracy: 0.7190
Epoch 2/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.5612 - accuracy: 0.7207
Epoch 2: val_loss improved from 0.56072 to 0.55076, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5611 - accuracy: 0.7210 - val_loss: 0.5508 - val_accuracy: 0.7298
Epoch 3/150
820/852 [===========================&gt;..] - ETA: 0s - loss: 0.5562 - accuracy: 0.7235
Epoch 3: val_loss improved from 0.55076 to 0.54971, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5563 - accuracy: 0.7231 - val_loss: 0.5497 - val_accuracy: 0.7292
Epoch 4/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.5550 - accuracy: 0.7264
Epoch 4: val_loss did not improve from 0.54971
852/852 [==============================] - 2s 2ms/step - loss: 0.5547 - accuracy: 0.7273 - val_loss: 0.5523 - val_accuracy: 0.7312
Epoch 5/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.5544 - accuracy: 0.7267
Epoch 5: val_loss improved from 0.54971 to 0.54664, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5542 - accuracy: 0.7267 - val_loss: 0.5466 - val_accuracy: 0.7284
Epoch 6/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.5505 - accuracy: 0.7284
Epoch 6: val_loss improved from 0.54664 to 0.54409, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5502 - accuracy: 0.7286 - val_loss: 0.5441 - val_accuracy: 0.7336
Epoch 7/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.5488 - accuracy: 0.7317
Epoch 7: val_loss improved from 0.54409 to 0.54011, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5491 - accuracy: 0.7313 - val_loss: 0.5401 - val_accuracy: 0.7381
Epoch 8/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.5474 - accuracy: 0.7337
Epoch 8: val_loss improved from 0.54011 to 0.53930, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5468 - accuracy: 0.7344 - val_loss: 0.5393 - val_accuracy: 0.7334
Epoch 9/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.5436 - accuracy: 0.7343
Epoch 9: val_loss improved from 0.53930 to 0.53573, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5432 - accuracy: 0.7349 - val_loss: 0.5357 - val_accuracy: 0.7383
Epoch 10/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5417 - accuracy: 0.7353
Epoch 10: val_loss improved from 0.53573 to 0.53402, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5416 - accuracy: 0.7357 - val_loss: 0.5340 - val_accuracy: 0.7424
Epoch 11/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.5370 - accuracy: 0.7400
Epoch 11: val_loss did not improve from 0.53402
852/852 [==============================] - 2s 3ms/step - loss: 0.5382 - accuracy: 0.7386 - val_loss: 0.5344 - val_accuracy: 0.7395
Epoch 12/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.5362 - accuracy: 0.7377
Epoch 12: val_loss improved from 0.53402 to 0.52562, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5372 - accuracy: 0.7370 - val_loss: 0.5256 - val_accuracy: 0.7438
Epoch 13/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.5341 - accuracy: 0.7393
Epoch 13: val_loss did not improve from 0.52562
852/852 [==============================] - 2s 2ms/step - loss: 0.5341 - accuracy: 0.7390 - val_loss: 0.5295 - val_accuracy: 0.7395
Epoch 14/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.5295 - accuracy: 0.7417
Epoch 14: val_loss did not improve from 0.52562
852/852 [==============================] - 2s 2ms/step - loss: 0.5306 - accuracy: 0.7399 - val_loss: 0.5369 - val_accuracy: 0.7323
Epoch 15/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.5266 - accuracy: 0.7442
Epoch 15: val_loss improved from 0.52562 to 0.52332, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.5282 - accuracy: 0.7427 - val_loss: 0.5233 - val_accuracy: 0.7417
Epoch 16/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.5292 - accuracy: 0.7398
Epoch 16: val_loss improved from 0.52332 to 0.52242, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5275 - accuracy: 0.7408 - val_loss: 0.5224 - val_accuracy: 0.7439
Epoch 17/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.5242 - accuracy: 0.7456
Epoch 17: val_loss improved from 0.52242 to 0.51040, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5243 - accuracy: 0.7454 - val_loss: 0.5104 - val_accuracy: 0.7511
Epoch 18/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.5202 - accuracy: 0.7429
Epoch 18: val_loss did not improve from 0.51040
852/852 [==============================] - 2s 3ms/step - loss: 0.5206 - accuracy: 0.7425 - val_loss: 0.5133 - val_accuracy: 0.7464
Epoch 19/150
852/852 [==============================] - ETA: 0s - loss: 0.5205 - accuracy: 0.7454
Epoch 19: val_loss improved from 0.51040 to 0.50654, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5205 - accuracy: 0.7454 - val_loss: 0.5065 - val_accuracy: 0.7561
Epoch 20/150
815/852 [===========================&gt;..] - ETA: 0s - loss: 0.5155 - accuracy: 0.7481
Epoch 20: val_loss improved from 0.50654 to 0.50195, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5162 - accuracy: 0.7472 - val_loss: 0.5020 - val_accuracy: 0.7553
Epoch 21/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.5130 - accuracy: 0.7492
Epoch 21: val_loss improved from 0.50195 to 0.49690, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5121 - accuracy: 0.7499 - val_loss: 0.4969 - val_accuracy: 0.7567
Epoch 22/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.5096 - accuracy: 0.7487
Epoch 22: val_loss did not improve from 0.49690
852/852 [==============================] - 2s 2ms/step - loss: 0.5113 - accuracy: 0.7479 - val_loss: 0.4981 - val_accuracy: 0.7563
Epoch 23/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.5069 - accuracy: 0.7530
Epoch 23: val_loss did not improve from 0.49690
852/852 [==============================] - 2s 2ms/step - loss: 0.5069 - accuracy: 0.7531 - val_loss: 0.4979 - val_accuracy: 0.7538
Epoch 24/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.5058 - accuracy: 0.7505
Epoch 24: val_loss improved from 0.49690 to 0.49043, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5066 - accuracy: 0.7500 - val_loss: 0.4904 - val_accuracy: 0.7622
Epoch 25/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.5000 - accuracy: 0.7542
Epoch 25: val_loss did not improve from 0.49043
852/852 [==============================] - 2s 2ms/step - loss: 0.5008 - accuracy: 0.7539 - val_loss: 0.4989 - val_accuracy: 0.7506
Epoch 26/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.4997 - accuracy: 0.7539
Epoch 26: val_loss improved from 0.49043 to 0.48642, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.5007 - accuracy: 0.7528 - val_loss: 0.4864 - val_accuracy: 0.7616
Epoch 27/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.4972 - accuracy: 0.7560
Epoch 27: val_loss improved from 0.48642 to 0.48394, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4968 - accuracy: 0.7567 - val_loss: 0.4839 - val_accuracy: 0.7626
Epoch 28/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.4931 - accuracy: 0.7592
Epoch 28: val_loss improved from 0.48394 to 0.48270, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4946 - accuracy: 0.7573 - val_loss: 0.4827 - val_accuracy: 0.7606
Epoch 29/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.4957 - accuracy: 0.7567
Epoch 29: val_loss did not improve from 0.48270
852/852 [==============================] - 2s 2ms/step - loss: 0.4950 - accuracy: 0.7578 - val_loss: 0.4829 - val_accuracy: 0.7630
Epoch 30/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.4904 - accuracy: 0.7572
Epoch 30: val_loss improved from 0.48270 to 0.47557, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4912 - accuracy: 0.7568 - val_loss: 0.4756 - val_accuracy: 0.7669
Epoch 31/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.4913 - accuracy: 0.7549
Epoch 31: val_loss improved from 0.47557 to 0.47485, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4914 - accuracy: 0.7549 - val_loss: 0.4748 - val_accuracy: 0.7662
Epoch 32/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.4881 - accuracy: 0.7569
Epoch 32: val_loss improved from 0.47485 to 0.47045, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4889 - accuracy: 0.7567 - val_loss: 0.4705 - val_accuracy: 0.7696
Epoch 33/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.4836 - accuracy: 0.7626
Epoch 33: val_loss did not improve from 0.47045
852/852 [==============================] - 2s 2ms/step - loss: 0.4837 - accuracy: 0.7626 - val_loss: 0.4730 - val_accuracy: 0.7663
Epoch 34/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.4811 - accuracy: 0.7622
Epoch 34: val_loss did not improve from 0.47045
852/852 [==============================] - 2s 3ms/step - loss: 0.4828 - accuracy: 0.7612 - val_loss: 0.4791 - val_accuracy: 0.7705
Epoch 35/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.4809 - accuracy: 0.7592
Epoch 35: val_loss improved from 0.47045 to 0.46906, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4811 - accuracy: 0.7594 - val_loss: 0.4691 - val_accuracy: 0.7688
Epoch 36/150
820/852 [===========================&gt;..] - ETA: 0s - loss: 0.4755 - accuracy: 0.7620
Epoch 36: val_loss improved from 0.46906 to 0.45954, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4760 - accuracy: 0.7620 - val_loss: 0.4595 - val_accuracy: 0.7721
Epoch 37/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.4735 - accuracy: 0.7634
Epoch 37: val_loss did not improve from 0.45954
852/852 [==============================] - 2s 2ms/step - loss: 0.4739 - accuracy: 0.7632 - val_loss: 0.4599 - val_accuracy: 0.7709
Epoch 38/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.4717 - accuracy: 0.7671
Epoch 38: val_loss improved from 0.45954 to 0.45661, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4713 - accuracy: 0.7675 - val_loss: 0.4566 - val_accuracy: 0.7723
Epoch 39/150
820/852 [===========================&gt;..] - ETA: 0s - loss: 0.4701 - accuracy: 0.7639
Epoch 39: val_loss improved from 0.45661 to 0.45069, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4717 - accuracy: 0.7633 - val_loss: 0.4507 - val_accuracy: 0.7778
Epoch 40/150
820/852 [===========================&gt;..] - ETA: 0s - loss: 0.4656 - accuracy: 0.7700
Epoch 40: val_loss did not improve from 0.45069
852/852 [==============================] - 2s 2ms/step - loss: 0.4663 - accuracy: 0.7695 - val_loss: 0.4531 - val_accuracy: 0.7747
Epoch 41/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.4643 - accuracy: 0.7666
Epoch 41: val_loss improved from 0.45069 to 0.44929, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4651 - accuracy: 0.7661 - val_loss: 0.4493 - val_accuracy: 0.7776
Epoch 42/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.4636 - accuracy: 0.7683
Epoch 42: val_loss improved from 0.44929 to 0.43571, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.4633 - accuracy: 0.7686 - val_loss: 0.4357 - val_accuracy: 0.7822
Epoch 43/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.4604 - accuracy: 0.7710
Epoch 43: val_loss did not improve from 0.43571
852/852 [==============================] - 2s 2ms/step - loss: 0.4603 - accuracy: 0.7711 - val_loss: 0.4478 - val_accuracy: 0.7802
Epoch 44/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.4592 - accuracy: 0.7709
Epoch 44: val_loss did not improve from 0.43571
852/852 [==============================] - 2s 2ms/step - loss: 0.4590 - accuracy: 0.7709 - val_loss: 0.4506 - val_accuracy: 0.7783
Epoch 45/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.4576 - accuracy: 0.7719
Epoch 45: val_loss did not improve from 0.43571
852/852 [==============================] - 2s 2ms/step - loss: 0.4581 - accuracy: 0.7718 - val_loss: 0.4365 - val_accuracy: 0.7801
Epoch 46/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.4581 - accuracy: 0.7676
Epoch 46: val_loss did not improve from 0.43571
852/852 [==============================] - 2s 2ms/step - loss: 0.4564 - accuracy: 0.7689 - val_loss: 0.4407 - val_accuracy: 0.7775
Epoch 47/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.4515 - accuracy: 0.7748
Epoch 47: val_loss did not improve from 0.43571
852/852 [==============================] - 2s 2ms/step - loss: 0.4515 - accuracy: 0.7749 - val_loss: 0.4379 - val_accuracy: 0.7795
Epoch 48/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.4507 - accuracy: 0.7758
Epoch 48: val_loss improved from 0.43571 to 0.42748, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4507 - accuracy: 0.7756 - val_loss: 0.4275 - val_accuracy: 0.7868
Epoch 49/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.4459 - accuracy: 0.7787
Epoch 49: val_loss did not improve from 0.42748
852/852 [==============================] - 2s 2ms/step - loss: 0.4450 - accuracy: 0.7797 - val_loss: 0.4367 - val_accuracy: 0.7792
Epoch 50/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.4439 - accuracy: 0.7814
Epoch 50: val_loss improved from 0.42748 to 0.42515, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4448 - accuracy: 0.7804 - val_loss: 0.4252 - val_accuracy: 0.7842
Epoch 51/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.4424 - accuracy: 0.7770
Epoch 51: val_loss improved from 0.42515 to 0.42446, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4429 - accuracy: 0.7769 - val_loss: 0.4245 - val_accuracy: 0.7905
Epoch 52/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.4407 - accuracy: 0.7807
Epoch 52: val_loss did not improve from 0.42446
852/852 [==============================] - 2s 2ms/step - loss: 0.4412 - accuracy: 0.7803 - val_loss: 0.4328 - val_accuracy: 0.7804
Epoch 53/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.4389 - accuracy: 0.7755
Epoch 53: val_loss did not improve from 0.42446
852/852 [==============================] - 2s 2ms/step - loss: 0.4385 - accuracy: 0.7763 - val_loss: 0.4333 - val_accuracy: 0.7879
Epoch 54/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.4404 - accuracy: 0.7787
Epoch 54: val_loss improved from 0.42446 to 0.41822, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4394 - accuracy: 0.7794 - val_loss: 0.4182 - val_accuracy: 0.7909
Epoch 55/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.4330 - accuracy: 0.7825
Epoch 55: val_loss did not improve from 0.41822
852/852 [==============================] - 2s 2ms/step - loss: 0.4336 - accuracy: 0.7817 - val_loss: 0.4239 - val_accuracy: 0.7886
Epoch 56/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.4314 - accuracy: 0.7805
Epoch 56: val_loss improved from 0.41822 to 0.41123, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4315 - accuracy: 0.7805 - val_loss: 0.4112 - val_accuracy: 0.7947
Epoch 57/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.4251 - accuracy: 0.7858
Epoch 57: val_loss did not improve from 0.41123
852/852 [==============================] - 2s 2ms/step - loss: 0.4268 - accuracy: 0.7838 - val_loss: 0.4176 - val_accuracy: 0.7920
Epoch 58/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.4273 - accuracy: 0.7823
Epoch 58: val_loss did not improve from 0.41123
852/852 [==============================] - 2s 3ms/step - loss: 0.4273 - accuracy: 0.7823 - val_loss: 0.4211 - val_accuracy: 0.7877
Epoch 59/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.4252 - accuracy: 0.7859
Epoch 59: val_loss improved from 0.41123 to 0.40381, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4255 - accuracy: 0.7857 - val_loss: 0.4038 - val_accuracy: 0.8003
Epoch 60/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.4189 - accuracy: 0.7906
Epoch 60: val_loss improved from 0.40381 to 0.40347, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4190 - accuracy: 0.7905 - val_loss: 0.4035 - val_accuracy: 0.7996
Epoch 61/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.4187 - accuracy: 0.7927
Epoch 61: val_loss did not improve from 0.40347
852/852 [==============================] - 2s 2ms/step - loss: 0.4188 - accuracy: 0.7927 - val_loss: 0.4059 - val_accuracy: 0.7993
Epoch 62/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.4202 - accuracy: 0.7905
Epoch 62: val_loss improved from 0.40347 to 0.39992, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4213 - accuracy: 0.7897 - val_loss: 0.3999 - val_accuracy: 0.7999
Epoch 63/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.4124 - accuracy: 0.7932
Epoch 63: val_loss improved from 0.39992 to 0.39824, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4125 - accuracy: 0.7936 - val_loss: 0.3982 - val_accuracy: 0.7994
Epoch 64/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.4173 - accuracy: 0.7916
Epoch 64: val_loss improved from 0.39824 to 0.39068, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4163 - accuracy: 0.7915 - val_loss: 0.3907 - val_accuracy: 0.8066
Epoch 65/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.4121 - accuracy: 0.7930
Epoch 65: val_loss improved from 0.39068 to 0.38865, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.4116 - accuracy: 0.7937 - val_loss: 0.3886 - val_accuracy: 0.8031
Epoch 66/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.4094 - accuracy: 0.7978
Epoch 66: val_loss improved from 0.38865 to 0.38754, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.4111 - accuracy: 0.7970 - val_loss: 0.3875 - val_accuracy: 0.8111
Epoch 67/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.4024 - accuracy: 0.8009
Epoch 67: val_loss did not improve from 0.38754
852/852 [==============================] - 2s 2ms/step - loss: 0.4021 - accuracy: 0.8014 - val_loss: 0.3879 - val_accuracy: 0.8054
Epoch 68/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.4056 - accuracy: 0.7968
Epoch 68: val_loss did not improve from 0.38754
852/852 [==============================] - 2s 2ms/step - loss: 0.4057 - accuracy: 0.7967 - val_loss: 0.3914 - val_accuracy: 0.8053
Epoch 69/150
819/852 [===========================&gt;..] - ETA: 0s - loss: 0.3991 - accuracy: 0.8006
Epoch 69: val_loss improved from 0.38754 to 0.36900, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3982 - accuracy: 0.8006 - val_loss: 0.3690 - val_accuracy: 0.8165
Epoch 70/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.4033 - accuracy: 0.7992
Epoch 70: val_loss did not improve from 0.36900
852/852 [==============================] - 2s 2ms/step - loss: 0.4038 - accuracy: 0.7984 - val_loss: 0.3932 - val_accuracy: 0.8092
Epoch 71/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.3974 - accuracy: 0.8075
Epoch 71: val_loss did not improve from 0.36900
852/852 [==============================] - 2s 2ms/step - loss: 0.3975 - accuracy: 0.8074 - val_loss: 0.3768 - val_accuracy: 0.8138
Epoch 72/150
814/852 [===========================&gt;..] - ETA: 0s - loss: 0.3882 - accuracy: 0.8052
Epoch 72: val_loss did not improve from 0.36900
852/852 [==============================] - 2s 2ms/step - loss: 0.3889 - accuracy: 0.8057 - val_loss: 0.3780 - val_accuracy: 0.8106
Epoch 73/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.3945 - accuracy: 0.8018
Epoch 73: val_loss did not improve from 0.36900
852/852 [==============================] - 2s 2ms/step - loss: 0.3951 - accuracy: 0.8019 - val_loss: 0.3737 - val_accuracy: 0.8163
Epoch 74/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.3857 - accuracy: 0.8073
Epoch 74: val_loss did not improve from 0.36900
852/852 [==============================] - 2s 3ms/step - loss: 0.3857 - accuracy: 0.8074 - val_loss: 0.3719 - val_accuracy: 0.8111
Epoch 75/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.3855 - accuracy: 0.8051
Epoch 75: val_loss did not improve from 0.36900
852/852 [==============================] - 2s 2ms/step - loss: 0.3878 - accuracy: 0.8031 - val_loss: 0.3700 - val_accuracy: 0.8187
Epoch 76/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.3875 - accuracy: 0.8062
Epoch 76: val_loss improved from 0.36900 to 0.36556, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3889 - accuracy: 0.8050 - val_loss: 0.3656 - val_accuracy: 0.8205
Epoch 77/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.3823 - accuracy: 0.8110
Epoch 77: val_loss did not improve from 0.36556
852/852 [==============================] - 2s 2ms/step - loss: 0.3829 - accuracy: 0.8105 - val_loss: 0.3674 - val_accuracy: 0.8223
Epoch 78/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.3815 - accuracy: 0.8075
Epoch 78: val_loss improved from 0.36556 to 0.35516, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3842 - accuracy: 0.8061 - val_loss: 0.3552 - val_accuracy: 0.8284
Epoch 79/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.3794 - accuracy: 0.8144
Epoch 79: val_loss did not improve from 0.35516
852/852 [==============================] - 2s 2ms/step - loss: 0.3788 - accuracy: 0.8147 - val_loss: 0.3810 - val_accuracy: 0.8153
Epoch 80/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.3795 - accuracy: 0.8107
Epoch 80: val_loss did not improve from 0.35516
852/852 [==============================] - 2s 3ms/step - loss: 0.3802 - accuracy: 0.8107 - val_loss: 0.3670 - val_accuracy: 0.8217
Epoch 81/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.3755 - accuracy: 0.8153
Epoch 81: val_loss improved from 0.35516 to 0.35316, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.3753 - accuracy: 0.8149 - val_loss: 0.3532 - val_accuracy: 0.8281
Epoch 82/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.3754 - accuracy: 0.8133
Epoch 82: val_loss did not improve from 0.35316
852/852 [==============================] - 2s 3ms/step - loss: 0.3756 - accuracy: 0.8133 - val_loss: 0.3568 - val_accuracy: 0.8262
Epoch 83/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.3696 - accuracy: 0.8142
Epoch 83: val_loss improved from 0.35316 to 0.34508, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3698 - accuracy: 0.8148 - val_loss: 0.3451 - val_accuracy: 0.8326
Epoch 84/150
852/852 [==============================] - ETA: 0s - loss: 0.3678 - accuracy: 0.8163
Epoch 84: val_loss did not improve from 0.34508
852/852 [==============================] - 2s 2ms/step - loss: 0.3678 - accuracy: 0.8163 - val_loss: 0.3562 - val_accuracy: 0.8317
Epoch 85/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.3699 - accuracy: 0.8169
Epoch 85: val_loss did not improve from 0.34508
852/852 [==============================] - 2s 2ms/step - loss: 0.3697 - accuracy: 0.8171 - val_loss: 0.3524 - val_accuracy: 0.8262
Epoch 86/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.3643 - accuracy: 0.8187
Epoch 86: val_loss did not improve from 0.34508
852/852 [==============================] - 2s 2ms/step - loss: 0.3650 - accuracy: 0.8186 - val_loss: 0.3476 - val_accuracy: 0.8328
Epoch 87/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.3596 - accuracy: 0.8234
Epoch 87: val_loss did not improve from 0.34508
852/852 [==============================] - 2s 2ms/step - loss: 0.3598 - accuracy: 0.8230 - val_loss: 0.3502 - val_accuracy: 0.8313
Epoch 88/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.3691 - accuracy: 0.8176
Epoch 88: val_loss improved from 0.34508 to 0.33931, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3700 - accuracy: 0.8166 - val_loss: 0.3393 - val_accuracy: 0.8310
Epoch 89/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.3566 - accuracy: 0.8248
Epoch 89: val_loss did not improve from 0.33931
852/852 [==============================] - 2s 3ms/step - loss: 0.3565 - accuracy: 0.8250 - val_loss: 0.3421 - val_accuracy: 0.8334
Epoch 90/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.3539 - accuracy: 0.8263
Epoch 90: val_loss did not improve from 0.33931
852/852 [==============================] - 2s 2ms/step - loss: 0.3538 - accuracy: 0.8263 - val_loss: 0.3419 - val_accuracy: 0.8351
Epoch 91/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.3590 - accuracy: 0.8222
Epoch 91: val_loss did not improve from 0.33931
852/852 [==============================] - 2s 2ms/step - loss: 0.3588 - accuracy: 0.8223 - val_loss: 0.3409 - val_accuracy: 0.8343
Epoch 92/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.3585 - accuracy: 0.8229
Epoch 92: val_loss did not improve from 0.33931
852/852 [==============================] - 2s 2ms/step - loss: 0.3587 - accuracy: 0.8228 - val_loss: 0.3499 - val_accuracy: 0.8331
Epoch 93/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.3565 - accuracy: 0.8251
Epoch 93: val_loss improved from 0.33931 to 0.32426, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3560 - accuracy: 0.8256 - val_loss: 0.3243 - val_accuracy: 0.8431
Epoch 94/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.3515 - accuracy: 0.8282
Epoch 94: val_loss did not improve from 0.32426
852/852 [==============================] - 2s 2ms/step - loss: 0.3523 - accuracy: 0.8277 - val_loss: 0.3248 - val_accuracy: 0.8445
Epoch 95/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.3425 - accuracy: 0.8313
Epoch 95: val_loss improved from 0.32426 to 0.32216, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3425 - accuracy: 0.8315 - val_loss: 0.3222 - val_accuracy: 0.8421
Epoch 96/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.3451 - accuracy: 0.8327
Epoch 96: val_loss did not improve from 0.32216
852/852 [==============================] - 2s 2ms/step - loss: 0.3455 - accuracy: 0.8327 - val_loss: 0.3230 - val_accuracy: 0.8458
Epoch 97/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.3488 - accuracy: 0.8280
Epoch 97: val_loss did not improve from 0.32216
852/852 [==============================] - 2s 3ms/step - loss: 0.3484 - accuracy: 0.8287 - val_loss: 0.3311 - val_accuracy: 0.8394
Epoch 98/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.3427 - accuracy: 0.8296
Epoch 98: val_loss did not improve from 0.32216
852/852 [==============================] - 2s 2ms/step - loss: 0.3436 - accuracy: 0.8283 - val_loss: 0.3353 - val_accuracy: 0.8388
Epoch 99/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.3481 - accuracy: 0.8293
Epoch 99: val_loss improved from 0.32216 to 0.31310, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3480 - accuracy: 0.8288 - val_loss: 0.3131 - val_accuracy: 0.8476
Epoch 100/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.3355 - accuracy: 0.8351
Epoch 100: val_loss improved from 0.31310 to 0.30826, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3362 - accuracy: 0.8347 - val_loss: 0.3083 - val_accuracy: 0.8526
Epoch 101/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.3282 - accuracy: 0.8390
Epoch 101: val_loss did not improve from 0.30826
852/852 [==============================] - 2s 2ms/step - loss: 0.3292 - accuracy: 0.8381 - val_loss: 0.3086 - val_accuracy: 0.8490
Epoch 102/150
815/852 [===========================&gt;..] - ETA: 0s - loss: 0.3361 - accuracy: 0.8315
Epoch 102: val_loss improved from 0.30826 to 0.30697, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3348 - accuracy: 0.8330 - val_loss: 0.3070 - val_accuracy: 0.8484
Epoch 103/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.3323 - accuracy: 0.8342
Epoch 103: val_loss improved from 0.30697 to 0.30022, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3324 - accuracy: 0.8348 - val_loss: 0.3002 - val_accuracy: 0.8544
Epoch 104/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.3272 - accuracy: 0.8380
Epoch 104: val_loss did not improve from 0.30022
852/852 [==============================] - 2s 2ms/step - loss: 0.3284 - accuracy: 0.8375 - val_loss: 0.3026 - val_accuracy: 0.8532
Epoch 105/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.3285 - accuracy: 0.8380
Epoch 105: val_loss did not improve from 0.30022
852/852 [==============================] - 2s 3ms/step - loss: 0.3296 - accuracy: 0.8377 - val_loss: 0.3102 - val_accuracy: 0.8511
Epoch 106/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.3270 - accuracy: 0.8413
Epoch 106: val_loss did not improve from 0.30022
852/852 [==============================] - 2s 2ms/step - loss: 0.3277 - accuracy: 0.8417 - val_loss: 0.3175 - val_accuracy: 0.8496
Epoch 107/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.3222 - accuracy: 0.8430
Epoch 107: val_loss did not improve from 0.30022
852/852 [==============================] - 2s 2ms/step - loss: 0.3223 - accuracy: 0.8426 - val_loss: 0.3058 - val_accuracy: 0.8497
Epoch 108/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.3265 - accuracy: 0.8419
Epoch 108: val_loss did not improve from 0.30022
852/852 [==============================] - 2s 2ms/step - loss: 0.3267 - accuracy: 0.8415 - val_loss: 0.3019 - val_accuracy: 0.8520
Epoch 109/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.3187 - accuracy: 0.8436
Epoch 109: val_loss improved from 0.30022 to 0.29488, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3190 - accuracy: 0.8435 - val_loss: 0.2949 - val_accuracy: 0.8594
Epoch 110/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.3243 - accuracy: 0.8407
Epoch 110: val_loss did not improve from 0.29488
852/852 [==============================] - 2s 2ms/step - loss: 0.3241 - accuracy: 0.8404 - val_loss: 0.3118 - val_accuracy: 0.8489
Epoch 111/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.3212 - accuracy: 0.8415
Epoch 111: val_loss did not improve from 0.29488
852/852 [==============================] - 2s 2ms/step - loss: 0.3214 - accuracy: 0.8415 - val_loss: 0.2956 - val_accuracy: 0.8597
Epoch 112/150
818/852 [===========================&gt;..] - ETA: 0s - loss: 0.3110 - accuracy: 0.8472
Epoch 112: val_loss did not improve from 0.29488
852/852 [==============================] - 2s 2ms/step - loss: 0.3118 - accuracy: 0.8461 - val_loss: 0.3012 - val_accuracy: 0.8538
Epoch 113/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.3199 - accuracy: 0.8431
Epoch 113: val_loss did not improve from 0.29488
852/852 [==============================] - 2s 3ms/step - loss: 0.3201 - accuracy: 0.8435 - val_loss: 0.3088 - val_accuracy: 0.8532
Epoch 114/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.3144 - accuracy: 0.8473
Epoch 114: val_loss did not improve from 0.29488
852/852 [==============================] - 2s 2ms/step - loss: 0.3142 - accuracy: 0.8475 - val_loss: 0.3058 - val_accuracy: 0.8556
Epoch 115/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.3146 - accuracy: 0.8438
Epoch 115: val_loss did not improve from 0.29488
852/852 [==============================] - 2s 2ms/step - loss: 0.3150 - accuracy: 0.8435 - val_loss: 0.3036 - val_accuracy: 0.8515
Epoch 116/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.3107 - accuracy: 0.8512
Epoch 116: val_loss improved from 0.29488 to 0.28223, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3121 - accuracy: 0.8498 - val_loss: 0.2822 - val_accuracy: 0.8646
Epoch 117/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.3138 - accuracy: 0.8507
Epoch 117: val_loss improved from 0.28223 to 0.28017, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3137 - accuracy: 0.8502 - val_loss: 0.2802 - val_accuracy: 0.8681
Epoch 118/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.3036 - accuracy: 0.8485
Epoch 118: val_loss did not improve from 0.28017
852/852 [==============================] - 2s 2ms/step - loss: 0.3034 - accuracy: 0.8490 - val_loss: 0.2851 - val_accuracy: 0.8650
Epoch 119/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.3047 - accuracy: 0.8548
Epoch 119: val_loss did not improve from 0.28017
852/852 [==============================] - 2s 2ms/step - loss: 0.3048 - accuracy: 0.8540 - val_loss: 0.2810 - val_accuracy: 0.8643
Epoch 120/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.3021 - accuracy: 0.8524
Epoch 120: val_loss did not improve from 0.28017
852/852 [==============================] - 2s 2ms/step - loss: 0.3019 - accuracy: 0.8525 - val_loss: 0.2980 - val_accuracy: 0.8553
Epoch 121/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.3033 - accuracy: 0.8517
Epoch 121: val_loss did not improve from 0.28017
852/852 [==============================] - 2s 2ms/step - loss: 0.3039 - accuracy: 0.8513 - val_loss: 0.2866 - val_accuracy: 0.8636
Epoch 122/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.2996 - accuracy: 0.8540
Epoch 122: val_loss did not improve from 0.28017
852/852 [==============================] - 2s 2ms/step - loss: 0.3005 - accuracy: 0.8539 - val_loss: 0.2976 - val_accuracy: 0.8591
Epoch 123/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.3057 - accuracy: 0.8528
Epoch 123: val_loss improved from 0.28017 to 0.26993, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3080 - accuracy: 0.8506 - val_loss: 0.2699 - val_accuracy: 0.8725
Epoch 124/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.2970 - accuracy: 0.8593
Epoch 124: val_loss did not improve from 0.26993
852/852 [==============================] - 2s 2ms/step - loss: 0.2975 - accuracy: 0.8587 - val_loss: 0.2732 - val_accuracy: 0.8691
Epoch 125/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.2982 - accuracy: 0.8544
Epoch 125: val_loss did not improve from 0.26993
852/852 [==============================] - 2s 2ms/step - loss: 0.2988 - accuracy: 0.8542 - val_loss: 0.2742 - val_accuracy: 0.8679
Epoch 126/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.2905 - accuracy: 0.8596
Epoch 126: val_loss did not improve from 0.26993
852/852 [==============================] - 2s 2ms/step - loss: 0.2926 - accuracy: 0.8585 - val_loss: 0.2845 - val_accuracy: 0.8660
Epoch 127/150
815/852 [===========================&gt;..] - ETA: 0s - loss: 0.2904 - accuracy: 0.8604
Epoch 127: val_loss did not improve from 0.26993
852/852 [==============================] - 2s 2ms/step - loss: 0.2918 - accuracy: 0.8593 - val_loss: 0.2761 - val_accuracy: 0.8665
Epoch 128/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.2922 - accuracy: 0.8538
Epoch 128: val_loss improved from 0.26993 to 0.26978, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.2924 - accuracy: 0.8544 - val_loss: 0.2698 - val_accuracy: 0.8667
Epoch 129/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.2909 - accuracy: 0.8598
Epoch 129: val_loss did not improve from 0.26978
852/852 [==============================] - 2s 2ms/step - loss: 0.2908 - accuracy: 0.8598 - val_loss: 0.2708 - val_accuracy: 0.8636
Epoch 130/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.2870 - accuracy: 0.8598
Epoch 130: val_loss improved from 0.26978 to 0.26285, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.2871 - accuracy: 0.8597 - val_loss: 0.2628 - val_accuracy: 0.8731
Epoch 131/150
852/852 [==============================] - ETA: 0s - loss: 0.2892 - accuracy: 0.8578
Epoch 131: val_loss did not improve from 0.26285
852/852 [==============================] - 2s 2ms/step - loss: 0.2892 - accuracy: 0.8578 - val_loss: 0.2641 - val_accuracy: 0.8713
Epoch 132/150
814/852 [===========================&gt;..] - ETA: 0s - loss: 0.2860 - accuracy: 0.8586
Epoch 132: val_loss improved from 0.26285 to 0.26114, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.2848 - accuracy: 0.8603 - val_loss: 0.2611 - val_accuracy: 0.8717
Epoch 133/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.2914 - accuracy: 0.8573
Epoch 133: val_loss did not improve from 0.26114
852/852 [==============================] - 2s 2ms/step - loss: 0.2927 - accuracy: 0.8571 - val_loss: 0.2843 - val_accuracy: 0.8660
Epoch 134/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.3020 - accuracy: 0.8591
Epoch 134: val_loss improved from 0.26114 to 0.25871, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.3020 - accuracy: 0.8590 - val_loss: 0.2587 - val_accuracy: 0.8789
Epoch 135/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.2827 - accuracy: 0.8618
Epoch 135: val_loss did not improve from 0.25871
852/852 [==============================] - 2s 2ms/step - loss: 0.2840 - accuracy: 0.8605 - val_loss: 0.2587 - val_accuracy: 0.8776
Epoch 136/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.2849 - accuracy: 0.8590
Epoch 136: val_loss improved from 0.25871 to 0.25206, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.2841 - accuracy: 0.8594 - val_loss: 0.2521 - val_accuracy: 0.8828
Epoch 137/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.2764 - accuracy: 0.8630
Epoch 137: val_loss did not improve from 0.25206
852/852 [==============================] - 2s 2ms/step - loss: 0.2764 - accuracy: 0.8633 - val_loss: 0.2871 - val_accuracy: 0.8627
Epoch 138/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.2848 - accuracy: 0.8594
Epoch 138: val_loss did not improve from 0.25206
852/852 [==============================] - 2s 2ms/step - loss: 0.2838 - accuracy: 0.8606 - val_loss: 0.2661 - val_accuracy: 0.8714
Epoch 139/150
814/852 [===========================&gt;..] - ETA: 0s - loss: 0.2886 - accuracy: 0.8593
Epoch 139: val_loss did not improve from 0.25206
852/852 [==============================] - 2s 2ms/step - loss: 0.2883 - accuracy: 0.8600 - val_loss: 0.2621 - val_accuracy: 0.8765
Epoch 140/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.2784 - accuracy: 0.8635
Epoch 140: val_loss improved from 0.25206 to 0.25201, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.2783 - accuracy: 0.8633 - val_loss: 0.2520 - val_accuracy: 0.8756
Epoch 141/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.2829 - accuracy: 0.8622
Epoch 141: val_loss did not improve from 0.25201
852/852 [==============================] - 2s 2ms/step - loss: 0.2828 - accuracy: 0.8630 - val_loss: 0.2528 - val_accuracy: 0.8780
Epoch 142/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.2702 - accuracy: 0.8681
Epoch 142: val_loss improved from 0.25201 to 0.24381, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.2699 - accuracy: 0.8678 - val_loss: 0.2438 - val_accuracy: 0.8807
Epoch 143/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.2714 - accuracy: 0.8665
Epoch 143: val_loss did not improve from 0.24381
852/852 [==============================] - 2s 2ms/step - loss: 0.2716 - accuracy: 0.8664 - val_loss: 0.2527 - val_accuracy: 0.8806
Epoch 144/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.2805 - accuracy: 0.8651
Epoch 144: val_loss did not improve from 0.24381
852/852 [==============================] - 2s 3ms/step - loss: 0.2815 - accuracy: 0.8643 - val_loss: 0.2452 - val_accuracy: 0.8806
Epoch 145/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.2793 - accuracy: 0.8644
Epoch 145: val_loss did not improve from 0.24381
852/852 [==============================] - 2s 2ms/step - loss: 0.2792 - accuracy: 0.8645 - val_loss: 0.2519 - val_accuracy: 0.8786
Epoch 146/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.2740 - accuracy: 0.8653
Epoch 146: val_loss did not improve from 0.24381
852/852 [==============================] - 2s 2ms/step - loss: 0.2742 - accuracy: 0.8650 - val_loss: 0.2462 - val_accuracy: 0.8769
Epoch 147/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.2736 - accuracy: 0.8652
Epoch 147: val_loss did not improve from 0.24381
852/852 [==============================] - 2s 2ms/step - loss: 0.2741 - accuracy: 0.8650 - val_loss: 0.2586 - val_accuracy: 0.8781
Epoch 148/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.2879 - accuracy: 0.8618
Epoch 148: val_loss did not improve from 0.24381
852/852 [==============================] - 2s 2ms/step - loss: 0.2870 - accuracy: 0.8625 - val_loss: 0.2451 - val_accuracy: 0.8836
Epoch 149/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.2631 - accuracy: 0.8708
Epoch 149: val_loss did not improve from 0.24381
852/852 [==============================] - 2s 2ms/step - loss: 0.2645 - accuracy: 0.8702 - val_loss: 0.2475 - val_accuracy: 0.8807
Epoch 150/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.2662 - accuracy: 0.8679
Epoch 150: val_loss did not improve from 0.24381
852/852 [==============================] - 2s 2ms/step - loss: 0.2665 - accuracy: 0.8674 - val_loss: 0.2518 - val_accuracy: 0.8803
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=b076685e-15bf-4996-ac6f-80c79aa9d095">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [82]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">history_497</span> <span class="o">=</span> <span class="n">four_ninetyseven_neuron</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">callback_a</span><span class="p">,</span> <span class="n">callback_b</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.2693 - accuracy: 0.8691
Epoch 1: val_loss improved from 0.24381 to 0.23592, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.2691 - accuracy: 0.8692 - val_loss: 0.2359 - val_accuracy: 0.8857
Epoch 2/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.2683 - accuracy: 0.8682
Epoch 2: val_loss did not improve from 0.23592
852/852 [==============================] - 2s 3ms/step - loss: 0.2683 - accuracy: 0.8681 - val_loss: 0.2446 - val_accuracy: 0.8801
Epoch 3/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.2582 - accuracy: 0.8718
Epoch 3: val_loss did not improve from 0.23592
852/852 [==============================] - 2s 2ms/step - loss: 0.2583 - accuracy: 0.8724 - val_loss: 0.2375 - val_accuracy: 0.8823
Epoch 4/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.2742 - accuracy: 0.8684
Epoch 4: val_loss did not improve from 0.23592
852/852 [==============================] - 2s 2ms/step - loss: 0.2734 - accuracy: 0.8681 - val_loss: 0.2454 - val_accuracy: 0.8833
Epoch 5/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.2608 - accuracy: 0.8746
Epoch 5: val_loss did not improve from 0.23592
852/852 [==============================] - 2s 2ms/step - loss: 0.2615 - accuracy: 0.8742 - val_loss: 0.2457 - val_accuracy: 0.8823
Epoch 6/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.2681 - accuracy: 0.8697
Epoch 6: val_loss did not improve from 0.23592
852/852 [==============================] - 2s 2ms/step - loss: 0.2688 - accuracy: 0.8697 - val_loss: 0.2522 - val_accuracy: 0.8808
Epoch 7/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.2553 - accuracy: 0.8755
Epoch 7: val_loss did not improve from 0.23592
852/852 [==============================] - 2s 2ms/step - loss: 0.2560 - accuracy: 0.8753 - val_loss: 0.2616 - val_accuracy: 0.8764
Epoch 8/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.2561 - accuracy: 0.8731
Epoch 8: val_loss did not improve from 0.23592
852/852 [==============================] - 2s 2ms/step - loss: 0.2575 - accuracy: 0.8720 - val_loss: 0.2521 - val_accuracy: 0.8779
Epoch 9/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.2597 - accuracy: 0.8716
Epoch 9: val_loss did not improve from 0.23592
852/852 [==============================] - 2s 2ms/step - loss: 0.2598 - accuracy: 0.8714 - val_loss: 0.2602 - val_accuracy: 0.8741
Epoch 10/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.2665 - accuracy: 0.8705
Epoch 10: val_loss improved from 0.23592 to 0.22919, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.2662 - accuracy: 0.8707 - val_loss: 0.2292 - val_accuracy: 0.8879
Epoch 11/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.2627 - accuracy: 0.8718
Epoch 11: val_loss did not improve from 0.22919
852/852 [==============================] - 2s 2ms/step - loss: 0.2628 - accuracy: 0.8717 - val_loss: 0.2690 - val_accuracy: 0.8698
Epoch 12/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.2554 - accuracy: 0.8746
Epoch 12: val_loss improved from 0.22919 to 0.22791, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.2552 - accuracy: 0.8744 - val_loss: 0.2279 - val_accuracy: 0.8943
Epoch 13/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.2514 - accuracy: 0.8791
Epoch 13: val_loss did not improve from 0.22791
852/852 [==============================] - 2s 2ms/step - loss: 0.2517 - accuracy: 0.8788 - val_loss: 0.2644 - val_accuracy: 0.8788
Epoch 14/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.2579 - accuracy: 0.8754
Epoch 14: val_loss did not improve from 0.22791
852/852 [==============================] - 2s 2ms/step - loss: 0.2576 - accuracy: 0.8759 - val_loss: 0.2418 - val_accuracy: 0.8825
Epoch 15/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.2532 - accuracy: 0.8765
Epoch 15: val_loss improved from 0.22791 to 0.22666, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.2525 - accuracy: 0.8768 - val_loss: 0.2267 - val_accuracy: 0.8900
Epoch 16/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.2549 - accuracy: 0.8740
Epoch 16: val_loss did not improve from 0.22666
852/852 [==============================] - 2s 2ms/step - loss: 0.2554 - accuracy: 0.8736 - val_loss: 0.2327 - val_accuracy: 0.8867
Epoch 17/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.2521 - accuracy: 0.8769
Epoch 17: val_loss did not improve from 0.22666
852/852 [==============================] - 2s 2ms/step - loss: 0.2525 - accuracy: 0.8766 - val_loss: 0.2430 - val_accuracy: 0.8832
Epoch 18/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.2505 - accuracy: 0.8768
Epoch 18: val_loss did not improve from 0.22666
852/852 [==============================] - 2s 2ms/step - loss: 0.2508 - accuracy: 0.8765 - val_loss: 0.2490 - val_accuracy: 0.8848
Epoch 19/150
818/852 [===========================&gt;..] - ETA: 0s - loss: 0.2524 - accuracy: 0.8773
Epoch 19: val_loss did not improve from 0.22666
852/852 [==============================] - 2s 2ms/step - loss: 0.2532 - accuracy: 0.8764 - val_loss: 0.2495 - val_accuracy: 0.8776
Epoch 20/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.2529 - accuracy: 0.8788
Epoch 20: val_loss did not improve from 0.22666
852/852 [==============================] - 2s 2ms/step - loss: 0.2530 - accuracy: 0.8783 - val_loss: 0.2324 - val_accuracy: 0.8883
Epoch 21/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.2520 - accuracy: 0.8789
Epoch 21: val_loss did not improve from 0.22666
852/852 [==============================] - 2s 2ms/step - loss: 0.2538 - accuracy: 0.8780 - val_loss: 0.2438 - val_accuracy: 0.8833
Epoch 22/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.2504 - accuracy: 0.8768
Epoch 22: val_loss did not improve from 0.22666
852/852 [==============================] - 2s 2ms/step - loss: 0.2512 - accuracy: 0.8766 - val_loss: 0.2314 - val_accuracy: 0.8890
Epoch 23/150
819/852 [===========================&gt;..] - ETA: 0s - loss: 0.2399 - accuracy: 0.8810
Epoch 23: val_loss did not improve from 0.22666
852/852 [==============================] - 2s 2ms/step - loss: 0.2401 - accuracy: 0.8801 - val_loss: 0.2382 - val_accuracy: 0.8861
Epoch 24/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.2590 - accuracy: 0.8785
Epoch 24: val_loss did not improve from 0.22666
852/852 [==============================] - 2s 2ms/step - loss: 0.2590 - accuracy: 0.8783 - val_loss: 0.2339 - val_accuracy: 0.8854
Epoch 25/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.2463 - accuracy: 0.8803
Epoch 25: val_loss did not improve from 0.22666
852/852 [==============================] - 2s 2ms/step - loss: 0.2465 - accuracy: 0.8803 - val_loss: 0.2396 - val_accuracy: 0.8842
Epoch 26/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.2390 - accuracy: 0.8832
Epoch 26: val_loss improved from 0.22666 to 0.22228, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.2398 - accuracy: 0.8828 - val_loss: 0.2223 - val_accuracy: 0.8903
Epoch 27/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.2410 - accuracy: 0.8797
Epoch 27: val_loss did not improve from 0.22228
852/852 [==============================] - 2s 2ms/step - loss: 0.2430 - accuracy: 0.8786 - val_loss: 0.2322 - val_accuracy: 0.8887
Epoch 28/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.2427 - accuracy: 0.8814
Epoch 28: val_loss did not improve from 0.22228
852/852 [==============================] - 2s 2ms/step - loss: 0.2436 - accuracy: 0.8809 - val_loss: 0.2355 - val_accuracy: 0.8890
Epoch 29/150
814/852 [===========================&gt;..] - ETA: 0s - loss: 0.2429 - accuracy: 0.8848
Epoch 29: val_loss improved from 0.22228 to 0.22173, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.2468 - accuracy: 0.8832 - val_loss: 0.2217 - val_accuracy: 0.8960
Epoch 30/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.2416 - accuracy: 0.8855
Epoch 30: val_loss improved from 0.22173 to 0.21494, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.2410 - accuracy: 0.8859 - val_loss: 0.2149 - val_accuracy: 0.8958
Epoch 31/150
814/852 [===========================&gt;..] - ETA: 0s - loss: 0.2427 - accuracy: 0.8801
Epoch 31: val_loss did not improve from 0.21494
852/852 [==============================] - 2s 2ms/step - loss: 0.2421 - accuracy: 0.8805 - val_loss: 0.2202 - val_accuracy: 0.8953
Epoch 32/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.2422 - accuracy: 0.8842
Epoch 32: val_loss did not improve from 0.21494
852/852 [==============================] - 2s 2ms/step - loss: 0.2416 - accuracy: 0.8845 - val_loss: 0.2216 - val_accuracy: 0.8951
Epoch 33/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.2354 - accuracy: 0.8856
Epoch 33: val_loss did not improve from 0.21494
852/852 [==============================] - 2s 2ms/step - loss: 0.2365 - accuracy: 0.8850 - val_loss: 0.2286 - val_accuracy: 0.8931
Epoch 34/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.2429 - accuracy: 0.8815
Epoch 34: val_loss did not improve from 0.21494
852/852 [==============================] - 2s 2ms/step - loss: 0.2439 - accuracy: 0.8812 - val_loss: 0.2249 - val_accuracy: 0.8914
Epoch 35/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.2407 - accuracy: 0.8830
Epoch 35: val_loss did not improve from 0.21494
852/852 [==============================] - 2s 2ms/step - loss: 0.2409 - accuracy: 0.8826 - val_loss: 0.2313 - val_accuracy: 0.8894
Epoch 36/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.2418 - accuracy: 0.8818
Epoch 36: val_loss did not improve from 0.21494
852/852 [==============================] - 2s 2ms/step - loss: 0.2413 - accuracy: 0.8819 - val_loss: 0.2232 - val_accuracy: 0.8926
Epoch 37/150
815/852 [===========================&gt;..] - ETA: 0s - loss: 0.2346 - accuracy: 0.8842
Epoch 37: val_loss did not improve from 0.21494
852/852 [==============================] - 2s 2ms/step - loss: 0.2361 - accuracy: 0.8835 - val_loss: 0.2151 - val_accuracy: 0.8955
Epoch 38/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.2337 - accuracy: 0.8808
Epoch 38: val_loss did not improve from 0.21494
852/852 [==============================] - 2s 2ms/step - loss: 0.2343 - accuracy: 0.8807 - val_loss: 0.2323 - val_accuracy: 0.8868
Epoch 39/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.2396 - accuracy: 0.8826
Epoch 39: val_loss improved from 0.21494 to 0.21473, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.2399 - accuracy: 0.8820 - val_loss: 0.2147 - val_accuracy: 0.8957
Epoch 40/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.2356 - accuracy: 0.8851
Epoch 40: val_loss improved from 0.21473 to 0.21372, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.2345 - accuracy: 0.8856 - val_loss: 0.2137 - val_accuracy: 0.8950
Epoch 41/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.2347 - accuracy: 0.8844
Epoch 41: val_loss did not improve from 0.21372
852/852 [==============================] - 2s 2ms/step - loss: 0.2344 - accuracy: 0.8846 - val_loss: 0.2198 - val_accuracy: 0.8957
Epoch 42/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.2306 - accuracy: 0.8893
Epoch 42: val_loss did not improve from 0.21372
852/852 [==============================] - 2s 2ms/step - loss: 0.2311 - accuracy: 0.8888 - val_loss: 0.2165 - val_accuracy: 0.8982
Epoch 43/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.2429 - accuracy: 0.8808
Epoch 43: val_loss improved from 0.21372 to 0.21167, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.2428 - accuracy: 0.8812 - val_loss: 0.2117 - val_accuracy: 0.8967
Epoch 44/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.2329 - accuracy: 0.8862
Epoch 44: val_loss did not improve from 0.21167
852/852 [==============================] - 2s 2ms/step - loss: 0.2322 - accuracy: 0.8866 - val_loss: 0.2328 - val_accuracy: 0.8920
Epoch 45/150
818/852 [===========================&gt;..] - ETA: 0s - loss: 0.2357 - accuracy: 0.8890
Epoch 45: val_loss improved from 0.21167 to 0.20027, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.2360 - accuracy: 0.8888 - val_loss: 0.2003 - val_accuracy: 0.9045
Epoch 46/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.2245 - accuracy: 0.8893
Epoch 46: val_loss did not improve from 0.20027
852/852 [==============================] - 2s 2ms/step - loss: 0.2249 - accuracy: 0.8891 - val_loss: 0.2047 - val_accuracy: 0.9001
Epoch 47/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.2295 - accuracy: 0.8908
Epoch 47: val_loss did not improve from 0.20027
852/852 [==============================] - 2s 2ms/step - loss: 0.2296 - accuracy: 0.8903 - val_loss: 0.2235 - val_accuracy: 0.8917
Epoch 48/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.2304 - accuracy: 0.8884
Epoch 48: val_loss did not improve from 0.20027
852/852 [==============================] - 2s 2ms/step - loss: 0.2303 - accuracy: 0.8883 - val_loss: 0.2278 - val_accuracy: 0.8935
Epoch 49/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.2374 - accuracy: 0.8861
Epoch 49: val_loss did not improve from 0.20027
852/852 [==============================] - 2s 2ms/step - loss: 0.2371 - accuracy: 0.8861 - val_loss: 0.2272 - val_accuracy: 0.8894
Epoch 50/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.2345 - accuracy: 0.8872
Epoch 50: val_loss did not improve from 0.20027
852/852 [==============================] - 2s 2ms/step - loss: 0.2337 - accuracy: 0.8876 - val_loss: 0.2185 - val_accuracy: 0.8950
Epoch 51/150
819/852 [===========================&gt;..] - ETA: 0s - loss: 0.2366 - accuracy: 0.8872
Epoch 51: val_loss did not improve from 0.20027
852/852 [==============================] - 2s 2ms/step - loss: 0.2374 - accuracy: 0.8872 - val_loss: 0.2125 - val_accuracy: 0.9005
Epoch 52/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.2231 - accuracy: 0.8916
Epoch 52: val_loss did not improve from 0.20027
852/852 [==============================] - 2s 2ms/step - loss: 0.2243 - accuracy: 0.8907 - val_loss: 0.2019 - val_accuracy: 0.9047
Epoch 53/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.2286 - accuracy: 0.8932
Epoch 53: val_loss did not improve from 0.20027
852/852 [==============================] - 2s 2ms/step - loss: 0.2289 - accuracy: 0.8931 - val_loss: 0.2167 - val_accuracy: 0.9009
Epoch 54/150
819/852 [===========================&gt;..] - ETA: 0s - loss: 0.2286 - accuracy: 0.8934
Epoch 54: val_loss did not improve from 0.20027
852/852 [==============================] - 2s 2ms/step - loss: 0.2287 - accuracy: 0.8941 - val_loss: 0.2136 - val_accuracy: 0.8984
Epoch 55/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.2297 - accuracy: 0.8872
Epoch 55: val_loss improved from 0.20027 to 0.19213, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.2297 - accuracy: 0.8868 - val_loss: 0.1921 - val_accuracy: 0.9113
Epoch 56/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.2151 - accuracy: 0.8969
Epoch 56: val_loss did not improve from 0.19213
852/852 [==============================] - 2s 2ms/step - loss: 0.2147 - accuracy: 0.8971 - val_loss: 0.2089 - val_accuracy: 0.8990
Epoch 57/150
814/852 [===========================&gt;..] - ETA: 0s - loss: 0.2252 - accuracy: 0.8902
Epoch 57: val_loss did not improve from 0.19213
852/852 [==============================] - 2s 2ms/step - loss: 0.2252 - accuracy: 0.8907 - val_loss: 0.2187 - val_accuracy: 0.8961
Epoch 58/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.2179 - accuracy: 0.8963
Epoch 58: val_loss did not improve from 0.19213
852/852 [==============================] - 2s 2ms/step - loss: 0.2182 - accuracy: 0.8962 - val_loss: 0.2014 - val_accuracy: 0.9055
Epoch 59/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.2313 - accuracy: 0.8913
Epoch 59: val_loss did not improve from 0.19213
852/852 [==============================] - 2s 2ms/step - loss: 0.2324 - accuracy: 0.8901 - val_loss: 0.2068 - val_accuracy: 0.9045
Epoch 60/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.2201 - accuracy: 0.8936
Epoch 60: val_loss did not improve from 0.19213
852/852 [==============================] - 2s 2ms/step - loss: 0.2194 - accuracy: 0.8941 - val_loss: 0.2004 - val_accuracy: 0.9023
Epoch 61/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.2171 - accuracy: 0.8965
Epoch 61: val_loss did not improve from 0.19213
852/852 [==============================] - 2s 2ms/step - loss: 0.2182 - accuracy: 0.8955 - val_loss: 0.1996 - val_accuracy: 0.9062
Epoch 62/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.2208 - accuracy: 0.8949
Epoch 62: val_loss did not improve from 0.19213
852/852 [==============================] - 2s 2ms/step - loss: 0.2202 - accuracy: 0.8954 - val_loss: 0.2239 - val_accuracy: 0.8926
Epoch 63/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.2235 - accuracy: 0.8940
Epoch 63: val_loss did not improve from 0.19213
852/852 [==============================] - 2s 2ms/step - loss: 0.2242 - accuracy: 0.8927 - val_loss: 0.2093 - val_accuracy: 0.9001
Epoch 64/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.2199 - accuracy: 0.8970
Epoch 64: val_loss did not improve from 0.19213
852/852 [==============================] - 2s 2ms/step - loss: 0.2200 - accuracy: 0.8971 - val_loss: 0.2464 - val_accuracy: 0.8848
Epoch 65/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.2290 - accuracy: 0.8905
Epoch 65: val_loss did not improve from 0.19213
852/852 [==============================] - 2s 2ms/step - loss: 0.2299 - accuracy: 0.8897 - val_loss: 0.1929 - val_accuracy: 0.9102
Epoch 66/150
814/852 [===========================&gt;..] - ETA: 0s - loss: 0.2142 - accuracy: 0.8989
Epoch 66: val_loss did not improve from 0.19213
852/852 [==============================] - 2s 2ms/step - loss: 0.2133 - accuracy: 0.8992 - val_loss: 0.2306 - val_accuracy: 0.8930
Epoch 67/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.2236 - accuracy: 0.8912
Epoch 67: val_loss did not improve from 0.19213
852/852 [==============================] - 2s 2ms/step - loss: 0.2237 - accuracy: 0.8913 - val_loss: 0.2000 - val_accuracy: 0.9062
Epoch 68/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.2163 - accuracy: 0.8971
Epoch 68: val_loss did not improve from 0.19213
852/852 [==============================] - 2s 2ms/step - loss: 0.2168 - accuracy: 0.8967 - val_loss: 0.2049 - val_accuracy: 0.8992
Epoch 69/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.2245 - accuracy: 0.8906
Epoch 69: val_loss did not improve from 0.19213
852/852 [==============================] - 2s 2ms/step - loss: 0.2247 - accuracy: 0.8904 - val_loss: 0.2100 - val_accuracy: 0.9002
Epoch 70/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.2077 - accuracy: 0.8970
Epoch 70: val_loss did not improve from 0.19213
852/852 [==============================] - 2s 2ms/step - loss: 0.2078 - accuracy: 0.8971 - val_loss: 0.2012 - val_accuracy: 0.9059
Epoch 71/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.2199 - accuracy: 0.8952
Epoch 71: val_loss improved from 0.19213 to 0.18440, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.2194 - accuracy: 0.8954 - val_loss: 0.1844 - val_accuracy: 0.9089
Epoch 72/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.2167 - accuracy: 0.8933
Epoch 72: val_loss did not improve from 0.18440
852/852 [==============================] - 2s 2ms/step - loss: 0.2159 - accuracy: 0.8940 - val_loss: 0.1953 - val_accuracy: 0.9083
Epoch 73/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.2141 - accuracy: 0.8944
Epoch 73: val_loss did not improve from 0.18440
852/852 [==============================] - 2s 2ms/step - loss: 0.2140 - accuracy: 0.8942 - val_loss: 0.2173 - val_accuracy: 0.9007
Epoch 74/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.2085 - accuracy: 0.8989
Epoch 74: val_loss did not improve from 0.18440
852/852 [==============================] - 2s 3ms/step - loss: 0.2092 - accuracy: 0.8984 - val_loss: 0.1920 - val_accuracy: 0.9056
Epoch 75/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.2177 - accuracy: 0.8969
Epoch 75: val_loss did not improve from 0.18440
852/852 [==============================] - 2s 2ms/step - loss: 0.2176 - accuracy: 0.8970 - val_loss: 0.1902 - val_accuracy: 0.9111
Epoch 76/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.2106 - accuracy: 0.8968
Epoch 76: val_loss did not improve from 0.18440
852/852 [==============================] - 2s 2ms/step - loss: 0.2106 - accuracy: 0.8970 - val_loss: 0.2025 - val_accuracy: 0.9071
Epoch 77/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.2133 - accuracy: 0.8978
Epoch 77: val_loss did not improve from 0.18440
852/852 [==============================] - 2s 2ms/step - loss: 0.2137 - accuracy: 0.8971 - val_loss: 0.2029 - val_accuracy: 0.9037
Epoch 78/150
818/852 [===========================&gt;..] - ETA: 0s - loss: 0.2152 - accuracy: 0.8924
Epoch 78: val_loss did not improve from 0.18440
852/852 [==============================] - 2s 2ms/step - loss: 0.2149 - accuracy: 0.8931 - val_loss: 0.2230 - val_accuracy: 0.8981
Epoch 79/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.2201 - accuracy: 0.8957
Epoch 79: val_loss did not improve from 0.18440
852/852 [==============================] - 2s 3ms/step - loss: 0.2208 - accuracy: 0.8953 - val_loss: 0.2139 - val_accuracy: 0.9012
Epoch 80/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.2086 - accuracy: 0.9006
Epoch 80: val_loss did not improve from 0.18440
852/852 [==============================] - 2s 3ms/step - loss: 0.2085 - accuracy: 0.9005 - val_loss: 0.1942 - val_accuracy: 0.9089
Epoch 81/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.2114 - accuracy: 0.8977
Epoch 81: val_loss did not improve from 0.18440
852/852 [==============================] - 2s 3ms/step - loss: 0.2119 - accuracy: 0.8970 - val_loss: 0.1851 - val_accuracy: 0.9101
Epoch 82/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.2087 - accuracy: 0.9010
Epoch 82: val_loss did not improve from 0.18440
852/852 [==============================] - 2s 3ms/step - loss: 0.2091 - accuracy: 0.9007 - val_loss: 0.1942 - val_accuracy: 0.9069
Epoch 83/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.2044 - accuracy: 0.9011
Epoch 83: val_loss did not improve from 0.18440
852/852 [==============================] - 2s 2ms/step - loss: 0.2047 - accuracy: 0.9008 - val_loss: 0.1918 - val_accuracy: 0.9088
Epoch 84/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.2146 - accuracy: 0.8977
Epoch 84: val_loss did not improve from 0.18440
852/852 [==============================] - 2s 2ms/step - loss: 0.2153 - accuracy: 0.8971 - val_loss: 0.2103 - val_accuracy: 0.9002
Epoch 85/150
819/852 [===========================&gt;..] - ETA: 0s - loss: 0.2169 - accuracy: 0.8995
Epoch 85: val_loss did not improve from 0.18440
852/852 [==============================] - 2s 2ms/step - loss: 0.2155 - accuracy: 0.8994 - val_loss: 0.2102 - val_accuracy: 0.9025
Epoch 86/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.2070 - accuracy: 0.9016
Epoch 86: val_loss improved from 0.18440 to 0.17823, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.2064 - accuracy: 0.9019 - val_loss: 0.1782 - val_accuracy: 0.9125
Epoch 87/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.2128 - accuracy: 0.8961
Epoch 87: val_loss improved from 0.17823 to 0.17184, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.2123 - accuracy: 0.8965 - val_loss: 0.1718 - val_accuracy: 0.9197
Epoch 88/150
815/852 [===========================&gt;..] - ETA: 0s - loss: 0.1992 - accuracy: 0.9065
Epoch 88: val_loss did not improve from 0.17184
852/852 [==============================] - 2s 2ms/step - loss: 0.1990 - accuracy: 0.9065 - val_loss: 0.1824 - val_accuracy: 0.9162
Epoch 89/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.2054 - accuracy: 0.9026
Epoch 89: val_loss did not improve from 0.17184
852/852 [==============================] - 2s 3ms/step - loss: 0.2060 - accuracy: 0.9022 - val_loss: 0.1794 - val_accuracy: 0.9151
Epoch 90/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.1985 - accuracy: 0.9048
Epoch 90: val_loss did not improve from 0.17184
852/852 [==============================] - 2s 2ms/step - loss: 0.1987 - accuracy: 0.9048 - val_loss: 0.1827 - val_accuracy: 0.9140
Epoch 91/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.2104 - accuracy: 0.9019
Epoch 91: val_loss did not improve from 0.17184
852/852 [==============================] - 2s 2ms/step - loss: 0.2105 - accuracy: 0.9014 - val_loss: 0.2013 - val_accuracy: 0.9052
Epoch 92/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.2051 - accuracy: 0.9031
Epoch 92: val_loss did not improve from 0.17184
852/852 [==============================] - 2s 2ms/step - loss: 0.2057 - accuracy: 0.9029 - val_loss: 0.2261 - val_accuracy: 0.8964
Epoch 93/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.2033 - accuracy: 0.9023
Epoch 93: val_loss did not improve from 0.17184
852/852 [==============================] - 2s 2ms/step - loss: 0.2041 - accuracy: 0.9027 - val_loss: 0.1891 - val_accuracy: 0.9130
Epoch 94/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.2106 - accuracy: 0.9000
Epoch 94: val_loss did not improve from 0.17184
852/852 [==============================] - 2s 2ms/step - loss: 0.2102 - accuracy: 0.9002 - val_loss: 0.1924 - val_accuracy: 0.9130
Epoch 95/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.2024 - accuracy: 0.9034
Epoch 95: val_loss did not improve from 0.17184
852/852 [==============================] - 2s 2ms/step - loss: 0.2030 - accuracy: 0.9031 - val_loss: 0.1823 - val_accuracy: 0.9111
Epoch 96/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.2000 - accuracy: 0.9030
Epoch 96: val_loss did not improve from 0.17184
852/852 [==============================] - 2s 2ms/step - loss: 0.1997 - accuracy: 0.9029 - val_loss: 0.1800 - val_accuracy: 0.9135
Epoch 97/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.2030 - accuracy: 0.9046
Epoch 97: val_loss did not improve from 0.17184
852/852 [==============================] - 2s 3ms/step - loss: 0.2022 - accuracy: 0.9051 - val_loss: 0.1820 - val_accuracy: 0.9180
Epoch 98/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.1997 - accuracy: 0.9027
Epoch 98: val_loss did not improve from 0.17184
852/852 [==============================] - 2s 2ms/step - loss: 0.2016 - accuracy: 0.9024 - val_loss: 0.2071 - val_accuracy: 0.9102
Epoch 99/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.2042 - accuracy: 0.9025
Epoch 99: val_loss improved from 0.17184 to 0.16824, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.2035 - accuracy: 0.9034 - val_loss: 0.1682 - val_accuracy: 0.9207
Epoch 100/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.1961 - accuracy: 0.9088
Epoch 100: val_loss did not improve from 0.16824
852/852 [==============================] - 2s 2ms/step - loss: 0.1969 - accuracy: 0.9088 - val_loss: 0.1850 - val_accuracy: 0.9119
Epoch 101/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.2014 - accuracy: 0.9018
Epoch 101: val_loss did not improve from 0.16824
852/852 [==============================] - 2s 2ms/step - loss: 0.2006 - accuracy: 0.9022 - val_loss: 0.1964 - val_accuracy: 0.9082
Epoch 102/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.1897 - accuracy: 0.9088
Epoch 102: val_loss did not improve from 0.16824
852/852 [==============================] - 2s 2ms/step - loss: 0.1893 - accuracy: 0.9091 - val_loss: 0.1714 - val_accuracy: 0.9178
Epoch 103/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.1985 - accuracy: 0.9062
Epoch 103: val_loss did not improve from 0.16824
852/852 [==============================] - 2s 2ms/step - loss: 0.2006 - accuracy: 0.9057 - val_loss: 0.1822 - val_accuracy: 0.9130
Epoch 104/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.1968 - accuracy: 0.9060
Epoch 104: val_loss did not improve from 0.16824
852/852 [==============================] - 2s 2ms/step - loss: 0.1973 - accuracy: 0.9058 - val_loss: 0.1747 - val_accuracy: 0.9155
Epoch 105/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.1917 - accuracy: 0.9077
Epoch 105: val_loss did not improve from 0.16824
852/852 [==============================] - 2s 2ms/step - loss: 0.1912 - accuracy: 0.9079 - val_loss: 0.1817 - val_accuracy: 0.9140
Epoch 106/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.1894 - accuracy: 0.9050
Epoch 106: val_loss did not improve from 0.16824
852/852 [==============================] - 2s 2ms/step - loss: 0.1898 - accuracy: 0.9043 - val_loss: 0.1700 - val_accuracy: 0.9165
Epoch 107/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.1988 - accuracy: 0.9034
Epoch 107: val_loss did not improve from 0.16824
852/852 [==============================] - 2s 2ms/step - loss: 0.1987 - accuracy: 0.9031 - val_loss: 0.1850 - val_accuracy: 0.9084
Epoch 108/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.1914 - accuracy: 0.9082
Epoch 108: val_loss improved from 0.16824 to 0.16812, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.1911 - accuracy: 0.9082 - val_loss: 0.1681 - val_accuracy: 0.9200
Epoch 109/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.1930 - accuracy: 0.9072
Epoch 109: val_loss did not improve from 0.16812
852/852 [==============================] - 2s 2ms/step - loss: 0.1927 - accuracy: 0.9081 - val_loss: 0.1831 - val_accuracy: 0.9135
Epoch 110/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.1975 - accuracy: 0.9035
Epoch 110: val_loss did not improve from 0.16812
852/852 [==============================] - 2s 2ms/step - loss: 0.1981 - accuracy: 0.9034 - val_loss: 0.1906 - val_accuracy: 0.9133
Epoch 111/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.2009 - accuracy: 0.9075
Epoch 111: val_loss improved from 0.16812 to 0.16593, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.2005 - accuracy: 0.9082 - val_loss: 0.1659 - val_accuracy: 0.9241
Epoch 112/150
820/852 [===========================&gt;..] - ETA: 0s - loss: 0.1910 - accuracy: 0.9094
Epoch 112: val_loss improved from 0.16593 to 0.16407, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.1901 - accuracy: 0.9093 - val_loss: 0.1641 - val_accuracy: 0.9214
Epoch 113/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.1854 - accuracy: 0.9137
Epoch 113: val_loss did not improve from 0.16407
852/852 [==============================] - 2s 2ms/step - loss: 0.1883 - accuracy: 0.9131 - val_loss: 0.1867 - val_accuracy: 0.9123
Epoch 114/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.1898 - accuracy: 0.9099
Epoch 114: val_loss did not improve from 0.16407
852/852 [==============================] - 2s 2ms/step - loss: 0.1903 - accuracy: 0.9092 - val_loss: 0.1756 - val_accuracy: 0.9192
Epoch 115/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.1805 - accuracy: 0.9115
Epoch 115: val_loss did not improve from 0.16407
852/852 [==============================] - 2s 2ms/step - loss: 0.1809 - accuracy: 0.9111 - val_loss: 0.1910 - val_accuracy: 0.9123
Epoch 116/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.1964 - accuracy: 0.9086
Epoch 116: val_loss did not improve from 0.16407
852/852 [==============================] - 2s 2ms/step - loss: 0.1970 - accuracy: 0.9084 - val_loss: 0.1738 - val_accuracy: 0.9198
Epoch 117/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.2038 - accuracy: 0.9052
Epoch 117: val_loss did not improve from 0.16407
852/852 [==============================] - 2s 2ms/step - loss: 0.2036 - accuracy: 0.9050 - val_loss: 0.1732 - val_accuracy: 0.9184
Epoch 118/150
818/852 [===========================&gt;..] - ETA: 0s - loss: 0.1830 - accuracy: 0.9110
Epoch 118: val_loss did not improve from 0.16407
852/852 [==============================] - 2s 2ms/step - loss: 0.1828 - accuracy: 0.9112 - val_loss: 0.1678 - val_accuracy: 0.9212
Epoch 119/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.1975 - accuracy: 0.9096
Epoch 119: val_loss did not improve from 0.16407
852/852 [==============================] - 2s 2ms/step - loss: 0.1972 - accuracy: 0.9098 - val_loss: 0.1761 - val_accuracy: 0.9163
Epoch 120/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.1929 - accuracy: 0.9077
Epoch 120: val_loss did not improve from 0.16407
852/852 [==============================] - 2s 2ms/step - loss: 0.1922 - accuracy: 0.9079 - val_loss: 0.1733 - val_accuracy: 0.9186
Epoch 121/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.1820 - accuracy: 0.9117
Epoch 121: val_loss did not improve from 0.16407
852/852 [==============================] - 2s 2ms/step - loss: 0.1816 - accuracy: 0.9123 - val_loss: 0.1800 - val_accuracy: 0.9123
Epoch 122/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.1924 - accuracy: 0.9060
Epoch 122: val_loss did not improve from 0.16407
852/852 [==============================] - 2s 2ms/step - loss: 0.1931 - accuracy: 0.9052 - val_loss: 0.1738 - val_accuracy: 0.9150
Epoch 123/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.1859 - accuracy: 0.9101
Epoch 123: val_loss did not improve from 0.16407
852/852 [==============================] - 2s 2ms/step - loss: 0.1876 - accuracy: 0.9082 - val_loss: 0.1871 - val_accuracy: 0.9142
Epoch 124/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.1941 - accuracy: 0.9113
Epoch 124: val_loss improved from 0.16407 to 0.15832, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.1956 - accuracy: 0.9109 - val_loss: 0.1583 - val_accuracy: 0.9253
Epoch 125/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.1831 - accuracy: 0.9145
Epoch 125: val_loss did not improve from 0.15832
852/852 [==============================] - 2s 2ms/step - loss: 0.1831 - accuracy: 0.9145 - val_loss: 0.1587 - val_accuracy: 0.9234
Epoch 126/150
819/852 [===========================&gt;..] - ETA: 0s - loss: 0.1818 - accuracy: 0.9151
Epoch 126: val_loss did not improve from 0.15832
852/852 [==============================] - 2s 2ms/step - loss: 0.1849 - accuracy: 0.9133 - val_loss: 0.1802 - val_accuracy: 0.9149
Epoch 127/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.1898 - accuracy: 0.9113
Epoch 127: val_loss did not improve from 0.15832
852/852 [==============================] - 2s 2ms/step - loss: 0.1905 - accuracy: 0.9108 - val_loss: 0.1770 - val_accuracy: 0.9159
Epoch 128/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.1874 - accuracy: 0.9101
Epoch 128: val_loss did not improve from 0.15832
852/852 [==============================] - 2s 3ms/step - loss: 0.1872 - accuracy: 0.9099 - val_loss: 0.1681 - val_accuracy: 0.9227
Epoch 129/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.1946 - accuracy: 0.9108
Epoch 129: val_loss did not improve from 0.15832
852/852 [==============================] - 2s 3ms/step - loss: 0.1945 - accuracy: 0.9108 - val_loss: 0.2004 - val_accuracy: 0.9079
Epoch 130/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.1865 - accuracy: 0.9139
Epoch 130: val_loss did not improve from 0.15832
852/852 [==============================] - 2s 2ms/step - loss: 0.1863 - accuracy: 0.9137 - val_loss: 0.1741 - val_accuracy: 0.9203
Epoch 131/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.1717 - accuracy: 0.9172
Epoch 131: val_loss did not improve from 0.15832
852/852 [==============================] - 2s 3ms/step - loss: 0.1727 - accuracy: 0.9170 - val_loss: 0.1658 - val_accuracy: 0.9243
Epoch 132/150
819/852 [===========================&gt;..] - ETA: 0s - loss: 0.1957 - accuracy: 0.9104
Epoch 132: val_loss did not improve from 0.15832
852/852 [==============================] - 2s 2ms/step - loss: 0.1963 - accuracy: 0.9102 - val_loss: 0.1805 - val_accuracy: 0.9144
Epoch 133/150
820/852 [===========================&gt;..] - ETA: 0s - loss: 0.1755 - accuracy: 0.9133
Epoch 133: val_loss did not improve from 0.15832
852/852 [==============================] - 2s 2ms/step - loss: 0.1770 - accuracy: 0.9125 - val_loss: 0.1718 - val_accuracy: 0.9225
Epoch 134/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.1903 - accuracy: 0.9110
Epoch 134: val_loss did not improve from 0.15832
852/852 [==============================] - 2s 2ms/step - loss: 0.1891 - accuracy: 0.9109 - val_loss: 0.1731 - val_accuracy: 0.9164
Epoch 135/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.1887 - accuracy: 0.9134
Epoch 135: val_loss did not improve from 0.15832
852/852 [==============================] - 2s 2ms/step - loss: 0.1885 - accuracy: 0.9132 - val_loss: 0.1726 - val_accuracy: 0.9194
Epoch 136/150
818/852 [===========================&gt;..] - ETA: 0s - loss: 0.1815 - accuracy: 0.9144
Epoch 136: val_loss did not improve from 0.15832
852/852 [==============================] - 2s 2ms/step - loss: 0.1817 - accuracy: 0.9143 - val_loss: 0.1687 - val_accuracy: 0.9172
Epoch 137/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.1915 - accuracy: 0.9119
Epoch 137: val_loss did not improve from 0.15832
852/852 [==============================] - 2s 2ms/step - loss: 0.1909 - accuracy: 0.9123 - val_loss: 0.1674 - val_accuracy: 0.9225
Epoch 138/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.1836 - accuracy: 0.9113
Epoch 138: val_loss did not improve from 0.15832
852/852 [==============================] - 2s 2ms/step - loss: 0.1846 - accuracy: 0.9103 - val_loss: 0.1718 - val_accuracy: 0.9209
Epoch 139/150
852/852 [==============================] - ETA: 0s - loss: 0.1807 - accuracy: 0.9130
Epoch 139: val_loss did not improve from 0.15832
852/852 [==============================] - 2s 3ms/step - loss: 0.1807 - accuracy: 0.9130 - val_loss: 0.1751 - val_accuracy: 0.9166
Epoch 140/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.1748 - accuracy: 0.9145
Epoch 140: val_loss did not improve from 0.15832
852/852 [==============================] - 2s 2ms/step - loss: 0.1749 - accuracy: 0.9146 - val_loss: 0.1659 - val_accuracy: 0.9211
Epoch 141/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.2023 - accuracy: 0.9095
Epoch 141: val_loss did not improve from 0.15832
852/852 [==============================] - 2s 2ms/step - loss: 0.2021 - accuracy: 0.9096 - val_loss: 0.1669 - val_accuracy: 0.9209
Epoch 142/150
818/852 [===========================&gt;..] - ETA: 0s - loss: 0.1729 - accuracy: 0.9171
Epoch 142: val_loss did not improve from 0.15832
852/852 [==============================] - 2s 2ms/step - loss: 0.1750 - accuracy: 0.9159 - val_loss: 0.1785 - val_accuracy: 0.9170
Epoch 143/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.1833 - accuracy: 0.9132
Epoch 143: val_loss did not improve from 0.15832
852/852 [==============================] - 2s 2ms/step - loss: 0.1830 - accuracy: 0.9129 - val_loss: 0.1718 - val_accuracy: 0.9182
Epoch 144/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.1817 - accuracy: 0.9142
Epoch 144: val_loss improved from 0.15832 to 0.15246, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.1817 - accuracy: 0.9139 - val_loss: 0.1525 - val_accuracy: 0.9279
Epoch 145/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.1699 - accuracy: 0.9169
Epoch 145: val_loss improved from 0.15246 to 0.14642, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.1683 - accuracy: 0.9180 - val_loss: 0.1464 - val_accuracy: 0.9284
Epoch 146/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.1693 - accuracy: 0.9201
Epoch 146: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.1690 - accuracy: 0.9205 - val_loss: 0.1667 - val_accuracy: 0.9198
Epoch 147/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.1890 - accuracy: 0.9132
Epoch 147: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.1889 - accuracy: 0.9129 - val_loss: 0.1579 - val_accuracy: 0.9257
Epoch 148/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.1778 - accuracy: 0.9165
Epoch 148: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.1771 - accuracy: 0.9170 - val_loss: 0.1631 - val_accuracy: 0.9240
Epoch 149/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.1814 - accuracy: 0.9136
Epoch 149: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 3ms/step - loss: 0.1809 - accuracy: 0.9138 - val_loss: 0.1601 - val_accuracy: 0.9247
Epoch 150/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.1792 - accuracy: 0.9133
Epoch 150: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 3ms/step - loss: 0.1792 - accuracy: 0.9135 - val_loss: 0.1730 - val_accuracy: 0.9187
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=276f1399-534d-437f-896c-1178d48ba7dc">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [92]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">history_497</span> <span class="o">=</span> <span class="n">four_ninetyseven_neuron</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">callback_a</span><span class="p">,</span> <span class="n">callback_b</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.1818 - accuracy: 0.9134
Epoch 1: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.1816 - accuracy: 0.9137 - val_loss: 0.1564 - val_accuracy: 0.9253
Epoch 2/150
814/852 [===========================&gt;..] - ETA: 0s - loss: 0.1690 - accuracy: 0.9201
Epoch 2: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.1707 - accuracy: 0.9182 - val_loss: 0.1505 - val_accuracy: 0.9307
Epoch 3/150
811/852 [===========================&gt;..] - ETA: 0s - loss: 0.1665 - accuracy: 0.9192
Epoch 3: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.1693 - accuracy: 0.9178 - val_loss: 0.1581 - val_accuracy: 0.9268
Epoch 4/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.1730 - accuracy: 0.9144
Epoch 4: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.1728 - accuracy: 0.9144 - val_loss: 0.2013 - val_accuracy: 0.9072
Epoch 5/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.1842 - accuracy: 0.9146
Epoch 5: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.1842 - accuracy: 0.9146 - val_loss: 0.1636 - val_accuracy: 0.9246
Epoch 6/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.1756 - accuracy: 0.9175
Epoch 6: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.1765 - accuracy: 0.9172 - val_loss: 0.1568 - val_accuracy: 0.9288
Epoch 7/150
819/852 [===========================&gt;..] - ETA: 0s - loss: 0.1675 - accuracy: 0.9205
Epoch 7: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.1676 - accuracy: 0.9209 - val_loss: 0.1495 - val_accuracy: 0.9300
Epoch 8/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.1810 - accuracy: 0.9127
Epoch 8: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.1817 - accuracy: 0.9118 - val_loss: 0.1572 - val_accuracy: 0.9300
Epoch 9/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.1777 - accuracy: 0.9170
Epoch 9: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.1777 - accuracy: 0.9167 - val_loss: 0.1631 - val_accuracy: 0.9245
Epoch 10/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.1705 - accuracy: 0.9179
Epoch 10: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.1707 - accuracy: 0.9177 - val_loss: 0.1645 - val_accuracy: 0.9259
Epoch 11/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.1808 - accuracy: 0.9162
Epoch 11: val_loss did not improve from 0.14642
852/852 [==============================] - 2s 2ms/step - loss: 0.1808 - accuracy: 0.9162 - val_loss: 0.1569 - val_accuracy: 0.9294
Epoch 12/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.1728 - accuracy: 0.9192
Epoch 12: val_loss improved from 0.14642 to 0.13964, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.1726 - accuracy: 0.9192 - val_loss: 0.1396 - val_accuracy: 0.9334
Epoch 13/150
852/852 [==============================] - ETA: 0s - loss: 0.1697 - accuracy: 0.9217
Epoch 13: val_loss did not improve from 0.13964
852/852 [==============================] - 2s 2ms/step - loss: 0.1697 - accuracy: 0.9217 - val_loss: 0.1560 - val_accuracy: 0.9311
Epoch 14/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.1719 - accuracy: 0.9192
Epoch 14: val_loss did not improve from 0.13964
852/852 [==============================] - 2s 2ms/step - loss: 0.1721 - accuracy: 0.9193 - val_loss: 0.1472 - val_accuracy: 0.9322
Epoch 15/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.1733 - accuracy: 0.9173
Epoch 15: val_loss did not improve from 0.13964
852/852 [==============================] - 2s 2ms/step - loss: 0.1728 - accuracy: 0.9177 - val_loss: 0.1572 - val_accuracy: 0.9266
Epoch 16/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.1797 - accuracy: 0.9159
Epoch 16: val_loss did not improve from 0.13964
852/852 [==============================] - 2s 2ms/step - loss: 0.1807 - accuracy: 0.9153 - val_loss: 0.1714 - val_accuracy: 0.9200
Epoch 17/150
812/852 [===========================&gt;..] - ETA: 0s - loss: 0.1776 - accuracy: 0.9133
Epoch 17: val_loss did not improve from 0.13964
852/852 [==============================] - 2s 2ms/step - loss: 0.1765 - accuracy: 0.9138 - val_loss: 0.1585 - val_accuracy: 0.9246
Epoch 18/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.1661 - accuracy: 0.9201
Epoch 18: val_loss did not improve from 0.13964
852/852 [==============================] - 2s 2ms/step - loss: 0.1664 - accuracy: 0.9205 - val_loss: 0.1565 - val_accuracy: 0.9251
Epoch 19/150
813/852 [===========================&gt;..] - ETA: 0s - loss: 0.1683 - accuracy: 0.9197
Epoch 19: val_loss did not improve from 0.13964
852/852 [==============================] - 2s 2ms/step - loss: 0.1693 - accuracy: 0.9191 - val_loss: 0.1578 - val_accuracy: 0.9280
Epoch 20/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.1666 - accuracy: 0.9195
Epoch 20: val_loss did not improve from 0.13964
852/852 [==============================] - 2s 2ms/step - loss: 0.1660 - accuracy: 0.9194 - val_loss: 0.1708 - val_accuracy: 0.9219
Epoch 21/150
809/852 [===========================&gt;..] - ETA: 0s - loss: 0.1708 - accuracy: 0.9163
Epoch 21: val_loss did not improve from 0.13964
852/852 [==============================] - 2s 2ms/step - loss: 0.1742 - accuracy: 0.9158 - val_loss: 0.1689 - val_accuracy: 0.9258
Epoch 22/150
811/852 [===========================&gt;..] - ETA: 0s - loss: 0.1769 - accuracy: 0.9180
Epoch 22: val_loss did not improve from 0.13964
852/852 [==============================] - 2s 2ms/step - loss: 0.1760 - accuracy: 0.9184 - val_loss: 0.1560 - val_accuracy: 0.9283
Epoch 23/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.1572 - accuracy: 0.9239
Epoch 23: val_loss did not improve from 0.13964
852/852 [==============================] - 2s 2ms/step - loss: 0.1582 - accuracy: 0.9236 - val_loss: 0.1690 - val_accuracy: 0.9211
Epoch 24/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.1810 - accuracy: 0.9149
Epoch 24: val_loss did not improve from 0.13964
852/852 [==============================] - 2s 2ms/step - loss: 0.1812 - accuracy: 0.9146 - val_loss: 0.1527 - val_accuracy: 0.9287
Epoch 25/150
852/852 [==============================] - ETA: 0s - loss: 0.1691 - accuracy: 0.9207
Epoch 25: val_loss did not improve from 0.13964
852/852 [==============================] - 2s 2ms/step - loss: 0.1691 - accuracy: 0.9207 - val_loss: 0.1504 - val_accuracy: 0.9285
Epoch 26/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.1636 - accuracy: 0.9255
Epoch 26: val_loss did not improve from 0.13964
852/852 [==============================] - 2s 2ms/step - loss: 0.1650 - accuracy: 0.9246 - val_loss: 0.1733 - val_accuracy: 0.9237
Epoch 27/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.1765 - accuracy: 0.9175
Epoch 27: val_loss did not improve from 0.13964
852/852 [==============================] - 2s 2ms/step - loss: 0.1771 - accuracy: 0.9172 - val_loss: 0.1656 - val_accuracy: 0.9240
Epoch 28/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.1710 - accuracy: 0.9204
Epoch 28: val_loss did not improve from 0.13964
852/852 [==============================] - 2s 2ms/step - loss: 0.1724 - accuracy: 0.9191 - val_loss: 0.1522 - val_accuracy: 0.9297
Epoch 29/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.1606 - accuracy: 0.9277
Epoch 29: val_loss did not improve from 0.13964
852/852 [==============================] - 2s 2ms/step - loss: 0.1616 - accuracy: 0.9271 - val_loss: 0.1854 - val_accuracy: 0.9165
Epoch 30/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.1844 - accuracy: 0.9180
Epoch 30: val_loss did not improve from 0.13964
852/852 [==============================] - 2s 2ms/step - loss: 0.1890 - accuracy: 0.9183 - val_loss: 0.1537 - val_accuracy: 0.9298
Epoch 31/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.1673 - accuracy: 0.9214
Epoch 31: val_loss did not improve from 0.13964
852/852 [==============================] - 2s 2ms/step - loss: 0.1678 - accuracy: 0.9213 - val_loss: 0.1408 - val_accuracy: 0.9335
Epoch 32/150
815/852 [===========================&gt;..] - ETA: 0s - loss: 0.1700 - accuracy: 0.9209
Epoch 32: val_loss did not improve from 0.13964
852/852 [==============================] - 2s 2ms/step - loss: 0.1701 - accuracy: 0.9202 - val_loss: 0.1438 - val_accuracy: 0.9318
Epoch 32: early stopping
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=f623e4c9-0627-43a8-8d4c-abac59af0310">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [97]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">history_497</span> <span class="o">=</span> <span class="n">four_ninetyseven_neuron</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">callback_a</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/150
810/852 [===========================&gt;..] - ETA: 0s - loss: 0.1720 - accuracy: 0.9180
Epoch 1: val_loss did not improve from 0.13964
852/852 [==============================] - 2s 2ms/step - loss: 0.1737 - accuracy: 0.9169 - val_loss: 0.1569 - val_accuracy: 0.9267
Epoch 2/150
852/852 [==============================] - ETA: 0s - loss: 0.1735 - accuracy: 0.9183
Epoch 2: val_loss did not improve from 0.13964
852/852 [==============================] - 2s 2ms/step - loss: 0.1735 - accuracy: 0.9183 - val_loss: 0.1451 - val_accuracy: 0.9326
Epoch 3/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.1687 - accuracy: 0.9232
Epoch 3: val_loss did not improve from 0.13964
852/852 [==============================] - 2s 2ms/step - loss: 0.1692 - accuracy: 0.9229 - val_loss: 0.1802 - val_accuracy: 0.9185
Epoch 4/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.1799 - accuracy: 0.9178
Epoch 4: val_loss did not improve from 0.13964
852/852 [==============================] - 2s 2ms/step - loss: 0.1791 - accuracy: 0.9180 - val_loss: 0.1574 - val_accuracy: 0.9270
Epoch 5/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.1739 - accuracy: 0.9157
Epoch 5: val_loss did not improve from 0.13964
852/852 [==============================] - 2s 2ms/step - loss: 0.1735 - accuracy: 0.9157 - val_loss: 0.1524 - val_accuracy: 0.9313
Epoch 6/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.1754 - accuracy: 0.9191
Epoch 6: val_loss did not improve from 0.13964
852/852 [==============================] - 2s 2ms/step - loss: 0.1753 - accuracy: 0.9190 - val_loss: 0.1910 - val_accuracy: 0.9145
Epoch 7/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.1742 - accuracy: 0.9177
Epoch 7: val_loss did not improve from 0.13964
852/852 [==============================] - 2s 2ms/step - loss: 0.1746 - accuracy: 0.9174 - val_loss: 0.1518 - val_accuracy: 0.9312
Epoch 8/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.1704 - accuracy: 0.9204
Epoch 8: val_loss did not improve from 0.13964
852/852 [==============================] - 2s 2ms/step - loss: 0.1702 - accuracy: 0.9203 - val_loss: 0.1533 - val_accuracy: 0.9272
Epoch 9/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.1666 - accuracy: 0.9213
Epoch 9: val_loss did not improve from 0.13964
852/852 [==============================] - 2s 2ms/step - loss: 0.1659 - accuracy: 0.9216 - val_loss: 0.1625 - val_accuracy: 0.9327
Epoch 10/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.1666 - accuracy: 0.9219
Epoch 10: val_loss did not improve from 0.13964
852/852 [==============================] - 2s 2ms/step - loss: 0.1674 - accuracy: 0.9213 - val_loss: 0.1605 - val_accuracy: 0.9253
Epoch 11/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.1754 - accuracy: 0.9147
Epoch 11: val_loss did not improve from 0.13964
852/852 [==============================] - 2s 2ms/step - loss: 0.1749 - accuracy: 0.9147 - val_loss: 0.1793 - val_accuracy: 0.9192
Epoch 12/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.1731 - accuracy: 0.9184
Epoch 12: val_loss improved from 0.13964 to 0.13758, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.1729 - accuracy: 0.9189 - val_loss: 0.1376 - val_accuracy: 0.9349
Epoch 13/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.1580 - accuracy: 0.9242
Epoch 13: val_loss did not improve from 0.13758
852/852 [==============================] - 2s 2ms/step - loss: 0.1580 - accuracy: 0.9237 - val_loss: 0.1445 - val_accuracy: 0.9307
Epoch 14/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.1635 - accuracy: 0.9210
Epoch 14: val_loss did not improve from 0.13758
852/852 [==============================] - 2s 2ms/step - loss: 0.1652 - accuracy: 0.9207 - val_loss: 0.1576 - val_accuracy: 0.9270
Epoch 15/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.1713 - accuracy: 0.9199
Epoch 15: val_loss did not improve from 0.13758
852/852 [==============================] - 2s 2ms/step - loss: 0.1713 - accuracy: 0.9202 - val_loss: 0.1628 - val_accuracy: 0.9252
Epoch 16/150
820/852 [===========================&gt;..] - ETA: 0s - loss: 0.1766 - accuracy: 0.9196
Epoch 16: val_loss did not improve from 0.13758
852/852 [==============================] - 2s 2ms/step - loss: 0.1758 - accuracy: 0.9198 - val_loss: 0.1495 - val_accuracy: 0.9290
Epoch 17/150
852/852 [==============================] - ETA: 0s - loss: 0.1629 - accuracy: 0.9251
Epoch 17: val_loss did not improve from 0.13758
852/852 [==============================] - 2s 2ms/step - loss: 0.1629 - accuracy: 0.9251 - val_loss: 0.1445 - val_accuracy: 0.9333
Epoch 18/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.1566 - accuracy: 0.9235
Epoch 18: val_loss did not improve from 0.13758
852/852 [==============================] - 2s 2ms/step - loss: 0.1576 - accuracy: 0.9238 - val_loss: 0.2457 - val_accuracy: 0.9166
Epoch 19/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.1831 - accuracy: 0.9155
Epoch 19: val_loss did not improve from 0.13758
852/852 [==============================] - 2s 2ms/step - loss: 0.1835 - accuracy: 0.9151 - val_loss: 0.1457 - val_accuracy: 0.9317
Epoch 20/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.1681 - accuracy: 0.9197
Epoch 20: val_loss did not improve from 0.13758
852/852 [==============================] - 2s 2ms/step - loss: 0.1701 - accuracy: 0.9186 - val_loss: 0.1766 - val_accuracy: 0.9199
Epoch 21/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.1613 - accuracy: 0.9228
Epoch 21: val_loss did not improve from 0.13758
852/852 [==============================] - 2s 2ms/step - loss: 0.1613 - accuracy: 0.9230 - val_loss: 0.1548 - val_accuracy: 0.9263
Epoch 22/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.1709 - accuracy: 0.9221
Epoch 22: val_loss did not improve from 0.13758
852/852 [==============================] - 2s 2ms/step - loss: 0.1711 - accuracy: 0.9220 - val_loss: 0.1556 - val_accuracy: 0.9265
Epoch 23/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.1726 - accuracy: 0.9183
Epoch 23: val_loss did not improve from 0.13758
852/852 [==============================] - 2s 2ms/step - loss: 0.1725 - accuracy: 0.9183 - val_loss: 0.1531 - val_accuracy: 0.9263
Epoch 24/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.1595 - accuracy: 0.9214
Epoch 24: val_loss improved from 0.13758 to 0.13463, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.1603 - accuracy: 0.9210 - val_loss: 0.1346 - val_accuracy: 0.9367
Epoch 25/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.1618 - accuracy: 0.9213
Epoch 25: val_loss did not improve from 0.13463
852/852 [==============================] - 2s 2ms/step - loss: 0.1617 - accuracy: 0.9214 - val_loss: 0.1578 - val_accuracy: 0.9297
Epoch 26/150
820/852 [===========================&gt;..] - ETA: 0s - loss: 0.1652 - accuracy: 0.9238
Epoch 26: val_loss did not improve from 0.13463
852/852 [==============================] - 2s 2ms/step - loss: 0.1670 - accuracy: 0.9227 - val_loss: 0.1461 - val_accuracy: 0.9295
Epoch 27/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.1604 - accuracy: 0.9227
Epoch 27: val_loss did not improve from 0.13463
852/852 [==============================] - 2s 2ms/step - loss: 0.1604 - accuracy: 0.9227 - val_loss: 0.1484 - val_accuracy: 0.9278
Epoch 28/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.1696 - accuracy: 0.9208
Epoch 28: val_loss did not improve from 0.13463
852/852 [==============================] - 2s 2ms/step - loss: 0.1695 - accuracy: 0.9209 - val_loss: 0.1462 - val_accuracy: 0.9342
Epoch 29/150
813/852 [===========================&gt;..] - ETA: 0s - loss: 0.1584 - accuracy: 0.9269
Epoch 29: val_loss did not improve from 0.13463
852/852 [==============================] - 2s 2ms/step - loss: 0.1600 - accuracy: 0.9266 - val_loss: 0.1490 - val_accuracy: 0.9301
Epoch 30/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.1608 - accuracy: 0.9242
Epoch 30: val_loss did not improve from 0.13463
852/852 [==============================] - 2s 2ms/step - loss: 0.1602 - accuracy: 0.9244 - val_loss: 0.1393 - val_accuracy: 0.9331
Epoch 31/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.1635 - accuracy: 0.9207
Epoch 31: val_loss did not improve from 0.13463
852/852 [==============================] - 2s 2ms/step - loss: 0.1634 - accuracy: 0.9211 - val_loss: 0.1601 - val_accuracy: 0.9274
Epoch 32/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.1737 - accuracy: 0.9181
Epoch 32: val_loss did not improve from 0.13463
852/852 [==============================] - 2s 2ms/step - loss: 0.1734 - accuracy: 0.9179 - val_loss: 0.1401 - val_accuracy: 0.9317
Epoch 33/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.1560 - accuracy: 0.9248
Epoch 33: val_loss did not improve from 0.13463
852/852 [==============================] - 2s 2ms/step - loss: 0.1560 - accuracy: 0.9250 - val_loss: 0.1591 - val_accuracy: 0.9280
Epoch 34/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.1664 - accuracy: 0.9225
Epoch 34: val_loss improved from 0.13463 to 0.12908, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.1658 - accuracy: 0.9224 - val_loss: 0.1291 - val_accuracy: 0.9394
Epoch 35/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.1627 - accuracy: 0.9270
Epoch 35: val_loss did not improve from 0.12908
852/852 [==============================] - 2s 2ms/step - loss: 0.1629 - accuracy: 0.9268 - val_loss: 0.1513 - val_accuracy: 0.9314
Epoch 36/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.1568 - accuracy: 0.9244
Epoch 36: val_loss did not improve from 0.12908
852/852 [==============================] - 2s 2ms/step - loss: 0.1571 - accuracy: 0.9240 - val_loss: 0.1464 - val_accuracy: 0.9342
Epoch 37/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.1628 - accuracy: 0.9235
Epoch 37: val_loss did not improve from 0.12908
852/852 [==============================] - 2s 2ms/step - loss: 0.1626 - accuracy: 0.9233 - val_loss: 0.1352 - val_accuracy: 0.9388
Epoch 38/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.1668 - accuracy: 0.9226
Epoch 38: val_loss did not improve from 0.12908
852/852 [==============================] - 2s 2ms/step - loss: 0.1668 - accuracy: 0.9226 - val_loss: 0.1429 - val_accuracy: 0.9313
Epoch 39/150
814/852 [===========================&gt;..] - ETA: 0s - loss: 0.1568 - accuracy: 0.9233
Epoch 39: val_loss did not improve from 0.12908
852/852 [==============================] - 2s 2ms/step - loss: 0.1557 - accuracy: 0.9240 - val_loss: 0.1311 - val_accuracy: 0.9371
Epoch 40/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.1564 - accuracy: 0.9282
Epoch 40: val_loss did not improve from 0.12908
852/852 [==============================] - 2s 2ms/step - loss: 0.1564 - accuracy: 0.9286 - val_loss: 0.1543 - val_accuracy: 0.9293
Epoch 41/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.1652 - accuracy: 0.9226
Epoch 41: val_loss did not improve from 0.12908
852/852 [==============================] - 2s 2ms/step - loss: 0.1664 - accuracy: 0.9224 - val_loss: 0.1534 - val_accuracy: 0.9294
Epoch 42/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.1640 - accuracy: 0.9264
Epoch 42: val_loss did not improve from 0.12908
852/852 [==============================] - 2s 2ms/step - loss: 0.1647 - accuracy: 0.9263 - val_loss: 0.1336 - val_accuracy: 0.9365
Epoch 43/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.1522 - accuracy: 0.9252
Epoch 43: val_loss did not improve from 0.12908
852/852 [==============================] - 2s 2ms/step - loss: 0.1525 - accuracy: 0.9248 - val_loss: 0.1299 - val_accuracy: 0.9400
Epoch 44/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.1566 - accuracy: 0.9266
Epoch 44: val_loss did not improve from 0.12908
852/852 [==============================] - 2s 2ms/step - loss: 0.1588 - accuracy: 0.9261 - val_loss: 0.1637 - val_accuracy: 0.9251
Epoch 45/150
819/852 [===========================&gt;..] - ETA: 0s - loss: 0.1674 - accuracy: 0.9248
Epoch 45: val_loss did not improve from 0.12908
852/852 [==============================] - 2s 2ms/step - loss: 0.1667 - accuracy: 0.9246 - val_loss: 0.1445 - val_accuracy: 0.9320
Epoch 46/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.1644 - accuracy: 0.9223
Epoch 46: val_loss did not improve from 0.12908
852/852 [==============================] - 2s 2ms/step - loss: 0.1655 - accuracy: 0.9207 - val_loss: 0.1740 - val_accuracy: 0.9220
Epoch 47/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.1601 - accuracy: 0.9243
Epoch 47: val_loss improved from 0.12908 to 0.12821, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.1591 - accuracy: 0.9248 - val_loss: 0.1282 - val_accuracy: 0.9381
Epoch 48/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.1601 - accuracy: 0.9241
Epoch 48: val_loss did not improve from 0.12821
852/852 [==============================] - 2s 2ms/step - loss: 0.1614 - accuracy: 0.9238 - val_loss: 0.1550 - val_accuracy: 0.9319
Epoch 49/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.1496 - accuracy: 0.9278
Epoch 49: val_loss did not improve from 0.12821
852/852 [==============================] - 2s 2ms/step - loss: 0.1502 - accuracy: 0.9277 - val_loss: 0.1336 - val_accuracy: 0.9371
Epoch 50/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.1622 - accuracy: 0.9252
Epoch 50: val_loss did not improve from 0.12821
852/852 [==============================] - 2s 2ms/step - loss: 0.1624 - accuracy: 0.9253 - val_loss: 0.1716 - val_accuracy: 0.9231
Epoch 51/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.1584 - accuracy: 0.9232
Epoch 51: val_loss did not improve from 0.12821
852/852 [==============================] - 2s 2ms/step - loss: 0.1582 - accuracy: 0.9230 - val_loss: 0.1388 - val_accuracy: 0.9354
Epoch 52/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.1556 - accuracy: 0.9305
Epoch 52: val_loss did not improve from 0.12821
852/852 [==============================] - 2s 2ms/step - loss: 0.1560 - accuracy: 0.9305 - val_loss: 0.1579 - val_accuracy: 0.9260
Epoch 53/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.1503 - accuracy: 0.9256
Epoch 53: val_loss did not improve from 0.12821
852/852 [==============================] - 2s 2ms/step - loss: 0.1515 - accuracy: 0.9246 - val_loss: 0.1476 - val_accuracy: 0.9346
Epoch 54/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.1565 - accuracy: 0.9274
Epoch 54: val_loss did not improve from 0.12821
852/852 [==============================] - 2s 2ms/step - loss: 0.1571 - accuracy: 0.9272 - val_loss: 0.1454 - val_accuracy: 0.9333
Epoch 55/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.1671 - accuracy: 0.9251
Epoch 55: val_loss did not improve from 0.12821
852/852 [==============================] - 2s 2ms/step - loss: 0.1677 - accuracy: 0.9251 - val_loss: 0.1508 - val_accuracy: 0.9334
Epoch 56/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.1537 - accuracy: 0.9282
Epoch 56: val_loss improved from 0.12821 to 0.12760, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.1543 - accuracy: 0.9280 - val_loss: 0.1276 - val_accuracy: 0.9398
Epoch 57/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.1427 - accuracy: 0.9286
Epoch 57: val_loss did not improve from 0.12760
852/852 [==============================] - 2s 2ms/step - loss: 0.1422 - accuracy: 0.9292 - val_loss: 0.1332 - val_accuracy: 0.9375
Epoch 58/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.1549 - accuracy: 0.9287
Epoch 58: val_loss did not improve from 0.12760
852/852 [==============================] - 2s 2ms/step - loss: 0.1551 - accuracy: 0.9281 - val_loss: 0.1459 - val_accuracy: 0.9317
Epoch 59/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.1624 - accuracy: 0.9267
Epoch 59: val_loss did not improve from 0.12760
852/852 [==============================] - 2s 2ms/step - loss: 0.1625 - accuracy: 0.9267 - val_loss: 0.2168 - val_accuracy: 0.9153
Epoch 60/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.1552 - accuracy: 0.9273
Epoch 60: val_loss did not improve from 0.12760
852/852 [==============================] - 2s 2ms/step - loss: 0.1549 - accuracy: 0.9280 - val_loss: 0.1334 - val_accuracy: 0.9389
Epoch 61/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.1577 - accuracy: 0.9253
Epoch 61: val_loss did not improve from 0.12760
852/852 [==============================] - 2s 2ms/step - loss: 0.1580 - accuracy: 0.9254 - val_loss: 0.1320 - val_accuracy: 0.9406
Epoch 62/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.1504 - accuracy: 0.9283
Epoch 62: val_loss did not improve from 0.12760
852/852 [==============================] - 2s 2ms/step - loss: 0.1502 - accuracy: 0.9284 - val_loss: 0.1380 - val_accuracy: 0.9362
Epoch 63/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.1494 - accuracy: 0.9280
Epoch 63: val_loss did not improve from 0.12760
852/852 [==============================] - 2s 2ms/step - loss: 0.1491 - accuracy: 0.9281 - val_loss: 0.1287 - val_accuracy: 0.9426
Epoch 64/150
812/852 [===========================&gt;..] - ETA: 0s - loss: 0.1484 - accuracy: 0.9329
Epoch 64: val_loss did not improve from 0.12760
852/852 [==============================] - 2s 2ms/step - loss: 0.1487 - accuracy: 0.9322 - val_loss: 0.1372 - val_accuracy: 0.9349
Epoch 65/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.1488 - accuracy: 0.9267
Epoch 65: val_loss improved from 0.12760 to 0.12671, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.1487 - accuracy: 0.9270 - val_loss: 0.1267 - val_accuracy: 0.9415
Epoch 66/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.1578 - accuracy: 0.9253
Epoch 66: val_loss did not improve from 0.12671
852/852 [==============================] - 2s 2ms/step - loss: 0.1582 - accuracy: 0.9246 - val_loss: 0.1362 - val_accuracy: 0.9349
Epoch 67/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.1688 - accuracy: 0.9246
Epoch 67: val_loss did not improve from 0.12671
852/852 [==============================] - 2s 2ms/step - loss: 0.1688 - accuracy: 0.9245 - val_loss: 0.1644 - val_accuracy: 0.9265
Epoch 68/150
819/852 [===========================&gt;..] - ETA: 0s - loss: 0.1484 - accuracy: 0.9309
Epoch 68: val_loss did not improve from 0.12671
852/852 [==============================] - 2s 2ms/step - loss: 0.1490 - accuracy: 0.9302 - val_loss: 0.1293 - val_accuracy: 0.9398
Epoch 69/150
812/852 [===========================&gt;..] - ETA: 0s - loss: 0.1430 - accuracy: 0.9294
Epoch 69: val_loss did not improve from 0.12671
852/852 [==============================] - 2s 2ms/step - loss: 0.1431 - accuracy: 0.9291 - val_loss: 0.1466 - val_accuracy: 0.9320
Epoch 70/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.1514 - accuracy: 0.9315
Epoch 70: val_loss did not improve from 0.12671
852/852 [==============================] - 2s 2ms/step - loss: 0.1519 - accuracy: 0.9310 - val_loss: 0.1401 - val_accuracy: 0.9344
Epoch 71/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.1522 - accuracy: 0.9278
Epoch 71: val_loss did not improve from 0.12671
852/852 [==============================] - 2s 2ms/step - loss: 0.1531 - accuracy: 0.9274 - val_loss: 0.1460 - val_accuracy: 0.9338
Epoch 72/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.1474 - accuracy: 0.9305
Epoch 72: val_loss did not improve from 0.12671
852/852 [==============================] - 2s 2ms/step - loss: 0.1469 - accuracy: 0.9310 - val_loss: 0.1424 - val_accuracy: 0.9320
Epoch 73/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.1453 - accuracy: 0.9268
Epoch 73: val_loss improved from 0.12671 to 0.12393, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.1455 - accuracy: 0.9273 - val_loss: 0.1239 - val_accuracy: 0.9421
Epoch 74/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.1477 - accuracy: 0.9287
Epoch 74: val_loss did not improve from 0.12393
852/852 [==============================] - 2s 2ms/step - loss: 0.1482 - accuracy: 0.9284 - val_loss: 0.1526 - val_accuracy: 0.9345
Epoch 75/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.1626 - accuracy: 0.9264
Epoch 75: val_loss did not improve from 0.12393
852/852 [==============================] - 2s 2ms/step - loss: 0.1623 - accuracy: 0.9264 - val_loss: 0.1790 - val_accuracy: 0.9150
Epoch 76/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.1568 - accuracy: 0.9271
Epoch 76: val_loss did not improve from 0.12393
852/852 [==============================] - 2s 2ms/step - loss: 0.1574 - accuracy: 0.9271 - val_loss: 0.1309 - val_accuracy: 0.9402
Epoch 77/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.1445 - accuracy: 0.9319
Epoch 77: val_loss did not improve from 0.12393
852/852 [==============================] - 2s 2ms/step - loss: 0.1446 - accuracy: 0.9319 - val_loss: 0.1467 - val_accuracy: 0.9327
Epoch 78/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.1617 - accuracy: 0.9289
Epoch 78: val_loss improved from 0.12393 to 0.11982, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.1616 - accuracy: 0.9287 - val_loss: 0.1198 - val_accuracy: 0.9430
Epoch 79/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.1415 - accuracy: 0.9332
Epoch 79: val_loss did not improve from 0.11982
852/852 [==============================] - 2s 2ms/step - loss: 0.1420 - accuracy: 0.9331 - val_loss: 0.1233 - val_accuracy: 0.9414
Epoch 80/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.1546 - accuracy: 0.9286
Epoch 80: val_loss did not improve from 0.11982
852/852 [==============================] - 2s 2ms/step - loss: 0.1549 - accuracy: 0.9283 - val_loss: 0.1345 - val_accuracy: 0.9357
Epoch 81/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.1432 - accuracy: 0.9319
Epoch 81: val_loss did not improve from 0.11982
852/852 [==============================] - 2s 2ms/step - loss: 0.1445 - accuracy: 0.9311 - val_loss: 0.1229 - val_accuracy: 0.9422
Epoch 82/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.1497 - accuracy: 0.9308
Epoch 82: val_loss did not improve from 0.11982
852/852 [==============================] - 2s 2ms/step - loss: 0.1510 - accuracy: 0.9299 - val_loss: 0.1385 - val_accuracy: 0.9381
Epoch 83/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.1543 - accuracy: 0.9287
Epoch 83: val_loss did not improve from 0.11982
852/852 [==============================] - 2s 2ms/step - loss: 0.1542 - accuracy: 0.9286 - val_loss: 0.1284 - val_accuracy: 0.9434
Epoch 84/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.1346 - accuracy: 0.9361
Epoch 84: val_loss did not improve from 0.11982
852/852 [==============================] - 2s 2ms/step - loss: 0.1355 - accuracy: 0.9355 - val_loss: 0.1289 - val_accuracy: 0.9403
Epoch 85/150
813/852 [===========================&gt;..] - ETA: 0s - loss: 0.1541 - accuracy: 0.9304
Epoch 85: val_loss did not improve from 0.11982
852/852 [==============================] - 2s 2ms/step - loss: 0.1561 - accuracy: 0.9288 - val_loss: 0.1435 - val_accuracy: 0.9348
Epoch 86/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.1481 - accuracy: 0.9291
Epoch 86: val_loss did not improve from 0.11982
852/852 [==============================] - 2s 2ms/step - loss: 0.1490 - accuracy: 0.9285 - val_loss: 0.1349 - val_accuracy: 0.9374
Epoch 87/150
815/852 [===========================&gt;..] - ETA: 0s - loss: 0.1586 - accuracy: 0.9259
Epoch 87: val_loss did not improve from 0.11982
852/852 [==============================] - 2s 2ms/step - loss: 0.1598 - accuracy: 0.9259 - val_loss: 0.1349 - val_accuracy: 0.9364
Epoch 88/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.1383 - accuracy: 0.9318
Epoch 88: val_loss did not improve from 0.11982
852/852 [==============================] - 2s 2ms/step - loss: 0.1395 - accuracy: 0.9313 - val_loss: 0.1299 - val_accuracy: 0.9402
Epoch 89/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.1452 - accuracy: 0.9321
Epoch 89: val_loss did not improve from 0.11982
852/852 [==============================] - 2s 2ms/step - loss: 0.1465 - accuracy: 0.9319 - val_loss: 0.1460 - val_accuracy: 0.9358
Epoch 90/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.1540 - accuracy: 0.9322
Epoch 90: val_loss did not improve from 0.11982
852/852 [==============================] - 2s 2ms/step - loss: 0.1542 - accuracy: 0.9320 - val_loss: 0.1355 - val_accuracy: 0.9385
Epoch 91/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.1399 - accuracy: 0.9364
Epoch 91: val_loss did not improve from 0.11982
852/852 [==============================] - 2s 2ms/step - loss: 0.1410 - accuracy: 0.9358 - val_loss: 0.1214 - val_accuracy: 0.9429
Epoch 92/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.1413 - accuracy: 0.9330
Epoch 92: val_loss improved from 0.11982 to 0.11773, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.1414 - accuracy: 0.9328 - val_loss: 0.1177 - val_accuracy: 0.9422
Epoch 93/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.1449 - accuracy: 0.9347
Epoch 93: val_loss did not improve from 0.11773
852/852 [==============================] - 2s 2ms/step - loss: 0.1454 - accuracy: 0.9347 - val_loss: 0.1431 - val_accuracy: 0.9412
Epoch 94/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.1509 - accuracy: 0.9325
Epoch 94: val_loss did not improve from 0.11773
852/852 [==============================] - 2s 2ms/step - loss: 0.1511 - accuracy: 0.9324 - val_loss: 0.1327 - val_accuracy: 0.9412
Epoch 95/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.1506 - accuracy: 0.9287
Epoch 95: val_loss did not improve from 0.11773
852/852 [==============================] - 2s 2ms/step - loss: 0.1507 - accuracy: 0.9288 - val_loss: 0.1310 - val_accuracy: 0.9405
Epoch 96/150
819/852 [===========================&gt;..] - ETA: 0s - loss: 0.1312 - accuracy: 0.9343
Epoch 96: val_loss improved from 0.11773 to 0.11604, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.1313 - accuracy: 0.9342 - val_loss: 0.1160 - val_accuracy: 0.9465
Epoch 97/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.1368 - accuracy: 0.9357
Epoch 97: val_loss did not improve from 0.11604
852/852 [==============================] - 2s 2ms/step - loss: 0.1379 - accuracy: 0.9346 - val_loss: 0.1354 - val_accuracy: 0.9388
Epoch 98/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.1521 - accuracy: 0.9295
Epoch 98: val_loss did not improve from 0.11604
852/852 [==============================] - 2s 2ms/step - loss: 0.1542 - accuracy: 0.9284 - val_loss: 0.1491 - val_accuracy: 0.9328
Epoch 99/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.1493 - accuracy: 0.9277
Epoch 99: val_loss did not improve from 0.11604
852/852 [==============================] - 2s 2ms/step - loss: 0.1495 - accuracy: 0.9279 - val_loss: 0.1257 - val_accuracy: 0.9427
Epoch 100/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.1395 - accuracy: 0.9328
Epoch 100: val_loss improved from 0.11604 to 0.11292, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.1401 - accuracy: 0.9324 - val_loss: 0.1129 - val_accuracy: 0.9454
Epoch 101/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.1351 - accuracy: 0.9317
Epoch 101: val_loss did not improve from 0.11292
852/852 [==============================] - 2s 2ms/step - loss: 0.1362 - accuracy: 0.9314 - val_loss: 0.1206 - val_accuracy: 0.9443
Epoch 102/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.1430 - accuracy: 0.9327
Epoch 102: val_loss did not improve from 0.11292
852/852 [==============================] - 2s 2ms/step - loss: 0.1438 - accuracy: 0.9325 - val_loss: 0.1265 - val_accuracy: 0.9406
Epoch 103/150
814/852 [===========================&gt;..] - ETA: 0s - loss: 0.1478 - accuracy: 0.9323
Epoch 103: val_loss did not improve from 0.11292
852/852 [==============================] - 2s 2ms/step - loss: 0.1465 - accuracy: 0.9331 - val_loss: 0.1194 - val_accuracy: 0.9436
Epoch 104/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.1412 - accuracy: 0.9339
Epoch 104: val_loss did not improve from 0.11292
852/852 [==============================] - 2s 2ms/step - loss: 0.1417 - accuracy: 0.9332 - val_loss: 0.1667 - val_accuracy: 0.9257
Epoch 105/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.1472 - accuracy: 0.9339
Epoch 105: val_loss did not improve from 0.11292
852/852 [==============================] - 2s 2ms/step - loss: 0.1478 - accuracy: 0.9338 - val_loss: 0.1673 - val_accuracy: 0.9286
Epoch 106/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.1585 - accuracy: 0.9302
Epoch 106: val_loss did not improve from 0.11292
852/852 [==============================] - 2s 2ms/step - loss: 0.1584 - accuracy: 0.9302 - val_loss: 0.1374 - val_accuracy: 0.9378
Epoch 107/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.1388 - accuracy: 0.9337
Epoch 107: val_loss did not improve from 0.11292
852/852 [==============================] - 2s 2ms/step - loss: 0.1386 - accuracy: 0.9339 - val_loss: 0.1153 - val_accuracy: 0.9462
Epoch 108/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.1395 - accuracy: 0.9356
Epoch 108: val_loss did not improve from 0.11292
852/852 [==============================] - 2s 2ms/step - loss: 0.1408 - accuracy: 0.9351 - val_loss: 0.1379 - val_accuracy: 0.9365
Epoch 109/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.1432 - accuracy: 0.9326
Epoch 109: val_loss did not improve from 0.11292
852/852 [==============================] - 2s 2ms/step - loss: 0.1442 - accuracy: 0.9322 - val_loss: 0.1334 - val_accuracy: 0.9385
Epoch 110/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.1469 - accuracy: 0.9327
Epoch 110: val_loss did not improve from 0.11292
852/852 [==============================] - 2s 2ms/step - loss: 0.1464 - accuracy: 0.9329 - val_loss: 0.1220 - val_accuracy: 0.9416
Epoch 111/150
819/852 [===========================&gt;..] - ETA: 0s - loss: 0.1377 - accuracy: 0.9370
Epoch 111: val_loss did not improve from 0.11292
852/852 [==============================] - 2s 2ms/step - loss: 0.1381 - accuracy: 0.9368 - val_loss: 0.1242 - val_accuracy: 0.9436
Epoch 112/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.1367 - accuracy: 0.9357
Epoch 112: val_loss improved from 0.11292 to 0.11129, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.1370 - accuracy: 0.9358 - val_loss: 0.1113 - val_accuracy: 0.9482
Epoch 113/150
852/852 [==============================] - ETA: 0s - loss: 0.1306 - accuracy: 0.9375
Epoch 113: val_loss did not improve from 0.11129
852/852 [==============================] - 2s 2ms/step - loss: 0.1306 - accuracy: 0.9375 - val_loss: 0.1287 - val_accuracy: 0.9386
Epoch 114/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.1491 - accuracy: 0.9318
Epoch 114: val_loss did not improve from 0.11129
852/852 [==============================] - 2s 2ms/step - loss: 0.1487 - accuracy: 0.9320 - val_loss: 0.1304 - val_accuracy: 0.9394
Epoch 115/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.1415 - accuracy: 0.9356
Epoch 115: val_loss did not improve from 0.11129
852/852 [==============================] - 2s 2ms/step - loss: 0.1414 - accuracy: 0.9358 - val_loss: 0.1287 - val_accuracy: 0.9419
Epoch 116/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.1337 - accuracy: 0.9379
Epoch 116: val_loss did not improve from 0.11129
852/852 [==============================] - 2s 2ms/step - loss: 0.1328 - accuracy: 0.9386 - val_loss: 0.1305 - val_accuracy: 0.9389
Epoch 117/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.1588 - accuracy: 0.9289
Epoch 117: val_loss did not improve from 0.11129
852/852 [==============================] - 2s 2ms/step - loss: 0.1591 - accuracy: 0.9286 - val_loss: 0.1293 - val_accuracy: 0.9426
Epoch 118/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.1386 - accuracy: 0.9352
Epoch 118: val_loss did not improve from 0.11129
852/852 [==============================] - 2s 2ms/step - loss: 0.1392 - accuracy: 0.9351 - val_loss: 0.1548 - val_accuracy: 0.9295
Epoch 119/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.1462 - accuracy: 0.9314
Epoch 119: val_loss did not improve from 0.11129
852/852 [==============================] - 2s 2ms/step - loss: 0.1463 - accuracy: 0.9312 - val_loss: 0.1341 - val_accuracy: 0.9402
Epoch 120/150
819/852 [===========================&gt;..] - ETA: 0s - loss: 0.1411 - accuracy: 0.9346
Epoch 120: val_loss did not improve from 0.11129
852/852 [==============================] - 2s 2ms/step - loss: 0.1423 - accuracy: 0.9333 - val_loss: 0.1259 - val_accuracy: 0.9427
Epoch 121/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.1308 - accuracy: 0.9355
Epoch 121: val_loss did not improve from 0.11129
852/852 [==============================] - 2s 2ms/step - loss: 0.1308 - accuracy: 0.9357 - val_loss: 0.1182 - val_accuracy: 0.9443
Epoch 122/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.1419 - accuracy: 0.9319
Epoch 122: val_loss did not improve from 0.11129
852/852 [==============================] - 2s 2ms/step - loss: 0.1420 - accuracy: 0.9318 - val_loss: 0.1497 - val_accuracy: 0.9332
Epoch 123/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.1368 - accuracy: 0.9339
Epoch 123: val_loss did not improve from 0.11129
852/852 [==============================] - 2s 2ms/step - loss: 0.1360 - accuracy: 0.9340 - val_loss: 0.1238 - val_accuracy: 0.9428
Epoch 124/150
818/852 [===========================&gt;..] - ETA: 0s - loss: 0.1447 - accuracy: 0.9317
Epoch 124: val_loss did not improve from 0.11129
852/852 [==============================] - 2s 2ms/step - loss: 0.1458 - accuracy: 0.9308 - val_loss: 0.1409 - val_accuracy: 0.9394
Epoch 125/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.1372 - accuracy: 0.9359
Epoch 125: val_loss did not improve from 0.11129
852/852 [==============================] - 2s 2ms/step - loss: 0.1367 - accuracy: 0.9365 - val_loss: 0.1244 - val_accuracy: 0.9465
Epoch 126/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.1425 - accuracy: 0.9374
Epoch 126: val_loss did not improve from 0.11129
852/852 [==============================] - 2s 2ms/step - loss: 0.1427 - accuracy: 0.9373 - val_loss: 0.1186 - val_accuracy: 0.9459
Epoch 127/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.1344 - accuracy: 0.9354
Epoch 127: val_loss did not improve from 0.11129
852/852 [==============================] - 2s 2ms/step - loss: 0.1348 - accuracy: 0.9352 - val_loss: 0.1326 - val_accuracy: 0.9407
Epoch 128/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.1413 - accuracy: 0.9347
Epoch 128: val_loss did not improve from 0.11129
852/852 [==============================] - 2s 2ms/step - loss: 0.1416 - accuracy: 0.9344 - val_loss: 0.1318 - val_accuracy: 0.9374
Epoch 129/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.1353 - accuracy: 0.9350
Epoch 129: val_loss did not improve from 0.11129
852/852 [==============================] - 2s 2ms/step - loss: 0.1349 - accuracy: 0.9353 - val_loss: 0.1217 - val_accuracy: 0.9446
Epoch 130/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.1317 - accuracy: 0.9361
Epoch 130: val_loss did not improve from 0.11129
852/852 [==============================] - 2s 2ms/step - loss: 0.1318 - accuracy: 0.9360 - val_loss: 0.1365 - val_accuracy: 0.9346
Epoch 131/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.1417 - accuracy: 0.9301
Epoch 131: val_loss did not improve from 0.11129
852/852 [==============================] - 2s 2ms/step - loss: 0.1415 - accuracy: 0.9304 - val_loss: 0.1263 - val_accuracy: 0.9402
Epoch 132/150
811/852 [===========================&gt;..] - ETA: 0s - loss: 0.1336 - accuracy: 0.9367
Epoch 132: val_loss did not improve from 0.11129
852/852 [==============================] - 2s 2ms/step - loss: 0.1322 - accuracy: 0.9369 - val_loss: 0.1200 - val_accuracy: 0.9440
Epoch 133/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.1381 - accuracy: 0.9355
Epoch 133: val_loss did not improve from 0.11129
852/852 [==============================] - 2s 2ms/step - loss: 0.1385 - accuracy: 0.9351 - val_loss: 0.1520 - val_accuracy: 0.9354
Epoch 134/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.1410 - accuracy: 0.9367
Epoch 134: val_loss did not improve from 0.11129
852/852 [==============================] - 2s 2ms/step - loss: 0.1409 - accuracy: 0.9368 - val_loss: 0.1313 - val_accuracy: 0.9409
Epoch 135/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.1357 - accuracy: 0.9369
Epoch 135: val_loss did not improve from 0.11129
852/852 [==============================] - 2s 2ms/step - loss: 0.1363 - accuracy: 0.9366 - val_loss: 0.1292 - val_accuracy: 0.9407
Epoch 136/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.1343 - accuracy: 0.9355
Epoch 136: val_loss did not improve from 0.11129
852/852 [==============================] - 2s 2ms/step - loss: 0.1343 - accuracy: 0.9357 - val_loss: 0.1576 - val_accuracy: 0.9353
Epoch 137/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.1584 - accuracy: 0.9290
Epoch 137: val_loss did not improve from 0.11129
852/852 [==============================] - 2s 2ms/step - loss: 0.1576 - accuracy: 0.9293 - val_loss: 0.1213 - val_accuracy: 0.9432
Epoch 138/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.1324 - accuracy: 0.9394
Epoch 138: val_loss did not improve from 0.11129
852/852 [==============================] - 2s 2ms/step - loss: 0.1332 - accuracy: 0.9389 - val_loss: 0.1227 - val_accuracy: 0.9418
Epoch 139/150
816/852 [===========================&gt;..] - ETA: 0s - loss: 0.1359 - accuracy: 0.9359
Epoch 139: val_loss did not improve from 0.11129
852/852 [==============================] - 2s 2ms/step - loss: 0.1390 - accuracy: 0.9357 - val_loss: 0.1335 - val_accuracy: 0.9373
Epoch 140/150
819/852 [===========================&gt;..] - ETA: 0s - loss: 0.1265 - accuracy: 0.9409
Epoch 140: val_loss improved from 0.11129 to 0.10524, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 2ms/step - loss: 0.1251 - accuracy: 0.9411 - val_loss: 0.1052 - val_accuracy: 0.9508
Epoch 141/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.1231 - accuracy: 0.9422
Epoch 141: val_loss did not improve from 0.10524
852/852 [==============================] - 2s 2ms/step - loss: 0.1232 - accuracy: 0.9422 - val_loss: 0.1934 - val_accuracy: 0.9218
Epoch 142/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.1480 - accuracy: 0.9310
Epoch 142: val_loss did not improve from 0.10524
852/852 [==============================] - 2s 2ms/step - loss: 0.1488 - accuracy: 0.9306 - val_loss: 0.1384 - val_accuracy: 0.9430
Epoch 143/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.1432 - accuracy: 0.9365
Epoch 143: val_loss did not improve from 0.10524
852/852 [==============================] - 2s 2ms/step - loss: 0.1446 - accuracy: 0.9347 - val_loss: 0.1260 - val_accuracy: 0.9422
Epoch 144/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.1282 - accuracy: 0.9394
Epoch 144: val_loss did not improve from 0.10524
852/852 [==============================] - 2s 2ms/step - loss: 0.1293 - accuracy: 0.9394 - val_loss: 0.1209 - val_accuracy: 0.9434
Epoch 145/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.1297 - accuracy: 0.9351
Epoch 145: val_loss did not improve from 0.10524
852/852 [==============================] - 2s 2ms/step - loss: 0.1296 - accuracy: 0.9351 - val_loss: 0.1159 - val_accuracy: 0.9429
Epoch 146/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.1344 - accuracy: 0.9371
Epoch 146: val_loss did not improve from 0.10524
852/852 [==============================] - 2s 2ms/step - loss: 0.1350 - accuracy: 0.9367 - val_loss: 0.1353 - val_accuracy: 0.9375
Epoch 147/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.1443 - accuracy: 0.9315
Epoch 147: val_loss did not improve from 0.10524
852/852 [==============================] - 2s 2ms/step - loss: 0.1442 - accuracy: 0.9317 - val_loss: 0.1275 - val_accuracy: 0.9388
Epoch 148/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.1302 - accuracy: 0.9375
Epoch 148: val_loss did not improve from 0.10524
852/852 [==============================] - 2s 2ms/step - loss: 0.1311 - accuracy: 0.9376 - val_loss: 0.1220 - val_accuracy: 0.9447
Epoch 149/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.1288 - accuracy: 0.9404
Epoch 149: val_loss did not improve from 0.10524
852/852 [==============================] - 2s 2ms/step - loss: 0.1286 - accuracy: 0.9405 - val_loss: 0.1249 - val_accuracy: 0.9434
Epoch 150/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.1417 - accuracy: 0.9376
Epoch 150: val_loss did not improve from 0.10524
852/852 [==============================] - 2s 2ms/step - loss: 0.1437 - accuracy: 0.9365 - val_loss: 0.1222 - val_accuracy: 0.9447
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=0a87175a-ba20-4590-83b2-8b665a65b413">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [98]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">four_ninetyseven_neuron</span><span class="o">.</span><span class="n">load_weights</span><span class="p">(</span><span class="s1">'four_ninetyseven_model.hdf5'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=1e5e7e02-3969-4569-9d2d-da169f61d1f4">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [99]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Other Metrics</span>
<span class="n">four_ninetyseven_neuron_preds</span> <span class="o">=</span> <span class="n">four_ninetyseven_neuron</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">four_ninetyseven_neuron_preds</span><span class="o">.</span><span class="n">round</span><span class="p">())</span>
<span class="n">precision</span> <span class="o">=</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">four_ninetyseven_neuron_preds</span><span class="o">.</span><span class="n">round</span><span class="p">())</span>
<span class="n">recall</span> <span class="o">=</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">four_ninetyseven_neuron_preds</span><span class="o">.</span><span class="n">round</span><span class="p">())</span>
<span class="n">f1score</span> <span class="o">=</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">four_ninetyseven_neuron_preds</span><span class="o">.</span><span class="n">round</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Accuracy: </span><span class="si">%.2f</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">accuracy</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Precision: </span><span class="si">%.2f</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">precision</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Recall: </span><span class="si">%.2f</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">recall</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"F1-score: </span><span class="si">%.2f</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">f1score</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>267/267 [==============================] - 0s 938us/step
Accuracy: 0.95
Precision: 0.96
Recall: 0.94
F1-score: 0.95
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=66de8f94-d9ff-4c5d-83f9-5686b408aa91">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [100]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">history_497</span> <span class="o">=</span> <span class="n">four_ninetyseven_neuron</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">callback_a</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.1298 - accuracy: 0.9387
Epoch 1: val_loss did not improve from 0.10524
852/852 [==============================] - 2s 2ms/step - loss: 0.1298 - accuracy: 0.9387 - val_loss: 0.1151 - val_accuracy: 0.9469
Epoch 2/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.1339 - accuracy: 0.9378
Epoch 2: val_loss did not improve from 0.10524
852/852 [==============================] - 2s 2ms/step - loss: 0.1360 - accuracy: 0.9365 - val_loss: 0.1152 - val_accuracy: 0.9476
Epoch 3/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.1395 - accuracy: 0.9345
Epoch 3: val_loss did not improve from 0.10524
852/852 [==============================] - 2s 2ms/step - loss: 0.1388 - accuracy: 0.9349 - val_loss: 0.1206 - val_accuracy: 0.9461
Epoch 4/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.1231 - accuracy: 0.9422
Epoch 4: val_loss did not improve from 0.10524
852/852 [==============================] - 2s 2ms/step - loss: 0.1229 - accuracy: 0.9422 - val_loss: 0.1210 - val_accuracy: 0.9454
Epoch 5/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.1308 - accuracy: 0.9386
Epoch 5: val_loss did not improve from 0.10524
852/852 [==============================] - 2s 2ms/step - loss: 0.1310 - accuracy: 0.9387 - val_loss: 0.1121 - val_accuracy: 0.9460
Epoch 6/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.1333 - accuracy: 0.9351
Epoch 6: val_loss did not improve from 0.10524
852/852 [==============================] - 2s 3ms/step - loss: 0.1331 - accuracy: 0.9354 - val_loss: 0.1249 - val_accuracy: 0.9428
Epoch 7/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.1305 - accuracy: 0.9402
Epoch 7: val_loss did not improve from 0.10524
852/852 [==============================] - 2s 2ms/step - loss: 0.1305 - accuracy: 0.9401 - val_loss: 0.1162 - val_accuracy: 0.9438
Epoch 8/150
815/852 [===========================&gt;..] - ETA: 0s - loss: 0.1312 - accuracy: 0.9368
Epoch 8: val_loss did not improve from 0.10524
852/852 [==============================] - 2s 2ms/step - loss: 0.1321 - accuracy: 0.9362 - val_loss: 0.1294 - val_accuracy: 0.9398
Epoch 9/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.1433 - accuracy: 0.9336
Epoch 9: val_loss did not improve from 0.10524
852/852 [==============================] - 2s 2ms/step - loss: 0.1427 - accuracy: 0.9340 - val_loss: 0.1485 - val_accuracy: 0.9335
Epoch 10/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.1339 - accuracy: 0.9372
Epoch 10: val_loss did not improve from 0.10524
852/852 [==============================] - 3s 3ms/step - loss: 0.1344 - accuracy: 0.9368 - val_loss: 0.1236 - val_accuracy: 0.9422
Epoch 11/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.1403 - accuracy: 0.9365
Epoch 11: val_loss did not improve from 0.10524
852/852 [==============================] - 4s 4ms/step - loss: 0.1399 - accuracy: 0.9368 - val_loss: 0.1143 - val_accuracy: 0.9477
Epoch 12/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.1399 - accuracy: 0.9367
Epoch 12: val_loss did not improve from 0.10524
852/852 [==============================] - 3s 4ms/step - loss: 0.1390 - accuracy: 0.9369 - val_loss: 0.1254 - val_accuracy: 0.9439
Epoch 13/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.1359 - accuracy: 0.9353
Epoch 13: val_loss did not improve from 0.10524
852/852 [==============================] - 3s 4ms/step - loss: 0.1382 - accuracy: 0.9341 - val_loss: 0.1402 - val_accuracy: 0.9352
Epoch 14/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.1413 - accuracy: 0.9334
Epoch 14: val_loss did not improve from 0.10524
852/852 [==============================] - 3s 3ms/step - loss: 0.1413 - accuracy: 0.9334 - val_loss: 0.1288 - val_accuracy: 0.9401
Epoch 15/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.1288 - accuracy: 0.9396
Epoch 15: val_loss did not improve from 0.10524
852/852 [==============================] - 3s 3ms/step - loss: 0.1289 - accuracy: 0.9395 - val_loss: 0.1099 - val_accuracy: 0.9477
Epoch 16/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.1258 - accuracy: 0.9394
Epoch 16: val_loss did not improve from 0.10524
852/852 [==============================] - 3s 3ms/step - loss: 0.1256 - accuracy: 0.9393 - val_loss: 0.1077 - val_accuracy: 0.9496
Epoch 17/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.1366 - accuracy: 0.9360
Epoch 17: val_loss did not improve from 0.10524
852/852 [==============================] - 3s 3ms/step - loss: 0.1364 - accuracy: 0.9361 - val_loss: 0.1259 - val_accuracy: 0.9436
Epoch 18/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.1390 - accuracy: 0.9381
Epoch 18: val_loss did not improve from 0.10524
852/852 [==============================] - 2s 3ms/step - loss: 0.1386 - accuracy: 0.9384 - val_loss: 0.1187 - val_accuracy: 0.9460
Epoch 19/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.1245 - accuracy: 0.9395
Epoch 19: val_loss did not improve from 0.10524
852/852 [==============================] - 3s 3ms/step - loss: 0.1253 - accuracy: 0.9395 - val_loss: 0.1280 - val_accuracy: 0.9421
Epoch 20/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.1279 - accuracy: 0.9368
Epoch 20: val_loss did not improve from 0.10524
852/852 [==============================] - 3s 4ms/step - loss: 0.1283 - accuracy: 0.9365 - val_loss: 0.1142 - val_accuracy: 0.9455
Epoch 21/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.1424 - accuracy: 0.9337
Epoch 21: val_loss improved from 0.10524 to 0.10405, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.1415 - accuracy: 0.9341 - val_loss: 0.1041 - val_accuracy: 0.9519
Epoch 22/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.1336 - accuracy: 0.9389
Epoch 22: val_loss did not improve from 0.10405
852/852 [==============================] - 2s 2ms/step - loss: 0.1336 - accuracy: 0.9388 - val_loss: 0.1194 - val_accuracy: 0.9442
Epoch 23/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.1270 - accuracy: 0.9375
Epoch 23: val_loss did not improve from 0.10405
852/852 [==============================] - 3s 3ms/step - loss: 0.1267 - accuracy: 0.9378 - val_loss: 0.1418 - val_accuracy: 0.9319
Epoch 24/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.1279 - accuracy: 0.9410
Epoch 24: val_loss did not improve from 0.10405
852/852 [==============================] - 2s 3ms/step - loss: 0.1280 - accuracy: 0.9409 - val_loss: 0.1215 - val_accuracy: 0.9411
Epoch 25/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.1221 - accuracy: 0.9412
Epoch 25: val_loss did not improve from 0.10405
852/852 [==============================] - 2s 2ms/step - loss: 0.1216 - accuracy: 0.9414 - val_loss: 0.1246 - val_accuracy: 0.9427
Epoch 26/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.1375 - accuracy: 0.9368
Epoch 26: val_loss did not improve from 0.10405
852/852 [==============================] - 2s 2ms/step - loss: 0.1374 - accuracy: 0.9368 - val_loss: 0.1051 - val_accuracy: 0.9483
Epoch 27/150
819/852 [===========================&gt;..] - ETA: 0s - loss: 0.1267 - accuracy: 0.9414
Epoch 27: val_loss did not improve from 0.10405
852/852 [==============================] - 2s 2ms/step - loss: 0.1266 - accuracy: 0.9408 - val_loss: 0.1067 - val_accuracy: 0.9489
Epoch 28/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.1266 - accuracy: 0.9403
Epoch 28: val_loss did not improve from 0.10405
852/852 [==============================] - 2s 2ms/step - loss: 0.1268 - accuracy: 0.9396 - val_loss: 0.1388 - val_accuracy: 0.9408
Epoch 29/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.1419 - accuracy: 0.9375
Epoch 29: val_loss did not improve from 0.10405
852/852 [==============================] - 2s 2ms/step - loss: 0.1417 - accuracy: 0.9375 - val_loss: 0.1119 - val_accuracy: 0.9460
Epoch 30/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.1216 - accuracy: 0.9433
Epoch 30: val_loss did not improve from 0.10405
852/852 [==============================] - 2s 2ms/step - loss: 0.1214 - accuracy: 0.9433 - val_loss: 0.1167 - val_accuracy: 0.9450
Epoch 31/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.1409 - accuracy: 0.9380
Epoch 31: val_loss did not improve from 0.10405
852/852 [==============================] - 2s 3ms/step - loss: 0.1407 - accuracy: 0.9381 - val_loss: 0.1676 - val_accuracy: 0.9418
Epoch 32/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.1409 - accuracy: 0.9380
Epoch 32: val_loss did not improve from 0.10405
852/852 [==============================] - 2s 2ms/step - loss: 0.1404 - accuracy: 0.9386 - val_loss: 0.1082 - val_accuracy: 0.9500
Epoch 33/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.1270 - accuracy: 0.9417
Epoch 33: val_loss did not improve from 0.10405
852/852 [==============================] - 2s 2ms/step - loss: 0.1270 - accuracy: 0.9418 - val_loss: 0.1304 - val_accuracy: 0.9406
Epoch 34/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.1259 - accuracy: 0.9414
Epoch 34: val_loss did not improve from 0.10405
852/852 [==============================] - 2s 2ms/step - loss: 0.1269 - accuracy: 0.9408 - val_loss: 0.1202 - val_accuracy: 0.9467
Epoch 35/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.1349 - accuracy: 0.9369
Epoch 35: val_loss did not improve from 0.10405
852/852 [==============================] - 2s 2ms/step - loss: 0.1354 - accuracy: 0.9361 - val_loss: 0.1255 - val_accuracy: 0.9447
Epoch 36/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.1292 - accuracy: 0.9369
Epoch 36: val_loss did not improve from 0.10405
852/852 [==============================] - 3s 3ms/step - loss: 0.1279 - accuracy: 0.9376 - val_loss: 0.1123 - val_accuracy: 0.9468
Epoch 37/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.1303 - accuracy: 0.9385
Epoch 37: val_loss did not improve from 0.10405
852/852 [==============================] - 3s 3ms/step - loss: 0.1303 - accuracy: 0.9384 - val_loss: 0.1296 - val_accuracy: 0.9409
Epoch 38/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.1369 - accuracy: 0.9374
Epoch 38: val_loss did not improve from 0.10405
852/852 [==============================] - 3s 4ms/step - loss: 0.1376 - accuracy: 0.9366 - val_loss: 0.1285 - val_accuracy: 0.9416
Epoch 39/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.1306 - accuracy: 0.9386
Epoch 39: val_loss did not improve from 0.10405
852/852 [==============================] - 3s 3ms/step - loss: 0.1310 - accuracy: 0.9384 - val_loss: 0.1275 - val_accuracy: 0.9394
Epoch 40/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.1297 - accuracy: 0.9398
Epoch 40: val_loss did not improve from 0.10405
852/852 [==============================] - 2s 3ms/step - loss: 0.1303 - accuracy: 0.9394 - val_loss: 0.1528 - val_accuracy: 0.9353
Epoch 41/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.1428 - accuracy: 0.9347
Epoch 41: val_loss did not improve from 0.10405
852/852 [==============================] - 3s 3ms/step - loss: 0.1429 - accuracy: 0.9346 - val_loss: 0.1124 - val_accuracy: 0.9486
Epoch 42/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.1304 - accuracy: 0.9371
Epoch 42: val_loss did not improve from 0.10405
852/852 [==============================] - 3s 3ms/step - loss: 0.1302 - accuracy: 0.9373 - val_loss: 0.1041 - val_accuracy: 0.9489
Epoch 43/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.1162 - accuracy: 0.9421
Epoch 43: val_loss did not improve from 0.10405
852/852 [==============================] - 2s 3ms/step - loss: 0.1164 - accuracy: 0.9420 - val_loss: 0.1259 - val_accuracy: 0.9420
Epoch 44/150
820/852 [===========================&gt;..] - ETA: 0s - loss: 0.1198 - accuracy: 0.9412
Epoch 44: val_loss did not improve from 0.10405
852/852 [==============================] - 2s 3ms/step - loss: 0.1204 - accuracy: 0.9407 - val_loss: 0.1169 - val_accuracy: 0.9468
Epoch 45/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.1350 - accuracy: 0.9360
Epoch 45: val_loss did not improve from 0.10405
852/852 [==============================] - 3s 3ms/step - loss: 0.1349 - accuracy: 0.9359 - val_loss: 0.1190 - val_accuracy: 0.9459
Epoch 46/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.1348 - accuracy: 0.9380
Epoch 46: val_loss did not improve from 0.10405
852/852 [==============================] - 2s 2ms/step - loss: 0.1351 - accuracy: 0.9381 - val_loss: 0.1114 - val_accuracy: 0.9486
Epoch 47/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.1309 - accuracy: 0.9386
Epoch 47: val_loss did not improve from 0.10405
852/852 [==============================] - 2s 3ms/step - loss: 0.1320 - accuracy: 0.9381 - val_loss: 0.1205 - val_accuracy: 0.9455
Epoch 48/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.1198 - accuracy: 0.9428
Epoch 48: val_loss did not improve from 0.10405
852/852 [==============================] - 2s 2ms/step - loss: 0.1204 - accuracy: 0.9428 - val_loss: 0.1045 - val_accuracy: 0.9512
Epoch 49/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.1327 - accuracy: 0.9388
Epoch 49: val_loss did not improve from 0.10405
852/852 [==============================] - 2s 2ms/step - loss: 0.1330 - accuracy: 0.9384 - val_loss: 0.1275 - val_accuracy: 0.9420
Epoch 50/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.1246 - accuracy: 0.9425
Epoch 50: val_loss did not improve from 0.10405
852/852 [==============================] - 2s 2ms/step - loss: 0.1256 - accuracy: 0.9421 - val_loss: 0.1492 - val_accuracy: 0.9380
Epoch 51/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.1173 - accuracy: 0.9440
Epoch 51: val_loss did not improve from 0.10405
852/852 [==============================] - 2s 2ms/step - loss: 0.1172 - accuracy: 0.9441 - val_loss: 0.1103 - val_accuracy: 0.9481
Epoch 52/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.1266 - accuracy: 0.9390
Epoch 52: val_loss did not improve from 0.10405
852/852 [==============================] - 3s 3ms/step - loss: 0.1268 - accuracy: 0.9387 - val_loss: 0.1055 - val_accuracy: 0.9497
Epoch 53/150
852/852 [==============================] - ETA: 0s - loss: 0.1391 - accuracy: 0.9379
Epoch 53: val_loss improved from 0.10405 to 0.10308, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.1391 - accuracy: 0.9379 - val_loss: 0.1031 - val_accuracy: 0.9514
Epoch 54/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.1194 - accuracy: 0.9419
Epoch 54: val_loss did not improve from 0.10308
852/852 [==============================] - 2s 2ms/step - loss: 0.1201 - accuracy: 0.9415 - val_loss: 0.1116 - val_accuracy: 0.9487
Epoch 55/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.1223 - accuracy: 0.9410
Epoch 55: val_loss did not improve from 0.10308
852/852 [==============================] - 2s 2ms/step - loss: 0.1239 - accuracy: 0.9398 - val_loss: 0.1168 - val_accuracy: 0.9466
Epoch 56/150
818/852 [===========================&gt;..] - ETA: 0s - loss: 0.1269 - accuracy: 0.9388
Epoch 56: val_loss did not improve from 0.10308
852/852 [==============================] - 2s 2ms/step - loss: 0.1269 - accuracy: 0.9384 - val_loss: 0.1098 - val_accuracy: 0.9483
Epoch 57/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.1200 - accuracy: 0.9419
Epoch 57: val_loss did not improve from 0.10308
852/852 [==============================] - 2s 3ms/step - loss: 0.1201 - accuracy: 0.9419 - val_loss: 0.1242 - val_accuracy: 0.9449
Epoch 58/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.1294 - accuracy: 0.9394
Epoch 58: val_loss did not improve from 0.10308
852/852 [==============================] - 2s 3ms/step - loss: 0.1299 - accuracy: 0.9391 - val_loss: 0.1072 - val_accuracy: 0.9479
Epoch 59/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.1305 - accuracy: 0.9384
Epoch 59: val_loss did not improve from 0.10308
852/852 [==============================] - 3s 3ms/step - loss: 0.1306 - accuracy: 0.9385 - val_loss: 0.1254 - val_accuracy: 0.9413
Epoch 60/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.1240 - accuracy: 0.9400
Epoch 60: val_loss did not improve from 0.10308
852/852 [==============================] - 2s 3ms/step - loss: 0.1240 - accuracy: 0.9395 - val_loss: 0.1152 - val_accuracy: 0.9469
Epoch 61/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.1332 - accuracy: 0.9373
Epoch 61: val_loss did not improve from 0.10308
852/852 [==============================] - 3s 4ms/step - loss: 0.1339 - accuracy: 0.9367 - val_loss: 0.1069 - val_accuracy: 0.9508
Epoch 62/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.1200 - accuracy: 0.9446
Epoch 62: val_loss improved from 0.10308 to 0.10084, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.1201 - accuracy: 0.9442 - val_loss: 0.1008 - val_accuracy: 0.9530
Epoch 63/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.1141 - accuracy: 0.9440
Epoch 63: val_loss did not improve from 0.10084
852/852 [==============================] - 3s 3ms/step - loss: 0.1148 - accuracy: 0.9433 - val_loss: 0.1561 - val_accuracy: 0.9277
Epoch 64/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.1316 - accuracy: 0.9391
Epoch 64: val_loss did not improve from 0.10084
852/852 [==============================] - 3s 3ms/step - loss: 0.1320 - accuracy: 0.9381 - val_loss: 0.1172 - val_accuracy: 0.9448
Epoch 65/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.1167 - accuracy: 0.9450
Epoch 65: val_loss did not improve from 0.10084
852/852 [==============================] - 3s 3ms/step - loss: 0.1191 - accuracy: 0.9438 - val_loss: 0.1197 - val_accuracy: 0.9466
Epoch 66/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.1397 - accuracy: 0.9380
Epoch 66: val_loss did not improve from 0.10084
852/852 [==============================] - 3s 3ms/step - loss: 0.1396 - accuracy: 0.9380 - val_loss: 0.1083 - val_accuracy: 0.9501
Epoch 67/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.1205 - accuracy: 0.9446
Epoch 67: val_loss did not improve from 0.10084
852/852 [==============================] - 3s 4ms/step - loss: 0.1208 - accuracy: 0.9445 - val_loss: 0.1092 - val_accuracy: 0.9492
Epoch 68/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.1242 - accuracy: 0.9403
Epoch 68: val_loss improved from 0.10084 to 0.09887, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.1235 - accuracy: 0.9412 - val_loss: 0.0989 - val_accuracy: 0.9533
Epoch 69/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.1152 - accuracy: 0.9472
Epoch 69: val_loss did not improve from 0.09887
852/852 [==============================] - 3s 3ms/step - loss: 0.1148 - accuracy: 0.9475 - val_loss: 0.1032 - val_accuracy: 0.9514
Epoch 70/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.1254 - accuracy: 0.9417
Epoch 70: val_loss did not improve from 0.09887
852/852 [==============================] - 3s 3ms/step - loss: 0.1255 - accuracy: 0.9413 - val_loss: 0.1096 - val_accuracy: 0.9499
Epoch 71/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.1189 - accuracy: 0.9421
Epoch 71: val_loss did not improve from 0.09887
852/852 [==============================] - 3s 3ms/step - loss: 0.1197 - accuracy: 0.9415 - val_loss: 0.1436 - val_accuracy: 0.9357
Epoch 72/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.1270 - accuracy: 0.9396
Epoch 72: val_loss did not improve from 0.09887
852/852 [==============================] - 3s 3ms/step - loss: 0.1266 - accuracy: 0.9394 - val_loss: 0.1020 - val_accuracy: 0.9521
Epoch 73/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.1294 - accuracy: 0.9432
Epoch 73: val_loss did not improve from 0.09887
852/852 [==============================] - 4s 4ms/step - loss: 0.1294 - accuracy: 0.9430 - val_loss: 0.1095 - val_accuracy: 0.9503
Epoch 74/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.1225 - accuracy: 0.9426
Epoch 74: val_loss did not improve from 0.09887
852/852 [==============================] - 3s 3ms/step - loss: 0.1233 - accuracy: 0.9421 - val_loss: 0.1134 - val_accuracy: 0.9461
Epoch 75/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.1188 - accuracy: 0.9418
Epoch 75: val_loss did not improve from 0.09887
852/852 [==============================] - 2s 2ms/step - loss: 0.1185 - accuracy: 0.9421 - val_loss: 0.1102 - val_accuracy: 0.9506
Epoch 76/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.1185 - accuracy: 0.9450
Epoch 76: val_loss did not improve from 0.09887
852/852 [==============================] - 2s 3ms/step - loss: 0.1182 - accuracy: 0.9452 - val_loss: 0.1050 - val_accuracy: 0.9500
Epoch 77/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.1224 - accuracy: 0.9404
Epoch 77: val_loss did not improve from 0.09887
852/852 [==============================] - 3s 3ms/step - loss: 0.1226 - accuracy: 0.9405 - val_loss: 0.1155 - val_accuracy: 0.9473
Epoch 78/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.1265 - accuracy: 0.9407
Epoch 78: val_loss did not improve from 0.09887
852/852 [==============================] - 3s 4ms/step - loss: 0.1265 - accuracy: 0.9408 - val_loss: 0.1146 - val_accuracy: 0.9486
Epoch 79/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.1386 - accuracy: 0.9411
Epoch 79: val_loss did not improve from 0.09887
852/852 [==============================] - 2s 3ms/step - loss: 0.1381 - accuracy: 0.9411 - val_loss: 0.1143 - val_accuracy: 0.9479
Epoch 80/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.1190 - accuracy: 0.9427
Epoch 80: val_loss did not improve from 0.09887
852/852 [==============================] - 3s 3ms/step - loss: 0.1184 - accuracy: 0.9433 - val_loss: 0.1012 - val_accuracy: 0.9513
Epoch 81/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.1173 - accuracy: 0.9454
Epoch 81: val_loss did not improve from 0.09887
852/852 [==============================] - 3s 3ms/step - loss: 0.1195 - accuracy: 0.9443 - val_loss: 0.1325 - val_accuracy: 0.9400
Epoch 82/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.1199 - accuracy: 0.9420
Epoch 82: val_loss did not improve from 0.09887
852/852 [==============================] - 2s 3ms/step - loss: 0.1203 - accuracy: 0.9418 - val_loss: 0.1023 - val_accuracy: 0.9510
Epoch 83/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.1232 - accuracy: 0.9398
Epoch 83: val_loss did not improve from 0.09887
852/852 [==============================] - 3s 3ms/step - loss: 0.1228 - accuracy: 0.9400 - val_loss: 0.1029 - val_accuracy: 0.9506
Epoch 84/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.1179 - accuracy: 0.9445
Epoch 84: val_loss did not improve from 0.09887
852/852 [==============================] - 3s 3ms/step - loss: 0.1179 - accuracy: 0.9445 - val_loss: 0.1071 - val_accuracy: 0.9488
Epoch 85/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.1175 - accuracy: 0.9462
Epoch 85: val_loss did not improve from 0.09887
852/852 [==============================] - 2s 3ms/step - loss: 0.1169 - accuracy: 0.9465 - val_loss: 0.1118 - val_accuracy: 0.9502
Epoch 86/150
852/852 [==============================] - ETA: 0s - loss: 0.1293 - accuracy: 0.9406
Epoch 86: val_loss did not improve from 0.09887
852/852 [==============================] - 3s 3ms/step - loss: 0.1293 - accuracy: 0.9406 - val_loss: 0.1220 - val_accuracy: 0.9427
Epoch 87/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.1201 - accuracy: 0.9427
Epoch 87: val_loss did not improve from 0.09887
852/852 [==============================] - 3s 3ms/step - loss: 0.1205 - accuracy: 0.9423 - val_loss: 0.1080 - val_accuracy: 0.9501
Epoch 88/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.1223 - accuracy: 0.9433
Epoch 88: val_loss did not improve from 0.09887
852/852 [==============================] - 3s 3ms/step - loss: 0.1225 - accuracy: 0.9433 - val_loss: 0.1090 - val_accuracy: 0.9519
Epoch 89/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.1201 - accuracy: 0.9439
Epoch 89: val_loss did not improve from 0.09887
852/852 [==============================] - 3s 3ms/step - loss: 0.1200 - accuracy: 0.9440 - val_loss: 0.1161 - val_accuracy: 0.9476
Epoch 90/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.1285 - accuracy: 0.9420
Epoch 90: val_loss did not improve from 0.09887
852/852 [==============================] - 2s 3ms/step - loss: 0.1287 - accuracy: 0.9420 - val_loss: 0.1081 - val_accuracy: 0.9500
Epoch 91/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.1205 - accuracy: 0.9456
Epoch 91: val_loss did not improve from 0.09887
852/852 [==============================] - 2s 3ms/step - loss: 0.1207 - accuracy: 0.9457 - val_loss: 0.1097 - val_accuracy: 0.9493
Epoch 92/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.1209 - accuracy: 0.9436
Epoch 92: val_loss did not improve from 0.09887
852/852 [==============================] - 2s 3ms/step - loss: 0.1207 - accuracy: 0.9435 - val_loss: 0.1096 - val_accuracy: 0.9493
Epoch 93/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.1193 - accuracy: 0.9433
Epoch 93: val_loss did not improve from 0.09887
852/852 [==============================] - 3s 3ms/step - loss: 0.1191 - accuracy: 0.9433 - val_loss: 0.1227 - val_accuracy: 0.9415
Epoch 94/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.1177 - accuracy: 0.9436
Epoch 94: val_loss did not improve from 0.09887
852/852 [==============================] - 2s 3ms/step - loss: 0.1178 - accuracy: 0.9435 - val_loss: 0.1177 - val_accuracy: 0.9462
Epoch 95/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.1156 - accuracy: 0.9444
Epoch 95: val_loss did not improve from 0.09887
852/852 [==============================] - 2s 3ms/step - loss: 0.1155 - accuracy: 0.9445 - val_loss: 0.1018 - val_accuracy: 0.9516
Epoch 96/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.1243 - accuracy: 0.9415
Epoch 96: val_loss did not improve from 0.09887
852/852 [==============================] - 3s 3ms/step - loss: 0.1246 - accuracy: 0.9416 - val_loss: 0.1116 - val_accuracy: 0.9513
Epoch 97/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.1123 - accuracy: 0.9439
Epoch 97: val_loss improved from 0.09887 to 0.09717, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.1122 - accuracy: 0.9440 - val_loss: 0.0972 - val_accuracy: 0.9544
Epoch 98/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.1230 - accuracy: 0.9396
Epoch 98: val_loss did not improve from 0.09717
852/852 [==============================] - 2s 3ms/step - loss: 0.1227 - accuracy: 0.9399 - val_loss: 0.1163 - val_accuracy: 0.9452
Epoch 99/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.1226 - accuracy: 0.9396
Epoch 99: val_loss did not improve from 0.09717
852/852 [==============================] - 2s 3ms/step - loss: 0.1239 - accuracy: 0.9394 - val_loss: 0.1084 - val_accuracy: 0.9527
Epoch 100/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.1339 - accuracy: 0.9394
Epoch 100: val_loss did not improve from 0.09717
852/852 [==============================] - 2s 3ms/step - loss: 0.1326 - accuracy: 0.9402 - val_loss: 0.0981 - val_accuracy: 0.9533
Epoch 101/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.1158 - accuracy: 0.9432
Epoch 101: val_loss did not improve from 0.09717
852/852 [==============================] - 2s 3ms/step - loss: 0.1153 - accuracy: 0.9435 - val_loss: 0.1067 - val_accuracy: 0.9499
Epoch 102/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.1121 - accuracy: 0.9454
Epoch 102: val_loss did not improve from 0.09717
852/852 [==============================] - 2s 3ms/step - loss: 0.1123 - accuracy: 0.9452 - val_loss: 0.0990 - val_accuracy: 0.9543
Epoch 103/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.1127 - accuracy: 0.9456
Epoch 103: val_loss did not improve from 0.09717
852/852 [==============================] - 2s 3ms/step - loss: 0.1131 - accuracy: 0.9453 - val_loss: 0.1030 - val_accuracy: 0.9521
Epoch 104/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.1260 - accuracy: 0.9395
Epoch 104: val_loss did not improve from 0.09717
852/852 [==============================] - 2s 2ms/step - loss: 0.1257 - accuracy: 0.9393 - val_loss: 0.1126 - val_accuracy: 0.9484
Epoch 105/150
820/852 [===========================&gt;..] - ETA: 0s - loss: 0.1266 - accuracy: 0.9400
Epoch 105: val_loss did not improve from 0.09717
852/852 [==============================] - 2s 2ms/step - loss: 0.1265 - accuracy: 0.9399 - val_loss: 0.1123 - val_accuracy: 0.9492
Epoch 106/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.1121 - accuracy: 0.9471
Epoch 106: val_loss did not improve from 0.09717
852/852 [==============================] - 2s 2ms/step - loss: 0.1138 - accuracy: 0.9465 - val_loss: 0.1081 - val_accuracy: 0.9493
Epoch 107/150
818/852 [===========================&gt;..] - ETA: 0s - loss: 0.1215 - accuracy: 0.9419
Epoch 107: val_loss did not improve from 0.09717
852/852 [==============================] - 2s 2ms/step - loss: 0.1206 - accuracy: 0.9425 - val_loss: 0.1014 - val_accuracy: 0.9519
Epoch 108/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.1277 - accuracy: 0.9422
Epoch 108: val_loss did not improve from 0.09717
852/852 [==============================] - 2s 2ms/step - loss: 0.1269 - accuracy: 0.9427 - val_loss: 0.1049 - val_accuracy: 0.9508
Epoch 109/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.1138 - accuracy: 0.9474
Epoch 109: val_loss did not improve from 0.09717
852/852 [==============================] - 2s 2ms/step - loss: 0.1138 - accuracy: 0.9479 - val_loss: 0.1015 - val_accuracy: 0.9506
Epoch 110/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.1189 - accuracy: 0.9422
Epoch 110: val_loss did not improve from 0.09717
852/852 [==============================] - 3s 3ms/step - loss: 0.1179 - accuracy: 0.9426 - val_loss: 0.1068 - val_accuracy: 0.9506
Epoch 111/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.1222 - accuracy: 0.9407
Epoch 111: val_loss did not improve from 0.09717
852/852 [==============================] - 2s 3ms/step - loss: 0.1221 - accuracy: 0.9411 - val_loss: 0.1167 - val_accuracy: 0.9454
Epoch 112/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.1141 - accuracy: 0.9461
Epoch 112: val_loss did not improve from 0.09717
852/852 [==============================] - 2s 3ms/step - loss: 0.1141 - accuracy: 0.9459 - val_loss: 0.1047 - val_accuracy: 0.9514
Epoch 113/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.1216 - accuracy: 0.9449
Epoch 113: val_loss did not improve from 0.09717
852/852 [==============================] - 2s 2ms/step - loss: 0.1215 - accuracy: 0.9453 - val_loss: 0.0983 - val_accuracy: 0.9539
Epoch 114/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.1140 - accuracy: 0.9458
Epoch 114: val_loss did not improve from 0.09717
852/852 [==============================] - 3s 3ms/step - loss: 0.1138 - accuracy: 0.9460 - val_loss: 0.1521 - val_accuracy: 0.9327
Epoch 115/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.1286 - accuracy: 0.9417
Epoch 115: val_loss did not improve from 0.09717
852/852 [==============================] - 2s 3ms/step - loss: 0.1284 - accuracy: 0.9415 - val_loss: 0.1201 - val_accuracy: 0.9497
Epoch 116/150
852/852 [==============================] - ETA: 0s - loss: 0.1259 - accuracy: 0.9411
Epoch 116: val_loss did not improve from 0.09717
852/852 [==============================] - 2s 3ms/step - loss: 0.1259 - accuracy: 0.9411 - val_loss: 0.1159 - val_accuracy: 0.9472
Epoch 117/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.1294 - accuracy: 0.9443
Epoch 117: val_loss did not improve from 0.09717
852/852 [==============================] - 2s 3ms/step - loss: 0.1298 - accuracy: 0.9441 - val_loss: 0.1260 - val_accuracy: 0.9423
Epoch 118/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.1133 - accuracy: 0.9455
Epoch 118: val_loss did not improve from 0.09717
852/852 [==============================] - 2s 3ms/step - loss: 0.1140 - accuracy: 0.9450 - val_loss: 0.1395 - val_accuracy: 0.9414
Epoch 119/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.1242 - accuracy: 0.9408
Epoch 119: val_loss did not improve from 0.09717
852/852 [==============================] - 3s 3ms/step - loss: 0.1241 - accuracy: 0.9407 - val_loss: 0.1250 - val_accuracy: 0.9484
Epoch 120/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.1216 - accuracy: 0.9434
Epoch 120: val_loss did not improve from 0.09717
852/852 [==============================] - 2s 3ms/step - loss: 0.1228 - accuracy: 0.9432 - val_loss: 0.1189 - val_accuracy: 0.9435
Epoch 121/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.1135 - accuracy: 0.9468
Epoch 121: val_loss did not improve from 0.09717
852/852 [==============================] - 3s 3ms/step - loss: 0.1135 - accuracy: 0.9469 - val_loss: 0.1031 - val_accuracy: 0.9516
Epoch 122/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.1106 - accuracy: 0.9473
Epoch 122: val_loss did not improve from 0.09717
852/852 [==============================] - 3s 3ms/step - loss: 0.1106 - accuracy: 0.9472 - val_loss: 0.0989 - val_accuracy: 0.9531
Epoch 123/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.1207 - accuracy: 0.9441
Epoch 123: val_loss did not improve from 0.09717
852/852 [==============================] - 2s 3ms/step - loss: 0.1203 - accuracy: 0.9440 - val_loss: 0.0987 - val_accuracy: 0.9522
Epoch 124/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.1253 - accuracy: 0.9440
Epoch 124: val_loss did not improve from 0.09717
852/852 [==============================] - 3s 3ms/step - loss: 0.1241 - accuracy: 0.9446 - val_loss: 0.1041 - val_accuracy: 0.9537
Epoch 125/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.1085 - accuracy: 0.9480
Epoch 125: val_loss did not improve from 0.09717
852/852 [==============================] - 3s 3ms/step - loss: 0.1086 - accuracy: 0.9479 - val_loss: 0.1093 - val_accuracy: 0.9494
Epoch 126/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.1217 - accuracy: 0.9434
Epoch 126: val_loss did not improve from 0.09717
852/852 [==============================] - 2s 3ms/step - loss: 0.1224 - accuracy: 0.9430 - val_loss: 0.1183 - val_accuracy: 0.9459
Epoch 127/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.1276 - accuracy: 0.9411
Epoch 127: val_loss did not improve from 0.09717
852/852 [==============================] - 3s 3ms/step - loss: 0.1290 - accuracy: 0.9406 - val_loss: 0.1134 - val_accuracy: 0.9453
Epoch 128/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.1144 - accuracy: 0.9471
Epoch 128: val_loss improved from 0.09717 to 0.09536, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.1143 - accuracy: 0.9469 - val_loss: 0.0954 - val_accuracy: 0.9558
Epoch 129/150
817/852 [===========================&gt;..] - ETA: 0s - loss: 0.1141 - accuracy: 0.9453
Epoch 129: val_loss did not improve from 0.09536
852/852 [==============================] - 2s 3ms/step - loss: 0.1149 - accuracy: 0.9454 - val_loss: 0.1026 - val_accuracy: 0.9519
Epoch 130/150
852/852 [==============================] - ETA: 0s - loss: 0.1194 - accuracy: 0.9460
Epoch 130: val_loss did not improve from 0.09536
852/852 [==============================] - 3s 3ms/step - loss: 0.1194 - accuracy: 0.9460 - val_loss: 0.1265 - val_accuracy: 0.9432
Epoch 131/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.1195 - accuracy: 0.9468
Epoch 131: val_loss did not improve from 0.09536
852/852 [==============================] - 2s 3ms/step - loss: 0.1196 - accuracy: 0.9470 - val_loss: 0.1053 - val_accuracy: 0.9534
Epoch 132/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.1146 - accuracy: 0.9472
Epoch 132: val_loss did not improve from 0.09536
852/852 [==============================] - 2s 3ms/step - loss: 0.1152 - accuracy: 0.9467 - val_loss: 0.1160 - val_accuracy: 0.9482
Epoch 133/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.1123 - accuracy: 0.9467
Epoch 133: val_loss did not improve from 0.09536
852/852 [==============================] - 2s 2ms/step - loss: 0.1141 - accuracy: 0.9457 - val_loss: 0.1140 - val_accuracy: 0.9489
Epoch 134/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.1177 - accuracy: 0.9438
Epoch 134: val_loss did not improve from 0.09536
852/852 [==============================] - 2s 3ms/step - loss: 0.1174 - accuracy: 0.9442 - val_loss: 0.1014 - val_accuracy: 0.9515
Epoch 135/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.1147 - accuracy: 0.9462
Epoch 135: val_loss did not improve from 0.09536
852/852 [==============================] - 3s 4ms/step - loss: 0.1152 - accuracy: 0.9460 - val_loss: 0.0980 - val_accuracy: 0.9546
Epoch 136/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.1148 - accuracy: 0.9445
Epoch 136: val_loss did not improve from 0.09536
852/852 [==============================] - 3s 4ms/step - loss: 0.1147 - accuracy: 0.9446 - val_loss: 0.1012 - val_accuracy: 0.9513
Epoch 137/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.1176 - accuracy: 0.9464
Epoch 137: val_loss did not improve from 0.09536
852/852 [==============================] - 3s 3ms/step - loss: 0.1176 - accuracy: 0.9463 - val_loss: 0.1066 - val_accuracy: 0.9489
Epoch 138/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.1200 - accuracy: 0.9432
Epoch 138: val_loss did not improve from 0.09536
852/852 [==============================] - 2s 3ms/step - loss: 0.1200 - accuracy: 0.9432 - val_loss: 0.1211 - val_accuracy: 0.9462
Epoch 139/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.1125 - accuracy: 0.9462
Epoch 139: val_loss did not improve from 0.09536
852/852 [==============================] - 3s 3ms/step - loss: 0.1132 - accuracy: 0.9463 - val_loss: 0.1092 - val_accuracy: 0.9506
Epoch 140/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.1184 - accuracy: 0.9447
Epoch 140: val_loss improved from 0.09536 to 0.09105, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.1187 - accuracy: 0.9446 - val_loss: 0.0910 - val_accuracy: 0.9587
Epoch 141/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.1103 - accuracy: 0.9468
Epoch 141: val_loss did not improve from 0.09105
852/852 [==============================] - 3s 3ms/step - loss: 0.1095 - accuracy: 0.9474 - val_loss: 0.1016 - val_accuracy: 0.9515
Epoch 142/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.1192 - accuracy: 0.9458
Epoch 142: val_loss did not improve from 0.09105
852/852 [==============================] - 3s 3ms/step - loss: 0.1190 - accuracy: 0.9459 - val_loss: 0.1033 - val_accuracy: 0.9519
Epoch 143/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.1150 - accuracy: 0.9481
Epoch 143: val_loss did not improve from 0.09105
852/852 [==============================] - 2s 2ms/step - loss: 0.1155 - accuracy: 0.9476 - val_loss: 0.1159 - val_accuracy: 0.9450
Epoch 144/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.1240 - accuracy: 0.9441
Epoch 144: val_loss did not improve from 0.09105
852/852 [==============================] - 2s 3ms/step - loss: 0.1238 - accuracy: 0.9441 - val_loss: 0.0949 - val_accuracy: 0.9555
Epoch 145/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.1088 - accuracy: 0.9480
Epoch 145: val_loss did not improve from 0.09105
852/852 [==============================] - 2s 3ms/step - loss: 0.1087 - accuracy: 0.9481 - val_loss: 0.0992 - val_accuracy: 0.9527
Epoch 146/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.1132 - accuracy: 0.9472
Epoch 146: val_loss did not improve from 0.09105
852/852 [==============================] - 2s 2ms/step - loss: 0.1134 - accuracy: 0.9470 - val_loss: 0.1240 - val_accuracy: 0.9449
Epoch 147/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.1249 - accuracy: 0.9430
Epoch 147: val_loss did not improve from 0.09105
852/852 [==============================] - 2s 2ms/step - loss: 0.1249 - accuracy: 0.9428 - val_loss: 0.1004 - val_accuracy: 0.9526
Epoch 148/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.1206 - accuracy: 0.9427
Epoch 148: val_loss did not improve from 0.09105
852/852 [==============================] - 2s 3ms/step - loss: 0.1202 - accuracy: 0.9429 - val_loss: 0.1010 - val_accuracy: 0.9528
Epoch 149/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.1248 - accuracy: 0.9429
Epoch 149: val_loss did not improve from 0.09105
852/852 [==============================] - 2s 3ms/step - loss: 0.1247 - accuracy: 0.9430 - val_loss: 0.0961 - val_accuracy: 0.9560
Epoch 150/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.1182 - accuracy: 0.9459
Epoch 150: val_loss did not improve from 0.09105
852/852 [==============================] - 2s 3ms/step - loss: 0.1180 - accuracy: 0.9459 - val_loss: 0.1155 - val_accuracy: 0.9509
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=cab79f8f-83ba-41f9-be2a-0d6beec19a85">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [103]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">history_497</span> <span class="o">=</span> <span class="n">four_ninetyseven_neuron</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">callback_a</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.1035 - accuracy: 0.9501
Epoch 1: val_loss did not improve from 0.09105
852/852 [==============================] - 3s 3ms/step - loss: 0.1032 - accuracy: 0.9499 - val_loss: 0.0986 - val_accuracy: 0.9521
Epoch 2/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.1142 - accuracy: 0.9447
Epoch 2: val_loss did not improve from 0.09105
852/852 [==============================] - 2s 3ms/step - loss: 0.1143 - accuracy: 0.9446 - val_loss: 0.1273 - val_accuracy: 0.9434
Epoch 3/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.1212 - accuracy: 0.9418
Epoch 3: val_loss did not improve from 0.09105
852/852 [==============================] - 3s 3ms/step - loss: 0.1216 - accuracy: 0.9415 - val_loss: 0.1117 - val_accuracy: 0.9513
Epoch 4/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.1234 - accuracy: 0.9433
Epoch 4: val_loss did not improve from 0.09105
852/852 [==============================] - 3s 3ms/step - loss: 0.1236 - accuracy: 0.9432 - val_loss: 0.0923 - val_accuracy: 0.9564
Epoch 5/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.1076 - accuracy: 0.9463
Epoch 5: val_loss did not improve from 0.09105
852/852 [==============================] - 2s 3ms/step - loss: 0.1078 - accuracy: 0.9460 - val_loss: 0.1010 - val_accuracy: 0.9515
Epoch 6/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.1146 - accuracy: 0.9465
Epoch 6: val_loss did not improve from 0.09105
852/852 [==============================] - 2s 3ms/step - loss: 0.1146 - accuracy: 0.9467 - val_loss: 0.1118 - val_accuracy: 0.9499
Epoch 7/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.1277 - accuracy: 0.9397
Epoch 7: val_loss did not improve from 0.09105
852/852 [==============================] - 2s 3ms/step - loss: 0.1275 - accuracy: 0.9398 - val_loss: 0.1117 - val_accuracy: 0.9516
Epoch 8/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.1214 - accuracy: 0.9430
Epoch 8: val_loss did not improve from 0.09105
852/852 [==============================] - 3s 3ms/step - loss: 0.1211 - accuracy: 0.9433 - val_loss: 0.1043 - val_accuracy: 0.9508
Epoch 9/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.1200 - accuracy: 0.9426
Epoch 9: val_loss did not improve from 0.09105
852/852 [==============================] - 2s 3ms/step - loss: 0.1196 - accuracy: 0.9427 - val_loss: 0.0996 - val_accuracy: 0.9517
Epoch 10/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.1055 - accuracy: 0.9460
Epoch 10: val_loss did not improve from 0.09105
852/852 [==============================] - 2s 3ms/step - loss: 0.1051 - accuracy: 0.9463 - val_loss: 0.1024 - val_accuracy: 0.9531
Epoch 11/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.1126 - accuracy: 0.9437
Epoch 11: val_loss improved from 0.09105 to 0.09012, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.1122 - accuracy: 0.9441 - val_loss: 0.0901 - val_accuracy: 0.9563
Epoch 12/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.1183 - accuracy: 0.9437
Epoch 12: val_loss did not improve from 0.09012
852/852 [==============================] - 3s 3ms/step - loss: 0.1197 - accuracy: 0.9434 - val_loss: 0.1145 - val_accuracy: 0.9481
Epoch 13/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.1086 - accuracy: 0.9495
Epoch 13: val_loss did not improve from 0.09012
852/852 [==============================] - 4s 5ms/step - loss: 0.1086 - accuracy: 0.9494 - val_loss: 0.1000 - val_accuracy: 0.9522
Epoch 14/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.1141 - accuracy: 0.9462
Epoch 14: val_loss did not improve from 0.09012
852/852 [==============================] - 4s 5ms/step - loss: 0.1141 - accuracy: 0.9462 - val_loss: 0.1091 - val_accuracy: 0.9504
Epoch 15/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.1159 - accuracy: 0.9474
Epoch 15: val_loss did not improve from 0.09012
852/852 [==============================] - 4s 4ms/step - loss: 0.1155 - accuracy: 0.9477 - val_loss: 0.1075 - val_accuracy: 0.9496
Epoch 16/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.1147 - accuracy: 0.9438
Epoch 16: val_loss did not improve from 0.09012
852/852 [==============================] - 3s 3ms/step - loss: 0.1151 - accuracy: 0.9436 - val_loss: 0.0995 - val_accuracy: 0.9522
Epoch 17/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.1062 - accuracy: 0.9492
Epoch 17: val_loss did not improve from 0.09012
852/852 [==============================] - 2s 3ms/step - loss: 0.1065 - accuracy: 0.9489 - val_loss: 0.1133 - val_accuracy: 0.9480
Epoch 18/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.1210 - accuracy: 0.9418
Epoch 18: val_loss did not improve from 0.09012
852/852 [==============================] - 3s 3ms/step - loss: 0.1204 - accuracy: 0.9421 - val_loss: 0.0955 - val_accuracy: 0.9546
Epoch 19/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.1136 - accuracy: 0.9479
Epoch 19: val_loss did not improve from 0.09012
852/852 [==============================] - 3s 3ms/step - loss: 0.1135 - accuracy: 0.9480 - val_loss: 0.1048 - val_accuracy: 0.9527
Epoch 20/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.1171 - accuracy: 0.9455
Epoch 20: val_loss did not improve from 0.09012
852/852 [==============================] - 2s 3ms/step - loss: 0.1178 - accuracy: 0.9456 - val_loss: 0.1141 - val_accuracy: 0.9483
Epoch 21/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.1169 - accuracy: 0.9454
Epoch 21: val_loss did not improve from 0.09012
852/852 [==============================] - 2s 3ms/step - loss: 0.1172 - accuracy: 0.9454 - val_loss: 0.0973 - val_accuracy: 0.9554
Epoch 22/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.1133 - accuracy: 0.9476
Epoch 22: val_loss did not improve from 0.09012
852/852 [==============================] - 2s 3ms/step - loss: 0.1130 - accuracy: 0.9477 - val_loss: 0.0953 - val_accuracy: 0.9542
Epoch 23/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.1136 - accuracy: 0.9469
Epoch 23: val_loss did not improve from 0.09012
852/852 [==============================] - 3s 3ms/step - loss: 0.1142 - accuracy: 0.9468 - val_loss: 0.1115 - val_accuracy: 0.9524
Epoch 24/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.1140 - accuracy: 0.9462
Epoch 24: val_loss did not improve from 0.09012
852/852 [==============================] - 2s 3ms/step - loss: 0.1138 - accuracy: 0.9461 - val_loss: 0.0927 - val_accuracy: 0.9536
Epoch 25/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.1104 - accuracy: 0.9454
Epoch 25: val_loss did not improve from 0.09012
852/852 [==============================] - 3s 3ms/step - loss: 0.1105 - accuracy: 0.9453 - val_loss: 0.1055 - val_accuracy: 0.9509
Epoch 26/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.1118 - accuracy: 0.9460
Epoch 26: val_loss did not improve from 0.09012
852/852 [==============================] - 2s 3ms/step - loss: 0.1124 - accuracy: 0.9455 - val_loss: 0.1105 - val_accuracy: 0.9493
Epoch 27/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.1179 - accuracy: 0.9456
Epoch 27: val_loss did not improve from 0.09012
852/852 [==============================] - 2s 3ms/step - loss: 0.1185 - accuracy: 0.9452 - val_loss: 0.1010 - val_accuracy: 0.9530
Epoch 28/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.1031 - accuracy: 0.9493
Epoch 28: val_loss did not improve from 0.09012
852/852 [==============================] - 2s 3ms/step - loss: 0.1037 - accuracy: 0.9492 - val_loss: 0.1106 - val_accuracy: 0.9494
Epoch 29/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.1143 - accuracy: 0.9455
Epoch 29: val_loss did not improve from 0.09012
852/852 [==============================] - 3s 3ms/step - loss: 0.1142 - accuracy: 0.9454 - val_loss: 0.1038 - val_accuracy: 0.9490
Epoch 30/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.1210 - accuracy: 0.9444
Epoch 30: val_loss improved from 0.09012 to 0.08764, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.1208 - accuracy: 0.9445 - val_loss: 0.0876 - val_accuracy: 0.9571
Epoch 31/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.1029 - accuracy: 0.9471
Epoch 31: val_loss did not improve from 0.08764
852/852 [==============================] - 2s 3ms/step - loss: 0.1039 - accuracy: 0.9469 - val_loss: 0.0964 - val_accuracy: 0.9544
Epoch 32/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.1209 - accuracy: 0.9437
Epoch 32: val_loss did not improve from 0.08764
852/852 [==============================] - 3s 3ms/step - loss: 0.1216 - accuracy: 0.9433 - val_loss: 0.1101 - val_accuracy: 0.9484
Epoch 33/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.1142 - accuracy: 0.9469
Epoch 33: val_loss did not improve from 0.08764
852/852 [==============================] - 2s 3ms/step - loss: 0.1145 - accuracy: 0.9467 - val_loss: 0.0947 - val_accuracy: 0.9563
Epoch 34/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.1062 - accuracy: 0.9491
Epoch 34: val_loss did not improve from 0.08764
852/852 [==============================] - 3s 3ms/step - loss: 0.1051 - accuracy: 0.9494 - val_loss: 0.1059 - val_accuracy: 0.9515
Epoch 35/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.1272 - accuracy: 0.9432
Epoch 35: val_loss did not improve from 0.08764
852/852 [==============================] - 3s 3ms/step - loss: 0.1269 - accuracy: 0.9432 - val_loss: 0.0990 - val_accuracy: 0.9516
Epoch 36/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.1081 - accuracy: 0.9481
Epoch 36: val_loss did not improve from 0.08764
852/852 [==============================] - 4s 4ms/step - loss: 0.1080 - accuracy: 0.9481 - val_loss: 0.0947 - val_accuracy: 0.9555
Epoch 37/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.1188 - accuracy: 0.9454
Epoch 37: val_loss did not improve from 0.08764
852/852 [==============================] - 2s 3ms/step - loss: 0.1183 - accuracy: 0.9456 - val_loss: 0.1100 - val_accuracy: 0.9515
Epoch 38/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.1240 - accuracy: 0.9439
Epoch 38: val_loss did not improve from 0.08764
852/852 [==============================] - 3s 3ms/step - loss: 0.1246 - accuracy: 0.9434 - val_loss: 0.0939 - val_accuracy: 0.9574
Epoch 39/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.1038 - accuracy: 0.9482
Epoch 39: val_loss did not improve from 0.08764
852/852 [==============================] - 3s 3ms/step - loss: 0.1034 - accuracy: 0.9483 - val_loss: 0.0920 - val_accuracy: 0.9558
Epoch 40/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.1061 - accuracy: 0.9472
Epoch 40: val_loss did not improve from 0.08764
852/852 [==============================] - 2s 3ms/step - loss: 0.1058 - accuracy: 0.9472 - val_loss: 0.1042 - val_accuracy: 0.9537
Epoch 41/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.1193 - accuracy: 0.9456
Epoch 41: val_loss did not improve from 0.08764
852/852 [==============================] - 2s 3ms/step - loss: 0.1194 - accuracy: 0.9455 - val_loss: 0.0954 - val_accuracy: 0.9556
Epoch 42/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.1187 - accuracy: 0.9451
Epoch 42: val_loss did not improve from 0.08764
852/852 [==============================] - 2s 3ms/step - loss: 0.1188 - accuracy: 0.9453 - val_loss: 0.0987 - val_accuracy: 0.9528
Epoch 43/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.1008 - accuracy: 0.9516
Epoch 43: val_loss did not improve from 0.08764
852/852 [==============================] - 2s 3ms/step - loss: 0.1007 - accuracy: 0.9516 - val_loss: 0.0998 - val_accuracy: 0.9516
Epoch 44/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.1186 - accuracy: 0.9427
Epoch 44: val_loss did not improve from 0.08764
852/852 [==============================] - 3s 3ms/step - loss: 0.1185 - accuracy: 0.9426 - val_loss: 0.0969 - val_accuracy: 0.9528
Epoch 45/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.1121 - accuracy: 0.9469
Epoch 45: val_loss did not improve from 0.08764
852/852 [==============================] - 2s 3ms/step - loss: 0.1113 - accuracy: 0.9473 - val_loss: 0.0967 - val_accuracy: 0.9530
Epoch 46/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.1101 - accuracy: 0.9475
Epoch 46: val_loss did not improve from 0.08764
852/852 [==============================] - 3s 3ms/step - loss: 0.1092 - accuracy: 0.9477 - val_loss: 0.1025 - val_accuracy: 0.9497
Epoch 47/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.1188 - accuracy: 0.9441
Epoch 47: val_loss did not improve from 0.08764
852/852 [==============================] - 2s 3ms/step - loss: 0.1201 - accuracy: 0.9438 - val_loss: 0.0987 - val_accuracy: 0.9524
Epoch 48/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.1107 - accuracy: 0.9474
Epoch 48: val_loss did not improve from 0.08764
852/852 [==============================] - 2s 3ms/step - loss: 0.1139 - accuracy: 0.9470 - val_loss: 0.1186 - val_accuracy: 0.9452
Epoch 49/150
852/852 [==============================] - ETA: 0s - loss: 0.1131 - accuracy: 0.9486
Epoch 49: val_loss did not improve from 0.08764
852/852 [==============================] - 2s 3ms/step - loss: 0.1131 - accuracy: 0.9486 - val_loss: 0.0923 - val_accuracy: 0.9569
Epoch 50/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.1031 - accuracy: 0.9498
Epoch 50: val_loss did not improve from 0.08764
852/852 [==============================] - 2s 3ms/step - loss: 0.1037 - accuracy: 0.9494 - val_loss: 0.0917 - val_accuracy: 0.9577
Epoch 51/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.1076 - accuracy: 0.9475
Epoch 51: val_loss did not improve from 0.08764
852/852 [==============================] - 2s 3ms/step - loss: 0.1081 - accuracy: 0.9473 - val_loss: 0.0963 - val_accuracy: 0.9566
Epoch 52/150
825/852 [============================&gt;.] - ETA: 0s - loss: 0.1154 - accuracy: 0.9472
Epoch 52: val_loss did not improve from 0.08764
852/852 [==============================] - 2s 2ms/step - loss: 0.1175 - accuracy: 0.9460 - val_loss: 0.1023 - val_accuracy: 0.9527
Epoch 53/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.1055 - accuracy: 0.9486
Epoch 53: val_loss did not improve from 0.08764
852/852 [==============================] - 2s 3ms/step - loss: 0.1055 - accuracy: 0.9487 - val_loss: 0.1004 - val_accuracy: 0.9529
Epoch 54/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.1156 - accuracy: 0.9456
Epoch 54: val_loss did not improve from 0.08764
852/852 [==============================] - 2s 3ms/step - loss: 0.1153 - accuracy: 0.9459 - val_loss: 0.1002 - val_accuracy: 0.9524
Epoch 55/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.1135 - accuracy: 0.9460
Epoch 55: val_loss did not improve from 0.08764
852/852 [==============================] - 2s 3ms/step - loss: 0.1131 - accuracy: 0.9462 - val_loss: 0.1126 - val_accuracy: 0.9480
Epoch 56/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.1101 - accuracy: 0.9472
Epoch 56: val_loss did not improve from 0.08764
852/852 [==============================] - 2s 3ms/step - loss: 0.1096 - accuracy: 0.9476 - val_loss: 0.1231 - val_accuracy: 0.9460
Epoch 57/150
821/852 [===========================&gt;..] - ETA: 0s - loss: 0.1098 - accuracy: 0.9490
Epoch 57: val_loss did not improve from 0.08764
852/852 [==============================] - 2s 3ms/step - loss: 0.1100 - accuracy: 0.9486 - val_loss: 0.1009 - val_accuracy: 0.9509
Epoch 58/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.1186 - accuracy: 0.9442
Epoch 58: val_loss did not improve from 0.08764
852/852 [==============================] - 2s 3ms/step - loss: 0.1186 - accuracy: 0.9442 - val_loss: 0.0938 - val_accuracy: 0.9537
Epoch 59/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.1059 - accuracy: 0.9497
Epoch 59: val_loss did not improve from 0.08764
852/852 [==============================] - 2s 3ms/step - loss: 0.1060 - accuracy: 0.9494 - val_loss: 0.0964 - val_accuracy: 0.9548
Epoch 60/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.1029 - accuracy: 0.9511
Epoch 60: val_loss did not improve from 0.08764
852/852 [==============================] - 4s 5ms/step - loss: 0.1031 - accuracy: 0.9514 - val_loss: 0.1095 - val_accuracy: 0.9499
Epoch 61/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.1055 - accuracy: 0.9504
Epoch 61: val_loss did not improve from 0.08764
852/852 [==============================] - 5s 6ms/step - loss: 0.1056 - accuracy: 0.9503 - val_loss: 0.1077 - val_accuracy: 0.9509
Epoch 62/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.1203 - accuracy: 0.9431
Epoch 62: val_loss did not improve from 0.08764
852/852 [==============================] - 5s 6ms/step - loss: 0.1202 - accuracy: 0.9432 - val_loss: 0.0921 - val_accuracy: 0.9560
Epoch 63/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.1149 - accuracy: 0.9475
Epoch 63: val_loss did not improve from 0.08764
852/852 [==============================] - 4s 5ms/step - loss: 0.1144 - accuracy: 0.9479 - val_loss: 0.0968 - val_accuracy: 0.9548
Epoch 64/150
850/852 [============================&gt;.] - ETA: 0s - loss: 0.1145 - accuracy: 0.9451
Epoch 64: val_loss improved from 0.08764 to 0.08556, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 4s 4ms/step - loss: 0.1144 - accuracy: 0.9452 - val_loss: 0.0856 - val_accuracy: 0.9591
Epoch 65/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.0996 - accuracy: 0.9499
Epoch 65: val_loss did not improve from 0.08556
852/852 [==============================] - 3s 3ms/step - loss: 0.0991 - accuracy: 0.9501 - val_loss: 0.0991 - val_accuracy: 0.9548
Epoch 66/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.1152 - accuracy: 0.9448
Epoch 66: val_loss did not improve from 0.08556
852/852 [==============================] - 4s 4ms/step - loss: 0.1150 - accuracy: 0.9448 - val_loss: 0.0971 - val_accuracy: 0.9549
Epoch 67/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.1118 - accuracy: 0.9462
Epoch 67: val_loss did not improve from 0.08556
852/852 [==============================] - 3s 4ms/step - loss: 0.1113 - accuracy: 0.9465 - val_loss: 0.0906 - val_accuracy: 0.9582
Epoch 68/150
828/852 [============================&gt;.] - ETA: 0s - loss: 0.1065 - accuracy: 0.9489
Epoch 68: val_loss did not improve from 0.08556
852/852 [==============================] - 2s 3ms/step - loss: 0.1070 - accuracy: 0.9486 - val_loss: 0.1184 - val_accuracy: 0.9434
Epoch 69/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.1238 - accuracy: 0.9407
Epoch 69: val_loss did not improve from 0.08556
852/852 [==============================] - 3s 3ms/step - loss: 0.1233 - accuracy: 0.9411 - val_loss: 0.1110 - val_accuracy: 0.9523
Epoch 70/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.1098 - accuracy: 0.9480
Epoch 70: val_loss did not improve from 0.08556
852/852 [==============================] - 3s 3ms/step - loss: 0.1101 - accuracy: 0.9479 - val_loss: 0.1191 - val_accuracy: 0.9448
Epoch 71/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.1298 - accuracy: 0.9476
Epoch 71: val_loss did not improve from 0.08556
852/852 [==============================] - 2s 3ms/step - loss: 0.1292 - accuracy: 0.9477 - val_loss: 0.1068 - val_accuracy: 0.9517
Epoch 72/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.1074 - accuracy: 0.9498
Epoch 72: val_loss improved from 0.08556 to 0.08356, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 3s 3ms/step - loss: 0.1076 - accuracy: 0.9494 - val_loss: 0.0836 - val_accuracy: 0.9595
Epoch 73/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.0955 - accuracy: 0.9522
Epoch 73: val_loss did not improve from 0.08356
852/852 [==============================] - 3s 3ms/step - loss: 0.0965 - accuracy: 0.9521 - val_loss: 0.0910 - val_accuracy: 0.9574
Epoch 74/150
852/852 [==============================] - ETA: 0s - loss: 0.1151 - accuracy: 0.9473
Epoch 74: val_loss did not improve from 0.08356
852/852 [==============================] - 2s 3ms/step - loss: 0.1151 - accuracy: 0.9473 - val_loss: 0.0976 - val_accuracy: 0.9553
Epoch 75/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.1012 - accuracy: 0.9510
Epoch 75: val_loss did not improve from 0.08356
852/852 [==============================] - 2s 3ms/step - loss: 0.1010 - accuracy: 0.9512 - val_loss: 0.0863 - val_accuracy: 0.9588
Epoch 76/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.1071 - accuracy: 0.9492
Epoch 76: val_loss did not improve from 0.08356
852/852 [==============================] - 2s 3ms/step - loss: 0.1068 - accuracy: 0.9493 - val_loss: 0.1006 - val_accuracy: 0.9530
Epoch 77/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.1084 - accuracy: 0.9479
Epoch 77: val_loss did not improve from 0.08356
852/852 [==============================] - 2s 3ms/step - loss: 0.1091 - accuracy: 0.9466 - val_loss: 0.0894 - val_accuracy: 0.9577
Epoch 78/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.1007 - accuracy: 0.9490
Epoch 78: val_loss did not improve from 0.08356
852/852 [==============================] - 2s 3ms/step - loss: 0.1014 - accuracy: 0.9488 - val_loss: 0.0916 - val_accuracy: 0.9571
Epoch 79/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.1198 - accuracy: 0.9456
Epoch 79: val_loss did not improve from 0.08356
852/852 [==============================] - 3s 4ms/step - loss: 0.1198 - accuracy: 0.9456 - val_loss: 0.0994 - val_accuracy: 0.9550
Epoch 80/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.0983 - accuracy: 0.9514
Epoch 80: val_loss did not improve from 0.08356
852/852 [==============================] - 3s 3ms/step - loss: 0.0982 - accuracy: 0.9514 - val_loss: 0.0941 - val_accuracy: 0.9561
Epoch 81/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.1156 - accuracy: 0.9454
Epoch 81: val_loss did not improve from 0.08356
852/852 [==============================] - 2s 3ms/step - loss: 0.1160 - accuracy: 0.9455 - val_loss: 0.1109 - val_accuracy: 0.9504
Epoch 82/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.1026 - accuracy: 0.9501
Epoch 82: val_loss did not improve from 0.08356
852/852 [==============================] - 2s 3ms/step - loss: 0.1019 - accuracy: 0.9506 - val_loss: 0.0851 - val_accuracy: 0.9588
Epoch 83/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.1097 - accuracy: 0.9501
Epoch 83: val_loss did not improve from 0.08356
852/852 [==============================] - 2s 3ms/step - loss: 0.1096 - accuracy: 0.9503 - val_loss: 0.0925 - val_accuracy: 0.9561
Epoch 84/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.1073 - accuracy: 0.9490
Epoch 84: val_loss did not improve from 0.08356
852/852 [==============================] - 2s 3ms/step - loss: 0.1075 - accuracy: 0.9489 - val_loss: 0.0883 - val_accuracy: 0.9597
Epoch 85/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.1096 - accuracy: 0.9482
Epoch 85: val_loss did not improve from 0.08356
852/852 [==============================] - 3s 3ms/step - loss: 0.1098 - accuracy: 0.9481 - val_loss: 0.0995 - val_accuracy: 0.9561
Epoch 86/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.1053 - accuracy: 0.9495
Epoch 86: val_loss did not improve from 0.08356
852/852 [==============================] - 2s 3ms/step - loss: 0.1058 - accuracy: 0.9490 - val_loss: 0.1162 - val_accuracy: 0.9507
Epoch 87/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.1082 - accuracy: 0.9486
Epoch 87: val_loss did not improve from 0.08356
852/852 [==============================] - 2s 3ms/step - loss: 0.1096 - accuracy: 0.9481 - val_loss: 0.0990 - val_accuracy: 0.9521
Epoch 88/150
832/852 [============================&gt;.] - ETA: 0s - loss: 0.1082 - accuracy: 0.9510
Epoch 88: val_loss did not improve from 0.08356
852/852 [==============================] - 2s 3ms/step - loss: 0.1082 - accuracy: 0.9508 - val_loss: 0.0932 - val_accuracy: 0.9546
Epoch 89/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.1068 - accuracy: 0.9488
Epoch 89: val_loss did not improve from 0.08356
852/852 [==============================] - 2s 3ms/step - loss: 0.1068 - accuracy: 0.9488 - val_loss: 0.0918 - val_accuracy: 0.9554
Epoch 90/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.1054 - accuracy: 0.9486
Epoch 90: val_loss did not improve from 0.08356
852/852 [==============================] - 2s 3ms/step - loss: 0.1053 - accuracy: 0.9487 - val_loss: 0.0920 - val_accuracy: 0.9558
Epoch 91/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.1085 - accuracy: 0.9489
Epoch 91: val_loss did not improve from 0.08356
852/852 [==============================] - 2s 3ms/step - loss: 0.1086 - accuracy: 0.9488 - val_loss: 0.1087 - val_accuracy: 0.9508
Epoch 92/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.1136 - accuracy: 0.9481
Epoch 92: val_loss did not improve from 0.08356
852/852 [==============================] - 3s 3ms/step - loss: 0.1133 - accuracy: 0.9481 - val_loss: 0.0956 - val_accuracy: 0.9549
Epoch 93/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.1063 - accuracy: 0.9486
Epoch 93: val_loss did not improve from 0.08356
852/852 [==============================] - 2s 3ms/step - loss: 0.1066 - accuracy: 0.9483 - val_loss: 0.0898 - val_accuracy: 0.9562
Epoch 94/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.1012 - accuracy: 0.9514
Epoch 94: val_loss improved from 0.08356 to 0.08279, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.1025 - accuracy: 0.9510 - val_loss: 0.0828 - val_accuracy: 0.9605
Epoch 95/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.1143 - accuracy: 0.9442
Epoch 95: val_loss did not improve from 0.08279
852/852 [==============================] - 2s 3ms/step - loss: 0.1147 - accuracy: 0.9438 - val_loss: 0.1055 - val_accuracy: 0.9501
Epoch 96/150
852/852 [==============================] - ETA: 0s - loss: 0.1099 - accuracy: 0.9483
Epoch 96: val_loss did not improve from 0.08279
852/852 [==============================] - 2s 3ms/step - loss: 0.1099 - accuracy: 0.9483 - val_loss: 0.0913 - val_accuracy: 0.9554
Epoch 97/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.1000 - accuracy: 0.9525
Epoch 97: val_loss did not improve from 0.08279
852/852 [==============================] - 2s 3ms/step - loss: 0.0999 - accuracy: 0.9526 - val_loss: 0.0925 - val_accuracy: 0.9554
Epoch 98/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.1113 - accuracy: 0.9447
Epoch 98: val_loss did not improve from 0.08279
852/852 [==============================] - 2s 3ms/step - loss: 0.1129 - accuracy: 0.9445 - val_loss: 0.1136 - val_accuracy: 0.9468
Epoch 99/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.1084 - accuracy: 0.9484
Epoch 99: val_loss did not improve from 0.08279
852/852 [==============================] - 2s 3ms/step - loss: 0.1085 - accuracy: 0.9475 - val_loss: 0.0879 - val_accuracy: 0.9566
Epoch 100/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.1041 - accuracy: 0.9479
Epoch 100: val_loss did not improve from 0.08279
852/852 [==============================] - 2s 3ms/step - loss: 0.1039 - accuracy: 0.9479 - val_loss: 0.0873 - val_accuracy: 0.9596
Epoch 101/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.1070 - accuracy: 0.9489
Epoch 101: val_loss did not improve from 0.08279
852/852 [==============================] - 2s 3ms/step - loss: 0.1066 - accuracy: 0.9490 - val_loss: 0.1034 - val_accuracy: 0.9547
Epoch 102/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.1200 - accuracy: 0.9481
Epoch 102: val_loss did not improve from 0.08279
852/852 [==============================] - 2s 3ms/step - loss: 0.1202 - accuracy: 0.9474 - val_loss: 0.0947 - val_accuracy: 0.9569
Epoch 103/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.0981 - accuracy: 0.9525
Epoch 103: val_loss did not improve from 0.08279
852/852 [==============================] - 2s 3ms/step - loss: 0.0987 - accuracy: 0.9522 - val_loss: 0.0897 - val_accuracy: 0.9569
Epoch 104/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.1169 - accuracy: 0.9477
Epoch 104: val_loss did not improve from 0.08279
852/852 [==============================] - 4s 4ms/step - loss: 0.1171 - accuracy: 0.9470 - val_loss: 0.0955 - val_accuracy: 0.9553
Epoch 105/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.1112 - accuracy: 0.9474
Epoch 105: val_loss did not improve from 0.08279
852/852 [==============================] - 5s 5ms/step - loss: 0.1115 - accuracy: 0.9473 - val_loss: 0.1037 - val_accuracy: 0.9504
Epoch 106/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.1013 - accuracy: 0.9506
Epoch 106: val_loss did not improve from 0.08279
852/852 [==============================] - 4s 4ms/step - loss: 0.1015 - accuracy: 0.9507 - val_loss: 0.0867 - val_accuracy: 0.9573
Epoch 107/150
844/852 [============================&gt;.] - ETA: 0s - loss: 0.1016 - accuracy: 0.9506
Epoch 107: val_loss did not improve from 0.08279
852/852 [==============================] - 4s 4ms/step - loss: 0.1024 - accuracy: 0.9504 - val_loss: 0.0909 - val_accuracy: 0.9561
Epoch 108/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.1156 - accuracy: 0.9466
Epoch 108: val_loss did not improve from 0.08279
852/852 [==============================] - 3s 3ms/step - loss: 0.1169 - accuracy: 0.9462 - val_loss: 0.1031 - val_accuracy: 0.9521
Epoch 109/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.0996 - accuracy: 0.9498
Epoch 109: val_loss did not improve from 0.08279
852/852 [==============================] - 3s 3ms/step - loss: 0.0996 - accuracy: 0.9499 - val_loss: 0.0900 - val_accuracy: 0.9567
Epoch 110/150
839/852 [============================&gt;.] - ETA: 0s - loss: 0.1105 - accuracy: 0.9476
Epoch 110: val_loss did not improve from 0.08279
852/852 [==============================] - 3s 3ms/step - loss: 0.1109 - accuracy: 0.9474 - val_loss: 0.1029 - val_accuracy: 0.9533
Epoch 111/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.0961 - accuracy: 0.9523
Epoch 111: val_loss did not improve from 0.08279
852/852 [==============================] - 2s 3ms/step - loss: 0.0970 - accuracy: 0.9519 - val_loss: 0.0985 - val_accuracy: 0.9529
Epoch 112/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.1111 - accuracy: 0.9478
Epoch 112: val_loss did not improve from 0.08279
852/852 [==============================] - 2s 3ms/step - loss: 0.1135 - accuracy: 0.9473 - val_loss: 0.1076 - val_accuracy: 0.9527
Epoch 113/150
837/852 [============================&gt;.] - ETA: 0s - loss: 0.1133 - accuracy: 0.9476
Epoch 113: val_loss did not improve from 0.08279
852/852 [==============================] - 3s 3ms/step - loss: 0.1125 - accuracy: 0.9477 - val_loss: 0.0889 - val_accuracy: 0.9588
Epoch 114/150
823/852 [===========================&gt;..] - ETA: 0s - loss: 0.1020 - accuracy: 0.9515
Epoch 114: val_loss did not improve from 0.08279
852/852 [==============================] - 2s 3ms/step - loss: 0.1016 - accuracy: 0.9514 - val_loss: 0.0960 - val_accuracy: 0.9548
Epoch 115/150
846/852 [============================&gt;.] - ETA: 0s - loss: 0.0960 - accuracy: 0.9535
Epoch 115: val_loss did not improve from 0.08279
852/852 [==============================] - 2s 3ms/step - loss: 0.0968 - accuracy: 0.9535 - val_loss: 0.0984 - val_accuracy: 0.9536
Epoch 116/150
849/852 [============================&gt;.] - ETA: 0s - loss: 0.1076 - accuracy: 0.9490
Epoch 116: val_loss did not improve from 0.08279
852/852 [==============================] - 2s 3ms/step - loss: 0.1074 - accuracy: 0.9490 - val_loss: 0.1294 - val_accuracy: 0.9432
Epoch 117/150
834/852 [============================&gt;.] - ETA: 0s - loss: 0.1042 - accuracy: 0.9518
Epoch 117: val_loss did not improve from 0.08279
852/852 [==============================] - 2s 3ms/step - loss: 0.1043 - accuracy: 0.9515 - val_loss: 0.0885 - val_accuracy: 0.9580
Epoch 118/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.1100 - accuracy: 0.9472
Epoch 118: val_loss did not improve from 0.08279
852/852 [==============================] - 2s 3ms/step - loss: 0.1099 - accuracy: 0.9472 - val_loss: 0.0988 - val_accuracy: 0.9544
Epoch 119/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.1058 - accuracy: 0.9503
Epoch 119: val_loss did not improve from 0.08279
852/852 [==============================] - 2s 3ms/step - loss: 0.1063 - accuracy: 0.9500 - val_loss: 0.1276 - val_accuracy: 0.9440
Epoch 120/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.1108 - accuracy: 0.9467
Epoch 120: val_loss did not improve from 0.08279
852/852 [==============================] - 2s 3ms/step - loss: 0.1108 - accuracy: 0.9467 - val_loss: 0.0927 - val_accuracy: 0.9561
Epoch 121/150
822/852 [===========================&gt;..] - ETA: 0s - loss: 0.1154 - accuracy: 0.9506
Epoch 121: val_loss did not improve from 0.08279
852/852 [==============================] - 2s 3ms/step - loss: 0.1154 - accuracy: 0.9503 - val_loss: 0.0949 - val_accuracy: 0.9575
Epoch 122/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.1012 - accuracy: 0.9525
Epoch 122: val_loss did not improve from 0.08279
852/852 [==============================] - 2s 3ms/step - loss: 0.1015 - accuracy: 0.9521 - val_loss: 0.0949 - val_accuracy: 0.9557
Epoch 123/150
851/852 [============================&gt;.] - ETA: 0s - loss: 0.1123 - accuracy: 0.9476
Epoch 123: val_loss did not improve from 0.08279
852/852 [==============================] - 2s 3ms/step - loss: 0.1122 - accuracy: 0.9476 - val_loss: 0.0936 - val_accuracy: 0.9561
Epoch 124/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.0950 - accuracy: 0.9534
Epoch 124: val_loss did not improve from 0.08279
852/852 [==============================] - 3s 3ms/step - loss: 0.0953 - accuracy: 0.9530 - val_loss: 0.0947 - val_accuracy: 0.9540
Epoch 125/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.1097 - accuracy: 0.9495
Epoch 125: val_loss did not improve from 0.08279
852/852 [==============================] - 2s 3ms/step - loss: 0.1099 - accuracy: 0.9489 - val_loss: 0.1009 - val_accuracy: 0.9528
Epoch 126/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.1059 - accuracy: 0.9502
Epoch 126: val_loss did not improve from 0.08279
852/852 [==============================] - 3s 3ms/step - loss: 0.1056 - accuracy: 0.9504 - val_loss: 0.1041 - val_accuracy: 0.9549
Epoch 127/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.1016 - accuracy: 0.9510
Epoch 127: val_loss did not improve from 0.08279
852/852 [==============================] - 3s 4ms/step - loss: 0.1019 - accuracy: 0.9508 - val_loss: 0.0901 - val_accuracy: 0.9582
Epoch 128/150
852/852 [==============================] - ETA: 0s - loss: 0.1156 - accuracy: 0.9492
Epoch 128: val_loss did not improve from 0.08279
852/852 [==============================] - 2s 2ms/step - loss: 0.1156 - accuracy: 0.9492 - val_loss: 0.1066 - val_accuracy: 0.9548
Epoch 129/150
824/852 [============================&gt;.] - ETA: 0s - loss: 0.1014 - accuracy: 0.9521
Epoch 129: val_loss did not improve from 0.08279
852/852 [==============================] - 2s 3ms/step - loss: 0.1007 - accuracy: 0.9528 - val_loss: 0.1030 - val_accuracy: 0.9517
Epoch 130/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.1089 - accuracy: 0.9504
Epoch 130: val_loss did not improve from 0.08279
852/852 [==============================] - 2s 3ms/step - loss: 0.1084 - accuracy: 0.9508 - val_loss: 0.1119 - val_accuracy: 0.9490
Epoch 131/150
826/852 [============================&gt;.] - ETA: 0s - loss: 0.1032 - accuracy: 0.9513
Epoch 131: val_loss did not improve from 0.08279
852/852 [==============================] - 2s 3ms/step - loss: 0.1022 - accuracy: 0.9520 - val_loss: 0.0886 - val_accuracy: 0.9569
Epoch 132/150
829/852 [============================&gt;.] - ETA: 0s - loss: 0.0988 - accuracy: 0.9511
Epoch 132: val_loss did not improve from 0.08279
852/852 [==============================] - 2s 3ms/step - loss: 0.1001 - accuracy: 0.9503 - val_loss: 0.1092 - val_accuracy: 0.9486
Epoch 133/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.1121 - accuracy: 0.9513
Epoch 133: val_loss did not improve from 0.08279
852/852 [==============================] - 2s 2ms/step - loss: 0.1124 - accuracy: 0.9509 - val_loss: 0.1036 - val_accuracy: 0.9539
Epoch 134/150
843/852 [============================&gt;.] - ETA: 0s - loss: 0.0979 - accuracy: 0.9526
Epoch 134: val_loss did not improve from 0.08279
852/852 [==============================] - 2s 3ms/step - loss: 0.0975 - accuracy: 0.9527 - val_loss: 0.0946 - val_accuracy: 0.9554
Epoch 135/150
833/852 [============================&gt;.] - ETA: 0s - loss: 0.0996 - accuracy: 0.9521
Epoch 135: val_loss did not improve from 0.08279
852/852 [==============================] - 2s 2ms/step - loss: 0.0995 - accuracy: 0.9524 - val_loss: 0.0898 - val_accuracy: 0.9574
Epoch 136/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.1001 - accuracy: 0.9521
Epoch 136: val_loss did not improve from 0.08279
852/852 [==============================] - 2s 2ms/step - loss: 0.1007 - accuracy: 0.9517 - val_loss: 0.0971 - val_accuracy: 0.9535
Epoch 137/150
848/852 [============================&gt;.] - ETA: 0s - loss: 0.1082 - accuracy: 0.9483
Epoch 137: val_loss did not improve from 0.08279
852/852 [==============================] - 2s 3ms/step - loss: 0.1085 - accuracy: 0.9482 - val_loss: 0.0855 - val_accuracy: 0.9587
Epoch 138/150
838/852 [============================&gt;.] - ETA: 0s - loss: 0.1143 - accuracy: 0.9461
Epoch 138: val_loss did not improve from 0.08279
852/852 [==============================] - 2s 2ms/step - loss: 0.1146 - accuracy: 0.9456 - val_loss: 0.1037 - val_accuracy: 0.9544
Epoch 139/150
831/852 [============================&gt;.] - ETA: 0s - loss: 0.1201 - accuracy: 0.9486
Epoch 139: val_loss did not improve from 0.08279
852/852 [==============================] - 2s 3ms/step - loss: 0.1199 - accuracy: 0.9484 - val_loss: 0.1058 - val_accuracy: 0.9535
Epoch 140/150
835/852 [============================&gt;.] - ETA: 0s - loss: 0.1128 - accuracy: 0.9499
Epoch 140: val_loss did not improve from 0.08279
852/852 [==============================] - 2s 2ms/step - loss: 0.1122 - accuracy: 0.9502 - val_loss: 0.0857 - val_accuracy: 0.9590
Epoch 141/150
847/852 [============================&gt;.] - ETA: 0s - loss: 0.0990 - accuracy: 0.9498
Epoch 141: val_loss did not improve from 0.08279
852/852 [==============================] - 2s 3ms/step - loss: 0.0999 - accuracy: 0.9494 - val_loss: 0.0902 - val_accuracy: 0.9564
Epoch 142/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.1010 - accuracy: 0.9518
Epoch 142: val_loss did not improve from 0.08279
852/852 [==============================] - 2s 2ms/step - loss: 0.1037 - accuracy: 0.9502 - val_loss: 0.1004 - val_accuracy: 0.9567
Epoch 143/150
845/852 [============================&gt;.] - ETA: 0s - loss: 0.0991 - accuracy: 0.9525
Epoch 143: val_loss did not improve from 0.08279
852/852 [==============================] - 2s 3ms/step - loss: 0.0990 - accuracy: 0.9526 - val_loss: 0.0834 - val_accuracy: 0.9590
Epoch 144/150
842/852 [============================&gt;.] - ETA: 0s - loss: 0.1054 - accuracy: 0.9494
Epoch 144: val_loss did not improve from 0.08279
852/852 [==============================] - 2s 3ms/step - loss: 0.1053 - accuracy: 0.9496 - val_loss: 0.0955 - val_accuracy: 0.9560
Epoch 145/150
836/852 [============================&gt;.] - ETA: 0s - loss: 0.1139 - accuracy: 0.9446
Epoch 145: val_loss did not improve from 0.08279
852/852 [==============================] - 2s 3ms/step - loss: 0.1140 - accuracy: 0.9447 - val_loss: 0.0831 - val_accuracy: 0.9617
Epoch 146/150
830/852 [============================&gt;.] - ETA: 0s - loss: 0.0989 - accuracy: 0.9519
Epoch 146: val_loss did not improve from 0.08279
852/852 [==============================] - 2s 2ms/step - loss: 0.0985 - accuracy: 0.9519 - val_loss: 0.1116 - val_accuracy: 0.9503
Epoch 147/150
840/852 [============================&gt;.] - ETA: 0s - loss: 0.1237 - accuracy: 0.9470
Epoch 147: val_loss did not improve from 0.08279
852/852 [==============================] - 2s 2ms/step - loss: 0.1233 - accuracy: 0.9473 - val_loss: 0.0928 - val_accuracy: 0.9543
Epoch 148/150
852/852 [==============================] - ETA: 0s - loss: 0.0924 - accuracy: 0.9536
Epoch 148: val_loss improved from 0.08279 to 0.08107, saving model to four_ninetyseven_model.hdf5
852/852 [==============================] - 2s 3ms/step - loss: 0.0924 - accuracy: 0.9536 - val_loss: 0.0811 - val_accuracy: 0.9611
Epoch 149/150
841/852 [============================&gt;.] - ETA: 0s - loss: 0.1031 - accuracy: 0.9516
Epoch 149: val_loss did not improve from 0.08107
852/852 [==============================] - 2s 2ms/step - loss: 0.1032 - accuracy: 0.9514 - val_loss: 0.1093 - val_accuracy: 0.9493
Epoch 150/150
827/852 [============================&gt;.] - ETA: 0s - loss: 0.1084 - accuracy: 0.9493
Epoch 150: val_loss did not improve from 0.08107
852/852 [==============================] - 2s 3ms/step - loss: 0.1099 - accuracy: 0.9489 - val_loss: 0.1126 - val_accuracy: 0.9542
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=625cf1b5-2da9-4d3b-b189-6f0d0bc21cbb">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [104]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">four_ninetyseven_neuron</span><span class="o">.</span><span class="n">load_weights</span><span class="p">(</span><span class="s1">'four_ninetyseven_model.hdf5'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=de835444-5561-43d0-9ab5-ffbad8dbb73e">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [105]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Other Metrics</span>
<span class="n">four_ninetyseven_neuron_preds</span> <span class="o">=</span> <span class="n">four_ninetyseven_neuron</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">four_ninetyseven_neuron_preds</span><span class="o">.</span><span class="n">round</span><span class="p">())</span>
<span class="n">precision</span> <span class="o">=</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">four_ninetyseven_neuron_preds</span><span class="o">.</span><span class="n">round</span><span class="p">())</span>
<span class="n">recall</span> <span class="o">=</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">four_ninetyseven_neuron_preds</span><span class="o">.</span><span class="n">round</span><span class="p">())</span>
<span class="n">f1score</span> <span class="o">=</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">four_ninetyseven_neuron_preds</span><span class="o">.</span><span class="n">round</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Accuracy: </span><span class="si">%.2f</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">accuracy</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Precision: </span><span class="si">%.2f</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">precision</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Recall: </span><span class="si">%.2f</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">recall</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"F1-score: </span><span class="si">%.2f</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">f1score</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>267/267 [==============================] - 0s 1ms/step
Accuracy: 0.96
Precision: 0.96
Recall: 0.96
F1-score: 0.96
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=e09e195e-203d-4d7f-8ffd-8df388c7c34c">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="Underfitting-vs.-Overfitting-in-a-Neural-Network">Underfitting vs. Overfitting in a Neural Network<a class="anchor-link" href="#Underfitting-vs.-Overfitting-in-a-Neural-Network">¶</a></h4><p>Underfit models have a high bias meaning they produce innaccurate results for the training data and test data sets. While overfit models have a high variance which means that they produce accurate results for the training set but not for the test set.</p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=64d88e86-732d-4276-a7f1-357284bf3909">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h5 id="Phase-2-Conclusion">Phase 2 Conclusion<a class="anchor-link" href="#Phase-2-Conclusion">¶</a></h5><p>I really struggled with this phase, I spent hours tweaking and removing and trying different input variable combinations trying to improve my accurracy in my initial models and was ultimately unsuccessful. I also attempted to adjust the learning speed to slower speeds such as 0.01,0.001, 0,0001, and 0.00001 as well as adjusting the batch sizes to 10, 20, 50, and number of epochs from 20, 60, 100, and 150 but accuracy for the 1 neuron model remained at the same 22%, the 3 neuron model with 2 layers had about 18% accuracy and the 7 neuron model with 3 layers had such a small accuracy it appeared to be 0 even in scientific notation, with some adjustments I was able to get the 7 neuron model accuracy up to 19%. Ultimately I moved on because it appeared that when using all the data my model was immediately overfit and performed poorly no matter what I attempted. My conclusion at this time was perhaps the data had too much noise so it was already overfit and that was why it had such poor results. <br/> 
I eventually moved onto Phase 3 and tried using some different normalization processes and had better results on those models. I eventually returned and redid Phase 2 with the same methods of normalization that worked best in Phase 3 and immediately had better results. There must have been something wrong with my initial normalization in the first models I built so they were not performing properly. With the corrected normalization my one neuron model was able to get up to about 72% accuracy pretty quickly. From there my other 8 larger models all improved, where as my initial poor models were decreasing in accuracy.</p>
<br/>
Due to time constraints, and having returned to improve this phase I am stopping with my last overfit model attempt with its 96% accuracy. Unfortunately I did not make it to 100% accurancy on any of these models but I do now feel more confident that I could identify an overfit model and am now more aware of the factors that can lead to an overfit model.

</div>
</div>
</div>
</div>
</main>
</body>
</html>
