{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38d056bb-061f-460c-a2eb-7226fe89027f",
   "metadata": {},
   "source": [
    "##  Banking Binary Classification Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5b55c3-7fb0-4f80-ae6b-48620875032a",
   "metadata": {},
   "source": [
    "### Problem\n",
    "We would like to predict if given a bank client, are they likely to make a term deposit or not at the bank.\n",
    "\n",
    "Dataset: bankingInfo.csv\n",
    "     - if output == yes, then the client has made a term deposit at this bank, else they have not.\n",
    "\n",
    "Project was built using the bc_ml_breast_cancer_pred.ipynb and the deep_learning.ipynb files provided by Dr. Sambriddhi Mainali along with the *A hands-on introduction to feed-forward neural networks using Tensorflow and Keras* Github Repository by Dr. Badri Adhikari found [here.](https://badriadhikari.github.io/AI-2022spring/NN-using-TF.html) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05813c1f-4424-4604-a872-e1b8d7252173",
   "metadata": {},
   "source": [
    "### Phase 4: Feature Importance and Reduction\n",
    "____\n",
    "#### 4.1 Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "020f6e3b-943f-4513-9f06-cbc2333260e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import keras\n",
    "import tensorflow \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68479ec1-9d39-4d9e-8c9d-b5499f75c812",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>...</th>\n",
       "      <th>entrepreneur</th>\n",
       "      <th>housemaid</th>\n",
       "      <th>management</th>\n",
       "      <th>retired</th>\n",
       "      <th>self-employed</th>\n",
       "      <th>services</th>\n",
       "      <th>student</th>\n",
       "      <th>technician</th>\n",
       "      <th>unemployed</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>93.444</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>93.918</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>93.918</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  education  housing  loan  campaign  pdays  previous  poutcome  \\\n",
       "0   36          2        0     0         9    999         0         0   \n",
       "1   43          1        0     0         2    999         0         0   \n",
       "2   27          4        1     0         3    999         0         0   \n",
       "3   28          3        1     0         3    999         0         0   \n",
       "4   25          4        1     0         5    999         0         0   \n",
       "\n",
       "   emp.var.rate  cons.price.idx  ...  entrepreneur  housemaid  management  \\\n",
       "0           1.4          93.444  ...             0          1           0   \n",
       "1           1.1          93.994  ...             0          0           0   \n",
       "2           1.4          93.918  ...             0          0           0   \n",
       "3           1.1          93.994  ...             0          0           0   \n",
       "4           1.4          93.918  ...             0          0           0   \n",
       "\n",
       "   retired  self-employed  services  student  technician  unemployed  output  \n",
       "0        0              0         0        0           0           0       0  \n",
       "1        0              0         0        0           0           0       0  \n",
       "2        0              0         1        0           0           0       0  \n",
       "3        0              0         0        0           0           0       0  \n",
       "4        0              0         0        0           0           0       0  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#low_memory=False used to remove the warning for mixed data types in each columns\n",
    "banking_data = pd.read_csv('./cleaned_banking_data.csv', low_memory=False)\n",
    "banking_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "315a683f-126a-402f-a2c8-0d65b4344f91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8516, 27)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banking_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbafb1ed-b321-45f9-baa4-dff7bb662ce2",
   "metadata": {},
   "source": [
    "#### 4.2 Find and Evaluate the Accuracy of the Model for each Feature\n",
    "To determine which features might not be good features for our model and could be adding bias or noise I will first build 26 models, each with only one input feature to see what the model's validation accuracy is for that feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe4e2784-2803-40ad-8f7e-54d458da1407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " y_train  has shape: (6813,)\n",
      "y_test has shape: (1703,)\n"
     ]
    }
   ],
   "source": [
    "# Shuffle Data\n",
    "banking_data = banking_data.sample(frac=1)\n",
    "\n",
    "#Get output data\n",
    "X\n",
    "y = banking_data['output']\n",
    "index_20 = int(0.2 * len(y.index))  \n",
    "# Split y into validation (20%) and training (80%)\n",
    "y_test = y.iloc[:index_20]\n",
    "y_train = y.iloc[index_20:]\n",
    "print(f\" y_train  has shape: {y_train.shape}\")\n",
    "print(f\"y_test has shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bc9cdfd-bc79-455f-82f0-def5f6e50727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing feature: age\n",
      "data frame x has shape: (8516, 1)\n",
      "Epoch 1/60\n",
      "653/682 [===========================>..] - ETA: 0s - loss: 0.6944 - accuracy: 0.5181\n",
      "Epoch 1: val_loss improved from inf to 0.69332, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.6944 - accuracy: 0.5196 - val_loss: 0.6933 - val_accuracy: 0.5549\n",
      "Epoch 2/60\n",
      "648/682 [===========================>..] - ETA: 0s - loss: 0.6938 - accuracy: 0.5116\n",
      "Epoch 2: val_loss improved from 0.69332 to 0.69326, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.6938 - accuracy: 0.5096 - val_loss: 0.6933 - val_accuracy: 0.4833\n",
      "Epoch 3/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4876\n",
      "Epoch 3: val_loss improved from 0.69326 to 0.69325, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.6936 - accuracy: 0.4876 - val_loss: 0.6933 - val_accuracy: 0.4809\n",
      "Epoch 4/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.6934 - accuracy: 0.4873\n",
      "Epoch 4: val_loss improved from 0.69325 to 0.69320, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.6934 - accuracy: 0.4870 - val_loss: 0.6932 - val_accuracy: 0.4991\n",
      "Epoch 5/60\n",
      "649/682 [===========================>..] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 5: val_loss improved from 0.69320 to 0.69319, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.6933 - accuracy: 0.5002 - val_loss: 0.6932 - val_accuracy: 0.4991\n",
      "Epoch 6/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5021\n",
      "Epoch 6: val_loss improved from 0.69319 to 0.69317, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6931 - accuracy: 0.5021 - val_loss: 0.6932 - val_accuracy: 0.4985\n",
      "Epoch 7/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.4909\n",
      "Epoch 7: val_loss did not improve from 0.69317\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.6930 - accuracy: 0.4905 - val_loss: 0.6932 - val_accuracy: 0.4885\n",
      "Epoch 8/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.6929 - accuracy: 0.4835\n",
      "Epoch 8: val_loss improved from 0.69317 to 0.69314, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.6929 - accuracy: 0.4844 - val_loss: 0.6931 - val_accuracy: 0.4427\n",
      "Epoch 9/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.6928 - accuracy: 0.4837\n",
      "Epoch 9: val_loss did not improve from 0.69314\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.6928 - accuracy: 0.4825 - val_loss: 0.6931 - val_accuracy: 0.4563\n",
      "Epoch 10/60\n",
      "653/682 [===========================>..] - ETA: 0s - loss: 0.6927 - accuracy: 0.4779\n",
      "Epoch 10: val_loss did not improve from 0.69314\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.6927 - accuracy: 0.4776 - val_loss: 0.6932 - val_accuracy: 0.4451\n",
      "Epoch 11/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.6926 - accuracy: 0.4886\n",
      "Epoch 11: val_loss did not improve from 0.69314\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.6926 - accuracy: 0.4882 - val_loss: 0.6932 - val_accuracy: 0.4427\n",
      "Epoch 12/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.6925 - accuracy: 0.4832\n",
      "Epoch 12: val_loss did not improve from 0.69314\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.6925 - accuracy: 0.4817 - val_loss: 0.6932 - val_accuracy: 0.4427\n",
      "Epoch 13/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.6925 - accuracy: 0.4843\n",
      "Epoch 13: val_loss did not improve from 0.69314\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.6925 - accuracy: 0.4850 - val_loss: 0.6932 - val_accuracy: 0.4645\n",
      "Epoch 14/60\n",
      "649/682 [===========================>..] - ETA: 0s - loss: 0.6924 - accuracy: 0.4847\n",
      "Epoch 14: val_loss did not improve from 0.69314\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.6924 - accuracy: 0.4847 - val_loss: 0.6932 - val_accuracy: 0.4756\n",
      "Epoch 15/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.6924 - accuracy: 0.4931\n",
      "Epoch 15: val_loss did not improve from 0.69314\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.6924 - accuracy: 0.4951 - val_loss: 0.6932 - val_accuracy: 0.4885\n",
      "Epoch 16/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.6923 - accuracy: 0.4928\n",
      "Epoch 16: val_loss did not improve from 0.69314\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.6923 - accuracy: 0.4930 - val_loss: 0.6933 - val_accuracy: 0.4610\n",
      "Epoch 17/60\n",
      "654/682 [===========================>..] - ETA: 0s - loss: 0.6923 - accuracy: 0.4951\n",
      "Epoch 17: val_loss did not improve from 0.69314\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6923 - accuracy: 0.4951 - val_loss: 0.6934 - val_accuracy: 0.4427\n",
      "Epoch 18/60\n",
      "641/682 [===========================>..] - ETA: 0s - loss: 0.6923 - accuracy: 0.4819\n",
      "Epoch 18: val_loss did not improve from 0.69314\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6923 - accuracy: 0.4838 - val_loss: 0.6934 - val_accuracy: 0.4610\n",
      "Epoch 19/60\n",
      "654/682 [===========================>..] - ETA: 0s - loss: 0.6923 - accuracy: 0.4864\n",
      "Epoch 19: val_loss did not improve from 0.69314\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.6922 - accuracy: 0.4870 - val_loss: 0.6934 - val_accuracy: 0.4645\n",
      "Epoch 20/60\n",
      "642/682 [===========================>..] - ETA: 0s - loss: 0.6921 - accuracy: 0.4952\n",
      "Epoch 20: val_loss did not improve from 0.69314\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.6922 - accuracy: 0.4938 - val_loss: 0.6934 - val_accuracy: 0.4610\n",
      "Epoch 21/60\n",
      "640/682 [===========================>..] - ETA: 0s - loss: 0.6920 - accuracy: 0.4863\n",
      "Epoch 21: val_loss did not improve from 0.69314\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.6922 - accuracy: 0.4854 - val_loss: 0.6934 - val_accuracy: 0.4645\n",
      "Epoch 22/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.6921 - accuracy: 0.4913\n",
      "Epoch 22: val_loss did not improve from 0.69314\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6922 - accuracy: 0.4914 - val_loss: 0.6935 - val_accuracy: 0.4610\n",
      "Epoch 23/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.6921 - accuracy: 0.4883\n",
      "Epoch 23: val_loss did not improve from 0.69314\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6921 - accuracy: 0.4880 - val_loss: 0.6935 - val_accuracy: 0.4885\n",
      "Epoch 24/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.6921 - accuracy: 0.4961\n",
      "Epoch 24: val_loss did not improve from 0.69314\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.6921 - accuracy: 0.4961 - val_loss: 0.6935 - val_accuracy: 0.4756\n",
      "Epoch 25/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.6921 - accuracy: 0.4914\n",
      "Epoch 25: val_loss did not improve from 0.69314\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.6921 - accuracy: 0.4911 - val_loss: 0.6935 - val_accuracy: 0.4756\n",
      "Epoch 26/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.6921 - accuracy: 0.4836\n",
      "Epoch 26: val_loss did not improve from 0.69314\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.6921 - accuracy: 0.4836 - val_loss: 0.6935 - val_accuracy: 0.4885\n",
      "Epoch 27/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.6920 - accuracy: 0.4930\n",
      "Epoch 27: val_loss did not improve from 0.69314\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.6921 - accuracy: 0.4919 - val_loss: 0.6936 - val_accuracy: 0.4885\n",
      "Epoch 28/60\n",
      "642/682 [===========================>..] - ETA: 0s - loss: 0.6921 - accuracy: 0.5023\n",
      "Epoch 28: val_loss did not improve from 0.69314\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.6920 - accuracy: 0.5026 - val_loss: 0.6936 - val_accuracy: 0.4645\n",
      "Epoch 28: early stopping\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.4427\n",
      "Processing feature: education\n",
      "data frame x has shape: (8516, 1)\n",
      "Epoch 1/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.6940 - accuracy: 0.5011\n",
      "Epoch 1: val_loss improved from inf to 0.68992, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6938 - accuracy: 0.5029 - val_loss: 0.6899 - val_accuracy: 0.5161\n",
      "Epoch 2/60\n",
      "646/682 [===========================>..] - ETA: 0s - loss: 0.6906 - accuracy: 0.5356\n",
      "Epoch 2: val_loss improved from 0.68992 to 0.68937, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.6906 - accuracy: 0.5362 - val_loss: 0.6894 - val_accuracy: 0.5490\n",
      "Epoch 3/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.6905 - accuracy: 0.5372\n",
      "Epoch 3: val_loss improved from 0.68937 to 0.68913, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.6904 - accuracy: 0.5387 - val_loss: 0.6891 - val_accuracy: 0.5490\n",
      "Epoch 4/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.6901 - accuracy: 0.5386\n",
      "Epoch 4: val_loss improved from 0.68913 to 0.68891, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.6901 - accuracy: 0.5375 - val_loss: 0.6889 - val_accuracy: 0.5590\n",
      "Epoch 5/60\n",
      "648/682 [===========================>..] - ETA: 0s - loss: 0.6901 - accuracy: 0.5485\n",
      "Epoch 5: val_loss improved from 0.68891 to 0.68874, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6901 - accuracy: 0.5490 - val_loss: 0.6887 - val_accuracy: 0.5590\n",
      "Epoch 6/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.6900 - accuracy: 0.5375\n",
      "Epoch 6: val_loss improved from 0.68874 to 0.68856, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6899 - accuracy: 0.5379 - val_loss: 0.6886 - val_accuracy: 0.5590\n",
      "Epoch 7/60\n",
      "651/682 [===========================>..] - ETA: 0s - loss: 0.6896 - accuracy: 0.5481\n",
      "Epoch 7: val_loss improved from 0.68856 to 0.68840, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6899 - accuracy: 0.5462 - val_loss: 0.6884 - val_accuracy: 0.5590\n",
      "Epoch 8/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.6899 - accuracy: 0.5479\n",
      "Epoch 8: val_loss improved from 0.68840 to 0.68825, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6898 - accuracy: 0.5491 - val_loss: 0.6883 - val_accuracy: 0.5590\n",
      "Epoch 9/60\n",
      "649/682 [===========================>..] - ETA: 0s - loss: 0.6893 - accuracy: 0.5455\n",
      "Epoch 9: val_loss improved from 0.68825 to 0.68813, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6897 - accuracy: 0.5445 - val_loss: 0.6881 - val_accuracy: 0.5590\n",
      "Epoch 10/60\n",
      "652/682 [===========================>..] - ETA: 0s - loss: 0.6897 - accuracy: 0.5396\n",
      "Epoch 10: val_loss improved from 0.68813 to 0.68802, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6896 - accuracy: 0.5404 - val_loss: 0.6880 - val_accuracy: 0.5590\n",
      "Epoch 11/60\n",
      "642/682 [===========================>..] - ETA: 0s - loss: 0.6893 - accuracy: 0.5512\n",
      "Epoch 11: val_loss improved from 0.68802 to 0.68790, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.6896 - accuracy: 0.5503 - val_loss: 0.6879 - val_accuracy: 0.5590\n",
      "Epoch 12/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.6895 - accuracy: 0.5482\n",
      "Epoch 12: val_loss improved from 0.68790 to 0.68788, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6895 - accuracy: 0.5478 - val_loss: 0.6879 - val_accuracy: 0.5490\n",
      "Epoch 13/60\n",
      "650/682 [===========================>..] - ETA: 0s - loss: 0.6893 - accuracy: 0.5508\n",
      "Epoch 13: val_loss improved from 0.68788 to 0.68773, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6894 - accuracy: 0.5506 - val_loss: 0.6877 - val_accuracy: 0.5590\n",
      "Epoch 14/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.6894 - accuracy: 0.5429\n",
      "Epoch 14: val_loss improved from 0.68773 to 0.68766, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.6894 - accuracy: 0.5429 - val_loss: 0.6877 - val_accuracy: 0.5590\n",
      "Epoch 15/60\n",
      "648/682 [===========================>..] - ETA: 0s - loss: 0.6897 - accuracy: 0.5522\n",
      "Epoch 15: val_loss improved from 0.68766 to 0.68758, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.6894 - accuracy: 0.5522 - val_loss: 0.6876 - val_accuracy: 0.5590\n",
      "Epoch 16/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.6894 - accuracy: 0.5519\n",
      "Epoch 16: val_loss improved from 0.68758 to 0.68757, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6893 - accuracy: 0.5514 - val_loss: 0.6876 - val_accuracy: 0.5490\n",
      "Epoch 17/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.6893 - accuracy: 0.5510\n",
      "Epoch 17: val_loss improved from 0.68757 to 0.68745, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.6893 - accuracy: 0.5513 - val_loss: 0.6874 - val_accuracy: 0.5590\n",
      "Epoch 18/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.6893 - accuracy: 0.5488\n",
      "Epoch 18: val_loss improved from 0.68745 to 0.68738, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.6893 - accuracy: 0.5488 - val_loss: 0.6874 - val_accuracy: 0.5590\n",
      "Epoch 19/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.6891 - accuracy: 0.5498\n",
      "Epoch 19: val_loss improved from 0.68738 to 0.68733, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.6893 - accuracy: 0.5487 - val_loss: 0.6873 - val_accuracy: 0.5590\n",
      "Epoch 20/60\n",
      "642/682 [===========================>..] - ETA: 0s - loss: 0.6891 - accuracy: 0.5533\n",
      "Epoch 20: val_loss improved from 0.68733 to 0.68730, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.6892 - accuracy: 0.5507 - val_loss: 0.6873 - val_accuracy: 0.5590\n",
      "Epoch 21/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.6891 - accuracy: 0.5495\n",
      "Epoch 21: val_loss improved from 0.68730 to 0.68724, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.6892 - accuracy: 0.5482 - val_loss: 0.6872 - val_accuracy: 0.5590\n",
      "Epoch 22/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.6892 - accuracy: 0.5482\n",
      "Epoch 22: val_loss improved from 0.68724 to 0.68720, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.6892 - accuracy: 0.5482 - val_loss: 0.6872 - val_accuracy: 0.5590\n",
      "Epoch 23/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.6891 - accuracy: 0.5528\n",
      "Epoch 23: val_loss improved from 0.68720 to 0.68717, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6892 - accuracy: 0.5528 - val_loss: 0.6872 - val_accuracy: 0.5590\n",
      "Epoch 24/60\n",
      "648/682 [===========================>..] - ETA: 0s - loss: 0.6893 - accuracy: 0.5502\n",
      "Epoch 24: val_loss improved from 0.68717 to 0.68713, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.6892 - accuracy: 0.5522 - val_loss: 0.6871 - val_accuracy: 0.5590\n",
      "Epoch 25/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.6891 - accuracy: 0.5523\n",
      "Epoch 25: val_loss did not improve from 0.68713\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6891 - accuracy: 0.5522 - val_loss: 0.6871 - val_accuracy: 0.5590\n",
      "Epoch 26/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.6889 - accuracy: 0.5501\n",
      "Epoch 26: val_loss improved from 0.68713 to 0.68707, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.6892 - accuracy: 0.5484 - val_loss: 0.6871 - val_accuracy: 0.5590\n",
      "Epoch 27/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.6890 - accuracy: 0.5551\n",
      "Epoch 27: val_loss did not improve from 0.68707\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6890 - accuracy: 0.5548 - val_loss: 0.6872 - val_accuracy: 0.5490\n",
      "Epoch 28/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.6893 - accuracy: 0.5452\n",
      "Epoch 28: val_loss improved from 0.68707 to 0.68702, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6892 - accuracy: 0.5466 - val_loss: 0.6870 - val_accuracy: 0.5590\n",
      "Epoch 29/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.6889 - accuracy: 0.5531\n",
      "Epoch 29: val_loss improved from 0.68702 to 0.68700, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6891 - accuracy: 0.5522 - val_loss: 0.6870 - val_accuracy: 0.5590\n",
      "Epoch 30/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.6891 - accuracy: 0.5515\n",
      "Epoch 30: val_loss improved from 0.68700 to 0.68698, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.6891 - accuracy: 0.5506 - val_loss: 0.6870 - val_accuracy: 0.5590\n",
      "Epoch 31/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.6892 - accuracy: 0.5525\n",
      "Epoch 31: val_loss improved from 0.68698 to 0.68696, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6891 - accuracy: 0.5522 - val_loss: 0.6870 - val_accuracy: 0.5590\n",
      "Epoch 32/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.6890 - accuracy: 0.5530\n",
      "Epoch 32: val_loss improved from 0.68696 to 0.68694, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6891 - accuracy: 0.5522 - val_loss: 0.6869 - val_accuracy: 0.5590\n",
      "Epoch 33/60\n",
      "653/682 [===========================>..] - ETA: 0s - loss: 0.6888 - accuracy: 0.5528\n",
      "Epoch 33: val_loss improved from 0.68694 to 0.68692, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.6891 - accuracy: 0.5522 - val_loss: 0.6869 - val_accuracy: 0.5590\n",
      "Epoch 34/60\n",
      "652/682 [===========================>..] - ETA: 0s - loss: 0.6885 - accuracy: 0.5563\n",
      "Epoch 34: val_loss improved from 0.68692 to 0.68691, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6891 - accuracy: 0.5522 - val_loss: 0.6869 - val_accuracy: 0.5590\n",
      "Epoch 35/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.6892 - accuracy: 0.5516\n",
      "Epoch 35: val_loss improved from 0.68691 to 0.68690, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.6891 - accuracy: 0.5522 - val_loss: 0.6869 - val_accuracy: 0.5590\n",
      "Epoch 36/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.6891 - accuracy: 0.5520\n",
      "Epoch 36: val_loss improved from 0.68690 to 0.68688, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6891 - accuracy: 0.5522 - val_loss: 0.6869 - val_accuracy: 0.5590\n",
      "Epoch 37/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.6891 - accuracy: 0.5497\n",
      "Epoch 37: val_loss improved from 0.68688 to 0.68687, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.6891 - accuracy: 0.5506 - val_loss: 0.6869 - val_accuracy: 0.5590\n",
      "Epoch 38/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.6893 - accuracy: 0.5512\n",
      "Epoch 38: val_loss improved from 0.68687 to 0.68686, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6891 - accuracy: 0.5525 - val_loss: 0.6869 - val_accuracy: 0.5590\n",
      "Epoch 39/60\n",
      "649/682 [===========================>..] - ETA: 0s - loss: 0.6890 - accuracy: 0.5541\n",
      "Epoch 39: val_loss improved from 0.68686 to 0.68684, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6891 - accuracy: 0.5522 - val_loss: 0.6868 - val_accuracy: 0.5590\n",
      "Epoch 40/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.6894 - accuracy: 0.5505\n",
      "Epoch 40: val_loss improved from 0.68684 to 0.68684, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6891 - accuracy: 0.5522 - val_loss: 0.6868 - val_accuracy: 0.5590\n",
      "Epoch 41/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.6891 - accuracy: 0.5484\n",
      "Epoch 41: val_loss did not improve from 0.68684\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6891 - accuracy: 0.5482 - val_loss: 0.6869 - val_accuracy: 0.5590\n",
      "Epoch 42/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.6892 - accuracy: 0.5510\n",
      "Epoch 42: val_loss improved from 0.68684 to 0.68683, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6891 - accuracy: 0.5522 - val_loss: 0.6868 - val_accuracy: 0.5590\n",
      "Epoch 43/60\n",
      "648/682 [===========================>..] - ETA: 0s - loss: 0.6889 - accuracy: 0.5537\n",
      "Epoch 43: val_loss improved from 0.68683 to 0.68681, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6891 - accuracy: 0.5522 - val_loss: 0.6868 - val_accuracy: 0.5590\n",
      "Epoch 44/60\n",
      "649/682 [===========================>..] - ETA: 0s - loss: 0.6888 - accuracy: 0.5522\n",
      "Epoch 44: val_loss improved from 0.68681 to 0.68680, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6891 - accuracy: 0.5522 - val_loss: 0.6868 - val_accuracy: 0.5590\n",
      "Epoch 45/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.6892 - accuracy: 0.5501\n",
      "Epoch 45: val_loss improved from 0.68680 to 0.68679, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.6891 - accuracy: 0.5506 - val_loss: 0.6868 - val_accuracy: 0.5590\n",
      "Epoch 46/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.6894 - accuracy: 0.5502\n",
      "Epoch 46: val_loss did not improve from 0.68679\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.6891 - accuracy: 0.5522 - val_loss: 0.6868 - val_accuracy: 0.5590\n",
      "Epoch 47/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.6889 - accuracy: 0.5534\n",
      "Epoch 47: val_loss improved from 0.68679 to 0.68679, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.6891 - accuracy: 0.5522 - val_loss: 0.6868 - val_accuracy: 0.5590\n",
      "Epoch 48/60\n",
      "652/682 [===========================>..] - ETA: 0s - loss: 0.6890 - accuracy: 0.5520\n",
      "Epoch 48: val_loss improved from 0.68679 to 0.68678, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.6891 - accuracy: 0.5522 - val_loss: 0.6868 - val_accuracy: 0.5590\n",
      "Epoch 49/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.6890 - accuracy: 0.5519\n",
      "Epoch 49: val_loss improved from 0.68678 to 0.68677, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.6890 - accuracy: 0.5522 - val_loss: 0.6868 - val_accuracy: 0.5590\n",
      "Epoch 50/60\n",
      "645/682 [===========================>..] - ETA: 0s - loss: 0.6891 - accuracy: 0.5522\n",
      "Epoch 50: val_loss improved from 0.68677 to 0.68676, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.6891 - accuracy: 0.5522 - val_loss: 0.6868 - val_accuracy: 0.5590\n",
      "Epoch 51/60\n",
      "644/682 [===========================>..] - ETA: 0s - loss: 0.6888 - accuracy: 0.5542\n",
      "Epoch 51: val_loss did not improve from 0.68676\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6891 - accuracy: 0.5522 - val_loss: 0.6868 - val_accuracy: 0.5590\n",
      "Epoch 52/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.6891 - accuracy: 0.5458\n",
      "Epoch 52: val_loss improved from 0.68676 to 0.68676, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6891 - accuracy: 0.5467 - val_loss: 0.6868 - val_accuracy: 0.5590\n",
      "Epoch 53/60\n",
      "653/682 [===========================>..] - ETA: 0s - loss: 0.6889 - accuracy: 0.5492\n",
      "Epoch 53: val_loss improved from 0.68676 to 0.68675, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6891 - accuracy: 0.5495 - val_loss: 0.6868 - val_accuracy: 0.5590\n",
      "Epoch 54/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.6894 - accuracy: 0.5500\n",
      "Epoch 54: val_loss did not improve from 0.68675\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6891 - accuracy: 0.5522 - val_loss: 0.6868 - val_accuracy: 0.5590\n",
      "Epoch 55/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.6889 - accuracy: 0.5534\n",
      "Epoch 55: val_loss did not improve from 0.68675\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6891 - accuracy: 0.5522 - val_loss: 0.6868 - val_accuracy: 0.5590\n",
      "Epoch 56/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.6890 - accuracy: 0.5529\n",
      "Epoch 56: val_loss improved from 0.68675 to 0.68674, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6891 - accuracy: 0.5522 - val_loss: 0.6867 - val_accuracy: 0.5590\n",
      "Epoch 57/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.6888 - accuracy: 0.5536\n",
      "Epoch 57: val_loss did not improve from 0.68674\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6891 - accuracy: 0.5522 - val_loss: 0.6867 - val_accuracy: 0.5590\n",
      "Epoch 58/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.6888 - accuracy: 0.5516\n",
      "Epoch 58: val_loss improved from 0.68674 to 0.68673, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6891 - accuracy: 0.5513 - val_loss: 0.6867 - val_accuracy: 0.5590\n",
      "Epoch 59/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.6894 - accuracy: 0.5510\n",
      "Epoch 59: val_loss did not improve from 0.68673\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6891 - accuracy: 0.5522 - val_loss: 0.6867 - val_accuracy: 0.5590\n",
      "Epoch 60/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.6892 - accuracy: 0.5495\n",
      "Epoch 60: val_loss did not improve from 0.68673\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6891 - accuracy: 0.5504 - val_loss: 0.6867 - val_accuracy: 0.5590\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.6867 - accuracy: 0.5590\n",
      "Processing feature: housing\n",
      "data frame x has shape: (8516, 1)\n",
      "Epoch 1/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.6984 - accuracy: 0.4891\n",
      "Epoch 1: val_loss improved from inf to 0.69514, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6984 - accuracy: 0.4891 - val_loss: 0.6951 - val_accuracy: 0.4927\n",
      "Epoch 2/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.6950 - accuracy: 0.4886\n",
      "Epoch 2: val_loss improved from 0.69514 to 0.69410, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6949 - accuracy: 0.4897 - val_loss: 0.6941 - val_accuracy: 0.4927\n",
      "Epoch 3/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.6941 - accuracy: 0.4887\n",
      "Epoch 3: val_loss improved from 0.69410 to 0.69366, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6941 - accuracy: 0.4886 - val_loss: 0.6937 - val_accuracy: 0.4927\n",
      "Epoch 4/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.6937 - accuracy: 0.4972\n",
      "Epoch 4: val_loss improved from 0.69366 to 0.69342, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6937 - accuracy: 0.4971 - val_loss: 0.6934 - val_accuracy: 0.4991\n",
      "Epoch 5/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.6935 - accuracy: 0.4920\n",
      "Epoch 5: val_loss improved from 0.69342 to 0.69325, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6934 - accuracy: 0.4941 - val_loss: 0.6932 - val_accuracy: 0.4991\n",
      "Epoch 6/60\n",
      "654/682 [===========================>..] - ETA: 0s - loss: 0.6933 - accuracy: 0.4920\n",
      "Epoch 6: val_loss improved from 0.69325 to 0.69314, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6933 - accuracy: 0.4941 - val_loss: 0.6931 - val_accuracy: 0.4991\n",
      "Epoch 7/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.6932 - accuracy: 0.5005\n",
      "Epoch 7: val_loss improved from 0.69314 to 0.69311, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.4999 - val_loss: 0.6931 - val_accuracy: 0.5073\n",
      "Epoch 8/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.6932 - accuracy: 0.5047\n",
      "Epoch 8: val_loss did not improve from 0.69311\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6931 - accuracy: 0.5062 - val_loss: 0.6931 - val_accuracy: 0.5073\n",
      "Epoch 9/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5089\n",
      "Epoch 9: val_loss improved from 0.69311 to 0.69304, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6931 - accuracy: 0.5092 - val_loss: 0.6930 - val_accuracy: 0.5073\n",
      "Epoch 10/60\n",
      "654/682 [===========================>..] - ETA: 0s - loss: 0.6931 - accuracy: 0.5078\n",
      "Epoch 10: val_loss did not improve from 0.69304\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5089 - val_loss: 0.6931 - val_accuracy: 0.5073\n",
      "Epoch 11/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.6931 - accuracy: 0.5108\n",
      "Epoch 11: val_loss improved from 0.69304 to 0.69304, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5103 - val_loss: 0.6930 - val_accuracy: 0.5073\n",
      "Epoch 12/60\n",
      "652/682 [===========================>..] - ETA: 0s - loss: 0.6930 - accuracy: 0.5123\n",
      "Epoch 12: val_loss did not improve from 0.69304\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5103 - val_loss: 0.6931 - val_accuracy: 0.5073\n",
      "Epoch 13/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5097\n",
      "Epoch 13: val_loss improved from 0.69304 to 0.69303, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6931 - accuracy: 0.5103 - val_loss: 0.6930 - val_accuracy: 0.5073\n",
      "Epoch 14/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5107\n",
      "Epoch 14: val_loss did not improve from 0.69303\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5103 - val_loss: 0.6930 - val_accuracy: 0.5073\n",
      "Epoch 15/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5104\n",
      "Epoch 15: val_loss did not improve from 0.69303\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5103 - val_loss: 0.6930 - val_accuracy: 0.5073\n",
      "Epoch 16/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5106\n",
      "Epoch 16: val_loss did not improve from 0.69303\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.5103 - val_loss: 0.6930 - val_accuracy: 0.5073\n",
      "Epoch 17/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.6931 - accuracy: 0.5099\n",
      "Epoch 17: val_loss did not improve from 0.69303\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6931 - accuracy: 0.5103 - val_loss: 0.6930 - val_accuracy: 0.5073\n",
      "Epoch 18/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5111\n",
      "Epoch 18: val_loss did not improve from 0.69303\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6930 - accuracy: 0.5103 - val_loss: 0.6930 - val_accuracy: 0.5073\n",
      "Epoch 19/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5094\n",
      "Epoch 19: val_loss did not improve from 0.69303\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6930 - accuracy: 0.5103 - val_loss: 0.6930 - val_accuracy: 0.5073\n",
      "Epoch 20/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5103\n",
      "Epoch 20: val_loss did not improve from 0.69303\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.5103 - val_loss: 0.6931 - val_accuracy: 0.5073\n",
      "Epoch 21/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5103\n",
      "Epoch 21: val_loss did not improve from 0.69303\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.5103 - val_loss: 0.6931 - val_accuracy: 0.5073\n",
      "Epoch 22/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5103\n",
      "Epoch 22: val_loss did not improve from 0.69303\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5103 - val_loss: 0.6931 - val_accuracy: 0.5073\n",
      "Epoch 23/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.6931 - accuracy: 0.5081\n",
      "Epoch 23: val_loss did not improve from 0.69303\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6930 - accuracy: 0.5103 - val_loss: 0.6930 - val_accuracy: 0.5073\n",
      "Epoch 24/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5060\n",
      "Epoch 24: val_loss did not improve from 0.69303\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6930 - accuracy: 0.5067 - val_loss: 0.6932 - val_accuracy: 0.5073\n",
      "Epoch 25/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5108\n",
      "Epoch 25: val_loss did not improve from 0.69303\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5103 - val_loss: 0.6931 - val_accuracy: 0.5073\n",
      "Epoch 26/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5108\n",
      "Epoch 26: val_loss did not improve from 0.69303\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5103 - val_loss: 0.6930 - val_accuracy: 0.5073\n",
      "Epoch 27/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.6931 - accuracy: 0.5090\n",
      "Epoch 27: val_loss did not improve from 0.69303\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6931 - accuracy: 0.5103 - val_loss: 0.6930 - val_accuracy: 0.5073\n",
      "Epoch 28/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5072\n",
      "Epoch 28: val_loss did not improve from 0.69303\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6931 - val_accuracy: 0.5073\n",
      "Epoch 29/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5094\n",
      "Epoch 29: val_loss did not improve from 0.69303\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6930 - accuracy: 0.5103 - val_loss: 0.6930 - val_accuracy: 0.5073\n",
      "Epoch 30/60\n",
      "654/682 [===========================>..] - ETA: 0s - loss: 0.6929 - accuracy: 0.5135\n",
      "Epoch 30: val_loss did not improve from 0.69303\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6930 - accuracy: 0.5103 - val_loss: 0.6931 - val_accuracy: 0.5073\n",
      "Epoch 31/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.6930 - accuracy: 0.5123\n",
      "Epoch 31: val_loss did not improve from 0.69303\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5103 - val_loss: 0.6931 - val_accuracy: 0.5073\n",
      "Epoch 32/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.6930 - accuracy: 0.5115\n",
      "Epoch 32: val_loss did not improve from 0.69303\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6930 - accuracy: 0.5103 - val_loss: 0.6930 - val_accuracy: 0.5073\n",
      "Epoch 33/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5106\n",
      "Epoch 33: val_loss did not improve from 0.69303\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.5103 - val_loss: 0.6931 - val_accuracy: 0.5073\n",
      "Epoch 33: early stopping\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.6930 - accuracy: 0.5073\n",
      "Processing feature: loan\n",
      "data frame x has shape: (8516, 1)\n",
      "Epoch 1/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.7045 - accuracy: 0.5000\n",
      "Epoch 1: val_loss improved from inf to 0.70221, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.7044 - accuracy: 0.5004 - val_loss: 0.7022 - val_accuracy: 0.4979\n",
      "Epoch 2/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.6986 - accuracy: 0.5000\n",
      "Epoch 2: val_loss improved from 0.70221 to 0.69805, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6989 - accuracy: 0.4992 - val_loss: 0.6981 - val_accuracy: 0.4979\n",
      "Epoch 3/60\n",
      "653/682 [===========================>..] - ETA: 0s - loss: 0.6961 - accuracy: 0.4998\n",
      "Epoch 3: val_loss improved from 0.69805 to 0.69579, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6963 - accuracy: 0.4992 - val_loss: 0.6958 - val_accuracy: 0.4979\n",
      "Epoch 4/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.6949 - accuracy: 0.4987\n",
      "Epoch 4: val_loss improved from 0.69579 to 0.69465, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6949 - accuracy: 0.4992 - val_loss: 0.6946 - val_accuracy: 0.4979\n",
      "Epoch 5/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.6940 - accuracy: 0.5006\n",
      "Epoch 5: val_loss improved from 0.69465 to 0.69401, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6941 - accuracy: 0.4992 - val_loss: 0.6940 - val_accuracy: 0.4979\n",
      "Epoch 6/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.6937 - accuracy: 0.4991\n",
      "Epoch 6: val_loss improved from 0.69401 to 0.69363, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6937 - accuracy: 0.4992 - val_loss: 0.6936 - val_accuracy: 0.4979\n",
      "Epoch 7/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.6935 - accuracy: 0.4987\n",
      "Epoch 7: val_loss improved from 0.69363 to 0.69342, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6935 - accuracy: 0.4992 - val_loss: 0.6934 - val_accuracy: 0.4979\n",
      "Epoch 8/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.6934 - accuracy: 0.4961\n",
      "Epoch 8: val_loss improved from 0.69342 to 0.69329, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6934 - accuracy: 0.4961 - val_loss: 0.6933 - val_accuracy: 0.4979\n",
      "Epoch 9/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.6933 - accuracy: 0.4957\n",
      "Epoch 9: val_loss improved from 0.69329 to 0.69324, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6933 - accuracy: 0.4955 - val_loss: 0.6932 - val_accuracy: 0.4979\n",
      "Epoch 10/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.6933 - accuracy: 0.4979\n",
      "Epoch 10: val_loss improved from 0.69324 to 0.69320, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6933 - accuracy: 0.4982 - val_loss: 0.6932 - val_accuracy: 0.4979\n",
      "Epoch 11/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.6933 - accuracy: 0.4901\n",
      "Epoch 11: val_loss did not improve from 0.69320\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.4897 - val_loss: 0.6932 - val_accuracy: 0.4991\n",
      "Epoch 12/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4916\n",
      "Epoch 12: val_loss improved from 0.69320 to 0.69319, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6933 - accuracy: 0.4914 - val_loss: 0.6932 - val_accuracy: 0.4991\n",
      "Epoch 13/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4874\n",
      "Epoch 13: val_loss improved from 0.69319 to 0.69316, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4872 - val_loss: 0.6932 - val_accuracy: 0.4991\n",
      "Epoch 14/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4899\n",
      "Epoch 14: val_loss improved from 0.69316 to 0.69314, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.4910 - val_loss: 0.6931 - val_accuracy: 0.5009\n",
      "Epoch 15/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4939\n",
      "Epoch 15: val_loss improved from 0.69314 to 0.69314, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.4941 - val_loss: 0.6931 - val_accuracy: 0.5009\n",
      "Epoch 16/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4946\n",
      "Epoch 16: val_loss improved from 0.69314 to 0.69314, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.4943 - val_loss: 0.6931 - val_accuracy: 0.5009\n",
      "Epoch 17/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4972\n",
      "Epoch 17: val_loss did not improve from 0.69314\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.4968 - val_loss: 0.6931 - val_accuracy: 0.5009\n",
      "Epoch 18/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.5006\n",
      "Epoch 18: val_loss did not improve from 0.69314\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6933 - accuracy: 0.4986 - val_loss: 0.6931 - val_accuracy: 0.5021\n",
      "Epoch 19/60\n",
      "651/682 [===========================>..] - ETA: 0s - loss: 0.6932 - accuracy: 0.4886\n",
      "Epoch 19: val_loss did not improve from 0.69314\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.4883 - val_loss: 0.6932 - val_accuracy: 0.4991\n",
      "Epoch 20/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4987\n",
      "Epoch 20: val_loss improved from 0.69314 to 0.69313, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4982 - val_loss: 0.6931 - val_accuracy: 0.5021\n",
      "Epoch 21/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4968\n",
      "Epoch 21: val_loss did not improve from 0.69313\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.4952 - val_loss: 0.6931 - val_accuracy: 0.5021\n",
      "Epoch 22/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.6933 - accuracy: 0.4898\n",
      "Epoch 22: val_loss did not improve from 0.69313\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4908 - val_loss: 0.6931 - val_accuracy: 0.5009\n",
      "Epoch 23/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.6933 - accuracy: 0.4897\n",
      "Epoch 23: val_loss did not improve from 0.69313\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6933 - accuracy: 0.4889 - val_loss: 0.6931 - val_accuracy: 0.5021\n",
      "Epoch 24/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.6933 - accuracy: 0.4866\n",
      "Epoch 24: val_loss improved from 0.69313 to 0.69313, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6933 - accuracy: 0.4860 - val_loss: 0.6931 - val_accuracy: 0.5021\n",
      "Epoch 25/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4893\n",
      "Epoch 25: val_loss did not improve from 0.69313\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.4888 - val_loss: 0.6931 - val_accuracy: 0.5021\n",
      "Epoch 26/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.6933 - accuracy: 0.4859\n",
      "Epoch 26: val_loss did not improve from 0.69313\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6933 - accuracy: 0.4850 - val_loss: 0.6931 - val_accuracy: 0.5021\n",
      "Epoch 27/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4973\n",
      "Epoch 27: val_loss did not improve from 0.69313\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4968 - val_loss: 0.6931 - val_accuracy: 0.5021\n",
      "Epoch 28/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.6933 - accuracy: 0.4925\n",
      "Epoch 28: val_loss did not improve from 0.69313\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6933 - accuracy: 0.4930 - val_loss: 0.6931 - val_accuracy: 0.5021\n",
      "Epoch 29/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4966\n",
      "Epoch 29: val_loss did not improve from 0.69313\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.4967 - val_loss: 0.6931 - val_accuracy: 0.5009\n",
      "Epoch 30/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.6932 - accuracy: 0.4901\n",
      "Epoch 30: val_loss did not improve from 0.69313\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.4897 - val_loss: 0.6931 - val_accuracy: 0.5021\n",
      "Epoch 31/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.6933 - accuracy: 0.4877\n",
      "Epoch 31: val_loss did not improve from 0.69313\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6933 - accuracy: 0.4873 - val_loss: 0.6931 - val_accuracy: 0.5021\n",
      "Epoch 32/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.6932 - accuracy: 0.4948\n",
      "Epoch 32: val_loss did not improve from 0.69313\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.4970 - val_loss: 0.6932 - val_accuracy: 0.4991\n",
      "Epoch 33/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4988\n",
      "Epoch 33: val_loss did not improve from 0.69313\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6933 - accuracy: 0.4980 - val_loss: 0.6932 - val_accuracy: 0.4991\n",
      "Epoch 34/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.5013\n",
      "Epoch 34: val_loss did not improve from 0.69313\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.5014 - val_loss: 0.6931 - val_accuracy: 0.5009\n",
      "Epoch 35/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4925\n",
      "Epoch 35: val_loss did not improve from 0.69313\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4926 - val_loss: 0.6931 - val_accuracy: 0.5021\n",
      "Epoch 36/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4984\n",
      "Epoch 36: val_loss did not improve from 0.69313\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.4986 - val_loss: 0.6932 - val_accuracy: 0.4991\n",
      "Epoch 37/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4891\n",
      "Epoch 37: val_loss did not improve from 0.69313\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.4888 - val_loss: 0.6931 - val_accuracy: 0.5021\n",
      "Epoch 38/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.6933 - accuracy: 0.4939\n",
      "Epoch 38: val_loss did not improve from 0.69313\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6933 - accuracy: 0.4948 - val_loss: 0.6931 - val_accuracy: 0.5009\n",
      "Epoch 39/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4973\n",
      "Epoch 39: val_loss did not improve from 0.69313\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4974 - val_loss: 0.6931 - val_accuracy: 0.5021\n",
      "Epoch 40/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4925\n",
      "Epoch 40: val_loss did not improve from 0.69313\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.4929 - val_loss: 0.6931 - val_accuracy: 0.5021\n",
      "Epoch 41/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.5010\n",
      "Epoch 41: val_loss did not improve from 0.69313\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.5007 - val_loss: 0.6931 - val_accuracy: 0.5021\n",
      "Epoch 42/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.6933 - accuracy: 0.4959\n",
      "Epoch 42: val_loss did not improve from 0.69313\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6933 - accuracy: 0.4954 - val_loss: 0.6931 - val_accuracy: 0.5021\n",
      "Epoch 43/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4895\n",
      "Epoch 43: val_loss did not improve from 0.69313\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4891 - val_loss: 0.6931 - val_accuracy: 0.5009\n",
      "Epoch 44/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4945\n",
      "Epoch 44: val_loss did not improve from 0.69313\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4939 - val_loss: 0.6931 - val_accuracy: 0.5021\n",
      "Epoch 44: early stopping\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5021\n",
      "Processing feature: campaign\n",
      "data frame x has shape: (8516, 1)\n",
      "Epoch 1/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.7003 - accuracy: 0.4592\n",
      "Epoch 1: val_loss improved from inf to 0.69897, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 3ms/step - loss: 0.7004 - accuracy: 0.4584 - val_loss: 0.6990 - val_accuracy: 0.4686\n",
      "Epoch 2/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.6990 - accuracy: 0.4602\n",
      "Epoch 2: val_loss improved from 0.69897 to 0.69781, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6990 - accuracy: 0.4596 - val_loss: 0.6978 - val_accuracy: 0.4692\n",
      "Epoch 3/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.6979 - accuracy: 0.4567\n",
      "Epoch 3: val_loss improved from 0.69781 to 0.69682, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6978 - accuracy: 0.4584 - val_loss: 0.6968 - val_accuracy: 0.4692\n",
      "Epoch 4/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.6967 - accuracy: 0.4595\n",
      "Epoch 4: val_loss improved from 0.69682 to 0.69588, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6968 - accuracy: 0.4587 - val_loss: 0.6959 - val_accuracy: 0.4692\n",
      "Epoch 5/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.6957 - accuracy: 0.4588\n",
      "Epoch 5: val_loss improved from 0.69588 to 0.69497, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6957 - accuracy: 0.4590 - val_loss: 0.6950 - val_accuracy: 0.4662\n",
      "Epoch 6/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.6948 - accuracy: 0.4617\n",
      "Epoch 6: val_loss improved from 0.69497 to 0.69413, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6948 - accuracy: 0.4616 - val_loss: 0.6941 - val_accuracy: 0.4662\n",
      "Epoch 7/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.6939 - accuracy: 0.4742\n",
      "Epoch 7: val_loss improved from 0.69413 to 0.69337, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6939 - accuracy: 0.4748 - val_loss: 0.6934 - val_accuracy: 0.4780\n",
      "Epoch 8/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.4991\n",
      "Epoch 8: val_loss improved from 0.69337 to 0.69265, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4995 - val_loss: 0.6927 - val_accuracy: 0.5314\n",
      "Epoch 9/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.6923 - accuracy: 0.5125\n",
      "Epoch 9: val_loss improved from 0.69265 to 0.69201, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6923 - accuracy: 0.5130 - val_loss: 0.6920 - val_accuracy: 0.5308\n",
      "Epoch 10/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.6917 - accuracy: 0.5267\n",
      "Epoch 10: val_loss improved from 0.69201 to 0.69141, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6917 - accuracy: 0.5274 - val_loss: 0.6914 - val_accuracy: 0.5308\n",
      "Epoch 11/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.6910 - accuracy: 0.5343\n",
      "Epoch 11: val_loss improved from 0.69141 to 0.69082, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5350 - val_loss: 0.6908 - val_accuracy: 0.5308\n",
      "Epoch 12/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.6904 - accuracy: 0.5421\n",
      "Epoch 12: val_loss improved from 0.69082 to 0.69029, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6904 - accuracy: 0.5410 - val_loss: 0.6903 - val_accuracy: 0.5308\n",
      "Epoch 13/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.6898 - accuracy: 0.5360\n",
      "Epoch 13: val_loss improved from 0.69029 to 0.68982, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6899 - accuracy: 0.5360 - val_loss: 0.6898 - val_accuracy: 0.5308\n",
      "Epoch 14/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.6894 - accuracy: 0.5355\n",
      "Epoch 14: val_loss improved from 0.68982 to 0.68938, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6894 - accuracy: 0.5354 - val_loss: 0.6894 - val_accuracy: 0.5308\n",
      "Epoch 15/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.6889 - accuracy: 0.5425\n",
      "Epoch 15: val_loss improved from 0.68938 to 0.68898, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6889 - accuracy: 0.5426 - val_loss: 0.6890 - val_accuracy: 0.5308\n",
      "Epoch 16/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.6884 - accuracy: 0.5424\n",
      "Epoch 16: val_loss improved from 0.68898 to 0.68861, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6885 - accuracy: 0.5416 - val_loss: 0.6886 - val_accuracy: 0.5308\n",
      "Epoch 17/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.6882 - accuracy: 0.5411\n",
      "Epoch 17: val_loss improved from 0.68861 to 0.68827, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6881 - accuracy: 0.5416 - val_loss: 0.6883 - val_accuracy: 0.5308\n",
      "Epoch 18/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.6877 - accuracy: 0.5389\n",
      "Epoch 18: val_loss improved from 0.68827 to 0.68796, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6878 - accuracy: 0.5387 - val_loss: 0.6880 - val_accuracy: 0.5308\n",
      "Epoch 19/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.6874 - accuracy: 0.5408\n",
      "Epoch 19: val_loss improved from 0.68796 to 0.68769, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6875 - accuracy: 0.5416 - val_loss: 0.6877 - val_accuracy: 0.5308\n",
      "Epoch 20/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.6872 - accuracy: 0.5419\n",
      "Epoch 20: val_loss improved from 0.68769 to 0.68746, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6872 - accuracy: 0.5416 - val_loss: 0.6875 - val_accuracy: 0.5308\n",
      "Epoch 21/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.6872 - accuracy: 0.5399\n",
      "Epoch 21: val_loss improved from 0.68746 to 0.68722, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6870 - accuracy: 0.5416 - val_loss: 0.6872 - val_accuracy: 0.5308\n",
      "Epoch 22/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.6868 - accuracy: 0.5409\n",
      "Epoch 22: val_loss improved from 0.68722 to 0.68702, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6867 - accuracy: 0.5416 - val_loss: 0.6870 - val_accuracy: 0.5308\n",
      "Epoch 23/60\n",
      "654/682 [===========================>..] - ETA: 0s - loss: 0.6862 - accuracy: 0.5442\n",
      "Epoch 23: val_loss improved from 0.68702 to 0.68681, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6865 - accuracy: 0.5416 - val_loss: 0.6868 - val_accuracy: 0.5308\n",
      "Epoch 24/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.6864 - accuracy: 0.5415\n",
      "Epoch 24: val_loss improved from 0.68681 to 0.68664, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6863 - accuracy: 0.5416 - val_loss: 0.6866 - val_accuracy: 0.5308\n",
      "Epoch 25/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.6862 - accuracy: 0.5405\n",
      "Epoch 25: val_loss improved from 0.68664 to 0.68650, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6862 - accuracy: 0.5406 - val_loss: 0.6865 - val_accuracy: 0.5308\n",
      "Epoch 26/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.6861 - accuracy: 0.5408\n",
      "Epoch 26: val_loss improved from 0.68650 to 0.68635, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6860 - accuracy: 0.5416 - val_loss: 0.6863 - val_accuracy: 0.5308\n",
      "Epoch 27/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.6858 - accuracy: 0.5423\n",
      "Epoch 27: val_loss improved from 0.68635 to 0.68625, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6858 - accuracy: 0.5415 - val_loss: 0.6862 - val_accuracy: 0.5308\n",
      "Epoch 28/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.6857 - accuracy: 0.5417\n",
      "Epoch 28: val_loss improved from 0.68625 to 0.68611, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6857 - accuracy: 0.5416 - val_loss: 0.6861 - val_accuracy: 0.5308\n",
      "Epoch 29/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.6858 - accuracy: 0.5408\n",
      "Epoch 29: val_loss improved from 0.68611 to 0.68601, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6857 - accuracy: 0.5416 - val_loss: 0.6860 - val_accuracy: 0.5308\n",
      "Epoch 30/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.6856 - accuracy: 0.5419\n",
      "Epoch 30: val_loss improved from 0.68601 to 0.68591, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6855 - accuracy: 0.5416 - val_loss: 0.6859 - val_accuracy: 0.5308\n",
      "Epoch 31/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.6855 - accuracy: 0.5405\n",
      "Epoch 31: val_loss improved from 0.68591 to 0.68582, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6855 - accuracy: 0.5416 - val_loss: 0.6858 - val_accuracy: 0.5308\n",
      "Epoch 32/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.6854 - accuracy: 0.5416\n",
      "Epoch 32: val_loss improved from 0.68582 to 0.68574, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6854 - accuracy: 0.5416 - val_loss: 0.6857 - val_accuracy: 0.5308\n",
      "Epoch 33/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.6853 - accuracy: 0.5410\n",
      "Epoch 33: val_loss improved from 0.68574 to 0.68567, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6853 - accuracy: 0.5416 - val_loss: 0.6857 - val_accuracy: 0.5308\n",
      "Epoch 34/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.6853 - accuracy: 0.5412\n",
      "Epoch 34: val_loss improved from 0.68567 to 0.68560, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6852 - accuracy: 0.5416 - val_loss: 0.6856 - val_accuracy: 0.5308\n",
      "Epoch 35/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.6854 - accuracy: 0.5409\n",
      "Epoch 35: val_loss improved from 0.68560 to 0.68554, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6851 - accuracy: 0.5416 - val_loss: 0.6855 - val_accuracy: 0.5308\n",
      "Epoch 36/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.6850 - accuracy: 0.5431\n",
      "Epoch 36: val_loss improved from 0.68554 to 0.68549, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6852 - accuracy: 0.5416 - val_loss: 0.6855 - val_accuracy: 0.5308\n",
      "Epoch 37/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.6849 - accuracy: 0.5423\n",
      "Epoch 37: val_loss improved from 0.68549 to 0.68544, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6851 - accuracy: 0.5416 - val_loss: 0.6854 - val_accuracy: 0.5308\n",
      "Epoch 38/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.6850 - accuracy: 0.5420\n",
      "Epoch 38: val_loss improved from 0.68544 to 0.68540, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6850 - accuracy: 0.5416 - val_loss: 0.6854 - val_accuracy: 0.5308\n",
      "Epoch 39/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.6850 - accuracy: 0.5408\n",
      "Epoch 39: val_loss improved from 0.68540 to 0.68538, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6850 - accuracy: 0.5416 - val_loss: 0.6854 - val_accuracy: 0.5308\n",
      "Epoch 40/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.6847 - accuracy: 0.5414\n",
      "Epoch 40: val_loss improved from 0.68538 to 0.68532, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6849 - accuracy: 0.5416 - val_loss: 0.6853 - val_accuracy: 0.5308\n",
      "Epoch 41/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.6849 - accuracy: 0.5413\n",
      "Epoch 41: val_loss improved from 0.68532 to 0.68528, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6849 - accuracy: 0.5416 - val_loss: 0.6853 - val_accuracy: 0.5308\n",
      "Epoch 42/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.6848 - accuracy: 0.5390\n",
      "Epoch 42: val_loss improved from 0.68528 to 0.68526, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6849 - accuracy: 0.5410 - val_loss: 0.6853 - val_accuracy: 0.5308\n",
      "Epoch 43/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.6848 - accuracy: 0.5423\n",
      "Epoch 43: val_loss improved from 0.68526 to 0.68522, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6848 - accuracy: 0.5416 - val_loss: 0.6852 - val_accuracy: 0.5308\n",
      "Epoch 44/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.6849 - accuracy: 0.5423\n",
      "Epoch 44: val_loss improved from 0.68522 to 0.68519, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6848 - accuracy: 0.5416 - val_loss: 0.6852 - val_accuracy: 0.5308\n",
      "Epoch 45/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.6850 - accuracy: 0.5401\n",
      "Epoch 45: val_loss improved from 0.68519 to 0.68517, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6848 - accuracy: 0.5416 - val_loss: 0.6852 - val_accuracy: 0.5308\n",
      "Epoch 46/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.6845 - accuracy: 0.5419\n",
      "Epoch 46: val_loss improved from 0.68517 to 0.68515, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6848 - accuracy: 0.5416 - val_loss: 0.6851 - val_accuracy: 0.5308\n",
      "Epoch 47/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.6848 - accuracy: 0.5412\n",
      "Epoch 47: val_loss improved from 0.68515 to 0.68512, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6848 - accuracy: 0.5416 - val_loss: 0.6851 - val_accuracy: 0.5308\n",
      "Epoch 48/60\n",
      "654/682 [===========================>..] - ETA: 0s - loss: 0.6849 - accuracy: 0.5419\n",
      "Epoch 48: val_loss improved from 0.68512 to 0.68512, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6847 - accuracy: 0.5416 - val_loss: 0.6851 - val_accuracy: 0.5308\n",
      "Epoch 49/60\n",
      "652/682 [===========================>..] - ETA: 0s - loss: 0.6846 - accuracy: 0.5402\n",
      "Epoch 49: val_loss improved from 0.68512 to 0.68509, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6847 - accuracy: 0.5416 - val_loss: 0.6851 - val_accuracy: 0.5308\n",
      "Epoch 50/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.6847 - accuracy: 0.5412\n",
      "Epoch 50: val_loss improved from 0.68509 to 0.68507, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6847 - accuracy: 0.5416 - val_loss: 0.6851 - val_accuracy: 0.5308\n",
      "Epoch 51/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.6846 - accuracy: 0.5417\n",
      "Epoch 51: val_loss improved from 0.68507 to 0.68506, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6847 - accuracy: 0.5416 - val_loss: 0.6851 - val_accuracy: 0.5308\n",
      "Epoch 52/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.6847 - accuracy: 0.5415\n",
      "Epoch 52: val_loss improved from 0.68506 to 0.68504, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6847 - accuracy: 0.5416 - val_loss: 0.6850 - val_accuracy: 0.5308\n",
      "Epoch 53/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.6846 - accuracy: 0.5419\n",
      "Epoch 53: val_loss improved from 0.68504 to 0.68503, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6847 - accuracy: 0.5416 - val_loss: 0.6850 - val_accuracy: 0.5308\n",
      "Epoch 54/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.6845 - accuracy: 0.5426\n",
      "Epoch 54: val_loss improved from 0.68503 to 0.68501, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6847 - accuracy: 0.5416 - val_loss: 0.6850 - val_accuracy: 0.5308\n",
      "Epoch 55/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.6847 - accuracy: 0.5399\n",
      "Epoch 55: val_loss improved from 0.68501 to 0.68501, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6847 - accuracy: 0.5416 - val_loss: 0.6850 - val_accuracy: 0.5308\n",
      "Epoch 56/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.6844 - accuracy: 0.5437\n",
      "Epoch 56: val_loss did not improve from 0.68501\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6847 - accuracy: 0.5416 - val_loss: 0.6850 - val_accuracy: 0.5308\n",
      "Epoch 57/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.6844 - accuracy: 0.5426\n",
      "Epoch 57: val_loss improved from 0.68501 to 0.68501, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6846 - accuracy: 0.5416 - val_loss: 0.6850 - val_accuracy: 0.5308\n",
      "Epoch 58/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.6845 - accuracy: 0.5418\n",
      "Epoch 58: val_loss improved from 0.68501 to 0.68497, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6846 - accuracy: 0.5416 - val_loss: 0.6850 - val_accuracy: 0.5308\n",
      "Epoch 59/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.6846 - accuracy: 0.5418\n",
      "Epoch 59: val_loss improved from 0.68497 to 0.68496, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6846 - accuracy: 0.5416 - val_loss: 0.6850 - val_accuracy: 0.5308\n",
      "Epoch 60/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.6846 - accuracy: 0.5416\n",
      "Epoch 60: val_loss improved from 0.68496 to 0.68496, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6846 - accuracy: 0.5416 - val_loss: 0.6850 - val_accuracy: 0.5308\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.6850 - accuracy: 0.5308\n",
      "Processing feature: pdays\n",
      "data frame x has shape: (8516, 1)\n",
      "Epoch 1/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.6837 - accuracy: 0.5951\n",
      "Epoch 1: val_loss improved from inf to 0.68131, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6838 - accuracy: 0.5943 - val_loss: 0.6813 - val_accuracy: 0.5907\n",
      "Epoch 2/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.6788 - accuracy: 0.5956\n",
      "Epoch 2: val_loss improved from 0.68131 to 0.67786, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6789 - accuracy: 0.5945 - val_loss: 0.6779 - val_accuracy: 0.5907\n",
      "Epoch 3/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.6757 - accuracy: 0.5935\n",
      "Epoch 3: val_loss improved from 0.67786 to 0.67486, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6755 - accuracy: 0.5945 - val_loss: 0.6749 - val_accuracy: 0.5907\n",
      "Epoch 4/60\n",
      "653/682 [===========================>..] - ETA: 0s - loss: 0.6727 - accuracy: 0.5939\n",
      "Epoch 4: val_loss improved from 0.67486 to 0.67208, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6724 - accuracy: 0.5945 - val_loss: 0.6721 - val_accuracy: 0.5907\n",
      "Epoch 5/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.6695 - accuracy: 0.5957\n",
      "Epoch 5: val_loss improved from 0.67208 to 0.66963, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6696 - accuracy: 0.5945 - val_loss: 0.6696 - val_accuracy: 0.5907\n",
      "Epoch 6/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.6673 - accuracy: 0.5936\n",
      "Epoch 6: val_loss improved from 0.66963 to 0.66746, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6671 - accuracy: 0.5945 - val_loss: 0.6675 - val_accuracy: 0.5907\n",
      "Epoch 7/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.6647 - accuracy: 0.5948\n",
      "Epoch 7: val_loss improved from 0.66746 to 0.66547, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6649 - accuracy: 0.5945 - val_loss: 0.6655 - val_accuracy: 0.5907\n",
      "Epoch 8/60\n",
      "652/682 [===========================>..] - ETA: 0s - loss: 0.6632 - accuracy: 0.5939\n",
      "Epoch 8: val_loss improved from 0.66547 to 0.66372, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6629 - accuracy: 0.5945 - val_loss: 0.6637 - val_accuracy: 0.5907\n",
      "Epoch 9/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.6609 - accuracy: 0.5945\n",
      "Epoch 9: val_loss improved from 0.66372 to 0.66213, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6610 - accuracy: 0.5945 - val_loss: 0.6621 - val_accuracy: 0.5907\n",
      "Epoch 10/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.6593 - accuracy: 0.5942\n",
      "Epoch 10: val_loss improved from 0.66213 to 0.66069, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6594 - accuracy: 0.5945 - val_loss: 0.6607 - val_accuracy: 0.5907\n",
      "Epoch 11/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.6575 - accuracy: 0.5957\n",
      "Epoch 11: val_loss improved from 0.66069 to 0.65942, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6579 - accuracy: 0.5945 - val_loss: 0.6594 - val_accuracy: 0.5907\n",
      "Epoch 12/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.6565 - accuracy: 0.5945\n",
      "Epoch 12: val_loss improved from 0.65942 to 0.65825, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6565 - accuracy: 0.5945 - val_loss: 0.6582 - val_accuracy: 0.5907\n",
      "Epoch 13/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.6554 - accuracy: 0.5939\n",
      "Epoch 13: val_loss improved from 0.65825 to 0.65720, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6552 - accuracy: 0.5945 - val_loss: 0.6572 - val_accuracy: 0.5907\n",
      "Epoch 14/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.6539 - accuracy: 0.5955\n",
      "Epoch 14: val_loss improved from 0.65720 to 0.65625, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6541 - accuracy: 0.5945 - val_loss: 0.6563 - val_accuracy: 0.5907\n",
      "Epoch 15/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.6531 - accuracy: 0.5942\n",
      "Epoch 15: val_loss improved from 0.65625 to 0.65537, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6531 - accuracy: 0.5945 - val_loss: 0.6554 - val_accuracy: 0.5907\n",
      "Epoch 16/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.6513 - accuracy: 0.5963\n",
      "Epoch 16: val_loss improved from 0.65537 to 0.65461, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6522 - accuracy: 0.5945 - val_loss: 0.6546 - val_accuracy: 0.5907\n",
      "Epoch 17/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.6511 - accuracy: 0.5946\n",
      "Epoch 17: val_loss improved from 0.65461 to 0.65391, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6513 - accuracy: 0.5945 - val_loss: 0.6539 - val_accuracy: 0.5907\n",
      "Epoch 18/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.6507 - accuracy: 0.5932\n",
      "Epoch 18: val_loss improved from 0.65391 to 0.65326, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6505 - accuracy: 0.5945 - val_loss: 0.6533 - val_accuracy: 0.5907\n",
      "Epoch 19/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.6501 - accuracy: 0.5942\n",
      "Epoch 19: val_loss improved from 0.65326 to 0.65267, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6499 - accuracy: 0.5945 - val_loss: 0.6527 - val_accuracy: 0.5907\n",
      "Epoch 20/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.6490 - accuracy: 0.5955\n",
      "Epoch 20: val_loss improved from 0.65267 to 0.65216, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6491 - accuracy: 0.5945 - val_loss: 0.6522 - val_accuracy: 0.5907\n",
      "Epoch 21/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.6486 - accuracy: 0.5943\n",
      "Epoch 21: val_loss improved from 0.65216 to 0.65165, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6485 - accuracy: 0.5945 - val_loss: 0.6517 - val_accuracy: 0.5907\n",
      "Epoch 22/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.6480 - accuracy: 0.5938\n",
      "Epoch 22: val_loss improved from 0.65165 to 0.65118, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6480 - accuracy: 0.5945 - val_loss: 0.6512 - val_accuracy: 0.5907\n",
      "Epoch 23/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.6475 - accuracy: 0.5946\n",
      "Epoch 23: val_loss improved from 0.65118 to 0.65078, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6475 - accuracy: 0.5945 - val_loss: 0.6508 - val_accuracy: 0.5907\n",
      "Epoch 24/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.6471 - accuracy: 0.5935\n",
      "Epoch 24: val_loss improved from 0.65078 to 0.65043, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6470 - accuracy: 0.5945 - val_loss: 0.6504 - val_accuracy: 0.5907\n",
      "Epoch 25/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.6462 - accuracy: 0.5941\n",
      "Epoch 25: val_loss improved from 0.65043 to 0.65014, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6465 - accuracy: 0.5945 - val_loss: 0.6501 - val_accuracy: 0.5907\n",
      "Epoch 26/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.6453 - accuracy: 0.5982\n",
      "Epoch 26: val_loss improved from 0.65014 to 0.64977, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6461 - accuracy: 0.5945 - val_loss: 0.6498 - val_accuracy: 0.5907\n",
      "Epoch 27/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.6456 - accuracy: 0.5936\n",
      "Epoch 27: val_loss improved from 0.64977 to 0.64945, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6458 - accuracy: 0.5945 - val_loss: 0.6495 - val_accuracy: 0.5907\n",
      "Epoch 28/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.6461 - accuracy: 0.5934\n",
      "Epoch 28: val_loss improved from 0.64945 to 0.64917, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6454 - accuracy: 0.5945 - val_loss: 0.6492 - val_accuracy: 0.5907\n",
      "Epoch 29/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.6457 - accuracy: 0.5940\n",
      "Epoch 29: val_loss improved from 0.64917 to 0.64891, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6450 - accuracy: 0.5945 - val_loss: 0.6489 - val_accuracy: 0.5907\n",
      "Epoch 30/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.6446 - accuracy: 0.5942\n",
      "Epoch 30: val_loss improved from 0.64891 to 0.64867, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6448 - accuracy: 0.5945 - val_loss: 0.6487 - val_accuracy: 0.5907\n",
      "Epoch 31/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.6446 - accuracy: 0.5938\n",
      "Epoch 31: val_loss improved from 0.64867 to 0.64847, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6445 - accuracy: 0.5945 - val_loss: 0.6485 - val_accuracy: 0.5907\n",
      "Epoch 32/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.6442 - accuracy: 0.5960\n",
      "Epoch 32: val_loss improved from 0.64847 to 0.64831, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6442 - accuracy: 0.5945 - val_loss: 0.6483 - val_accuracy: 0.5907\n",
      "Epoch 33/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.6437 - accuracy: 0.5950\n",
      "Epoch 33: val_loss improved from 0.64831 to 0.64808, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6439 - accuracy: 0.5945 - val_loss: 0.6481 - val_accuracy: 0.5907\n",
      "Epoch 34/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.6438 - accuracy: 0.5947\n",
      "Epoch 34: val_loss improved from 0.64808 to 0.64797, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6437 - accuracy: 0.5945 - val_loss: 0.6480 - val_accuracy: 0.5907\n",
      "Epoch 35/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.6431 - accuracy: 0.5961\n",
      "Epoch 35: val_loss improved from 0.64797 to 0.64781, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6435 - accuracy: 0.5945 - val_loss: 0.6478 - val_accuracy: 0.5907\n",
      "Epoch 36/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.6434 - accuracy: 0.5932\n",
      "Epoch 36: val_loss improved from 0.64781 to 0.64764, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6433 - accuracy: 0.5945 - val_loss: 0.6476 - val_accuracy: 0.5907\n",
      "Epoch 37/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.6436 - accuracy: 0.5931\n",
      "Epoch 37: val_loss improved from 0.64764 to 0.64751, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6431 - accuracy: 0.5945 - val_loss: 0.6475 - val_accuracy: 0.5907\n",
      "Epoch 38/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.6427 - accuracy: 0.5941\n",
      "Epoch 38: val_loss improved from 0.64751 to 0.64735, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 3ms/step - loss: 0.6429 - accuracy: 0.5945 - val_loss: 0.6474 - val_accuracy: 0.5907\n",
      "Epoch 39/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.6418 - accuracy: 0.5958\n",
      "Epoch 39: val_loss improved from 0.64735 to 0.64724, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6427 - accuracy: 0.5945 - val_loss: 0.6472 - val_accuracy: 0.5907\n",
      "Epoch 40/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.6424 - accuracy: 0.5943\n",
      "Epoch 40: val_loss improved from 0.64724 to 0.64714, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6426 - accuracy: 0.5945 - val_loss: 0.6471 - val_accuracy: 0.5907\n",
      "Epoch 41/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.6422 - accuracy: 0.5943\n",
      "Epoch 41: val_loss improved from 0.64714 to 0.64703, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6424 - accuracy: 0.5945 - val_loss: 0.6470 - val_accuracy: 0.5907\n",
      "Epoch 42/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.6423 - accuracy: 0.5940\n",
      "Epoch 42: val_loss improved from 0.64703 to 0.64698, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6422 - accuracy: 0.5945 - val_loss: 0.6470 - val_accuracy: 0.5907\n",
      "Epoch 43/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.6423 - accuracy: 0.5944\n",
      "Epoch 43: val_loss improved from 0.64698 to 0.64686, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6422 - accuracy: 0.5945 - val_loss: 0.6469 - val_accuracy: 0.5907\n",
      "Epoch 44/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.6420 - accuracy: 0.5943\n",
      "Epoch 44: val_loss improved from 0.64686 to 0.64677, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6420 - accuracy: 0.5945 - val_loss: 0.6468 - val_accuracy: 0.5907\n",
      "Epoch 45/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.6421 - accuracy: 0.5944\n",
      "Epoch 45: val_loss improved from 0.64677 to 0.64670, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6419 - accuracy: 0.5945 - val_loss: 0.6467 - val_accuracy: 0.5907\n",
      "Epoch 46/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.6417 - accuracy: 0.5948\n",
      "Epoch 46: val_loss improved from 0.64670 to 0.64662, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6418 - accuracy: 0.5945 - val_loss: 0.6466 - val_accuracy: 0.5907\n",
      "Epoch 47/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.6414 - accuracy: 0.5958\n",
      "Epoch 47: val_loss improved from 0.64662 to 0.64658, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6417 - accuracy: 0.5945 - val_loss: 0.6466 - val_accuracy: 0.5907\n",
      "Epoch 48/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.6415 - accuracy: 0.5945\n",
      "Epoch 48: val_loss improved from 0.64658 to 0.64651, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6415 - accuracy: 0.5945 - val_loss: 0.6465 - val_accuracy: 0.5907\n",
      "Epoch 49/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.6410 - accuracy: 0.5952\n",
      "Epoch 49: val_loss improved from 0.64651 to 0.64647, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6415 - accuracy: 0.5945 - val_loss: 0.6465 - val_accuracy: 0.5907\n",
      "Epoch 50/60\n",
      "652/682 [===========================>..] - ETA: 0s - loss: 0.6415 - accuracy: 0.5951\n",
      "Epoch 50: val_loss improved from 0.64647 to 0.64638, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6414 - accuracy: 0.5945 - val_loss: 0.6464 - val_accuracy: 0.5907\n",
      "Epoch 51/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.6415 - accuracy: 0.5949\n",
      "Epoch 51: val_loss improved from 0.64638 to 0.64633, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6413 - accuracy: 0.5945 - val_loss: 0.6463 - val_accuracy: 0.5907\n",
      "Epoch 52/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.6408 - accuracy: 0.5966\n",
      "Epoch 52: val_loss improved from 0.64633 to 0.64628, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6412 - accuracy: 0.5945 - val_loss: 0.6463 - val_accuracy: 0.5907\n",
      "Epoch 53/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.6412 - accuracy: 0.5950\n",
      "Epoch 53: val_loss improved from 0.64628 to 0.64625, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6411 - accuracy: 0.5945 - val_loss: 0.6463 - val_accuracy: 0.5907\n",
      "Epoch 54/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.6411 - accuracy: 0.5951\n",
      "Epoch 54: val_loss improved from 0.64625 to 0.64620, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6411 - accuracy: 0.5945 - val_loss: 0.6462 - val_accuracy: 0.5907\n",
      "Epoch 55/60\n",
      "654/682 [===========================>..] - ETA: 0s - loss: 0.6409 - accuracy: 0.5956\n",
      "Epoch 55: val_loss improved from 0.64620 to 0.64616, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6410 - accuracy: 0.5945 - val_loss: 0.6462 - val_accuracy: 0.5907\n",
      "Epoch 56/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.6413 - accuracy: 0.5930\n",
      "Epoch 56: val_loss improved from 0.64616 to 0.64612, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6409 - accuracy: 0.5945 - val_loss: 0.6461 - val_accuracy: 0.5907\n",
      "Epoch 57/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.6414 - accuracy: 0.5941\n",
      "Epoch 57: val_loss did not improve from 0.64612\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6408 - accuracy: 0.5945 - val_loss: 0.6462 - val_accuracy: 0.5907\n",
      "Epoch 58/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.6409 - accuracy: 0.5950\n",
      "Epoch 58: val_loss improved from 0.64612 to 0.64607, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6408 - accuracy: 0.5945 - val_loss: 0.6461 - val_accuracy: 0.5907\n",
      "Epoch 59/60\n",
      "651/682 [===========================>..] - ETA: 0s - loss: 0.6411 - accuracy: 0.5931\n",
      "Epoch 59: val_loss improved from 0.64607 to 0.64606, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6407 - accuracy: 0.5945 - val_loss: 0.6461 - val_accuracy: 0.5907\n",
      "Epoch 60/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.6403 - accuracy: 0.5953\n",
      "Epoch 60: val_loss improved from 0.64606 to 0.64602, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6407 - accuracy: 0.5945 - val_loss: 0.6460 - val_accuracy: 0.5907\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.6460 - accuracy: 0.5907\n",
      "Processing feature: previous\n",
      "data frame x has shape: (8516, 1)\n",
      "Epoch 1/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.6836 - accuracy: 0.5643\n",
      "Epoch 1: val_loss improved from inf to 0.68261, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6836 - accuracy: 0.5658 - val_loss: 0.6826 - val_accuracy: 0.5972\n",
      "Epoch 2/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.6801 - accuracy: 0.5996\n",
      "Epoch 2: val_loss improved from 0.68261 to 0.67966, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6801 - accuracy: 0.5994 - val_loss: 0.6797 - val_accuracy: 0.5972\n",
      "Epoch 3/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.6773 - accuracy: 0.5979\n",
      "Epoch 3: val_loss improved from 0.67966 to 0.67715, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6771 - accuracy: 0.5994 - val_loss: 0.6771 - val_accuracy: 0.5972\n",
      "Epoch 4/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.6747 - accuracy: 0.5968\n",
      "Epoch 4: val_loss improved from 0.67715 to 0.67495, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6746 - accuracy: 0.5994 - val_loss: 0.6749 - val_accuracy: 0.5972\n",
      "Epoch 5/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.6723 - accuracy: 0.5994\n",
      "Epoch 5: val_loss improved from 0.67495 to 0.67303, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6723 - accuracy: 0.5994 - val_loss: 0.6730 - val_accuracy: 0.5972\n",
      "Epoch 6/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.6704 - accuracy: 0.5996\n",
      "Epoch 6: val_loss improved from 0.67303 to 0.67138, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6703 - accuracy: 0.5994 - val_loss: 0.6714 - val_accuracy: 0.5972\n",
      "Epoch 7/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.6687 - accuracy: 0.5996\n",
      "Epoch 7: val_loss improved from 0.67138 to 0.66996, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6687 - accuracy: 0.5994 - val_loss: 0.6700 - val_accuracy: 0.5972\n",
      "Epoch 8/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.6675 - accuracy: 0.5987\n",
      "Epoch 8: val_loss improved from 0.66996 to 0.66876, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6672 - accuracy: 0.5994 - val_loss: 0.6688 - val_accuracy: 0.5972\n",
      "Epoch 9/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.6661 - accuracy: 0.5991\n",
      "Epoch 9: val_loss improved from 0.66876 to 0.66772, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6660 - accuracy: 0.5994 - val_loss: 0.6677 - val_accuracy: 0.5972\n",
      "Epoch 10/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.6646 - accuracy: 0.5994\n",
      "Epoch 10: val_loss improved from 0.66772 to 0.66682, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6649 - accuracy: 0.5994 - val_loss: 0.6668 - val_accuracy: 0.5972\n",
      "Epoch 11/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.6641 - accuracy: 0.6006\n",
      "Epoch 11: val_loss improved from 0.66682 to 0.66608, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6640 - accuracy: 0.5994 - val_loss: 0.6661 - val_accuracy: 0.5972\n",
      "Epoch 12/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.6635 - accuracy: 0.5985\n",
      "Epoch 12: val_loss improved from 0.66608 to 0.66544, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6632 - accuracy: 0.5994 - val_loss: 0.6654 - val_accuracy: 0.5972\n",
      "Epoch 13/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.6627 - accuracy: 0.5997\n",
      "Epoch 13: val_loss improved from 0.66544 to 0.66488, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6625 - accuracy: 0.5994 - val_loss: 0.6649 - val_accuracy: 0.5972\n",
      "Epoch 14/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.6619 - accuracy: 0.6005\n",
      "Epoch 14: val_loss improved from 0.66488 to 0.66441, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6620 - accuracy: 0.5994 - val_loss: 0.6644 - val_accuracy: 0.5972\n",
      "Epoch 15/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.6619 - accuracy: 0.5992\n",
      "Epoch 15: val_loss improved from 0.66441 to 0.66404, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6614 - accuracy: 0.5994 - val_loss: 0.6640 - val_accuracy: 0.5972\n",
      "Epoch 16/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.6614 - accuracy: 0.5994\n",
      "Epoch 16: val_loss improved from 0.66404 to 0.66368, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6610 - accuracy: 0.5994 - val_loss: 0.6637 - val_accuracy: 0.5972\n",
      "Epoch 17/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.6606 - accuracy: 0.5996\n",
      "Epoch 17: val_loss improved from 0.66368 to 0.66339, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6606 - accuracy: 0.5994 - val_loss: 0.6634 - val_accuracy: 0.5972\n",
      "Epoch 18/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.6605 - accuracy: 0.5982\n",
      "Epoch 18: val_loss improved from 0.66339 to 0.66315, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6604 - accuracy: 0.5994 - val_loss: 0.6631 - val_accuracy: 0.5972\n",
      "Epoch 19/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.6603 - accuracy: 0.5993\n",
      "Epoch 19: val_loss improved from 0.66315 to 0.66295, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6601 - accuracy: 0.5994 - val_loss: 0.6629 - val_accuracy: 0.5972\n",
      "Epoch 20/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.6598 - accuracy: 0.5991\n",
      "Epoch 20: val_loss improved from 0.66295 to 0.66277, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6598 - accuracy: 0.5994 - val_loss: 0.6628 - val_accuracy: 0.5972\n",
      "Epoch 21/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.6588 - accuracy: 0.6018\n",
      "Epoch 21: val_loss improved from 0.66277 to 0.66261, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6597 - accuracy: 0.5994 - val_loss: 0.6626 - val_accuracy: 0.5972\n",
      "Epoch 22/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.6598 - accuracy: 0.5982\n",
      "Epoch 22: val_loss improved from 0.66261 to 0.66251, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6595 - accuracy: 0.5994 - val_loss: 0.6625 - val_accuracy: 0.5972\n",
      "Epoch 23/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.6598 - accuracy: 0.5979\n",
      "Epoch 23: val_loss improved from 0.66251 to 0.66240, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6594 - accuracy: 0.5994 - val_loss: 0.6624 - val_accuracy: 0.5972\n",
      "Epoch 24/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.6593 - accuracy: 0.5998\n",
      "Epoch 24: val_loss improved from 0.66240 to 0.66231, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6592 - accuracy: 0.5994 - val_loss: 0.6623 - val_accuracy: 0.5972\n",
      "Epoch 25/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.6593 - accuracy: 0.5986\n",
      "Epoch 25: val_loss improved from 0.66231 to 0.66223, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6591 - accuracy: 0.5994 - val_loss: 0.6622 - val_accuracy: 0.5972\n",
      "Epoch 26/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.6598 - accuracy: 0.5988\n",
      "Epoch 26: val_loss improved from 0.66223 to 0.66215, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6590 - accuracy: 0.5994 - val_loss: 0.6622 - val_accuracy: 0.5972\n",
      "Epoch 27/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.6586 - accuracy: 0.5997\n",
      "Epoch 27: val_loss improved from 0.66215 to 0.66211, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6589 - accuracy: 0.5994 - val_loss: 0.6621 - val_accuracy: 0.5972\n",
      "Epoch 28/60\n",
      "651/682 [===========================>..] - ETA: 0s - loss: 0.6585 - accuracy: 0.6012\n",
      "Epoch 28: val_loss improved from 0.66211 to 0.66208, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6589 - accuracy: 0.5994 - val_loss: 0.6621 - val_accuracy: 0.5972\n",
      "Epoch 29/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.6588 - accuracy: 0.5994\n",
      "Epoch 29: val_loss improved from 0.66208 to 0.66202, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6588 - accuracy: 0.5994 - val_loss: 0.6620 - val_accuracy: 0.5972\n",
      "Epoch 30/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.6596 - accuracy: 0.5978\n",
      "Epoch 30: val_loss improved from 0.66202 to 0.66198, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6588 - accuracy: 0.5994 - val_loss: 0.6620 - val_accuracy: 0.5972\n",
      "Epoch 31/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.6587 - accuracy: 0.5994\n",
      "Epoch 31: val_loss improved from 0.66198 to 0.66195, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6587 - accuracy: 0.5994 - val_loss: 0.6619 - val_accuracy: 0.5972\n",
      "Epoch 32/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.6587 - accuracy: 0.5991\n",
      "Epoch 32: val_loss improved from 0.66195 to 0.66193, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6587 - accuracy: 0.5994 - val_loss: 0.6619 - val_accuracy: 0.5972\n",
      "Epoch 33/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.6578 - accuracy: 0.6014\n",
      "Epoch 33: val_loss did not improve from 0.66193\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6587 - accuracy: 0.5994 - val_loss: 0.6619 - val_accuracy: 0.5972\n",
      "Epoch 34/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.6587 - accuracy: 0.5996\n",
      "Epoch 34: val_loss improved from 0.66193 to 0.66189, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6586 - accuracy: 0.5994 - val_loss: 0.6619 - val_accuracy: 0.5972\n",
      "Epoch 35/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.6585 - accuracy: 0.5990\n",
      "Epoch 35: val_loss improved from 0.66189 to 0.66187, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6586 - accuracy: 0.5994 - val_loss: 0.6619 - val_accuracy: 0.5972\n",
      "Epoch 36/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.6584 - accuracy: 0.5999\n",
      "Epoch 36: val_loss improved from 0.66187 to 0.66186, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6586 - accuracy: 0.5994 - val_loss: 0.6619 - val_accuracy: 0.5972\n",
      "Epoch 37/60\n",
      "654/682 [===========================>..] - ETA: 0s - loss: 0.6592 - accuracy: 0.5989\n",
      "Epoch 37: val_loss improved from 0.66186 to 0.66185, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6585 - accuracy: 0.5994 - val_loss: 0.6618 - val_accuracy: 0.5972\n",
      "Epoch 38/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.6584 - accuracy: 0.5997\n",
      "Epoch 38: val_loss improved from 0.66185 to 0.66184, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6585 - accuracy: 0.5994 - val_loss: 0.6618 - val_accuracy: 0.5972\n",
      "Epoch 39/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.6586 - accuracy: 0.5984\n",
      "Epoch 39: val_loss did not improve from 0.66184\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6585 - accuracy: 0.5994 - val_loss: 0.6619 - val_accuracy: 0.5972\n",
      "Epoch 40/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.6577 - accuracy: 0.6006\n",
      "Epoch 40: val_loss did not improve from 0.66184\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6585 - accuracy: 0.5994 - val_loss: 0.6618 - val_accuracy: 0.5972\n",
      "Epoch 41/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.6584 - accuracy: 0.6000\n",
      "Epoch 41: val_loss improved from 0.66184 to 0.66183, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6585 - accuracy: 0.5994 - val_loss: 0.6618 - val_accuracy: 0.5972\n",
      "Epoch 42/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.6585 - accuracy: 0.5993\n",
      "Epoch 42: val_loss did not improve from 0.66183\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6584 - accuracy: 0.5994 - val_loss: 0.6618 - val_accuracy: 0.5972\n",
      "Epoch 43/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.6584 - accuracy: 0.5991\n",
      "Epoch 43: val_loss did not improve from 0.66183\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6585 - accuracy: 0.5994 - val_loss: 0.6618 - val_accuracy: 0.5972\n",
      "Epoch 44/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.6584 - accuracy: 0.5999\n",
      "Epoch 44: val_loss did not improve from 0.66183\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6585 - accuracy: 0.5994 - val_loss: 0.6618 - val_accuracy: 0.5972\n",
      "Epoch 45/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.6589 - accuracy: 0.5983\n",
      "Epoch 45: val_loss improved from 0.66183 to 0.66181, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6585 - accuracy: 0.5994 - val_loss: 0.6618 - val_accuracy: 0.5972\n",
      "Epoch 46/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.6578 - accuracy: 0.5997\n",
      "Epoch 46: val_loss did not improve from 0.66181\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6584 - accuracy: 0.5994 - val_loss: 0.6618 - val_accuracy: 0.5972\n",
      "Epoch 47/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.6585 - accuracy: 0.5994\n",
      "Epoch 47: val_loss did not improve from 0.66181\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6585 - accuracy: 0.5994 - val_loss: 0.6618 - val_accuracy: 0.5972\n",
      "Epoch 48/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.6587 - accuracy: 0.5999\n",
      "Epoch 48: val_loss did not improve from 0.66181\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6584 - accuracy: 0.5994 - val_loss: 0.6618 - val_accuracy: 0.5972\n",
      "Epoch 49/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.6585 - accuracy: 0.5996\n",
      "Epoch 49: val_loss did not improve from 0.66181\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6584 - accuracy: 0.5994 - val_loss: 0.6618 - val_accuracy: 0.5972\n",
      "Epoch 50/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.6576 - accuracy: 0.6009\n",
      "Epoch 50: val_loss improved from 0.66181 to 0.66180, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6584 - accuracy: 0.5994 - val_loss: 0.6618 - val_accuracy: 0.5972\n",
      "Epoch 51/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.6572 - accuracy: 0.6011\n",
      "Epoch 51: val_loss did not improve from 0.66180\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6584 - accuracy: 0.5994 - val_loss: 0.6618 - val_accuracy: 0.5972\n",
      "Epoch 52/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.6584 - accuracy: 0.5998\n",
      "Epoch 52: val_loss did not improve from 0.66180\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6584 - accuracy: 0.5994 - val_loss: 0.6618 - val_accuracy: 0.5972\n",
      "Epoch 53/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.6587 - accuracy: 0.5997\n",
      "Epoch 53: val_loss did not improve from 0.66180\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6584 - accuracy: 0.5994 - val_loss: 0.6618 - val_accuracy: 0.5972\n",
      "Epoch 54/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.6583 - accuracy: 0.5989\n",
      "Epoch 54: val_loss did not improve from 0.66180\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6584 - accuracy: 0.5994 - val_loss: 0.6618 - val_accuracy: 0.5972\n",
      "Epoch 55/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.6585 - accuracy: 0.5993\n",
      "Epoch 55: val_loss did not improve from 0.66180\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6584 - accuracy: 0.5994 - val_loss: 0.6618 - val_accuracy: 0.5972\n",
      "Epoch 56/60\n",
      "652/682 [===========================>..] - ETA: 0s - loss: 0.6590 - accuracy: 0.5980\n",
      "Epoch 56: val_loss did not improve from 0.66180\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6584 - accuracy: 0.5994 - val_loss: 0.6618 - val_accuracy: 0.5972\n",
      "Epoch 57/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.6584 - accuracy: 0.5997\n",
      "Epoch 57: val_loss did not improve from 0.66180\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6584 - accuracy: 0.5994 - val_loss: 0.6618 - val_accuracy: 0.5972\n",
      "Epoch 58/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.6598 - accuracy: 0.5976\n",
      "Epoch 58: val_loss did not improve from 0.66180\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6584 - accuracy: 0.5994 - val_loss: 0.6618 - val_accuracy: 0.5972\n",
      "Epoch 59/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.6576 - accuracy: 0.6018\n",
      "Epoch 59: val_loss did not improve from 0.66180\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6584 - accuracy: 0.5994 - val_loss: 0.6618 - val_accuracy: 0.5972\n",
      "Epoch 60/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.6582 - accuracy: 0.5997\n",
      "Epoch 60: val_loss did not improve from 0.66180\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6584 - accuracy: 0.5994 - val_loss: 0.6618 - val_accuracy: 0.5972\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.6618 - accuracy: 0.5972\n",
      "Processing feature: poutcome\n",
      "data frame x has shape: (8516, 1)\n",
      "Epoch 1/60\n",
      "650/682 [===========================>..] - ETA: 0s - loss: 0.6505 - accuracy: 0.5894\n",
      "Epoch 1: val_loss improved from inf to 0.65059, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6511 - accuracy: 0.5874 - val_loss: 0.6506 - val_accuracy: 0.5872\n",
      "Epoch 2/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.6477 - accuracy: 0.5876\n",
      "Epoch 2: val_loss improved from 0.65059 to 0.64850, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6478 - accuracy: 0.5874 - val_loss: 0.6485 - val_accuracy: 0.5872\n",
      "Epoch 3/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.6458 - accuracy: 0.5879\n",
      "Epoch 3: val_loss improved from 0.64850 to 0.64742, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6461 - accuracy: 0.5874 - val_loss: 0.6474 - val_accuracy: 0.5872\n",
      "Epoch 4/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.6450 - accuracy: 0.5876\n",
      "Epoch 4: val_loss improved from 0.64742 to 0.64690, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6451 - accuracy: 0.5874 - val_loss: 0.6469 - val_accuracy: 0.5872\n",
      "Epoch 5/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.6446 - accuracy: 0.5873\n",
      "Epoch 5: val_loss improved from 0.64690 to 0.64657, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6445 - accuracy: 0.5874 - val_loss: 0.6466 - val_accuracy: 0.5872\n",
      "Epoch 6/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.6434 - accuracy: 0.5862\n",
      "Epoch 6: val_loss improved from 0.64657 to 0.64639, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6442 - accuracy: 0.5874 - val_loss: 0.6464 - val_accuracy: 0.5872\n",
      "Epoch 7/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.6436 - accuracy: 0.5883\n",
      "Epoch 7: val_loss improved from 0.64639 to 0.64630, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6440 - accuracy: 0.5874 - val_loss: 0.6463 - val_accuracy: 0.5872\n",
      "Epoch 8/60\n",
      "653/682 [===========================>..] - ETA: 0s - loss: 0.6439 - accuracy: 0.5896\n",
      "Epoch 8: val_loss improved from 0.64630 to 0.64628, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6438 - accuracy: 0.5874 - val_loss: 0.6463 - val_accuracy: 0.5872\n",
      "Epoch 9/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.6428 - accuracy: 0.5889\n",
      "Epoch 9: val_loss improved from 0.64628 to 0.64624, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6437 - accuracy: 0.5874 - val_loss: 0.6462 - val_accuracy: 0.5872\n",
      "Epoch 10/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.6436 - accuracy: 0.5874\n",
      "Epoch 10: val_loss did not improve from 0.64624\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6436 - accuracy: 0.5874 - val_loss: 0.6463 - val_accuracy: 0.5872\n",
      "Epoch 11/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.6430 - accuracy: 0.5871\n",
      "Epoch 11: val_loss did not improve from 0.64624\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6436 - accuracy: 0.5874 - val_loss: 0.6463 - val_accuracy: 0.5872\n",
      "Epoch 12/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.6438 - accuracy: 0.5877\n",
      "Epoch 12: val_loss did not improve from 0.64624\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6436 - accuracy: 0.5874 - val_loss: 0.6463 - val_accuracy: 0.5872\n",
      "Epoch 13/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.6439 - accuracy: 0.5871\n",
      "Epoch 13: val_loss did not improve from 0.64624\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6435 - accuracy: 0.5874 - val_loss: 0.6463 - val_accuracy: 0.5872\n",
      "Epoch 14/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.6438 - accuracy: 0.5882\n",
      "Epoch 14: val_loss did not improve from 0.64624\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6435 - accuracy: 0.5874 - val_loss: 0.6463 - val_accuracy: 0.5872\n",
      "Epoch 15/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.6434 - accuracy: 0.5880\n",
      "Epoch 15: val_loss did not improve from 0.64624\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6435 - accuracy: 0.5874 - val_loss: 0.6463 - val_accuracy: 0.5872\n",
      "Epoch 16/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.6437 - accuracy: 0.5859\n",
      "Epoch 16: val_loss did not improve from 0.64624\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6435 - accuracy: 0.5874 - val_loss: 0.6463 - val_accuracy: 0.5872\n",
      "Epoch 17/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.6430 - accuracy: 0.5870\n",
      "Epoch 17: val_loss did not improve from 0.64624\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6435 - accuracy: 0.5874 - val_loss: 0.6463 - val_accuracy: 0.5872\n",
      "Epoch 18/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.6438 - accuracy: 0.5860\n",
      "Epoch 18: val_loss did not improve from 0.64624\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6435 - accuracy: 0.5874 - val_loss: 0.6463 - val_accuracy: 0.5872\n",
      "Epoch 19/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.6433 - accuracy: 0.5877\n",
      "Epoch 19: val_loss did not improve from 0.64624\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6435 - accuracy: 0.5874 - val_loss: 0.6463 - val_accuracy: 0.5872\n",
      "Epoch 20/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.6435 - accuracy: 0.5883\n",
      "Epoch 20: val_loss did not improve from 0.64624\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6435 - accuracy: 0.5874 - val_loss: 0.6464 - val_accuracy: 0.5872\n",
      "Epoch 21/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.6435 - accuracy: 0.5876\n",
      "Epoch 21: val_loss did not improve from 0.64624\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6435 - accuracy: 0.5874 - val_loss: 0.6464 - val_accuracy: 0.5872\n",
      "Epoch 22/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.6442 - accuracy: 0.5866\n",
      "Epoch 22: val_loss did not improve from 0.64624\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6435 - accuracy: 0.5874 - val_loss: 0.6464 - val_accuracy: 0.5872\n",
      "Epoch 23/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.6436 - accuracy: 0.5871\n",
      "Epoch 23: val_loss did not improve from 0.64624\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6435 - accuracy: 0.5874 - val_loss: 0.6464 - val_accuracy: 0.5872\n",
      "Epoch 24/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.6432 - accuracy: 0.5867\n",
      "Epoch 24: val_loss did not improve from 0.64624\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6435 - accuracy: 0.5874 - val_loss: 0.6464 - val_accuracy: 0.5872\n",
      "Epoch 25/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.6435 - accuracy: 0.5876\n",
      "Epoch 25: val_loss did not improve from 0.64624\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6435 - accuracy: 0.5874 - val_loss: 0.6464 - val_accuracy: 0.5872\n",
      "Epoch 26/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.6440 - accuracy: 0.5850\n",
      "Epoch 26: val_loss did not improve from 0.64624\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6435 - accuracy: 0.5874 - val_loss: 0.6464 - val_accuracy: 0.5872\n",
      "Epoch 27/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.6435 - accuracy: 0.5875\n",
      "Epoch 27: val_loss did not improve from 0.64624\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6435 - accuracy: 0.5874 - val_loss: 0.6464 - val_accuracy: 0.5872\n",
      "Epoch 28/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.6430 - accuracy: 0.5878\n",
      "Epoch 28: val_loss did not improve from 0.64624\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6435 - accuracy: 0.5874 - val_loss: 0.6464 - val_accuracy: 0.5872\n",
      "Epoch 29/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.6435 - accuracy: 0.5866\n",
      "Epoch 29: val_loss did not improve from 0.64624\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6435 - accuracy: 0.5874 - val_loss: 0.6464 - val_accuracy: 0.5872\n",
      "Epoch 29: early stopping\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.6462 - accuracy: 0.5872\n",
      "Processing feature: emp.var.rate\n",
      "data frame x has shape: (8516, 1)\n",
      "Epoch 1/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.6675 - accuracy: 0.5783\n",
      "Epoch 1: val_loss improved from inf to 0.63609, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6670 - accuracy: 0.5780 - val_loss: 0.6361 - val_accuracy: 0.6025\n",
      "Epoch 2/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.6305 - accuracy: 0.6712\n",
      "Epoch 2: val_loss improved from 0.63609 to 0.62274, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6306 - accuracy: 0.6712 - val_loss: 0.6227 - val_accuracy: 0.7158\n",
      "Epoch 3/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.6225 - accuracy: 0.7139\n",
      "Epoch 3: val_loss improved from 0.62274 to 0.61732, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6226 - accuracy: 0.7132 - val_loss: 0.6173 - val_accuracy: 0.7158\n",
      "Epoch 4/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.6180 - accuracy: 0.7130\n",
      "Epoch 4: val_loss improved from 0.61732 to 0.61318, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6180 - accuracy: 0.7132 - val_loss: 0.6132 - val_accuracy: 0.7158\n",
      "Epoch 5/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.6145 - accuracy: 0.7134\n",
      "Epoch 5: val_loss improved from 0.61318 to 0.60980, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6143 - accuracy: 0.7132 - val_loss: 0.6098 - val_accuracy: 0.7158\n",
      "Epoch 6/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.6123 - accuracy: 0.7121\n",
      "Epoch 6: val_loss improved from 0.60980 to 0.60707, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6115 - accuracy: 0.7132 - val_loss: 0.6071 - val_accuracy: 0.7158\n",
      "Epoch 7/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.6095 - accuracy: 0.7126\n",
      "Epoch 7: val_loss improved from 0.60707 to 0.60485, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6092 - accuracy: 0.7132 - val_loss: 0.6049 - val_accuracy: 0.7158\n",
      "Epoch 8/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.6080 - accuracy: 0.7116\n",
      "Epoch 8: val_loss improved from 0.60485 to 0.60313, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6073 - accuracy: 0.7132 - val_loss: 0.6031 - val_accuracy: 0.7158\n",
      "Epoch 9/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.6067 - accuracy: 0.7115\n",
      "Epoch 9: val_loss improved from 0.60313 to 0.60173, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6058 - accuracy: 0.7132 - val_loss: 0.6017 - val_accuracy: 0.7158\n",
      "Epoch 10/60\n",
      "654/682 [===========================>..] - ETA: 0s - loss: 0.6041 - accuracy: 0.7144\n",
      "Epoch 10: val_loss improved from 0.60173 to 0.60059, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6047 - accuracy: 0.7132 - val_loss: 0.6006 - val_accuracy: 0.7158\n",
      "Epoch 11/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.6037 - accuracy: 0.7132\n",
      "Epoch 11: val_loss improved from 0.60059 to 0.59978, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6037 - accuracy: 0.7132 - val_loss: 0.5998 - val_accuracy: 0.7158\n",
      "Epoch 12/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.6027 - accuracy: 0.7139\n",
      "Epoch 12: val_loss improved from 0.59978 to 0.59907, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6031 - accuracy: 0.7132 - val_loss: 0.5991 - val_accuracy: 0.7158\n",
      "Epoch 13/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.6026 - accuracy: 0.7133\n",
      "Epoch 13: val_loss improved from 0.59907 to 0.59849, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6025 - accuracy: 0.7132 - val_loss: 0.5985 - val_accuracy: 0.7158\n",
      "Epoch 14/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.6021 - accuracy: 0.7131\n",
      "Epoch 14: val_loss improved from 0.59849 to 0.59804, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6020 - accuracy: 0.7132 - val_loss: 0.5980 - val_accuracy: 0.7158\n",
      "Epoch 15/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.6010 - accuracy: 0.7136\n",
      "Epoch 15: val_loss improved from 0.59804 to 0.59765, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6016 - accuracy: 0.7132 - val_loss: 0.5976 - val_accuracy: 0.7158\n",
      "Epoch 16/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.6006 - accuracy: 0.7148\n",
      "Epoch 16: val_loss improved from 0.59765 to 0.59732, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6013 - accuracy: 0.7132 - val_loss: 0.5973 - val_accuracy: 0.7158\n",
      "Epoch 17/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.6020 - accuracy: 0.7126\n",
      "Epoch 17: val_loss improved from 0.59732 to 0.59708, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6011 - accuracy: 0.7132 - val_loss: 0.5971 - val_accuracy: 0.7158\n",
      "Epoch 18/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.6007 - accuracy: 0.7135\n",
      "Epoch 18: val_loss improved from 0.59708 to 0.59685, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6009 - accuracy: 0.7132 - val_loss: 0.5969 - val_accuracy: 0.7158\n",
      "Epoch 19/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.6011 - accuracy: 0.7135\n",
      "Epoch 19: val_loss improved from 0.59685 to 0.59669, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6007 - accuracy: 0.7132 - val_loss: 0.5967 - val_accuracy: 0.7158\n",
      "Epoch 20/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.6010 - accuracy: 0.7128\n",
      "Epoch 20: val_loss improved from 0.59669 to 0.59655, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6006 - accuracy: 0.7132 - val_loss: 0.5966 - val_accuracy: 0.7158\n",
      "Epoch 21/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.6011 - accuracy: 0.7130\n",
      "Epoch 21: val_loss improved from 0.59655 to 0.59645, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6005 - accuracy: 0.7132 - val_loss: 0.5964 - val_accuracy: 0.7158\n",
      "Epoch 22/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.5991 - accuracy: 0.7144\n",
      "Epoch 22: val_loss improved from 0.59645 to 0.59635, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6004 - accuracy: 0.7132 - val_loss: 0.5964 - val_accuracy: 0.7158\n",
      "Epoch 23/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.6004 - accuracy: 0.7131\n",
      "Epoch 23: val_loss improved from 0.59635 to 0.59631, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6003 - accuracy: 0.7132 - val_loss: 0.5963 - val_accuracy: 0.7158\n",
      "Epoch 24/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.6012 - accuracy: 0.7127\n",
      "Epoch 24: val_loss improved from 0.59631 to 0.59624, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6003 - accuracy: 0.7132 - val_loss: 0.5962 - val_accuracy: 0.7158\n",
      "Epoch 25/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.5990 - accuracy: 0.7148\n",
      "Epoch 25: val_loss improved from 0.59624 to 0.59614, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6002 - accuracy: 0.7132 - val_loss: 0.5961 - val_accuracy: 0.7158\n",
      "Epoch 26/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.6001 - accuracy: 0.7132\n",
      "Epoch 26: val_loss improved from 0.59614 to 0.59611, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6002 - accuracy: 0.7132 - val_loss: 0.5961 - val_accuracy: 0.7158\n",
      "Epoch 27/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.6007 - accuracy: 0.7131\n",
      "Epoch 27: val_loss improved from 0.59611 to 0.59605, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6001 - accuracy: 0.7132 - val_loss: 0.5961 - val_accuracy: 0.7158\n",
      "Epoch 28/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.6002 - accuracy: 0.7132\n",
      "Epoch 28: val_loss improved from 0.59605 to 0.59601, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6002 - accuracy: 0.7132 - val_loss: 0.5960 - val_accuracy: 0.7158\n",
      "Epoch 29/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.6001 - accuracy: 0.7131\n",
      "Epoch 29: val_loss improved from 0.59601 to 0.59600, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6001 - accuracy: 0.7132 - val_loss: 0.5960 - val_accuracy: 0.7158\n",
      "Epoch 30/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.6004 - accuracy: 0.7130\n",
      "Epoch 30: val_loss did not improve from 0.59600\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6000 - accuracy: 0.7132 - val_loss: 0.5962 - val_accuracy: 0.7158\n",
      "Epoch 31/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.6002 - accuracy: 0.7133\n",
      "Epoch 31: val_loss improved from 0.59600 to 0.59597, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6002 - accuracy: 0.7132 - val_loss: 0.5960 - val_accuracy: 0.7158\n",
      "Epoch 32/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.6005 - accuracy: 0.7124\n",
      "Epoch 32: val_loss did not improve from 0.59597\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6001 - accuracy: 0.7132 - val_loss: 0.5960 - val_accuracy: 0.7158\n",
      "Epoch 33/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.6005 - accuracy: 0.7127\n",
      "Epoch 33: val_loss improved from 0.59597 to 0.59592, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6001 - accuracy: 0.7132 - val_loss: 0.5959 - val_accuracy: 0.7158\n",
      "Epoch 34/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.5992 - accuracy: 0.7139\n",
      "Epoch 34: val_loss improved from 0.59592 to 0.59592, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6001 - accuracy: 0.7132 - val_loss: 0.5959 - val_accuracy: 0.7158\n",
      "Epoch 35/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.5995 - accuracy: 0.7137\n",
      "Epoch 35: val_loss improved from 0.59592 to 0.59590, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6001 - accuracy: 0.7132 - val_loss: 0.5959 - val_accuracy: 0.7158\n",
      "Epoch 36/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.6007 - accuracy: 0.7126\n",
      "Epoch 36: val_loss improved from 0.59590 to 0.59590, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6001 - accuracy: 0.7132 - val_loss: 0.5959 - val_accuracy: 0.7158\n",
      "Epoch 37/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.5994 - accuracy: 0.7142\n",
      "Epoch 37: val_loss improved from 0.59590 to 0.59589, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6000 - accuracy: 0.7132 - val_loss: 0.5959 - val_accuracy: 0.7158\n",
      "Epoch 38/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.6001 - accuracy: 0.7131\n",
      "Epoch 38: val_loss improved from 0.59589 to 0.59588, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6000 - accuracy: 0.7132 - val_loss: 0.5959 - val_accuracy: 0.7158\n",
      "Epoch 39/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.5999 - accuracy: 0.7139\n",
      "Epoch 39: val_loss improved from 0.59588 to 0.59587, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6001 - accuracy: 0.7132 - val_loss: 0.5959 - val_accuracy: 0.7158\n",
      "Epoch 40/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.6000 - accuracy: 0.7134\n",
      "Epoch 40: val_loss did not improve from 0.59587\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6000 - accuracy: 0.7132 - val_loss: 0.5959 - val_accuracy: 0.7158\n",
      "Epoch 41/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.6008 - accuracy: 0.7123\n",
      "Epoch 41: val_loss improved from 0.59587 to 0.59586, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6000 - accuracy: 0.7132 - val_loss: 0.5959 - val_accuracy: 0.7158\n",
      "Epoch 42/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.6011 - accuracy: 0.7122\n",
      "Epoch 42: val_loss did not improve from 0.59586\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6000 - accuracy: 0.7132 - val_loss: 0.5959 - val_accuracy: 0.7158\n",
      "Epoch 43/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.6001 - accuracy: 0.7133\n",
      "Epoch 43: val_loss did not improve from 0.59586\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6000 - accuracy: 0.7132 - val_loss: 0.5959 - val_accuracy: 0.7158\n",
      "Epoch 44/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.6003 - accuracy: 0.7124\n",
      "Epoch 44: val_loss improved from 0.59586 to 0.59585, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6001 - accuracy: 0.7132 - val_loss: 0.5958 - val_accuracy: 0.7158\n",
      "Epoch 45/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.5999 - accuracy: 0.7134\n",
      "Epoch 45: val_loss improved from 0.59585 to 0.59585, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6000 - accuracy: 0.7132 - val_loss: 0.5958 - val_accuracy: 0.7158\n",
      "Epoch 46/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.5983 - accuracy: 0.7150\n",
      "Epoch 46: val_loss improved from 0.59585 to 0.59585, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6000 - accuracy: 0.7132 - val_loss: 0.5958 - val_accuracy: 0.7158\n",
      "Epoch 47/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.6006 - accuracy: 0.7126\n",
      "Epoch 47: val_loss did not improve from 0.59585\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6000 - accuracy: 0.7132 - val_loss: 0.5959 - val_accuracy: 0.7158\n",
      "Epoch 48/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.6002 - accuracy: 0.7130\n",
      "Epoch 48: val_loss improved from 0.59585 to 0.59585, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6000 - accuracy: 0.7132 - val_loss: 0.5958 - val_accuracy: 0.7158\n",
      "Epoch 49/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.6013 - accuracy: 0.7115\n",
      "Epoch 49: val_loss improved from 0.59585 to 0.59584, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6000 - accuracy: 0.7132 - val_loss: 0.5958 - val_accuracy: 0.7158\n",
      "Epoch 50/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.5999 - accuracy: 0.7134\n",
      "Epoch 50: val_loss did not improve from 0.59584\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6000 - accuracy: 0.7132 - val_loss: 0.5959 - val_accuracy: 0.7158\n",
      "Epoch 51/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.5993 - accuracy: 0.7138\n",
      "Epoch 51: val_loss improved from 0.59584 to 0.59584, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6000 - accuracy: 0.7132 - val_loss: 0.5958 - val_accuracy: 0.7158\n",
      "Epoch 52/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.6000 - accuracy: 0.7133\n",
      "Epoch 52: val_loss did not improve from 0.59584\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6000 - accuracy: 0.7132 - val_loss: 0.5959 - val_accuracy: 0.7158\n",
      "Epoch 53/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.5996 - accuracy: 0.7134\n",
      "Epoch 53: val_loss improved from 0.59584 to 0.59584, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6000 - accuracy: 0.7132 - val_loss: 0.5958 - val_accuracy: 0.7158\n",
      "Epoch 54/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.6003 - accuracy: 0.7130\n",
      "Epoch 54: val_loss did not improve from 0.59584\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6000 - accuracy: 0.7132 - val_loss: 0.5959 - val_accuracy: 0.7158\n",
      "Epoch 55/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.6001 - accuracy: 0.7130\n",
      "Epoch 55: val_loss improved from 0.59584 to 0.59583, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6000 - accuracy: 0.7132 - val_loss: 0.5958 - val_accuracy: 0.7158\n",
      "Epoch 56/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.6005 - accuracy: 0.7127\n",
      "Epoch 56: val_loss did not improve from 0.59583\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6000 - accuracy: 0.7132 - val_loss: 0.5958 - val_accuracy: 0.7158\n",
      "Epoch 57/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.6015 - accuracy: 0.7116\n",
      "Epoch 57: val_loss did not improve from 0.59583\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6000 - accuracy: 0.7132 - val_loss: 0.5958 - val_accuracy: 0.7158\n",
      "Epoch 58/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.6016 - accuracy: 0.7120\n",
      "Epoch 58: val_loss did not improve from 0.59583\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6000 - accuracy: 0.7132 - val_loss: 0.5958 - val_accuracy: 0.7158\n",
      "Epoch 59/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.6006 - accuracy: 0.7125\n",
      "Epoch 59: val_loss improved from 0.59583 to 0.59583, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6000 - accuracy: 0.7132 - val_loss: 0.5958 - val_accuracy: 0.7158\n",
      "Epoch 60/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.6007 - accuracy: 0.7126\n",
      "Epoch 60: val_loss did not improve from 0.59583\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6000 - accuracy: 0.7132 - val_loss: 0.5959 - val_accuracy: 0.7158\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.5958 - accuracy: 0.7158\n",
      "Processing feature: cons.price.idx\n",
      "data frame x has shape: (8516, 1)\n",
      "Epoch 1/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.7053 - accuracy: 0.4361\n",
      "Epoch 1: val_loss improved from inf to 0.70063, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.7053 - accuracy: 0.4361 - val_loss: 0.7006 - val_accuracy: 0.3893\n",
      "Epoch 2/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.6976 - accuracy: 0.4747\n",
      "Epoch 2: val_loss improved from 0.70063 to 0.69615, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6975 - accuracy: 0.4750 - val_loss: 0.6962 - val_accuracy: 0.5103\n",
      "Epoch 3/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.6944 - accuracy: 0.5007\n",
      "Epoch 3: val_loss improved from 0.69615 to 0.69268, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6944 - accuracy: 0.5010 - val_loss: 0.6927 - val_accuracy: 0.5009\n",
      "Epoch 4/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.6917 - accuracy: 0.5050\n",
      "Epoch 4: val_loss improved from 0.69268 to 0.68969, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6917 - accuracy: 0.5052 - val_loss: 0.6897 - val_accuracy: 0.5531\n",
      "Epoch 5/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.6895 - accuracy: 0.5723\n",
      "Epoch 5: val_loss improved from 0.68969 to 0.68715, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6895 - accuracy: 0.5733 - val_loss: 0.6871 - val_accuracy: 0.6271\n",
      "Epoch 6/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.6875 - accuracy: 0.5974\n",
      "Epoch 6: val_loss improved from 0.68715 to 0.68478, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6875 - accuracy: 0.5986 - val_loss: 0.6848 - val_accuracy: 0.6271\n",
      "Epoch 7/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.6858 - accuracy: 0.5964\n",
      "Epoch 7: val_loss improved from 0.68478 to 0.68282, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6858 - accuracy: 0.5961 - val_loss: 0.6828 - val_accuracy: 0.5984\n",
      "Epoch 8/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.6841 - accuracy: 0.5860\n",
      "Epoch 8: val_loss improved from 0.68282 to 0.68102, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6844 - accuracy: 0.5840 - val_loss: 0.6810 - val_accuracy: 0.5984\n",
      "Epoch 9/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.6832 - accuracy: 0.5818\n",
      "Epoch 9: val_loss improved from 0.68102 to 0.67955, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6831 - accuracy: 0.5820 - val_loss: 0.6795 - val_accuracy: 0.5984\n",
      "Epoch 10/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.6821 - accuracy: 0.5824\n",
      "Epoch 10: val_loss improved from 0.67955 to 0.67824, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6821 - accuracy: 0.5827 - val_loss: 0.6782 - val_accuracy: 0.6107\n",
      "Epoch 11/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.6808 - accuracy: 0.5958\n",
      "Epoch 11: val_loss improved from 0.67824 to 0.67702, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6812 - accuracy: 0.5945 - val_loss: 0.6770 - val_accuracy: 0.5984\n",
      "Epoch 12/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.6803 - accuracy: 0.5937\n",
      "Epoch 12: val_loss improved from 0.67702 to 0.67605, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6804 - accuracy: 0.5930 - val_loss: 0.6761 - val_accuracy: 0.6107\n",
      "Epoch 13/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.6796 - accuracy: 0.5920\n",
      "Epoch 13: val_loss improved from 0.67605 to 0.67523, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6798 - accuracy: 0.5911 - val_loss: 0.6752 - val_accuracy: 0.6107\n",
      "Epoch 14/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.6791 - accuracy: 0.5881\n",
      "Epoch 14: val_loss improved from 0.67523 to 0.67436, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6792 - accuracy: 0.5876 - val_loss: 0.6744 - val_accuracy: 0.6107\n",
      "Epoch 15/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.6786 - accuracy: 0.5964\n",
      "Epoch 15: val_loss improved from 0.67436 to 0.67374, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6787 - accuracy: 0.5964 - val_loss: 0.6737 - val_accuracy: 0.6107\n",
      "Epoch 16/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.6782 - accuracy: 0.5889\n",
      "Epoch 16: val_loss improved from 0.67374 to 0.67320, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6783 - accuracy: 0.5881 - val_loss: 0.6732 - val_accuracy: 0.5784\n",
      "Epoch 17/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.6781 - accuracy: 0.5738\n",
      "Epoch 17: val_loss improved from 0.67320 to 0.67265, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6780 - accuracy: 0.5739 - val_loss: 0.6727 - val_accuracy: 0.6107\n",
      "Epoch 18/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.6775 - accuracy: 0.5819\n",
      "Epoch 18: val_loss improved from 0.67265 to 0.67212, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6777 - accuracy: 0.5805 - val_loss: 0.6721 - val_accuracy: 0.6107\n",
      "Epoch 19/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.6774 - accuracy: 0.5786\n",
      "Epoch 19: val_loss improved from 0.67212 to 0.67173, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6774 - accuracy: 0.5793 - val_loss: 0.6717 - val_accuracy: 0.6107\n",
      "Epoch 20/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.6772 - accuracy: 0.5739\n",
      "Epoch 20: val_loss improved from 0.67173 to 0.67135, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6772 - accuracy: 0.5742 - val_loss: 0.6714 - val_accuracy: 0.6107\n",
      "Epoch 21/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.6771 - accuracy: 0.5647\n",
      "Epoch 21: val_loss improved from 0.67135 to 0.67099, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6770 - accuracy: 0.5660 - val_loss: 0.6710 - val_accuracy: 0.6107\n",
      "Epoch 22/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.6769 - accuracy: 0.5779\n",
      "Epoch 22: val_loss improved from 0.67099 to 0.67074, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6769 - accuracy: 0.5785 - val_loss: 0.6707 - val_accuracy: 0.6107\n",
      "Epoch 23/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.6769 - accuracy: 0.5711\n",
      "Epoch 23: val_loss improved from 0.67074 to 0.67056, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6768 - accuracy: 0.5708 - val_loss: 0.6706 - val_accuracy: 0.5784\n",
      "Epoch 24/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.6766 - accuracy: 0.5633\n",
      "Epoch 24: val_loss improved from 0.67056 to 0.67032, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6766 - accuracy: 0.5632 - val_loss: 0.6703 - val_accuracy: 0.5784\n",
      "Epoch 25/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.6764 - accuracy: 0.5749\n",
      "Epoch 25: val_loss improved from 0.67032 to 0.67016, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6765 - accuracy: 0.5748 - val_loss: 0.6702 - val_accuracy: 0.5784\n",
      "Epoch 26/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.6764 - accuracy: 0.5674\n",
      "Epoch 26: val_loss improved from 0.67016 to 0.66995, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6764 - accuracy: 0.5669 - val_loss: 0.6699 - val_accuracy: 0.5784\n",
      "Epoch 27/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.6761 - accuracy: 0.5759\n",
      "Epoch 27: val_loss did not improve from 0.66995\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6763 - accuracy: 0.5746 - val_loss: 0.6700 - val_accuracy: 0.5784\n",
      "Epoch 28/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.6764 - accuracy: 0.5568\n",
      "Epoch 28: val_loss improved from 0.66995 to 0.66972, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6763 - accuracy: 0.5572 - val_loss: 0.6697 - val_accuracy: 0.5784\n",
      "Epoch 29/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.6770 - accuracy: 0.5583\n",
      "Epoch 29: val_loss improved from 0.66972 to 0.66954, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6762 - accuracy: 0.5598 - val_loss: 0.6695 - val_accuracy: 0.5784\n",
      "Epoch 30/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.6761 - accuracy: 0.5588\n",
      "Epoch 30: val_loss improved from 0.66954 to 0.66937, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6762 - accuracy: 0.5586 - val_loss: 0.6694 - val_accuracy: 0.5784\n",
      "Epoch 31/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.6762 - accuracy: 0.5609\n",
      "Epoch 31: val_loss improved from 0.66937 to 0.66926, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6761 - accuracy: 0.5614 - val_loss: 0.6693 - val_accuracy: 0.5784\n",
      "Epoch 32/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.6761 - accuracy: 0.5572\n",
      "Epoch 32: val_loss improved from 0.66926 to 0.66917, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6761 - accuracy: 0.5572 - val_loss: 0.6692 - val_accuracy: 0.5784\n",
      "Epoch 33/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.6762 - accuracy: 0.5573\n",
      "Epoch 33: val_loss improved from 0.66917 to 0.66910, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6761 - accuracy: 0.5572 - val_loss: 0.6691 - val_accuracy: 0.5784\n",
      "Epoch 34/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.6760 - accuracy: 0.5609\n",
      "Epoch 34: val_loss improved from 0.66910 to 0.66894, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6760 - accuracy: 0.5604 - val_loss: 0.6689 - val_accuracy: 0.5784\n",
      "Epoch 35/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.6757 - accuracy: 0.5702\n",
      "Epoch 35: val_loss did not improve from 0.66894\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6760 - accuracy: 0.5694 - val_loss: 0.6690 - val_accuracy: 0.5784\n",
      "Epoch 36/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.6758 - accuracy: 0.5623\n",
      "Epoch 36: val_loss did not improve from 0.66894\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6760 - accuracy: 0.5622 - val_loss: 0.6690 - val_accuracy: 0.5784\n",
      "Epoch 37/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.6762 - accuracy: 0.5571\n",
      "Epoch 37: val_loss improved from 0.66894 to 0.66887, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6760 - accuracy: 0.5572 - val_loss: 0.6689 - val_accuracy: 0.5784\n",
      "Epoch 38/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.6755 - accuracy: 0.5586\n",
      "Epoch 38: val_loss improved from 0.66887 to 0.66875, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6760 - accuracy: 0.5572 - val_loss: 0.6688 - val_accuracy: 0.5784\n",
      "Epoch 39/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.6763 - accuracy: 0.5560\n",
      "Epoch 39: val_loss improved from 0.66875 to 0.66871, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6759 - accuracy: 0.5572 - val_loss: 0.6687 - val_accuracy: 0.5784\n",
      "Epoch 40/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.6755 - accuracy: 0.5588\n",
      "Epoch 40: val_loss improved from 0.66871 to 0.66863, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6760 - accuracy: 0.5572 - val_loss: 0.6686 - val_accuracy: 0.5784\n",
      "Epoch 41/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.6758 - accuracy: 0.5582\n",
      "Epoch 41: val_loss improved from 0.66863 to 0.66860, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6759 - accuracy: 0.5572 - val_loss: 0.6686 - val_accuracy: 0.5784\n",
      "Epoch 42/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.6758 - accuracy: 0.5572\n",
      "Epoch 42: val_loss did not improve from 0.66860\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6759 - accuracy: 0.5572 - val_loss: 0.6686 - val_accuracy: 0.5784\n",
      "Epoch 43/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.6761 - accuracy: 0.5572\n",
      "Epoch 43: val_loss improved from 0.66860 to 0.66853, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6759 - accuracy: 0.5572 - val_loss: 0.6685 - val_accuracy: 0.5784\n",
      "Epoch 44/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.6756 - accuracy: 0.5577\n",
      "Epoch 44: val_loss improved from 0.66853 to 0.66842, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6759 - accuracy: 0.5570 - val_loss: 0.6684 - val_accuracy: 0.5784\n",
      "Epoch 45/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.6756 - accuracy: 0.5593\n",
      "Epoch 45: val_loss did not improve from 0.66842\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6759 - accuracy: 0.5583 - val_loss: 0.6685 - val_accuracy: 0.5784\n",
      "Epoch 46/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.6762 - accuracy: 0.5566\n",
      "Epoch 46: val_loss improved from 0.66842 to 0.66840, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6759 - accuracy: 0.5572 - val_loss: 0.6684 - val_accuracy: 0.5784\n",
      "Epoch 47/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.6754 - accuracy: 0.5588\n",
      "Epoch 47: val_loss did not improve from 0.66840\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6759 - accuracy: 0.5575 - val_loss: 0.6685 - val_accuracy: 0.5784\n",
      "Epoch 48/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.6759 - accuracy: 0.5595\n",
      "Epoch 48: val_loss improved from 0.66840 to 0.66837, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6759 - accuracy: 0.5598 - val_loss: 0.6684 - val_accuracy: 0.5784\n",
      "Epoch 49/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.6754 - accuracy: 0.5673\n",
      "Epoch 49: val_loss did not improve from 0.66837\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6759 - accuracy: 0.5657 - val_loss: 0.6685 - val_accuracy: 0.5784\n",
      "Epoch 50/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.6760 - accuracy: 0.5566\n",
      "Epoch 50: val_loss did not improve from 0.66837\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6759 - accuracy: 0.5572 - val_loss: 0.6684 - val_accuracy: 0.5784\n",
      "Epoch 51/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.6761 - accuracy: 0.5576\n",
      "Epoch 51: val_loss improved from 0.66837 to 0.66834, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6759 - accuracy: 0.5572 - val_loss: 0.6683 - val_accuracy: 0.5784\n",
      "Epoch 52/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.6758 - accuracy: 0.5655\n",
      "Epoch 52: val_loss did not improve from 0.66834\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6759 - accuracy: 0.5657 - val_loss: 0.6684 - val_accuracy: 0.5784\n",
      "Epoch 53/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.6760 - accuracy: 0.5565\n",
      "Epoch 53: val_loss improved from 0.66834 to 0.66829, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6759 - accuracy: 0.5572 - val_loss: 0.6683 - val_accuracy: 0.5784\n",
      "Epoch 54/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.6758 - accuracy: 0.5579\n",
      "Epoch 54: val_loss did not improve from 0.66829\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6759 - accuracy: 0.5572 - val_loss: 0.6683 - val_accuracy: 0.5784\n",
      "Epoch 55/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.6763 - accuracy: 0.5567\n",
      "Epoch 55: val_loss improved from 0.66829 to 0.66828, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6759 - accuracy: 0.5572 - val_loss: 0.6683 - val_accuracy: 0.5784\n",
      "Epoch 56/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.6758 - accuracy: 0.5573\n",
      "Epoch 56: val_loss improved from 0.66828 to 0.66816, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6758 - accuracy: 0.5573 - val_loss: 0.6682 - val_accuracy: 0.5784\n",
      "Epoch 57/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.6757 - accuracy: 0.5580\n",
      "Epoch 57: val_loss improved from 0.66816 to 0.66815, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6759 - accuracy: 0.5576 - val_loss: 0.6681 - val_accuracy: 0.5784\n",
      "Epoch 58/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.6764 - accuracy: 0.5571\n",
      "Epoch 58: val_loss did not improve from 0.66815\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6759 - accuracy: 0.5572 - val_loss: 0.6682 - val_accuracy: 0.5784\n",
      "Epoch 59/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.6760 - accuracy: 0.5572\n",
      "Epoch 59: val_loss did not improve from 0.66815\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6759 - accuracy: 0.5572 - val_loss: 0.6682 - val_accuracy: 0.5784\n",
      "Epoch 60/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.6762 - accuracy: 0.5565\n",
      "Epoch 60: val_loss did not improve from 0.66815\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6759 - accuracy: 0.5572 - val_loss: 0.6682 - val_accuracy: 0.5784\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.6681 - accuracy: 0.5784\n",
      "Processing feature: cons.conf.idx\n",
      "data frame x has shape: (8516, 1)\n",
      "Epoch 1/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.7236 - accuracy: 0.4714\n",
      "Epoch 1: val_loss improved from inf to 0.69876, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.7236 - accuracy: 0.4715 - val_loss: 0.6988 - val_accuracy: 0.4651\n",
      "Epoch 2/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.6951 - accuracy: 0.5111\n",
      "Epoch 2: val_loss improved from 0.69876 to 0.69109, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6951 - accuracy: 0.5130 - val_loss: 0.6911 - val_accuracy: 0.5825\n",
      "Epoch 3/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.6925 - accuracy: 0.5438\n",
      "Epoch 3: val_loss improved from 0.69109 to 0.69052, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6925 - accuracy: 0.5441 - val_loss: 0.6905 - val_accuracy: 0.5567\n",
      "Epoch 4/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.6921 - accuracy: 0.5302\n",
      "Epoch 4: val_loss improved from 0.69052 to 0.69034, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6921 - accuracy: 0.5302 - val_loss: 0.6903 - val_accuracy: 0.5314\n",
      "Epoch 5/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.6918 - accuracy: 0.5350\n",
      "Epoch 5: val_loss improved from 0.69034 to 0.69025, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6920 - accuracy: 0.5343 - val_loss: 0.6903 - val_accuracy: 0.5208\n",
      "Epoch 6/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.6920 - accuracy: 0.5185\n",
      "Epoch 6: val_loss improved from 0.69025 to 0.69014, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6919 - accuracy: 0.5187 - val_loss: 0.6901 - val_accuracy: 0.5208\n",
      "Epoch 7/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.6920 - accuracy: 0.5178\n",
      "Epoch 7: val_loss improved from 0.69014 to 0.69005, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6917 - accuracy: 0.5197 - val_loss: 0.6901 - val_accuracy: 0.5314\n",
      "Epoch 8/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.6913 - accuracy: 0.5250\n",
      "Epoch 8: val_loss improved from 0.69005 to 0.68999, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6916 - accuracy: 0.5228 - val_loss: 0.6900 - val_accuracy: 0.5314\n",
      "Epoch 9/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.6915 - accuracy: 0.5233\n",
      "Epoch 9: val_loss improved from 0.68999 to 0.68992, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6915 - accuracy: 0.5233 - val_loss: 0.6899 - val_accuracy: 0.5314\n",
      "Epoch 10/60\n",
      "651/682 [===========================>..] - ETA: 0s - loss: 0.6908 - accuracy: 0.5158\n",
      "Epoch 10: val_loss improved from 0.68992 to 0.68988, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6914 - accuracy: 0.5139 - val_loss: 0.6899 - val_accuracy: 0.5420\n",
      "Epoch 11/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5401\n",
      "Epoch 11: val_loss improved from 0.68988 to 0.68985, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6913 - accuracy: 0.5390 - val_loss: 0.6899 - val_accuracy: 0.5208\n",
      "Epoch 12/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5208\n",
      "Epoch 12: val_loss improved from 0.68985 to 0.68982, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5202 - val_loss: 0.6898 - val_accuracy: 0.5314\n",
      "Epoch 13/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5383\n",
      "Epoch 13: val_loss improved from 0.68982 to 0.68981, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5379 - val_loss: 0.6898 - val_accuracy: 0.5208\n",
      "Epoch 14/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.6914 - accuracy: 0.5202\n",
      "Epoch 14: val_loss improved from 0.68981 to 0.68977, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5206 - val_loss: 0.6898 - val_accuracy: 0.5314\n",
      "Epoch 15/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5195\n",
      "Epoch 15: val_loss improved from 0.68977 to 0.68975, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6911 - accuracy: 0.5211 - val_loss: 0.6898 - val_accuracy: 0.5420\n",
      "Epoch 16/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.6907 - accuracy: 0.5373\n",
      "Epoch 16: val_loss did not improve from 0.68975\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6911 - accuracy: 0.5352 - val_loss: 0.6898 - val_accuracy: 0.5208\n",
      "Epoch 17/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.6913 - accuracy: 0.5143\n",
      "Epoch 17: val_loss improved from 0.68975 to 0.68974, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6911 - accuracy: 0.5161 - val_loss: 0.6897 - val_accuracy: 0.5420\n",
      "Epoch 18/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.6913 - accuracy: 0.5267\n",
      "Epoch 18: val_loss did not improve from 0.68974\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6911 - accuracy: 0.5275 - val_loss: 0.6897 - val_accuracy: 0.5602\n",
      "Epoch 19/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.6914 - accuracy: 0.5441\n",
      "Epoch 19: val_loss improved from 0.68974 to 0.68973, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5456 - val_loss: 0.6897 - val_accuracy: 0.5420\n",
      "Epoch 20/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.6913 - accuracy: 0.5321\n",
      "Epoch 20: val_loss did not improve from 0.68973\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5337 - val_loss: 0.6897 - val_accuracy: 0.5314\n",
      "Epoch 21/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5141\n",
      "Epoch 21: val_loss did not improve from 0.68973\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5159 - val_loss: 0.6897 - val_accuracy: 0.5602\n",
      "Epoch 22/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.6909 - accuracy: 0.5501\n",
      "Epoch 22: val_loss did not improve from 0.68973\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6910 - accuracy: 0.5494 - val_loss: 0.6897 - val_accuracy: 0.5314\n",
      "Epoch 23/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.6910 - accuracy: 0.5371\n",
      "Epoch 23: val_loss did not improve from 0.68973\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6909 - accuracy: 0.5368 - val_loss: 0.6897 - val_accuracy: 0.5314\n",
      "Epoch 24/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.6910 - accuracy: 0.5112\n",
      "Epoch 24: val_loss did not improve from 0.68973\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5128 - val_loss: 0.6897 - val_accuracy: 0.5602\n",
      "Epoch 25/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.6909 - accuracy: 0.5469\n",
      "Epoch 25: val_loss did not improve from 0.68973\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5469 - val_loss: 0.6897 - val_accuracy: 0.5567\n",
      "Epoch 26/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.6909 - accuracy: 0.5527\n",
      "Epoch 26: val_loss did not improve from 0.68973\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6909 - accuracy: 0.5529 - val_loss: 0.6897 - val_accuracy: 0.5314\n",
      "Epoch 27/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.6910 - accuracy: 0.5329\n",
      "Epoch 27: val_loss did not improve from 0.68973\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6909 - accuracy: 0.5322 - val_loss: 0.6898 - val_accuracy: 0.5208\n",
      "Epoch 28/60\n",
      "653/682 [===========================>..] - ETA: 0s - loss: 0.6912 - accuracy: 0.5190\n",
      "Epoch 28: val_loss did not improve from 0.68973\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6909 - accuracy: 0.5208 - val_loss: 0.6897 - val_accuracy: 0.5420\n",
      "Epoch 29/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.6910 - accuracy: 0.5175\n",
      "Epoch 29: val_loss did not improve from 0.68973\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6909 - accuracy: 0.5184 - val_loss: 0.6897 - val_accuracy: 0.5602\n",
      "Epoch 30/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.6909 - accuracy: 0.5292\n",
      "Epoch 30: val_loss did not improve from 0.68973\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6909 - accuracy: 0.5303 - val_loss: 0.6897 - val_accuracy: 0.5567\n",
      "Epoch 31/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.6909 - accuracy: 0.5409\n",
      "Epoch 31: val_loss did not improve from 0.68973\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6909 - accuracy: 0.5407 - val_loss: 0.6897 - val_accuracy: 0.5420\n",
      "Epoch 32/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.6908 - accuracy: 0.5413\n",
      "Epoch 32: val_loss did not improve from 0.68973\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6909 - accuracy: 0.5407 - val_loss: 0.6898 - val_accuracy: 0.5420\n",
      "Epoch 33/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5186\n",
      "Epoch 33: val_loss did not improve from 0.68973\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6909 - accuracy: 0.5202 - val_loss: 0.6898 - val_accuracy: 0.5602\n",
      "Epoch 34/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.6909 - accuracy: 0.5324\n",
      "Epoch 34: val_loss did not improve from 0.68973\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6909 - accuracy: 0.5328 - val_loss: 0.6898 - val_accuracy: 0.5602\n",
      "Epoch 35/60\n",
      "652/682 [===========================>..] - ETA: 0s - loss: 0.6907 - accuracy: 0.5267\n",
      "Epoch 35: val_loss did not improve from 0.68973\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6909 - accuracy: 0.5261 - val_loss: 0.6898 - val_accuracy: 0.5602\n",
      "Epoch 36/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.6909 - accuracy: 0.5448\n",
      "Epoch 36: val_loss did not improve from 0.68973\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6909 - accuracy: 0.5448 - val_loss: 0.6898 - val_accuracy: 0.5602\n",
      "Epoch 37/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.6911 - accuracy: 0.5314\n",
      "Epoch 37: val_loss did not improve from 0.68973\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6909 - accuracy: 0.5330 - val_loss: 0.6898 - val_accuracy: 0.5602\n",
      "Epoch 38/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.6910 - accuracy: 0.5330\n",
      "Epoch 38: val_loss did not improve from 0.68973\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6909 - accuracy: 0.5335 - val_loss: 0.6898 - val_accuracy: 0.5602\n",
      "Epoch 39/60\n",
      "650/682 [===========================>..] - ETA: 0s - loss: 0.6910 - accuracy: 0.5486\n",
      "Epoch 39: val_loss did not improve from 0.68973\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6909 - accuracy: 0.5482 - val_loss: 0.6898 - val_accuracy: 0.5567\n",
      "Epoch 39: early stopping\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.6897 - accuracy: 0.5420\n",
      "Processing feature: euribor3m\n",
      "data frame x has shape: (8516, 1)\n",
      "Epoch 1/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.6399 - accuracy: 0.6800\n",
      "Epoch 1: val_loss improved from inf to 0.62941, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 3ms/step - loss: 0.6399 - accuracy: 0.6800 - val_loss: 0.6294 - val_accuracy: 0.7158\n",
      "Epoch 2/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.6238 - accuracy: 0.7139\n",
      "Epoch 2: val_loss improved from 0.62941 to 0.61746, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6238 - accuracy: 0.7132 - val_loss: 0.6175 - val_accuracy: 0.7158\n",
      "Epoch 3/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.6139 - accuracy: 0.7141\n",
      "Epoch 3: val_loss improved from 0.61746 to 0.60960, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6143 - accuracy: 0.7132 - val_loss: 0.6096 - val_accuracy: 0.7158\n",
      "Epoch 4/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.6076 - accuracy: 0.7132\n",
      "Epoch 4: val_loss improved from 0.60960 to 0.60405, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6077 - accuracy: 0.7132 - val_loss: 0.6040 - val_accuracy: 0.7158\n",
      "Epoch 5/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.6032 - accuracy: 0.7132\n",
      "Epoch 5: val_loss improved from 0.60405 to 0.60018, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6032 - accuracy: 0.7132 - val_loss: 0.6002 - val_accuracy: 0.7158\n",
      "Epoch 6/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.6006 - accuracy: 0.7124\n",
      "Epoch 6: val_loss improved from 0.60018 to 0.59748, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6001 - accuracy: 0.7132 - val_loss: 0.5975 - val_accuracy: 0.7158\n",
      "Epoch 7/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.5978 - accuracy: 0.7134\n",
      "Epoch 7: val_loss improved from 0.59748 to 0.59546, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5978 - accuracy: 0.7132 - val_loss: 0.5955 - val_accuracy: 0.7158\n",
      "Epoch 8/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.5961 - accuracy: 0.7134\n",
      "Epoch 8: val_loss improved from 0.59546 to 0.59403, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5962 - accuracy: 0.7132 - val_loss: 0.5940 - val_accuracy: 0.7158\n",
      "Epoch 9/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.5948 - accuracy: 0.7136\n",
      "Epoch 9: val_loss improved from 0.59403 to 0.59308, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5951 - accuracy: 0.7132 - val_loss: 0.5931 - val_accuracy: 0.7158\n",
      "Epoch 10/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.5954 - accuracy: 0.7123\n",
      "Epoch 10: val_loss improved from 0.59308 to 0.59239, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5944 - accuracy: 0.7132 - val_loss: 0.5924 - val_accuracy: 0.7158\n",
      "Epoch 11/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.5943 - accuracy: 0.7124\n",
      "Epoch 11: val_loss improved from 0.59239 to 0.59191, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5938 - accuracy: 0.7132 - val_loss: 0.5919 - val_accuracy: 0.7158\n",
      "Epoch 12/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.5937 - accuracy: 0.7129\n",
      "Epoch 12: val_loss improved from 0.59191 to 0.59153, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5935 - accuracy: 0.7132 - val_loss: 0.5915 - val_accuracy: 0.7158\n",
      "Epoch 13/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.5928 - accuracy: 0.7137\n",
      "Epoch 13: val_loss improved from 0.59153 to 0.59127, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5932 - accuracy: 0.7132 - val_loss: 0.5913 - val_accuracy: 0.7158\n",
      "Epoch 14/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.5935 - accuracy: 0.7124\n",
      "Epoch 14: val_loss improved from 0.59127 to 0.59112, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5930 - accuracy: 0.7132 - val_loss: 0.5911 - val_accuracy: 0.7158\n",
      "Epoch 15/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.5926 - accuracy: 0.7136\n",
      "Epoch 15: val_loss improved from 0.59112 to 0.59096, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5929 - accuracy: 0.7132 - val_loss: 0.5910 - val_accuracy: 0.7158\n",
      "Epoch 16/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.5920 - accuracy: 0.7142\n",
      "Epoch 16: val_loss improved from 0.59096 to 0.59086, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5928 - accuracy: 0.7132 - val_loss: 0.5909 - val_accuracy: 0.7158\n",
      "Epoch 17/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.5935 - accuracy: 0.7123\n",
      "Epoch 17: val_loss improved from 0.59086 to 0.59079, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5928 - accuracy: 0.7132 - val_loss: 0.5908 - val_accuracy: 0.7158\n",
      "Epoch 18/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.5933 - accuracy: 0.7125\n",
      "Epoch 18: val_loss improved from 0.59079 to 0.59074, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5927 - accuracy: 0.7132 - val_loss: 0.5907 - val_accuracy: 0.7158\n",
      "Epoch 19/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.5939 - accuracy: 0.7119\n",
      "Epoch 19: val_loss improved from 0.59074 to 0.59069, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5927 - accuracy: 0.7132 - val_loss: 0.5907 - val_accuracy: 0.7158\n",
      "Epoch 20/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.5939 - accuracy: 0.7118\n",
      "Epoch 20: val_loss improved from 0.59069 to 0.59066, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5907 - val_accuracy: 0.7158\n",
      "Epoch 21/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.5936 - accuracy: 0.7123\n",
      "Epoch 21: val_loss improved from 0.59066 to 0.59064, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5927 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158\n",
      "Epoch 22/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.5928 - accuracy: 0.7133\n",
      "Epoch 22: val_loss improved from 0.59064 to 0.59062, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158\n",
      "Epoch 23/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.5926 - accuracy: 0.7132\n",
      "Epoch 23: val_loss improved from 0.59062 to 0.59061, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158\n",
      "Epoch 24/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.5930 - accuracy: 0.7128\n",
      "Epoch 24: val_loss improved from 0.59061 to 0.59060, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158\n",
      "Epoch 25/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.5926 - accuracy: 0.7131\n",
      "Epoch 25: val_loss did not improve from 0.59060\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158\n",
      "Epoch 26/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.5926 - accuracy: 0.7135\n",
      "Epoch 26: val_loss improved from 0.59060 to 0.59058, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158\n",
      "Epoch 27/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.5917 - accuracy: 0.7142\n",
      "Epoch 27: val_loss improved from 0.59058 to 0.59057, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158\n",
      "Epoch 28/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.5914 - accuracy: 0.7143\n",
      "Epoch 28: val_loss did not improve from 0.59057\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158\n",
      "Epoch 29/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.5923 - accuracy: 0.7134\n",
      "Epoch 29: val_loss improved from 0.59057 to 0.59057, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158\n",
      "Epoch 30/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.5935 - accuracy: 0.7121\n",
      "Epoch 30: val_loss did not improve from 0.59057\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158\n",
      "Epoch 31/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.5916 - accuracy: 0.7143\n",
      "Epoch 31: val_loss improved from 0.59057 to 0.59056, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158\n",
      "Epoch 32/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.5915 - accuracy: 0.7146\n",
      "Epoch 32: val_loss did not improve from 0.59056\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158\n",
      "Epoch 33/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.5926 - accuracy: 0.7132\n",
      "Epoch 33: val_loss did not improve from 0.59056\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158\n",
      "Epoch 34/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.5917 - accuracy: 0.7141\n",
      "Epoch 34: val_loss did not improve from 0.59056\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158\n",
      "Epoch 35/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.5925 - accuracy: 0.7134\n",
      "Epoch 35: val_loss improved from 0.59056 to 0.59056, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158\n",
      "Epoch 36/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.5926 - accuracy: 0.7130\n",
      "Epoch 36: val_loss did not improve from 0.59056\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5925 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158\n",
      "Epoch 37/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.5922 - accuracy: 0.7136\n",
      "Epoch 37: val_loss improved from 0.59056 to 0.59056, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158\n",
      "Epoch 38/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.5925 - accuracy: 0.7133\n",
      "Epoch 38: val_loss did not improve from 0.59056\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158\n",
      "Epoch 39/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.5927 - accuracy: 0.7131\n",
      "Epoch 39: val_loss improved from 0.59056 to 0.59056, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158\n",
      "Epoch 40/60\n",
      "654/682 [===========================>..] - ETA: 0s - loss: 0.5924 - accuracy: 0.7135\n",
      "Epoch 40: val_loss improved from 0.59056 to 0.59055, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158\n",
      "Epoch 41/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.5926 - accuracy: 0.7131\n",
      "Epoch 41: val_loss did not improve from 0.59055\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5925 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158\n",
      "Epoch 42/60\n",
      "652/682 [===========================>..] - ETA: 0s - loss: 0.5933 - accuracy: 0.7126\n",
      "Epoch 42: val_loss did not improve from 0.59055\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158\n",
      "Epoch 43/60\n",
      "653/682 [===========================>..] - ETA: 0s - loss: 0.5922 - accuracy: 0.7132\n",
      "Epoch 43: val_loss did not improve from 0.59055\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158\n",
      "Epoch 44/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.5931 - accuracy: 0.7127\n",
      "Epoch 44: val_loss did not improve from 0.59055\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158\n",
      "Epoch 45/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.5927 - accuracy: 0.7131\n",
      "Epoch 45: val_loss did not improve from 0.59055\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158\n",
      "Epoch 46/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.5924 - accuracy: 0.7132\n",
      "Epoch 46: val_loss did not improve from 0.59055\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158\n",
      "Epoch 47/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.5911 - accuracy: 0.7147\n",
      "Epoch 47: val_loss improved from 0.59055 to 0.59055, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158\n",
      "Epoch 48/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.5928 - accuracy: 0.7131\n",
      "Epoch 48: val_loss did not improve from 0.59055\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158\n",
      "Epoch 49/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.5927 - accuracy: 0.7131\n",
      "Epoch 49: val_loss did not improve from 0.59055\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158\n",
      "Epoch 50/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.5930 - accuracy: 0.7128\n",
      "Epoch 50: val_loss did not improve from 0.59055\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158\n",
      "Epoch 51/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.5921 - accuracy: 0.7137\n",
      "Epoch 51: val_loss did not improve from 0.59055\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158\n",
      "Epoch 52/60\n",
      "653/682 [===========================>..] - ETA: 0s - loss: 0.5925 - accuracy: 0.7133\n",
      "Epoch 52: val_loss did not improve from 0.59055\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7133 - val_loss: 0.5906 - val_accuracy: 0.7158\n",
      "Epoch 53/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.5926 - accuracy: 0.7136\n",
      "Epoch 53: val_loss did not improve from 0.59055\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158\n",
      "Epoch 54/60\n",
      "654/682 [===========================>..] - ETA: 0s - loss: 0.5916 - accuracy: 0.7144\n",
      "Epoch 54: val_loss did not improve from 0.59055\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158\n",
      "Epoch 55/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.5931 - accuracy: 0.7127\n",
      "Epoch 55: val_loss did not improve from 0.59055\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158\n",
      "Epoch 56/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.5928 - accuracy: 0.7131\n",
      "Epoch 56: val_loss did not improve from 0.59055\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158\n",
      "Epoch 57/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.5907 - accuracy: 0.7152\n",
      "Epoch 57: val_loss improved from 0.59055 to 0.59055, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158\n",
      "Epoch 58/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.5929 - accuracy: 0.7129\n",
      "Epoch 58: val_loss did not improve from 0.59055\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158\n",
      "Epoch 59/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.5934 - accuracy: 0.7122\n",
      "Epoch 59: val_loss did not improve from 0.59055\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5925 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158\n",
      "Epoch 60/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.5912 - accuracy: 0.7148\n",
      "Epoch 60: val_loss improved from 0.59055 to 0.59055, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.5906 - accuracy: 0.7158\n",
      "Processing feature: divorced\n",
      "data frame x has shape: (8516, 1)\n",
      "Epoch 1/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.7156 - accuracy: 0.5043\n",
      "Epoch 1: val_loss improved from inf to 0.70783, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 3ms/step - loss: 0.7152 - accuracy: 0.5048 - val_loss: 0.7078 - val_accuracy: 0.5068\n",
      "Epoch 2/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.7068 - accuracy: 0.5048\n",
      "Epoch 2: val_loss improved from 0.70783 to 0.70189, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.7068 - accuracy: 0.5048 - val_loss: 0.7019 - val_accuracy: 0.5068\n",
      "Epoch 3/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.7017 - accuracy: 0.5024\n",
      "Epoch 3: val_loss improved from 0.70189 to 0.69819, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.7015 - accuracy: 0.5048 - val_loss: 0.6982 - val_accuracy: 0.5068\n",
      "Epoch 4/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.6983 - accuracy: 0.5042\n",
      "Epoch 4: val_loss improved from 0.69819 to 0.69586, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6981 - accuracy: 0.5048 - val_loss: 0.6959 - val_accuracy: 0.5068\n",
      "Epoch 5/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.6959 - accuracy: 0.5050\n",
      "Epoch 5: val_loss improved from 0.69586 to 0.69448, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6960 - accuracy: 0.5048 - val_loss: 0.6945 - val_accuracy: 0.5068\n",
      "Epoch 6/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.6946 - accuracy: 0.5044\n",
      "Epoch 6: val_loss improved from 0.69448 to 0.69368, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6948 - accuracy: 0.5048 - val_loss: 0.6937 - val_accuracy: 0.5068\n",
      "Epoch 7/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.6940 - accuracy: 0.5052\n",
      "Epoch 7: val_loss improved from 0.69368 to 0.69324, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6941 - accuracy: 0.5048 - val_loss: 0.6932 - val_accuracy: 0.5068\n",
      "Epoch 8/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.6936 - accuracy: 0.5056\n",
      "Epoch 8: val_loss improved from 0.69324 to 0.69304, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6936 - accuracy: 0.5048 - val_loss: 0.6930 - val_accuracy: 0.5068\n",
      "Epoch 9/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.6934 - accuracy: 0.5052\n",
      "Epoch 9: val_loss improved from 0.69304 to 0.69295, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6934 - accuracy: 0.5048 - val_loss: 0.6929 - val_accuracy: 0.5068\n",
      "Epoch 10/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.6934 - accuracy: 0.5039\n",
      "Epoch 10: val_loss improved from 0.69295 to 0.69289, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6933 - accuracy: 0.5048 - val_loss: 0.6929 - val_accuracy: 0.5068\n",
      "Epoch 11/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.5046\n",
      "Epoch 11: val_loss improved from 0.69289 to 0.69288, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.5048 - val_loss: 0.6929 - val_accuracy: 0.5068\n",
      "Epoch 12/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.5050\n",
      "Epoch 12: val_loss improved from 0.69288 to 0.69286, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.5048 - val_loss: 0.6929 - val_accuracy: 0.5068\n",
      "Epoch 13/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.5046\n",
      "Epoch 13: val_loss did not improve from 0.69286\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.5048 - val_loss: 0.6929 - val_accuracy: 0.5068\n",
      "Epoch 14/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.6931 - accuracy: 0.5046\n",
      "Epoch 14: val_loss did not improve from 0.69286\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5048 - val_loss: 0.6929 - val_accuracy: 0.5068\n",
      "Epoch 15/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5060\n",
      "Epoch 15: val_loss did not improve from 0.69286\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.5048 - val_loss: 0.6929 - val_accuracy: 0.5068\n",
      "Epoch 16/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5066\n",
      "Epoch 16: val_loss did not improve from 0.69286\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5048 - val_loss: 0.6929 - val_accuracy: 0.5068\n",
      "Epoch 17/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5021\n",
      "Epoch 17: val_loss did not improve from 0.69286\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5021 - val_loss: 0.6929 - val_accuracy: 0.5068\n",
      "Epoch 18/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.4994\n",
      "Epoch 18: val_loss did not improve from 0.69286\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4999 - val_loss: 0.6929 - val_accuracy: 0.5068\n",
      "Epoch 19/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5050\n",
      "Epoch 19: val_loss did not improve from 0.69286\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6931 - accuracy: 0.5048 - val_loss: 0.6929 - val_accuracy: 0.5068\n",
      "Epoch 20/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.6931 - accuracy: 0.5014\n",
      "Epoch 20: val_loss did not improve from 0.69286\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5001 - val_loss: 0.6929 - val_accuracy: 0.5068\n",
      "Epoch 21/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.4991\n",
      "Epoch 21: val_loss did not improve from 0.69286\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6931 - accuracy: 0.4992 - val_loss: 0.6929 - val_accuracy: 0.5068\n",
      "Epoch 22/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5051\n",
      "Epoch 22: val_loss did not improve from 0.69286\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5048 - val_loss: 0.6929 - val_accuracy: 0.5068\n",
      "Epoch 23/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.6931 - accuracy: 0.4940\n",
      "Epoch 23: val_loss did not improve from 0.69286\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4948 - val_loss: 0.6929 - val_accuracy: 0.5068\n",
      "Epoch 24/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5007\n",
      "Epoch 24: val_loss did not improve from 0.69286\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5008 - val_loss: 0.6929 - val_accuracy: 0.5068\n",
      "Epoch 25/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.5046\n",
      "Epoch 25: val_loss did not improve from 0.69286\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5048 - val_loss: 0.6929 - val_accuracy: 0.5068\n",
      "Epoch 26/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5048\n",
      "Epoch 26: val_loss did not improve from 0.69286\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5048 - val_loss: 0.6929 - val_accuracy: 0.5068\n",
      "Epoch 27/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5051\n",
      "Epoch 27: val_loss did not improve from 0.69286\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5057 - val_loss: 0.6929 - val_accuracy: 0.5068\n",
      "Epoch 28/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.5018\n",
      "Epoch 28: val_loss did not improve from 0.69286\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5023 - val_loss: 0.6929 - val_accuracy: 0.5068\n",
      "Epoch 29/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5065\n",
      "Epoch 29: val_loss did not improve from 0.69286\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5048 - val_loss: 0.6929 - val_accuracy: 0.5068\n",
      "Epoch 30/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5060\n",
      "Epoch 30: val_loss did not improve from 0.69286\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6931 - accuracy: 0.5048 - val_loss: 0.6929 - val_accuracy: 0.5068\n",
      "Epoch 31/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4955\n",
      "Epoch 31: val_loss did not improve from 0.69286\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4963 - val_loss: 0.6929 - val_accuracy: 0.5068\n",
      "Epoch 32/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5071\n",
      "Epoch 32: val_loss did not improve from 0.69286\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5048 - val_loss: 0.6929 - val_accuracy: 0.5068\n",
      "Epoch 32: early stopping\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.6929 - accuracy: 0.5068\n",
      "Processing feature: married\n",
      "data frame x has shape: (8516, 1)\n",
      "Epoch 1/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.7740 - accuracy: 0.4736\n",
      "Epoch 1: val_loss improved from inf to 0.73603, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 3ms/step - loss: 0.7740 - accuracy: 0.4737 - val_loss: 0.7360 - val_accuracy: 0.4680\n",
      "Epoch 2/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.7215 - accuracy: 0.4736\n",
      "Epoch 2: val_loss improved from 0.73603 to 0.71526, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.7215 - accuracy: 0.4737 - val_loss: 0.7153 - val_accuracy: 0.4680\n",
      "Epoch 3/60\n",
      "652/682 [===========================>..] - ETA: 0s - loss: 0.7094 - accuracy: 0.4735\n",
      "Epoch 3: val_loss improved from 0.71526 to 0.70727, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.7094 - accuracy: 0.4737 - val_loss: 0.7073 - val_accuracy: 0.4680\n",
      "Epoch 4/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.7038 - accuracy: 0.4731\n",
      "Epoch 4: val_loss improved from 0.70727 to 0.70204, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.7035 - accuracy: 0.4737 - val_loss: 0.7020 - val_accuracy: 0.4680\n",
      "Epoch 5/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.6995 - accuracy: 0.4737\n",
      "Epoch 5: val_loss improved from 0.70204 to 0.69850, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6996 - accuracy: 0.4737 - val_loss: 0.6985 - val_accuracy: 0.4680\n",
      "Epoch 6/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.6968 - accuracy: 0.4758\n",
      "Epoch 6: val_loss improved from 0.69850 to 0.69611, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6969 - accuracy: 0.4750 - val_loss: 0.6961 - val_accuracy: 0.4680\n",
      "Epoch 7/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.6952 - accuracy: 0.4787\n",
      "Epoch 7: val_loss improved from 0.69611 to 0.69444, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6951 - accuracy: 0.4814 - val_loss: 0.6944 - val_accuracy: 0.5009\n",
      "Epoch 8/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.6939 - accuracy: 0.5002\n",
      "Epoch 8: val_loss improved from 0.69444 to 0.69336, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6939 - accuracy: 0.4998 - val_loss: 0.6934 - val_accuracy: 0.5009\n",
      "Epoch 9/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.6932 - accuracy: 0.4998\n",
      "Epoch 9: val_loss improved from 0.69336 to 0.69272, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5008 - val_loss: 0.6927 - val_accuracy: 0.5320\n",
      "Epoch 10/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.6927 - accuracy: 0.5258\n",
      "Epoch 10: val_loss improved from 0.69272 to 0.69220, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6927 - accuracy: 0.5263 - val_loss: 0.6922 - val_accuracy: 0.5320\n",
      "Epoch 11/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.6924 - accuracy: 0.5269\n",
      "Epoch 11: val_loss improved from 0.69220 to 0.69186, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6924 - accuracy: 0.5263 - val_loss: 0.6919 - val_accuracy: 0.5320\n",
      "Epoch 12/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.6922 - accuracy: 0.5264\n",
      "Epoch 12: val_loss improved from 0.69186 to 0.69163, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6922 - accuracy: 0.5263 - val_loss: 0.6916 - val_accuracy: 0.5320\n",
      "Epoch 13/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.6920 - accuracy: 0.5277\n",
      "Epoch 13: val_loss improved from 0.69163 to 0.69148, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6921 - accuracy: 0.5263 - val_loss: 0.6915 - val_accuracy: 0.5320\n",
      "Epoch 14/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.6920 - accuracy: 0.5264\n",
      "Epoch 14: val_loss improved from 0.69148 to 0.69140, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6920 - accuracy: 0.5263 - val_loss: 0.6914 - val_accuracy: 0.5320\n",
      "Epoch 15/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.6918 - accuracy: 0.5275\n",
      "Epoch 15: val_loss improved from 0.69140 to 0.69136, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6919 - accuracy: 0.5263 - val_loss: 0.6914 - val_accuracy: 0.5320\n",
      "Epoch 16/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.6919 - accuracy: 0.5261\n",
      "Epoch 16: val_loss improved from 0.69136 to 0.69130, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6919 - accuracy: 0.5263 - val_loss: 0.6913 - val_accuracy: 0.5320\n",
      "Epoch 17/60\n",
      "654/682 [===========================>..] - ETA: 0s - loss: 0.6918 - accuracy: 0.5266\n",
      "Epoch 17: val_loss improved from 0.69130 to 0.69125, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6919 - accuracy: 0.5263 - val_loss: 0.6912 - val_accuracy: 0.5320\n",
      "Epoch 18/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.6919 - accuracy: 0.5263\n",
      "Epoch 18: val_loss improved from 0.69125 to 0.69123, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6919 - accuracy: 0.5263 - val_loss: 0.6912 - val_accuracy: 0.5320\n",
      "Epoch 19/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.6919 - accuracy: 0.5259\n",
      "Epoch 19: val_loss improved from 0.69123 to 0.69119, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6919 - accuracy: 0.5263 - val_loss: 0.6912 - val_accuracy: 0.5320\n",
      "Epoch 20/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.6918 - accuracy: 0.5268\n",
      "Epoch 20: val_loss improved from 0.69119 to 0.69118, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6912 - val_accuracy: 0.5320\n",
      "Epoch 21/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.6918 - accuracy: 0.5269\n",
      "Epoch 21: val_loss improved from 0.69118 to 0.69118, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6912 - val_accuracy: 0.5320\n",
      "Epoch 22/60\n",
      "653/682 [===========================>..] - ETA: 0s - loss: 0.6919 - accuracy: 0.5260\n",
      "Epoch 22: val_loss did not improve from 0.69118\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6912 - val_accuracy: 0.5320\n",
      "Epoch 23/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.6919 - accuracy: 0.5260\n",
      "Epoch 23: val_loss did not improve from 0.69118\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6912 - val_accuracy: 0.5320\n",
      "Epoch 24/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.6918 - accuracy: 0.5267\n",
      "Epoch 24: val_loss improved from 0.69118 to 0.69116, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6912 - val_accuracy: 0.5320\n",
      "Epoch 25/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.6916 - accuracy: 0.5285\n",
      "Epoch 25: val_loss improved from 0.69116 to 0.69113, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5320\n",
      "Epoch 26/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.6918 - accuracy: 0.5267\n",
      "Epoch 26: val_loss did not improve from 0.69113\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5320\n",
      "Epoch 27/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.6919 - accuracy: 0.5259\n",
      "Epoch 27: val_loss did not improve from 0.69113\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6919 - accuracy: 0.5263 - val_loss: 0.6912 - val_accuracy: 0.5320\n",
      "Epoch 28/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.6918 - accuracy: 0.5266\n",
      "Epoch 28: val_loss did not improve from 0.69113\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5320\n",
      "Epoch 29/60\n",
      "654/682 [===========================>..] - ETA: 0s - loss: 0.6917 - accuracy: 0.5274\n",
      "Epoch 29: val_loss did not improve from 0.69113\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6912 - val_accuracy: 0.5320\n",
      "Epoch 30/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.6919 - accuracy: 0.5258\n",
      "Epoch 30: val_loss did not improve from 0.69113\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6919 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5320\n",
      "Epoch 31/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.6918 - accuracy: 0.5257\n",
      "Epoch 31: val_loss did not improve from 0.69113\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6912 - val_accuracy: 0.5320\n",
      "Epoch 32/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.6918 - accuracy: 0.5263\n",
      "Epoch 32: val_loss did not improve from 0.69113\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6912 - val_accuracy: 0.5320\n",
      "Epoch 33/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.6919 - accuracy: 0.5261\n",
      "Epoch 33: val_loss did not improve from 0.69113\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6912 - val_accuracy: 0.5320\n",
      "Epoch 34/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.6918 - accuracy: 0.5268\n",
      "Epoch 34: val_loss did not improve from 0.69113\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6919 - accuracy: 0.5263 - val_loss: 0.6912 - val_accuracy: 0.5320\n",
      "Epoch 35/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.6919 - accuracy: 0.5263\n",
      "Epoch 35: val_loss did not improve from 0.69113\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6919 - accuracy: 0.5263 - val_loss: 0.6912 - val_accuracy: 0.5320\n",
      "Epoch 36/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.6918 - accuracy: 0.5267\n",
      "Epoch 36: val_loss did not improve from 0.69113\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6912 - val_accuracy: 0.5320\n",
      "Epoch 37/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.6919 - accuracy: 0.5264\n",
      "Epoch 37: val_loss did not improve from 0.69113\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6919 - accuracy: 0.5263 - val_loss: 0.6912 - val_accuracy: 0.5320\n",
      "Epoch 38/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.6917 - accuracy: 0.5269\n",
      "Epoch 38: val_loss improved from 0.69113 to 0.69112, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5320\n",
      "Epoch 39/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.6918 - accuracy: 0.5261\n",
      "Epoch 39: val_loss did not improve from 0.69112\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6917 - accuracy: 0.5263 - val_loss: 0.6913 - val_accuracy: 0.5320\n",
      "Epoch 40/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.6919 - accuracy: 0.5256\n",
      "Epoch 40: val_loss did not improve from 0.69112\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5320\n",
      "Epoch 41/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.6919 - accuracy: 0.5254\n",
      "Epoch 41: val_loss did not improve from 0.69112\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5320\n",
      "Epoch 42/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.6918 - accuracy: 0.5268\n",
      "Epoch 42: val_loss did not improve from 0.69112\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5320\n",
      "Epoch 43/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.6918 - accuracy: 0.5263\n",
      "Epoch 43: val_loss did not improve from 0.69112\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5320\n",
      "Epoch 44/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.6918 - accuracy: 0.5266\n",
      "Epoch 44: val_loss did not improve from 0.69112\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5320\n",
      "Epoch 45/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.6918 - accuracy: 0.5266\n",
      "Epoch 45: val_loss did not improve from 0.69112\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5320\n",
      "Epoch 46/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.6919 - accuracy: 0.5262\n",
      "Epoch 46: val_loss did not improve from 0.69112\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5320\n",
      "Epoch 47/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.6916 - accuracy: 0.5280\n",
      "Epoch 47: val_loss improved from 0.69112 to 0.69112, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5320\n",
      "Epoch 48/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.6918 - accuracy: 0.5265\n",
      "Epoch 48: val_loss improved from 0.69112 to 0.69112, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5320\n",
      "Epoch 49/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.6916 - accuracy: 0.5281\n",
      "Epoch 49: val_loss did not improve from 0.69112\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5320\n",
      "Epoch 50/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.6918 - accuracy: 0.5263\n",
      "Epoch 50: val_loss improved from 0.69112 to 0.69111, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5320\n",
      "Epoch 51/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.6919 - accuracy: 0.5261\n",
      "Epoch 51: val_loss did not improve from 0.69111\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5320\n",
      "Epoch 52/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.6918 - accuracy: 0.5269\n",
      "Epoch 52: val_loss did not improve from 0.69111\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6919 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5320\n",
      "Epoch 53/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.6918 - accuracy: 0.5267\n",
      "Epoch 53: val_loss did not improve from 0.69111\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6912 - val_accuracy: 0.5320\n",
      "Epoch 54/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.6918 - accuracy: 0.5263\n",
      "Epoch 54: val_loss did not improve from 0.69111\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5320\n",
      "Epoch 55/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.6920 - accuracy: 0.5250\n",
      "Epoch 55: val_loss did not improve from 0.69111\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5320\n",
      "Epoch 56/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.6919 - accuracy: 0.5258\n",
      "Epoch 56: val_loss did not improve from 0.69111\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5320\n",
      "Epoch 57/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.6919 - accuracy: 0.5261\n",
      "Epoch 57: val_loss did not improve from 0.69111\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5320\n",
      "Epoch 58/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.6918 - accuracy: 0.5270\n",
      "Epoch 58: val_loss did not improve from 0.69111\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6912 - val_accuracy: 0.5320\n",
      "Epoch 59/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.6919 - accuracy: 0.5261\n",
      "Epoch 59: val_loss did not improve from 0.69111\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6919 - accuracy: 0.5263 - val_loss: 0.6912 - val_accuracy: 0.5320\n",
      "Epoch 60/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.6918 - accuracy: 0.5259\n",
      "Epoch 60: val_loss did not improve from 0.69111\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5320\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.6911 - accuracy: 0.5320\n",
      "Processing feature: single\n",
      "data frame x has shape: (8516, 1)\n",
      "Epoch 1/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.6923 - accuracy: 0.5310\n",
      "Epoch 1: val_loss improved from inf to 0.68985, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6923 - accuracy: 0.5309 - val_loss: 0.6899 - val_accuracy: 0.5396\n",
      "Epoch 2/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.6914 - accuracy: 0.5284\n",
      "Epoch 2: val_loss improved from 0.68985 to 0.68964, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6911 - accuracy: 0.5309 - val_loss: 0.6896 - val_accuracy: 0.5396\n",
      "Epoch 3/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.6911 - accuracy: 0.5302\n",
      "Epoch 3: val_loss did not improve from 0.68964\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5309 - val_loss: 0.6897 - val_accuracy: 0.5396\n",
      "Epoch 4/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.6910 - accuracy: 0.5311\n",
      "Epoch 4: val_loss did not improve from 0.68964\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5309 - val_loss: 0.6897 - val_accuracy: 0.5396\n",
      "Epoch 5/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.6909 - accuracy: 0.5314\n",
      "Epoch 5: val_loss did not improve from 0.68964\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5309 - val_loss: 0.6897 - val_accuracy: 0.5396\n",
      "Epoch 6/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.6909 - accuracy: 0.5319\n",
      "Epoch 6: val_loss did not improve from 0.68964\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5309 - val_loss: 0.6897 - val_accuracy: 0.5396\n",
      "Epoch 7/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.6910 - accuracy: 0.5309\n",
      "Epoch 7: val_loss did not improve from 0.68964\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5309 - val_loss: 0.6897 - val_accuracy: 0.5396\n",
      "Epoch 8/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.6909 - accuracy: 0.5313\n",
      "Epoch 8: val_loss did not improve from 0.68964\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5309 - val_loss: 0.6897 - val_accuracy: 0.5396\n",
      "Epoch 9/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.6909 - accuracy: 0.5310\n",
      "Epoch 9: val_loss did not improve from 0.68964\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5309 - val_loss: 0.6897 - val_accuracy: 0.5396\n",
      "Epoch 10/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.6909 - accuracy: 0.5316\n",
      "Epoch 10: val_loss did not improve from 0.68964\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5309 - val_loss: 0.6897 - val_accuracy: 0.5396\n",
      "Epoch 11/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.6909 - accuracy: 0.5315\n",
      "Epoch 11: val_loss did not improve from 0.68964\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5309 - val_loss: 0.6897 - val_accuracy: 0.5396\n",
      "Epoch 12/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.6911 - accuracy: 0.5304\n",
      "Epoch 12: val_loss did not improve from 0.68964\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5309 - val_loss: 0.6897 - val_accuracy: 0.5396\n",
      "Epoch 13/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.6911 - accuracy: 0.5308\n",
      "Epoch 13: val_loss did not improve from 0.68964\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5309 - val_loss: 0.6897 - val_accuracy: 0.5396\n",
      "Epoch 14/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.6909 - accuracy: 0.5318\n",
      "Epoch 14: val_loss did not improve from 0.68964\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5309 - val_loss: 0.6897 - val_accuracy: 0.5396\n",
      "Epoch 15/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.6909 - accuracy: 0.5311\n",
      "Epoch 15: val_loss did not improve from 0.68964\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5309 - val_loss: 0.6898 - val_accuracy: 0.5396\n",
      "Epoch 16/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.6910 - accuracy: 0.5306\n",
      "Epoch 16: val_loss did not improve from 0.68964\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5309 - val_loss: 0.6897 - val_accuracy: 0.5396\n",
      "Epoch 17/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.6910 - accuracy: 0.5305\n",
      "Epoch 17: val_loss did not improve from 0.68964\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5309 - val_loss: 0.6898 - val_accuracy: 0.5396\n",
      "Epoch 18/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.6909 - accuracy: 0.5314\n",
      "Epoch 18: val_loss did not improve from 0.68964\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5309 - val_loss: 0.6897 - val_accuracy: 0.5396\n",
      "Epoch 19/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.6909 - accuracy: 0.5310\n",
      "Epoch 19: val_loss did not improve from 0.68964\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5309 - val_loss: 0.6897 - val_accuracy: 0.5396\n",
      "Epoch 20/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.6909 - accuracy: 0.5313\n",
      "Epoch 20: val_loss did not improve from 0.68964\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5309 - val_loss: 0.6898 - val_accuracy: 0.5396\n",
      "Epoch 21/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.6909 - accuracy: 0.5312\n",
      "Epoch 21: val_loss did not improve from 0.68964\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5309 - val_loss: 0.6898 - val_accuracy: 0.5396\n",
      "Epoch 22/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.6910 - accuracy: 0.5307\n",
      "Epoch 22: val_loss did not improve from 0.68964\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5309 - val_loss: 0.6898 - val_accuracy: 0.5396\n",
      "Epoch 22: early stopping\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.6896 - accuracy: 0.5396\n",
      "Processing feature: admin.\n",
      "data frame x has shape: (8516, 1)\n",
      "Epoch 1/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.7085 - accuracy: 0.5220\n",
      "Epoch 1: val_loss improved from inf to 0.69728, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 3ms/step - loss: 0.7084 - accuracy: 0.5222 - val_loss: 0.6973 - val_accuracy: 0.5326\n",
      "Epoch 2/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.6981 - accuracy: 0.5226\n",
      "Epoch 2: val_loss improved from 0.69728 to 0.69305, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6981 - accuracy: 0.5222 - val_loss: 0.6931 - val_accuracy: 0.5326\n",
      "Epoch 3/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.6950 - accuracy: 0.5219\n",
      "Epoch 3: val_loss improved from 0.69305 to 0.69148, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6949 - accuracy: 0.5222 - val_loss: 0.6915 - val_accuracy: 0.5326\n",
      "Epoch 4/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.6935 - accuracy: 0.5221\n",
      "Epoch 4: val_loss improved from 0.69148 to 0.69086, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6935 - accuracy: 0.5222 - val_loss: 0.6909 - val_accuracy: 0.5326\n",
      "Epoch 5/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.6926 - accuracy: 0.5233\n",
      "Epoch 5: val_loss improved from 0.69086 to 0.69064, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6927 - accuracy: 0.5222 - val_loss: 0.6906 - val_accuracy: 0.5326\n",
      "Epoch 6/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.6924 - accuracy: 0.5225\n",
      "Epoch 6: val_loss improved from 0.69064 to 0.69058, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6924 - accuracy: 0.5222 - val_loss: 0.6906 - val_accuracy: 0.5326\n",
      "Epoch 7/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.6920 - accuracy: 0.5238\n",
      "Epoch 7: val_loss did not improve from 0.69058\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6922 - accuracy: 0.5222 - val_loss: 0.6906 - val_accuracy: 0.5326\n",
      "Epoch 8/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.6920 - accuracy: 0.5228\n",
      "Epoch 8: val_loss did not improve from 0.69058\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6921 - accuracy: 0.5222 - val_loss: 0.6906 - val_accuracy: 0.5326\n",
      "Epoch 9/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.6921 - accuracy: 0.5220\n",
      "Epoch 9: val_loss did not improve from 0.69058\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6920 - accuracy: 0.5222 - val_loss: 0.6907 - val_accuracy: 0.5326\n",
      "Epoch 10/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.6920 - accuracy: 0.5224\n",
      "Epoch 10: val_loss did not improve from 0.69058\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6920 - accuracy: 0.5222 - val_loss: 0.6907 - val_accuracy: 0.5326\n",
      "Epoch 11/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.6920 - accuracy: 0.5222\n",
      "Epoch 11: val_loss did not improve from 0.69058\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6920 - accuracy: 0.5222 - val_loss: 0.6907 - val_accuracy: 0.5326\n",
      "Epoch 12/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.6918 - accuracy: 0.5246\n",
      "Epoch 12: val_loss did not improve from 0.69058\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6920 - accuracy: 0.5222 - val_loss: 0.6908 - val_accuracy: 0.5326\n",
      "Epoch 13/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.6920 - accuracy: 0.5235\n",
      "Epoch 13: val_loss did not improve from 0.69058\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6920 - accuracy: 0.5222 - val_loss: 0.6908 - val_accuracy: 0.5326\n",
      "Epoch 14/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.6920 - accuracy: 0.5228\n",
      "Epoch 14: val_loss did not improve from 0.69058\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6920 - accuracy: 0.5222 - val_loss: 0.6908 - val_accuracy: 0.5326\n",
      "Epoch 15/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.6919 - accuracy: 0.5227\n",
      "Epoch 15: val_loss did not improve from 0.69058\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6920 - accuracy: 0.5222 - val_loss: 0.6908 - val_accuracy: 0.5326\n",
      "Epoch 16/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.6919 - accuracy: 0.5229\n",
      "Epoch 16: val_loss did not improve from 0.69058\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6920 - accuracy: 0.5222 - val_loss: 0.6908 - val_accuracy: 0.5326\n",
      "Epoch 17/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.6920 - accuracy: 0.5219\n",
      "Epoch 17: val_loss did not improve from 0.69058\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6920 - accuracy: 0.5222 - val_loss: 0.6908 - val_accuracy: 0.5326\n",
      "Epoch 18/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.6920 - accuracy: 0.5231\n",
      "Epoch 18: val_loss did not improve from 0.69058\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6920 - accuracy: 0.5222 - val_loss: 0.6908 - val_accuracy: 0.5326\n",
      "Epoch 19/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.6919 - accuracy: 0.5228\n",
      "Epoch 19: val_loss did not improve from 0.69058\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6920 - accuracy: 0.5222 - val_loss: 0.6909 - val_accuracy: 0.5326\n",
      "Epoch 20/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.6918 - accuracy: 0.5236\n",
      "Epoch 20: val_loss did not improve from 0.69058\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6920 - accuracy: 0.5222 - val_loss: 0.6908 - val_accuracy: 0.5326\n",
      "Epoch 21/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.6918 - accuracy: 0.5244\n",
      "Epoch 21: val_loss did not improve from 0.69058\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6920 - accuracy: 0.5222 - val_loss: 0.6908 - val_accuracy: 0.5326\n",
      "Epoch 22/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.6920 - accuracy: 0.5221\n",
      "Epoch 22: val_loss did not improve from 0.69058\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6920 - accuracy: 0.5222 - val_loss: 0.6908 - val_accuracy: 0.5326\n",
      "Epoch 23/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.6920 - accuracy: 0.5223\n",
      "Epoch 23: val_loss did not improve from 0.69058\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6920 - accuracy: 0.5222 - val_loss: 0.6908 - val_accuracy: 0.5326\n",
      "Epoch 24/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.6920 - accuracy: 0.5215\n",
      "Epoch 24: val_loss did not improve from 0.69058\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6920 - accuracy: 0.5222 - val_loss: 0.6908 - val_accuracy: 0.5326\n",
      "Epoch 25/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.6921 - accuracy: 0.5208\n",
      "Epoch 25: val_loss did not improve from 0.69058\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6920 - accuracy: 0.5222 - val_loss: 0.6908 - val_accuracy: 0.5326\n",
      "Epoch 26/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.6921 - accuracy: 0.5225\n",
      "Epoch 26: val_loss did not improve from 0.69058\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6920 - accuracy: 0.5222 - val_loss: 0.6908 - val_accuracy: 0.5326\n",
      "Epoch 26: early stopping\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.6906 - accuracy: 0.5326\n",
      "Processing feature: blue-collar\n",
      "data frame x has shape: (8516, 1)\n",
      "Epoch 1/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.6884 - accuracy: 0.5337\n",
      "Epoch 1: val_loss improved from inf to 0.68628, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 3ms/step - loss: 0.6884 - accuracy: 0.5335 - val_loss: 0.6863 - val_accuracy: 0.5531\n",
      "Epoch 2/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.6873 - accuracy: 0.5458\n",
      "Epoch 2: val_loss improved from 0.68628 to 0.68528, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6873 - accuracy: 0.5462 - val_loss: 0.6853 - val_accuracy: 0.5531\n",
      "Epoch 3/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.6870 - accuracy: 0.5445\n",
      "Epoch 3: val_loss improved from 0.68528 to 0.68468, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6868 - accuracy: 0.5462 - val_loss: 0.6847 - val_accuracy: 0.5531\n",
      "Epoch 4/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.6865 - accuracy: 0.5456\n",
      "Epoch 4: val_loss improved from 0.68468 to 0.68431, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6865 - accuracy: 0.5462 - val_loss: 0.6843 - val_accuracy: 0.5531\n",
      "Epoch 5/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.6865 - accuracy: 0.5464\n",
      "Epoch 5: val_loss improved from 0.68431 to 0.68410, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6864 - accuracy: 0.5462 - val_loss: 0.6841 - val_accuracy: 0.5531\n",
      "Epoch 6/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.6865 - accuracy: 0.5451\n",
      "Epoch 6: val_loss improved from 0.68410 to 0.68398, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6863 - accuracy: 0.5462 - val_loss: 0.6840 - val_accuracy: 0.5531\n",
      "Epoch 7/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.6858 - accuracy: 0.5488\n",
      "Epoch 7: val_loss improved from 0.68398 to 0.68388, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6863 - accuracy: 0.5462 - val_loss: 0.6839 - val_accuracy: 0.5531\n",
      "Epoch 8/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.6863 - accuracy: 0.5458\n",
      "Epoch 8: val_loss improved from 0.68388 to 0.68382, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6863 - accuracy: 0.5462 - val_loss: 0.6838 - val_accuracy: 0.5531\n",
      "Epoch 9/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.6860 - accuracy: 0.5460\n",
      "Epoch 9: val_loss improved from 0.68382 to 0.68379, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6863 - accuracy: 0.5462 - val_loss: 0.6838 - val_accuracy: 0.5531\n",
      "Epoch 10/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.6861 - accuracy: 0.5468\n",
      "Epoch 10: val_loss improved from 0.68379 to 0.68373, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6862 - accuracy: 0.5462 - val_loss: 0.6837 - val_accuracy: 0.5531\n",
      "Epoch 11/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.6861 - accuracy: 0.5468\n",
      "Epoch 11: val_loss improved from 0.68373 to 0.68373, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6863 - accuracy: 0.5462 - val_loss: 0.6837 - val_accuracy: 0.5531\n",
      "Epoch 12/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.6863 - accuracy: 0.5470\n",
      "Epoch 12: val_loss did not improve from 0.68373\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6862 - accuracy: 0.5462 - val_loss: 0.6838 - val_accuracy: 0.5531\n",
      "Epoch 13/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.6862 - accuracy: 0.5462\n",
      "Epoch 13: val_loss did not improve from 0.68373\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6862 - accuracy: 0.5462 - val_loss: 0.6838 - val_accuracy: 0.5531\n",
      "Epoch 14/60\n",
      "650/682 [===========================>..] - ETA: 0s - loss: 0.6860 - accuracy: 0.5480\n",
      "Epoch 14: val_loss improved from 0.68373 to 0.68372, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6862 - accuracy: 0.5462 - val_loss: 0.6837 - val_accuracy: 0.5531\n",
      "Epoch 15/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.6861 - accuracy: 0.5469\n",
      "Epoch 15: val_loss did not improve from 0.68372\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6862 - accuracy: 0.5462 - val_loss: 0.6837 - val_accuracy: 0.5531\n",
      "Epoch 16/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.6864 - accuracy: 0.5458\n",
      "Epoch 16: val_loss improved from 0.68372 to 0.68371, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6862 - accuracy: 0.5462 - val_loss: 0.6837 - val_accuracy: 0.5531\n",
      "Epoch 17/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.6861 - accuracy: 0.5476\n",
      "Epoch 17: val_loss improved from 0.68371 to 0.68368, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6863 - accuracy: 0.5462 - val_loss: 0.6837 - val_accuracy: 0.5531\n",
      "Epoch 18/60\n",
      "649/682 [===========================>..] - ETA: 0s - loss: 0.6862 - accuracy: 0.5464\n",
      "Epoch 18: val_loss improved from 0.68368 to 0.68366, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6862 - accuracy: 0.5462 - val_loss: 0.6837 - val_accuracy: 0.5531\n",
      "Epoch 19/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.6861 - accuracy: 0.5473\n",
      "Epoch 19: val_loss did not improve from 0.68366\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6862 - accuracy: 0.5462 - val_loss: 0.6837 - val_accuracy: 0.5531\n",
      "Epoch 20/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.6865 - accuracy: 0.5456\n",
      "Epoch 20: val_loss did not improve from 0.68366\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6862 - accuracy: 0.5462 - val_loss: 0.6837 - val_accuracy: 0.5531\n",
      "Epoch 21/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.6866 - accuracy: 0.5440\n",
      "Epoch 21: val_loss did not improve from 0.68366\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6863 - accuracy: 0.5462 - val_loss: 0.6837 - val_accuracy: 0.5531\n",
      "Epoch 22/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.6862 - accuracy: 0.5465\n",
      "Epoch 22: val_loss did not improve from 0.68366\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6862 - accuracy: 0.5462 - val_loss: 0.6837 - val_accuracy: 0.5531\n",
      "Epoch 23/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.6862 - accuracy: 0.5473\n",
      "Epoch 23: val_loss did not improve from 0.68366\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6863 - accuracy: 0.5462 - val_loss: 0.6837 - val_accuracy: 0.5531\n",
      "Epoch 24/60\n",
      "648/682 [===========================>..] - ETA: 0s - loss: 0.6861 - accuracy: 0.5463\n",
      "Epoch 24: val_loss did not improve from 0.68366\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6863 - accuracy: 0.5462 - val_loss: 0.6837 - val_accuracy: 0.5531\n",
      "Epoch 25/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.6863 - accuracy: 0.5461\n",
      "Epoch 25: val_loss did not improve from 0.68366\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6862 - accuracy: 0.5462 - val_loss: 0.6838 - val_accuracy: 0.5531\n",
      "Epoch 26/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.6866 - accuracy: 0.5453\n",
      "Epoch 26: val_loss did not improve from 0.68366\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6863 - accuracy: 0.5462 - val_loss: 0.6837 - val_accuracy: 0.5531\n",
      "Epoch 27/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.6861 - accuracy: 0.5460\n",
      "Epoch 27: val_loss did not improve from 0.68366\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6862 - accuracy: 0.5462 - val_loss: 0.6837 - val_accuracy: 0.5531\n",
      "Epoch 28/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.6863 - accuracy: 0.5461\n",
      "Epoch 28: val_loss did not improve from 0.68366\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6862 - accuracy: 0.5462 - val_loss: 0.6837 - val_accuracy: 0.5531\n",
      "Epoch 29/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.6858 - accuracy: 0.5467\n",
      "Epoch 29: val_loss did not improve from 0.68366\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6863 - accuracy: 0.5462 - val_loss: 0.6838 - val_accuracy: 0.5531\n",
      "Epoch 30/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.6863 - accuracy: 0.5459\n",
      "Epoch 30: val_loss did not improve from 0.68366\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6863 - accuracy: 0.5462 - val_loss: 0.6837 - val_accuracy: 0.5531\n",
      "Epoch 31/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.6862 - accuracy: 0.5458\n",
      "Epoch 31: val_loss did not improve from 0.68366\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6862 - accuracy: 0.5462 - val_loss: 0.6837 - val_accuracy: 0.5531\n",
      "Epoch 32/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.6864 - accuracy: 0.5455\n",
      "Epoch 32: val_loss did not improve from 0.68366\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6862 - accuracy: 0.5462 - val_loss: 0.6837 - val_accuracy: 0.5531\n",
      "Epoch 33/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.6859 - accuracy: 0.5472\n",
      "Epoch 33: val_loss did not improve from 0.68366\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6862 - accuracy: 0.5462 - val_loss: 0.6837 - val_accuracy: 0.5531\n",
      "Epoch 34/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.6863 - accuracy: 0.5466\n",
      "Epoch 34: val_loss did not improve from 0.68366\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6862 - accuracy: 0.5462 - val_loss: 0.6837 - val_accuracy: 0.5531\n",
      "Epoch 35/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.6863 - accuracy: 0.5464\n",
      "Epoch 35: val_loss did not improve from 0.68366\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6862 - accuracy: 0.5462 - val_loss: 0.6837 - val_accuracy: 0.5531\n",
      "Epoch 36/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.6860 - accuracy: 0.5467\n",
      "Epoch 36: val_loss did not improve from 0.68366\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6863 - accuracy: 0.5462 - val_loss: 0.6837 - val_accuracy: 0.5531\n",
      "Epoch 37/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.6862 - accuracy: 0.5462\n",
      "Epoch 37: val_loss did not improve from 0.68366\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6862 - accuracy: 0.5462 - val_loss: 0.6837 - val_accuracy: 0.5531\n",
      "Epoch 38/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.6860 - accuracy: 0.5469\n",
      "Epoch 38: val_loss did not improve from 0.68366\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6863 - accuracy: 0.5462 - val_loss: 0.6837 - val_accuracy: 0.5531\n",
      "Epoch 38: early stopping\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.6837 - accuracy: 0.5531\n",
      "Processing feature: entrepreneur\n",
      "data frame x has shape: (8516, 1)\n",
      "Epoch 1/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.6986 - accuracy: 0.4942\n",
      "Epoch 1: val_loss improved from inf to 0.69769, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6985 - accuracy: 0.4943 - val_loss: 0.6977 - val_accuracy: 0.4962\n",
      "Epoch 2/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.6971 - accuracy: 0.4963\n",
      "Epoch 2: val_loss improved from 0.69769 to 0.69649, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6971 - accuracy: 0.4957 - val_loss: 0.6965 - val_accuracy: 0.4962\n",
      "Epoch 3/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.6961 - accuracy: 0.4955\n",
      "Epoch 3: val_loss improved from 0.69649 to 0.69556, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6960 - accuracy: 0.4957 - val_loss: 0.6956 - val_accuracy: 0.4962\n",
      "Epoch 4/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.6953 - accuracy: 0.4950\n",
      "Epoch 4: val_loss improved from 0.69556 to 0.69486, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6952 - accuracy: 0.4957 - val_loss: 0.6949 - val_accuracy: 0.4962\n",
      "Epoch 5/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.6946 - accuracy: 0.4895\n",
      "Epoch 5: val_loss improved from 0.69486 to 0.69430, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6946 - accuracy: 0.4904 - val_loss: 0.6943 - val_accuracy: 0.4962\n",
      "Epoch 6/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.6941 - accuracy: 0.4957\n",
      "Epoch 6: val_loss improved from 0.69430 to 0.69390, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6941 - accuracy: 0.4957 - val_loss: 0.6939 - val_accuracy: 0.4962\n",
      "Epoch 7/60\n",
      "653/682 [===========================>..] - ETA: 0s - loss: 0.6938 - accuracy: 0.4946\n",
      "Epoch 7: val_loss improved from 0.69390 to 0.69359, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6938 - accuracy: 0.4939 - val_loss: 0.6936 - val_accuracy: 0.4962\n",
      "Epoch 8/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.6935 - accuracy: 0.4928\n",
      "Epoch 8: val_loss improved from 0.69359 to 0.69339, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6935 - accuracy: 0.4921 - val_loss: 0.6934 - val_accuracy: 0.4991\n",
      "Epoch 9/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.6934 - accuracy: 0.4944\n",
      "Epoch 9: val_loss improved from 0.69339 to 0.69322, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6934 - accuracy: 0.4939 - val_loss: 0.6932 - val_accuracy: 0.4991\n",
      "Epoch 10/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.6933 - accuracy: 0.4876\n",
      "Epoch 10: val_loss improved from 0.69322 to 0.69309, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6933 - accuracy: 0.4876 - val_loss: 0.6931 - val_accuracy: 0.5009\n",
      "Epoch 11/60\n",
      "652/682 [===========================>..] - ETA: 0s - loss: 0.6932 - accuracy: 0.4943\n",
      "Epoch 11: val_loss improved from 0.69309 to 0.69301, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4933 - val_loss: 0.6930 - val_accuracy: 0.5009\n",
      "Epoch 12/60\n",
      "650/682 [===========================>..] - ETA: 0s - loss: 0.6931 - accuracy: 0.4980\n",
      "Epoch 12: val_loss improved from 0.69301 to 0.69296, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4985 - val_loss: 0.6930 - val_accuracy: 0.5038\n",
      "Epoch 13/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.4999\n",
      "Epoch 13: val_loss improved from 0.69296 to 0.69292, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4999 - val_loss: 0.6929 - val_accuracy: 0.5038\n",
      "Epoch 14/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5066\n",
      "Epoch 14: val_loss improved from 0.69292 to 0.69289, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.5043 - val_loss: 0.6929 - val_accuracy: 0.5038\n",
      "Epoch 15/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5025\n",
      "Epoch 15: val_loss improved from 0.69289 to 0.69285, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.5026 - val_loss: 0.6929 - val_accuracy: 0.5009\n",
      "Epoch 16/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.4968\n",
      "Epoch 16: val_loss improved from 0.69285 to 0.69284, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.4976 - val_loss: 0.6928 - val_accuracy: 0.5038\n",
      "Epoch 17/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5018\n",
      "Epoch 17: val_loss improved from 0.69284 to 0.69283, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.5021 - val_loss: 0.6928 - val_accuracy: 0.5038\n",
      "Epoch 18/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.4962\n",
      "Epoch 18: val_loss did not improve from 0.69283\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6929 - accuracy: 0.4982 - val_loss: 0.6928 - val_accuracy: 0.5038\n",
      "Epoch 19/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5027\n",
      "Epoch 19: val_loss improved from 0.69283 to 0.69282, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.5043 - val_loss: 0.6928 - val_accuracy: 0.5038\n",
      "Epoch 20/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.6929 - accuracy: 0.5058\n",
      "Epoch 20: val_loss improved from 0.69282 to 0.69281, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.5043 - val_loss: 0.6928 - val_accuracy: 0.5038\n",
      "Epoch 21/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.6929 - accuracy: 0.5046\n",
      "Epoch 21: val_loss improved from 0.69281 to 0.69279, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.5043 - val_loss: 0.6928 - val_accuracy: 0.5038\n",
      "Epoch 22/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.6930 - accuracy: 0.4992\n",
      "Epoch 22: val_loss did not improve from 0.69279\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.4989 - val_loss: 0.6928 - val_accuracy: 0.5038\n",
      "Epoch 23/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5023\n",
      "Epoch 23: val_loss did not improve from 0.69279\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6929 - accuracy: 0.5021 - val_loss: 0.6928 - val_accuracy: 0.5038\n",
      "Epoch 24/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5004\n",
      "Epoch 24: val_loss did not improve from 0.69279\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6930 - accuracy: 0.4993 - val_loss: 0.6928 - val_accuracy: 0.5038\n",
      "Epoch 25/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.6930 - accuracy: 0.5000\n",
      "Epoch 25: val_loss improved from 0.69279 to 0.69279, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.5011 - val_loss: 0.6928 - val_accuracy: 0.5038\n",
      "Epoch 26/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5015\n",
      "Epoch 26: val_loss improved from 0.69279 to 0.69279, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6929 - accuracy: 0.5024 - val_loss: 0.6928 - val_accuracy: 0.5038\n",
      "Epoch 27/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.6929 - accuracy: 0.5069\n",
      "Epoch 27: val_loss improved from 0.69279 to 0.69279, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6929 - accuracy: 0.5043 - val_loss: 0.6928 - val_accuracy: 0.5038\n",
      "Epoch 28/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.4964\n",
      "Epoch 28: val_loss did not improve from 0.69279\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.4964 - val_loss: 0.6928 - val_accuracy: 0.5038\n",
      "Epoch 29/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.4986\n",
      "Epoch 29: val_loss improved from 0.69279 to 0.69278, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.4996 - val_loss: 0.6928 - val_accuracy: 0.5038\n",
      "Epoch 30/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.6929 - accuracy: 0.5009\n",
      "Epoch 30: val_loss improved from 0.69278 to 0.69278, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6929 - accuracy: 0.5008 - val_loss: 0.6928 - val_accuracy: 0.5038\n",
      "Epoch 31/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.6929 - accuracy: 0.4933\n",
      "Epoch 31: val_loss improved from 0.69278 to 0.69278, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.4954 - val_loss: 0.6928 - val_accuracy: 0.5038\n",
      "Epoch 32/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.6929 - accuracy: 0.4940\n",
      "Epoch 32: val_loss did not improve from 0.69278\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6929 - accuracy: 0.4936 - val_loss: 0.6928 - val_accuracy: 0.5038\n",
      "Epoch 33/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.6929 - accuracy: 0.5034\n",
      "Epoch 33: val_loss did not improve from 0.69278\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6929 - accuracy: 0.5043 - val_loss: 0.6928 - val_accuracy: 0.5038\n",
      "Epoch 34/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.6929 - accuracy: 0.4988\n",
      "Epoch 34: val_loss did not improve from 0.69278\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6930 - accuracy: 0.5004 - val_loss: 0.6928 - val_accuracy: 0.5038\n",
      "Epoch 35/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.6929 - accuracy: 0.4980\n",
      "Epoch 35: val_loss did not improve from 0.69278\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6930 - accuracy: 0.4980 - val_loss: 0.6928 - val_accuracy: 0.5038\n",
      "Epoch 36/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.6929 - accuracy: 0.5027\n",
      "Epoch 36: val_loss improved from 0.69278 to 0.69278, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6929 - accuracy: 0.5023 - val_loss: 0.6928 - val_accuracy: 0.5038\n",
      "Epoch 37/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5007\n",
      "Epoch 37: val_loss did not improve from 0.69278\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6929 - accuracy: 0.5012 - val_loss: 0.6928 - val_accuracy: 0.5038\n",
      "Epoch 38/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5025\n",
      "Epoch 38: val_loss did not improve from 0.69278\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.5027 - val_loss: 0.6928 - val_accuracy: 0.5038\n",
      "Epoch 39/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.4994\n",
      "Epoch 39: val_loss did not improve from 0.69278\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6929 - accuracy: 0.5012 - val_loss: 0.6928 - val_accuracy: 0.5038\n",
      "Epoch 40/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5018\n",
      "Epoch 40: val_loss did not improve from 0.69278\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.5018 - val_loss: 0.6928 - val_accuracy: 0.5038\n",
      "Epoch 41/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.6929 - accuracy: 0.5047\n",
      "Epoch 41: val_loss improved from 0.69278 to 0.69278, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6929 - accuracy: 0.5020 - val_loss: 0.6928 - val_accuracy: 0.5009\n",
      "Epoch 42/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.6929 - accuracy: 0.4991\n",
      "Epoch 42: val_loss improved from 0.69278 to 0.69278, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.4985 - val_loss: 0.6928 - val_accuracy: 0.5038\n",
      "Epoch 43/60\n",
      "653/682 [===========================>..] - ETA: 0s - loss: 0.6931 - accuracy: 0.4968\n",
      "Epoch 43: val_loss did not improve from 0.69278\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.4982 - val_loss: 0.6928 - val_accuracy: 0.5038\n",
      "Epoch 44/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.6929 - accuracy: 0.5036\n",
      "Epoch 44: val_loss did not improve from 0.69278\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6929 - accuracy: 0.5043 - val_loss: 0.6928 - val_accuracy: 0.5038\n",
      "Epoch 45/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.6929 - accuracy: 0.5034\n",
      "Epoch 45: val_loss did not improve from 0.69278\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6929 - accuracy: 0.5027 - val_loss: 0.6928 - val_accuracy: 0.5038\n",
      "Epoch 46/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.4968\n",
      "Epoch 46: val_loss did not improve from 0.69278\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6929 - accuracy: 0.4970 - val_loss: 0.6928 - val_accuracy: 0.5038\n",
      "Epoch 47/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.6929 - accuracy: 0.5024\n",
      "Epoch 47: val_loss did not improve from 0.69278\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6929 - accuracy: 0.5024 - val_loss: 0.6928 - val_accuracy: 0.5038\n",
      "Epoch 48/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.4973\n",
      "Epoch 48: val_loss did not improve from 0.69278\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.4985 - val_loss: 0.6928 - val_accuracy: 0.5038\n",
      "Epoch 49/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.6929 - accuracy: 0.5044\n",
      "Epoch 49: val_loss did not improve from 0.69278\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.5043 - val_loss: 0.6928 - val_accuracy: 0.5038\n",
      "Epoch 50/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.6929 - accuracy: 0.5050\n",
      "Epoch 50: val_loss did not improve from 0.69278\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6929 - accuracy: 0.5043 - val_loss: 0.6928 - val_accuracy: 0.5038\n",
      "Epoch 51/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5005\n",
      "Epoch 51: val_loss did not improve from 0.69278\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.5011 - val_loss: 0.6928 - val_accuracy: 0.5038\n",
      "Epoch 52/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.4908\n",
      "Epoch 52: val_loss did not improve from 0.69278\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6929 - accuracy: 0.4921 - val_loss: 0.6928 - val_accuracy: 0.5038\n",
      "Epoch 53/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5043\n",
      "Epoch 53: val_loss did not improve from 0.69278\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6929 - accuracy: 0.5043 - val_loss: 0.6928 - val_accuracy: 0.5038\n",
      "Epoch 54/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.4993\n",
      "Epoch 54: val_loss did not improve from 0.69278\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6929 - accuracy: 0.4986 - val_loss: 0.6928 - val_accuracy: 0.5038\n",
      "Epoch 55/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.6929 - accuracy: 0.5043\n",
      "Epoch 55: val_loss did not improve from 0.69278\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.5043 - val_loss: 0.6928 - val_accuracy: 0.5038\n",
      "Epoch 56/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.4985\n",
      "Epoch 56: val_loss did not improve from 0.69278\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.4995 - val_loss: 0.6928 - val_accuracy: 0.5038\n",
      "Epoch 57/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.6929 - accuracy: 0.4916\n",
      "Epoch 57: val_loss did not improve from 0.69278\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6929 - accuracy: 0.4917 - val_loss: 0.6928 - val_accuracy: 0.5038\n",
      "Epoch 58/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.6930 - accuracy: 0.5055\n",
      "Epoch 58: val_loss did not improve from 0.69278\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.5043 - val_loss: 0.6928 - val_accuracy: 0.5038\n",
      "Epoch 59/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.6929 - accuracy: 0.5036\n",
      "Epoch 59: val_loss did not improve from 0.69278\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6929 - accuracy: 0.5033 - val_loss: 0.6928 - val_accuracy: 0.5038\n",
      "Epoch 60/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5044\n",
      "Epoch 60: val_loss did not improve from 0.69278\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.5043 - val_loss: 0.6928 - val_accuracy: 0.5038\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.6928 - accuracy: 0.5038\n",
      "Processing feature: housemaid\n",
      "data frame x has shape: (8516, 1)\n",
      "Epoch 1/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.6967 - accuracy: 0.5042\n",
      "Epoch 1: val_loss improved from inf to 0.70006, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 3s 3ms/step - loss: 0.6967 - accuracy: 0.5037 - val_loss: 0.7001 - val_accuracy: 0.4997\n",
      "Epoch 2/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.6960 - accuracy: 0.5027\n",
      "Epoch 2: val_loss improved from 0.70006 to 0.69882, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6960 - accuracy: 0.5027 - val_loss: 0.6988 - val_accuracy: 0.4997\n",
      "Epoch 3/60\n",
      "653/682 [===========================>..] - ETA: 0s - loss: 0.6953 - accuracy: 0.5032\n",
      "Epoch 3: val_loss improved from 0.69882 to 0.69786, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6954 - accuracy: 0.5027 - val_loss: 0.6979 - val_accuracy: 0.4997\n",
      "Epoch 4/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.6949 - accuracy: 0.5039\n",
      "Epoch 4: val_loss improved from 0.69786 to 0.69698, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6949 - accuracy: 0.5027 - val_loss: 0.6970 - val_accuracy: 0.4997\n",
      "Epoch 5/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.6945 - accuracy: 0.5031\n",
      "Epoch 5: val_loss improved from 0.69698 to 0.69626, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6945 - accuracy: 0.5027 - val_loss: 0.6963 - val_accuracy: 0.4997\n",
      "Epoch 6/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.6941 - accuracy: 0.5034\n",
      "Epoch 6: val_loss improved from 0.69626 to 0.69570, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6942 - accuracy: 0.5027 - val_loss: 0.6957 - val_accuracy: 0.4997\n",
      "Epoch 7/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.6939 - accuracy: 0.5017\n",
      "Epoch 7: val_loss improved from 0.69570 to 0.69522, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6939 - accuracy: 0.5027 - val_loss: 0.6952 - val_accuracy: 0.4997\n",
      "Epoch 8/60\n",
      "654/682 [===========================>..] - ETA: 0s - loss: 0.6939 - accuracy: 0.5012\n",
      "Epoch 8: val_loss improved from 0.69522 to 0.69484, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6937 - accuracy: 0.5027 - val_loss: 0.6948 - val_accuracy: 0.4997\n",
      "Epoch 9/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.6936 - accuracy: 0.5026\n",
      "Epoch 9: val_loss improved from 0.69484 to 0.69451, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6936 - accuracy: 0.5027 - val_loss: 0.6945 - val_accuracy: 0.4997\n",
      "Epoch 10/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.6936 - accuracy: 0.4967\n",
      "Epoch 10: val_loss improved from 0.69451 to 0.69426, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6934 - accuracy: 0.4992 - val_loss: 0.6943 - val_accuracy: 0.4997\n",
      "Epoch 11/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.6935 - accuracy: 0.5021\n",
      "Epoch 11: val_loss improved from 0.69426 to 0.69409, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6934 - accuracy: 0.5027 - val_loss: 0.6941 - val_accuracy: 0.4997\n",
      "Epoch 12/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.6933 - accuracy: 0.5017\n",
      "Epoch 12: val_loss improved from 0.69409 to 0.69394, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6933 - accuracy: 0.5010 - val_loss: 0.6939 - val_accuracy: 0.4997\n",
      "Epoch 13/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.6932 - accuracy: 0.4968\n",
      "Epoch 13: val_loss improved from 0.69394 to 0.69382, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4966 - val_loss: 0.6938 - val_accuracy: 0.4997\n",
      "Epoch 14/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4999\n",
      "Epoch 14: val_loss improved from 0.69382 to 0.69372, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4996 - val_loss: 0.6937 - val_accuracy: 0.4997\n",
      "Epoch 15/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4957\n",
      "Epoch 15: val_loss improved from 0.69372 to 0.69365, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4955 - val_loss: 0.6936 - val_accuracy: 0.4997\n",
      "Epoch 16/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5028\n",
      "Epoch 16: val_loss improved from 0.69365 to 0.69358, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5027 - val_loss: 0.6936 - val_accuracy: 0.4997\n",
      "Epoch 17/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.4994\n",
      "Epoch 17: val_loss improved from 0.69358 to 0.69355, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4973 - val_loss: 0.6935 - val_accuracy: 0.4997\n",
      "Epoch 18/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.5000\n",
      "Epoch 18: val_loss improved from 0.69355 to 0.69350, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4998 - val_loss: 0.6935 - val_accuracy: 0.4997\n",
      "Epoch 19/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.4978\n",
      "Epoch 19: val_loss improved from 0.69350 to 0.69348, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4973 - val_loss: 0.6935 - val_accuracy: 0.5009\n",
      "Epoch 20/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.4990\n",
      "Epoch 20: val_loss improved from 0.69348 to 0.69344, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4980 - val_loss: 0.6934 - val_accuracy: 0.4997\n",
      "Epoch 21/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5036\n",
      "Epoch 21: val_loss improved from 0.69344 to 0.69342, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5021 - val_loss: 0.6934 - val_accuracy: 0.4997\n",
      "Epoch 22/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.4961\n",
      "Epoch 22: val_loss improved from 0.69342 to 0.69340, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4966 - val_loss: 0.6934 - val_accuracy: 0.4997\n",
      "Epoch 23/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.6931 - accuracy: 0.5012\n",
      "Epoch 23: val_loss improved from 0.69340 to 0.69339, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5012 - val_loss: 0.6934 - val_accuracy: 0.4997\n",
      "Epoch 24/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.6930 - accuracy: 0.4924\n",
      "Epoch 24: val_loss improved from 0.69339 to 0.69337, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4917 - val_loss: 0.6934 - val_accuracy: 0.4997\n",
      "Epoch 25/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.4978\n",
      "Epoch 25: val_loss improved from 0.69337 to 0.69336, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4974 - val_loss: 0.6934 - val_accuracy: 0.4997\n",
      "Epoch 26/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.4967\n",
      "Epoch 26: val_loss improved from 0.69336 to 0.69335, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4977 - val_loss: 0.6933 - val_accuracy: 0.4997\n",
      "Epoch 27/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.4971\n",
      "Epoch 27: val_loss improved from 0.69335 to 0.69335, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4971 - val_loss: 0.6933 - val_accuracy: 0.4997\n",
      "Epoch 28/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.6931 - accuracy: 0.5006\n",
      "Epoch 28: val_loss improved from 0.69335 to 0.69333, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5002 - val_loss: 0.6933 - val_accuracy: 0.4997\n",
      "Epoch 29/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5024\n",
      "Epoch 29: val_loss did not improve from 0.69333\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5017 - val_loss: 0.6933 - val_accuracy: 0.4997\n",
      "Epoch 30/60\n",
      "653/682 [===========================>..] - ETA: 0s - loss: 0.6931 - accuracy: 0.4975\n",
      "Epoch 30: val_loss improved from 0.69333 to 0.69333, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4990 - val_loss: 0.6933 - val_accuracy: 0.4997\n",
      "Epoch 31/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.6931 - accuracy: 0.5015\n",
      "Epoch 31: val_loss did not improve from 0.69333\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5015 - val_loss: 0.6933 - val_accuracy: 0.4997\n",
      "Epoch 32/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5035\n",
      "Epoch 32: val_loss did not improve from 0.69333\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.5054 - val_loss: 0.6933 - val_accuracy: 0.4997\n",
      "Epoch 33/60\n",
      "653/682 [===========================>..] - ETA: 0s - loss: 0.6931 - accuracy: 0.5028\n",
      "Epoch 33: val_loss improved from 0.69333 to 0.69332, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5027 - val_loss: 0.6933 - val_accuracy: 0.4997\n",
      "Epoch 34/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.4946\n",
      "Epoch 34: val_loss did not improve from 0.69332\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4935 - val_loss: 0.6933 - val_accuracy: 0.4997\n",
      "Epoch 35/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.4956\n",
      "Epoch 35: val_loss improved from 0.69332 to 0.69331, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4955 - val_loss: 0.6933 - val_accuracy: 0.4997\n",
      "Epoch 36/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.6931 - accuracy: 0.4935\n",
      "Epoch 36: val_loss improved from 0.69331 to 0.69331, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4930 - val_loss: 0.6933 - val_accuracy: 0.4997\n",
      "Epoch 37/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.6932 - accuracy: 0.5030\n",
      "Epoch 37: val_loss improved from 0.69331 to 0.69331, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5027 - val_loss: 0.6933 - val_accuracy: 0.4997\n",
      "Epoch 38/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.4990\n",
      "Epoch 38: val_loss did not improve from 0.69331\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6931 - accuracy: 0.4988 - val_loss: 0.6933 - val_accuracy: 0.4997\n",
      "Epoch 39/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.4921\n",
      "Epoch 39: val_loss improved from 0.69331 to 0.69330, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4932 - val_loss: 0.6933 - val_accuracy: 0.4997\n",
      "Epoch 40/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.6931 - accuracy: 0.4907\n",
      "Epoch 40: val_loss did not improve from 0.69330\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4920 - val_loss: 0.6933 - val_accuracy: 0.4997\n",
      "Epoch 41/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.6931 - accuracy: 0.4953\n",
      "Epoch 41: val_loss did not improve from 0.69330\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4963 - val_loss: 0.6933 - val_accuracy: 0.4997\n",
      "Epoch 42/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5032\n",
      "Epoch 42: val_loss did not improve from 0.69330\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.5024 - val_loss: 0.6933 - val_accuracy: 0.5009\n",
      "Epoch 43/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.4981\n",
      "Epoch 43: val_loss did not improve from 0.69330\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4993 - val_loss: 0.6933 - val_accuracy: 0.5009\n",
      "Epoch 44/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.4972\n",
      "Epoch 44: val_loss did not improve from 0.69330\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4964 - val_loss: 0.6933 - val_accuracy: 0.5009\n",
      "Epoch 45/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.4946\n",
      "Epoch 45: val_loss improved from 0.69330 to 0.69330, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4945 - val_loss: 0.6933 - val_accuracy: 0.4997\n",
      "Epoch 46/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.4898\n",
      "Epoch 46: val_loss improved from 0.69330 to 0.69330, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4907 - val_loss: 0.6933 - val_accuracy: 0.4997\n",
      "Epoch 47/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.4998\n",
      "Epoch 47: val_loss did not improve from 0.69330\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5005 - val_loss: 0.6933 - val_accuracy: 0.4997\n",
      "Epoch 48/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.6931 - accuracy: 0.4992\n",
      "Epoch 48: val_loss did not improve from 0.69330\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5005 - val_loss: 0.6933 - val_accuracy: 0.4997\n",
      "Epoch 49/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.4968\n",
      "Epoch 49: val_loss did not improve from 0.69330\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4974 - val_loss: 0.6933 - val_accuracy: 0.4997\n",
      "Epoch 50/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.6930 - accuracy: 0.4998\n",
      "Epoch 50: val_loss did not improve from 0.69330\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4977 - val_loss: 0.6933 - val_accuracy: 0.4997\n",
      "Epoch 51/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.4935\n",
      "Epoch 51: val_loss did not improve from 0.69330\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4933 - val_loss: 0.6933 - val_accuracy: 0.4997\n",
      "Epoch 52/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.6930 - accuracy: 0.4986\n",
      "Epoch 52: val_loss did not improve from 0.69330\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4993 - val_loss: 0.6933 - val_accuracy: 0.4997\n",
      "Epoch 53/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5018\n",
      "Epoch 53: val_loss did not improve from 0.69330\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5027 - val_loss: 0.6933 - val_accuracy: 0.4997\n",
      "Epoch 54/60\n",
      "653/682 [===========================>..] - ETA: 0s - loss: 0.6930 - accuracy: 0.4959\n",
      "Epoch 54: val_loss did not improve from 0.69330\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4943 - val_loss: 0.6933 - val_accuracy: 0.4997\n",
      "Epoch 55/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.4993\n",
      "Epoch 55: val_loss did not improve from 0.69330\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6931 - accuracy: 0.4989 - val_loss: 0.6933 - val_accuracy: 0.4997\n",
      "Epoch 56/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.4949\n",
      "Epoch 56: val_loss did not improve from 0.69330\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4955 - val_loss: 0.6933 - val_accuracy: 0.4997\n",
      "Epoch 57/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5015\n",
      "Epoch 57: val_loss did not improve from 0.69330\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5024 - val_loss: 0.6933 - val_accuracy: 0.5009\n",
      "Epoch 58/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.4923\n",
      "Epoch 58: val_loss did not improve from 0.69330\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4924 - val_loss: 0.6933 - val_accuracy: 0.5009\n",
      "Epoch 59/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.6931 - accuracy: 0.5002\n",
      "Epoch 59: val_loss did not improve from 0.69330\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5004 - val_loss: 0.6933 - val_accuracy: 0.4997\n",
      "Epoch 60/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.4978\n",
      "Epoch 60: val_loss did not improve from 0.69330\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4983 - val_loss: 0.6933 - val_accuracy: 0.4997\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.6933 - accuracy: 0.4997\n",
      "Processing feature: management\n",
      "data frame x has shape: (8516, 1)\n",
      "Epoch 1/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.6944 - accuracy: 0.4980\n",
      "Epoch 1: val_loss improved from inf to 0.69357, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 3ms/step - loss: 0.6944 - accuracy: 0.4974 - val_loss: 0.6936 - val_accuracy: 0.5009\n",
      "Epoch 2/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.6939 - accuracy: 0.4985\n",
      "Epoch 2: val_loss improved from 0.69357 to 0.69334, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6939 - accuracy: 0.4988 - val_loss: 0.6933 - val_accuracy: 0.5009\n",
      "Epoch 3/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.6936 - accuracy: 0.4973\n",
      "Epoch 3: val_loss improved from 0.69334 to 0.69321, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6936 - accuracy: 0.4976 - val_loss: 0.6932 - val_accuracy: 0.5009\n",
      "Epoch 4/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.6935 - accuracy: 0.4949\n",
      "Epoch 4: val_loss improved from 0.69321 to 0.69315, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6935 - accuracy: 0.4952 - val_loss: 0.6932 - val_accuracy: 0.5009\n",
      "Epoch 5/60\n",
      "653/682 [===========================>..] - ETA: 0s - loss: 0.6934 - accuracy: 0.4928\n",
      "Epoch 5: val_loss improved from 0.69315 to 0.69313, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6934 - accuracy: 0.4930 - val_loss: 0.6931 - val_accuracy: 0.5009\n",
      "Epoch 6/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.6933 - accuracy: 0.5033\n",
      "Epoch 6: val_loss did not improve from 0.69313\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6933 - accuracy: 0.5043 - val_loss: 0.6932 - val_accuracy: 0.5009\n",
      "Epoch 7/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.6933 - accuracy: 0.4966\n",
      "Epoch 7: val_loss did not improve from 0.69313\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6933 - accuracy: 0.4966 - val_loss: 0.6931 - val_accuracy: 0.5009\n",
      "Epoch 8/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.6933 - accuracy: 0.4928\n",
      "Epoch 8: val_loss did not improve from 0.69313\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6933 - accuracy: 0.4920 - val_loss: 0.6931 - val_accuracy: 0.5009\n",
      "Epoch 9/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4923\n",
      "Epoch 9: val_loss did not improve from 0.69313\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4919 - val_loss: 0.6932 - val_accuracy: 0.4991\n",
      "Epoch 10/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4937\n",
      "Epoch 10: val_loss did not improve from 0.69313\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4938 - val_loss: 0.6932 - val_accuracy: 0.4991\n",
      "Epoch 11/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4961\n",
      "Epoch 11: val_loss did not improve from 0.69313\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4952 - val_loss: 0.6932 - val_accuracy: 0.4991\n",
      "Epoch 12/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4967\n",
      "Epoch 12: val_loss did not improve from 0.69313\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4960 - val_loss: 0.6932 - val_accuracy: 0.4991\n",
      "Epoch 13/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4872\n",
      "Epoch 13: val_loss did not improve from 0.69313\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4875 - val_loss: 0.6932 - val_accuracy: 0.4991\n",
      "Epoch 14/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4899\n",
      "Epoch 14: val_loss did not improve from 0.69313\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4879 - val_loss: 0.6932 - val_accuracy: 0.4991\n",
      "Epoch 15/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.5011\n",
      "Epoch 15: val_loss did not improve from 0.69313\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.5014 - val_loss: 0.6932 - val_accuracy: 0.4991\n",
      "Epoch 16/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4892\n",
      "Epoch 16: val_loss did not improve from 0.69313\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4892 - val_loss: 0.6932 - val_accuracy: 0.4991\n",
      "Epoch 17/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4982\n",
      "Epoch 17: val_loss did not improve from 0.69313\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4976 - val_loss: 0.6932 - val_accuracy: 0.4991\n",
      "Epoch 18/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4946\n",
      "Epoch 18: val_loss did not improve from 0.69313\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.4942 - val_loss: 0.6932 - val_accuracy: 0.4991\n",
      "Epoch 19/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4962\n",
      "Epoch 19: val_loss did not improve from 0.69313\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4958 - val_loss: 0.6932 - val_accuracy: 0.4991\n",
      "Epoch 20/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4949\n",
      "Epoch 20: val_loss did not improve from 0.69313\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4952 - val_loss: 0.6932 - val_accuracy: 0.4991\n",
      "Epoch 21/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4940\n",
      "Epoch 21: val_loss did not improve from 0.69313\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4932 - val_loss: 0.6932 - val_accuracy: 0.4991\n",
      "Epoch 22/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4896\n",
      "Epoch 22: val_loss did not improve from 0.69313\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.4910 - val_loss: 0.6932 - val_accuracy: 0.4991\n",
      "Epoch 23/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.6932 - accuracy: 0.5008\n",
      "Epoch 23: val_loss did not improve from 0.69313\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4990 - val_loss: 0.6932 - val_accuracy: 0.4991\n",
      "Epoch 24/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.6932 - accuracy: 0.4934\n",
      "Epoch 24: val_loss did not improve from 0.69313\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4933 - val_loss: 0.6932 - val_accuracy: 0.4991\n",
      "Epoch 25/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.6932 - accuracy: 0.4898\n",
      "Epoch 25: val_loss did not improve from 0.69313\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4891 - val_loss: 0.6932 - val_accuracy: 0.4991\n",
      "Epoch 25: early stopping\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5009\n",
      "Processing feature: retired\n",
      "data frame x has shape: (8516, 1)\n",
      "Epoch 1/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.7307 - accuracy: 0.4688\n",
      "Epoch 1: val_loss improved from inf to 0.71489, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.7311 - accuracy: 0.4692 - val_loss: 0.7149 - val_accuracy: 0.4821\n",
      "Epoch 2/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.7193 - accuracy: 0.4706\n",
      "Epoch 2: val_loss improved from 0.71489 to 0.70738, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.7196 - accuracy: 0.4691 - val_loss: 0.7074 - val_accuracy: 0.4821\n",
      "Epoch 3/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.7103 - accuracy: 0.4697\n",
      "Epoch 3: val_loss improved from 0.70738 to 0.70150, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.7104 - accuracy: 0.4691 - val_loss: 0.7015 - val_accuracy: 0.4821\n",
      "Epoch 4/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.7031 - accuracy: 0.4690\n",
      "Epoch 4: val_loss improved from 0.70150 to 0.69725, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.7030 - accuracy: 0.4691 - val_loss: 0.6973 - val_accuracy: 0.4821\n",
      "Epoch 5/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.6974 - accuracy: 0.4699\n",
      "Epoch 5: val_loss improved from 0.69725 to 0.69429, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6974 - accuracy: 0.4691 - val_loss: 0.6943 - val_accuracy: 0.4821\n",
      "Epoch 6/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.6935 - accuracy: 0.4840\n",
      "Epoch 6: val_loss improved from 0.69429 to 0.69239, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6934 - accuracy: 0.4839 - val_loss: 0.6924 - val_accuracy: 0.5179\n",
      "Epoch 7/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.6905 - accuracy: 0.5269\n",
      "Epoch 7: val_loss improved from 0.69239 to 0.69128, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6905 - accuracy: 0.5266 - val_loss: 0.6913 - val_accuracy: 0.5179\n",
      "Epoch 8/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.6886 - accuracy: 0.5203\n",
      "Epoch 8: val_loss improved from 0.69128 to 0.69070, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6886 - accuracy: 0.5203 - val_loss: 0.6907 - val_accuracy: 0.5179\n",
      "Epoch 9/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.6872 - accuracy: 0.5303\n",
      "Epoch 9: val_loss improved from 0.69070 to 0.69046, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6872 - accuracy: 0.5309 - val_loss: 0.6905 - val_accuracy: 0.5179\n",
      "Epoch 10/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.6862 - accuracy: 0.5295\n",
      "Epoch 10: val_loss improved from 0.69046 to 0.69044, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6864 - accuracy: 0.5309 - val_loss: 0.6904 - val_accuracy: 0.5179\n",
      "Epoch 11/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.6858 - accuracy: 0.5310\n",
      "Epoch 11: val_loss did not improve from 0.69044\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6858 - accuracy: 0.5309 - val_loss: 0.6905 - val_accuracy: 0.5179\n",
      "Epoch 12/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.6855 - accuracy: 0.5311\n",
      "Epoch 12: val_loss did not improve from 0.69044\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6855 - accuracy: 0.5309 - val_loss: 0.6907 - val_accuracy: 0.5179\n",
      "Epoch 13/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.6850 - accuracy: 0.5310\n",
      "Epoch 13: val_loss did not improve from 0.69044\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6853 - accuracy: 0.5309 - val_loss: 0.6908 - val_accuracy: 0.5179\n",
      "Epoch 14/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.6851 - accuracy: 0.5305\n",
      "Epoch 14: val_loss did not improve from 0.69044\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6851 - accuracy: 0.5309 - val_loss: 0.6910 - val_accuracy: 0.5179\n",
      "Epoch 15/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.6851 - accuracy: 0.5309\n",
      "Epoch 15: val_loss did not improve from 0.69044\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6851 - accuracy: 0.5309 - val_loss: 0.6911 - val_accuracy: 0.5179\n",
      "Epoch 16/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.6851 - accuracy: 0.5310\n",
      "Epoch 16: val_loss did not improve from 0.69044\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6850 - accuracy: 0.5309 - val_loss: 0.6912 - val_accuracy: 0.5179\n",
      "Epoch 17/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.6850 - accuracy: 0.5309\n",
      "Epoch 17: val_loss did not improve from 0.69044\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6849 - accuracy: 0.5309 - val_loss: 0.6913 - val_accuracy: 0.5179\n",
      "Epoch 18/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.6851 - accuracy: 0.5309\n",
      "Epoch 18: val_loss did not improve from 0.69044\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6849 - accuracy: 0.5309 - val_loss: 0.6914 - val_accuracy: 0.5179\n",
      "Epoch 19/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.6848 - accuracy: 0.5312\n",
      "Epoch 19: val_loss did not improve from 0.69044\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6849 - accuracy: 0.5309 - val_loss: 0.6915 - val_accuracy: 0.5179\n",
      "Epoch 20/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.6850 - accuracy: 0.5309\n",
      "Epoch 20: val_loss did not improve from 0.69044\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6849 - accuracy: 0.5309 - val_loss: 0.6915 - val_accuracy: 0.5179\n",
      "Epoch 21/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.6848 - accuracy: 0.5304\n",
      "Epoch 21: val_loss did not improve from 0.69044\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6849 - accuracy: 0.5309 - val_loss: 0.6916 - val_accuracy: 0.5179\n",
      "Epoch 22/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.6848 - accuracy: 0.5300\n",
      "Epoch 22: val_loss did not improve from 0.69044\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6849 - accuracy: 0.5309 - val_loss: 0.6916 - val_accuracy: 0.5179\n",
      "Epoch 23/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.6851 - accuracy: 0.5316\n",
      "Epoch 23: val_loss did not improve from 0.69044\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6850 - accuracy: 0.5309 - val_loss: 0.6917 - val_accuracy: 0.5179\n",
      "Epoch 24/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.6849 - accuracy: 0.5315\n",
      "Epoch 24: val_loss did not improve from 0.69044\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6849 - accuracy: 0.5309 - val_loss: 0.6917 - val_accuracy: 0.5179\n",
      "Epoch 25/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.6848 - accuracy: 0.5319\n",
      "Epoch 25: val_loss did not improve from 0.69044\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6849 - accuracy: 0.5309 - val_loss: 0.6917 - val_accuracy: 0.5179\n",
      "Epoch 26/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.6849 - accuracy: 0.5318\n",
      "Epoch 26: val_loss did not improve from 0.69044\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6849 - accuracy: 0.5309 - val_loss: 0.6917 - val_accuracy: 0.5179\n",
      "Epoch 27/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.6851 - accuracy: 0.5315\n",
      "Epoch 27: val_loss did not improve from 0.69044\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6849 - accuracy: 0.5309 - val_loss: 0.6917 - val_accuracy: 0.5179\n",
      "Epoch 28/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.6850 - accuracy: 0.5300\n",
      "Epoch 28: val_loss did not improve from 0.69044\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6849 - accuracy: 0.5309 - val_loss: 0.6918 - val_accuracy: 0.5179\n",
      "Epoch 29/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.6850 - accuracy: 0.5307\n",
      "Epoch 29: val_loss did not improve from 0.69044\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6849 - accuracy: 0.5309 - val_loss: 0.6918 - val_accuracy: 0.5179\n",
      "Epoch 30/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.6848 - accuracy: 0.5314\n",
      "Epoch 30: val_loss did not improve from 0.69044\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6849 - accuracy: 0.5309 - val_loss: 0.6918 - val_accuracy: 0.5179\n",
      "Epoch 30: early stopping\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.6904 - accuracy: 0.5179\n",
      "Processing feature: self-employed\n",
      "data frame x has shape: (8516, 1)\n",
      "Epoch 1/60\n",
      "653/682 [===========================>..] - ETA: 0s - loss: 0.6997 - accuracy: 0.4983\n",
      "Epoch 1: val_loss improved from inf to 0.69682, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 3ms/step - loss: 0.6997 - accuracy: 0.4966 - val_loss: 0.6968 - val_accuracy: 0.5044\n",
      "Epoch 2/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.6981 - accuracy: 0.4990\n",
      "Epoch 2: val_loss improved from 0.69682 to 0.69564, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6981 - accuracy: 0.4992 - val_loss: 0.6956 - val_accuracy: 0.5044\n",
      "Epoch 3/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.6969 - accuracy: 0.5017\n",
      "Epoch 3: val_loss improved from 0.69564 to 0.69472, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6969 - accuracy: 0.4992 - val_loss: 0.6947 - val_accuracy: 0.5044\n",
      "Epoch 4/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.6959 - accuracy: 0.4989\n",
      "Epoch 4: val_loss improved from 0.69472 to 0.69409, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6959 - accuracy: 0.4992 - val_loss: 0.6941 - val_accuracy: 0.5044\n",
      "Epoch 5/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.6952 - accuracy: 0.4993\n",
      "Epoch 5: val_loss improved from 0.69409 to 0.69363, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6952 - accuracy: 0.4992 - val_loss: 0.6936 - val_accuracy: 0.5044\n",
      "Epoch 6/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.6947 - accuracy: 0.4980\n",
      "Epoch 6: val_loss improved from 0.69363 to 0.69335, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6946 - accuracy: 0.4992 - val_loss: 0.6933 - val_accuracy: 0.5044\n",
      "Epoch 7/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.6942 - accuracy: 0.4996\n",
      "Epoch 7: val_loss improved from 0.69335 to 0.69314, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6942 - accuracy: 0.4992 - val_loss: 0.6931 - val_accuracy: 0.5044\n",
      "Epoch 8/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.6939 - accuracy: 0.4927\n",
      "Epoch 8: val_loss improved from 0.69314 to 0.69304, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6939 - accuracy: 0.4927 - val_loss: 0.6930 - val_accuracy: 0.5044\n",
      "Epoch 9/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.6937 - accuracy: 0.5000\n",
      "Epoch 9: val_loss improved from 0.69304 to 0.69298, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6937 - accuracy: 0.4992 - val_loss: 0.6930 - val_accuracy: 0.5044\n",
      "Epoch 10/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.6935 - accuracy: 0.4925\n",
      "Epoch 10: val_loss improved from 0.69298 to 0.69296, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6935 - accuracy: 0.4927 - val_loss: 0.6930 - val_accuracy: 0.5044\n",
      "Epoch 11/60\n",
      "653/682 [===========================>..] - ETA: 0s - loss: 0.6934 - accuracy: 0.4968\n",
      "Epoch 11: val_loss did not improve from 0.69296\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6934 - accuracy: 0.4968 - val_loss: 0.6930 - val_accuracy: 0.5044\n",
      "Epoch 12/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.6933 - accuracy: 0.5005\n",
      "Epoch 12: val_loss did not improve from 0.69296\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6934 - accuracy: 0.4992 - val_loss: 0.6930 - val_accuracy: 0.5044\n",
      "Epoch 13/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.6933 - accuracy: 0.4897\n",
      "Epoch 13: val_loss did not improve from 0.69296\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6933 - accuracy: 0.4892 - val_loss: 0.6930 - val_accuracy: 0.5044\n",
      "Epoch 14/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.6933 - accuracy: 0.4961\n",
      "Epoch 14: val_loss did not improve from 0.69296\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6933 - accuracy: 0.4948 - val_loss: 0.6930 - val_accuracy: 0.4991\n",
      "Epoch 15/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.6933 - accuracy: 0.4947\n",
      "Epoch 15: val_loss did not improve from 0.69296\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6933 - accuracy: 0.4948 - val_loss: 0.6931 - val_accuracy: 0.4991\n",
      "Epoch 16/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.6933 - accuracy: 0.4987\n",
      "Epoch 16: val_loss did not improve from 0.69296\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6933 - accuracy: 0.4985 - val_loss: 0.6931 - val_accuracy: 0.4991\n",
      "Epoch 17/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.6933 - accuracy: 0.4890\n",
      "Epoch 17: val_loss did not improve from 0.69296\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4897 - val_loss: 0.6931 - val_accuracy: 0.4991\n",
      "Epoch 18/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.6932 - accuracy: 0.4995\n",
      "Epoch 18: val_loss did not improve from 0.69296\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4977 - val_loss: 0.6931 - val_accuracy: 0.5044\n",
      "Epoch 19/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.5006\n",
      "Epoch 19: val_loss did not improve from 0.69296\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.5001 - val_loss: 0.6931 - val_accuracy: 0.4991\n",
      "Epoch 20/60\n",
      "653/682 [===========================>..] - ETA: 0s - loss: 0.6932 - accuracy: 0.4876\n",
      "Epoch 20: val_loss did not improve from 0.69296\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.4876 - val_loss: 0.6932 - val_accuracy: 0.4956\n",
      "Epoch 21/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4900\n",
      "Epoch 21: val_loss did not improve from 0.69296\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4895 - val_loss: 0.6932 - val_accuracy: 0.4956\n",
      "Epoch 22/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4835\n",
      "Epoch 22: val_loss did not improve from 0.69296\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4833 - val_loss: 0.6932 - val_accuracy: 0.4956\n",
      "Epoch 23/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4940\n",
      "Epoch 23: val_loss did not improve from 0.69296\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4933 - val_loss: 0.6932 - val_accuracy: 0.4956\n",
      "Epoch 24/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.5011\n",
      "Epoch 24: val_loss did not improve from 0.69296\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4988 - val_loss: 0.6932 - val_accuracy: 0.4956\n",
      "Epoch 25/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4941\n",
      "Epoch 25: val_loss did not improve from 0.69296\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4927 - val_loss: 0.6932 - val_accuracy: 0.4956\n",
      "Epoch 26/60\n",
      "654/682 [===========================>..] - ETA: 0s - loss: 0.6932 - accuracy: 0.4873\n",
      "Epoch 26: val_loss did not improve from 0.69296\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4875 - val_loss: 0.6932 - val_accuracy: 0.5009\n",
      "Epoch 27/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4991\n",
      "Epoch 27: val_loss did not improve from 0.69296\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4999 - val_loss: 0.6932 - val_accuracy: 0.4956\n",
      "Epoch 28/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.5018\n",
      "Epoch 28: val_loss did not improve from 0.69296\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.5020 - val_loss: 0.6932 - val_accuracy: 0.4956\n",
      "Epoch 29/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4932\n",
      "Epoch 29: val_loss did not improve from 0.69296\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4939 - val_loss: 0.6932 - val_accuracy: 0.4956\n",
      "Epoch 30/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4973\n",
      "Epoch 30: val_loss did not improve from 0.69296\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.4973 - val_loss: 0.6932 - val_accuracy: 0.4956\n",
      "Epoch 30: early stopping\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.6930 - accuracy: 0.5044\n",
      "Processing feature: services\n",
      "data frame x has shape: (8516, 1)\n",
      "Epoch 1/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.6911 - accuracy: 0.5176\n",
      "Epoch 1: val_loss improved from inf to 0.69114, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6913 - accuracy: 0.5181 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 2/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.6911 - accuracy: 0.5202\n",
      "Epoch 2: val_loss improved from 0.69114 to 0.69112, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 3/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.6913 - accuracy: 0.5188\n",
      "Epoch 3: val_loss improved from 0.69112 to 0.69110, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 4/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.6911 - accuracy: 0.5173\n",
      "Epoch 4: val_loss improved from 0.69110 to 0.69110, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 5/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.6911 - accuracy: 0.5206\n",
      "Epoch 5: val_loss improved from 0.69110 to 0.69110, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 6/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5183\n",
      "Epoch 6: val_loss improved from 0.69110 to 0.69109, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 7/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.6911 - accuracy: 0.5187\n",
      "Epoch 7: val_loss improved from 0.69109 to 0.69109, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 8/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.6913 - accuracy: 0.5162\n",
      "Epoch 8: val_loss did not improve from 0.69109\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 9/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.6912 - accuracy: 0.5184\n",
      "Epoch 9: val_loss did not improve from 0.69109\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 10/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5184\n",
      "Epoch 10: val_loss did not improve from 0.69109\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 11/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5182\n",
      "Epoch 11: val_loss did not improve from 0.69109\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 12/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.6910 - accuracy: 0.5206\n",
      "Epoch 12: val_loss did not improve from 0.69109\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 13/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.6911 - accuracy: 0.5177\n",
      "Epoch 13: val_loss improved from 0.69109 to 0.69109, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 14/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5177\n",
      "Epoch 14: val_loss did not improve from 0.69109\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 15/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5186\n",
      "Epoch 15: val_loss did not improve from 0.69109\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 16/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.6912 - accuracy: 0.5184\n",
      "Epoch 16: val_loss improved from 0.69109 to 0.69109, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 17/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5191\n",
      "Epoch 17: val_loss did not improve from 0.69109\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 18/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5186\n",
      "Epoch 18: val_loss did not improve from 0.69109\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 19/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.6909 - accuracy: 0.5192\n",
      "Epoch 19: val_loss did not improve from 0.69109\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 20/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5181\n",
      "Epoch 20: val_loss did not improve from 0.69109\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 21/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.6911 - accuracy: 0.5193\n",
      "Epoch 21: val_loss did not improve from 0.69109\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 22/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.6913 - accuracy: 0.5175\n",
      "Epoch 22: val_loss did not improve from 0.69109\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 23/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5182\n",
      "Epoch 23: val_loss did not improve from 0.69109\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 24/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.6911 - accuracy: 0.5174\n",
      "Epoch 24: val_loss did not improve from 0.69109\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 25/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.6911 - accuracy: 0.5188\n",
      "Epoch 25: val_loss did not improve from 0.69109\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 26/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5176\n",
      "Epoch 26: val_loss did not improve from 0.69109\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 27/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5196\n",
      "Epoch 27: val_loss did not improve from 0.69109\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 28/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.6911 - accuracy: 0.5190\n",
      "Epoch 28: val_loss did not improve from 0.69109\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 29/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.6911 - accuracy: 0.5185\n",
      "Epoch 29: val_loss did not improve from 0.69109\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 30/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.6910 - accuracy: 0.5186\n",
      "Epoch 30: val_loss improved from 0.69109 to 0.69109, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 31/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.6911 - accuracy: 0.5186\n",
      "Epoch 31: val_loss did not improve from 0.69109\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 32/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.6913 - accuracy: 0.5179\n",
      "Epoch 32: val_loss did not improve from 0.69109\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 33/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5180\n",
      "Epoch 33: val_loss did not improve from 0.69109\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 34/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5180\n",
      "Epoch 34: val_loss did not improve from 0.69109\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 35/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.6911 - accuracy: 0.5183\n",
      "Epoch 35: val_loss did not improve from 0.69109\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 36/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5185\n",
      "Epoch 36: val_loss did not improve from 0.69109\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 37/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5182\n",
      "Epoch 37: val_loss did not improve from 0.69109\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 38/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.6912 - accuracy: 0.5190\n",
      "Epoch 38: val_loss did not improve from 0.69109\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 39/60\n",
      "653/682 [===========================>..] - ETA: 0s - loss: 0.6910 - accuracy: 0.5196\n",
      "Epoch 39: val_loss did not improve from 0.69109\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 40/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.6913 - accuracy: 0.5180\n",
      "Epoch 40: val_loss did not improve from 0.69109\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 41/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5182\n",
      "Epoch 41: val_loss did not improve from 0.69109\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 42/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.6913 - accuracy: 0.5171\n",
      "Epoch 42: val_loss did not improve from 0.69109\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 43/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.6914 - accuracy: 0.5166\n",
      "Epoch 43: val_loss improved from 0.69109 to 0.69109, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 44/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.6911 - accuracy: 0.5182\n",
      "Epoch 44: val_loss did not improve from 0.69109\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 45/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5177\n",
      "Epoch 45: val_loss did not improve from 0.69109\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 46/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5189\n",
      "Epoch 46: val_loss did not improve from 0.69109\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 47/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5186\n",
      "Epoch 47: val_loss did not improve from 0.69109\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 48/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5182\n",
      "Epoch 48: val_loss did not improve from 0.69109\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 49/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.6911 - accuracy: 0.5198\n",
      "Epoch 49: val_loss did not improve from 0.69109\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 50/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.6911 - accuracy: 0.5192\n",
      "Epoch 50: val_loss did not improve from 0.69109\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 51/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.6915 - accuracy: 0.5183\n",
      "Epoch 51: val_loss did not improve from 0.69109\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 52/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.6914 - accuracy: 0.5153\n",
      "Epoch 52: val_loss did not improve from 0.69109\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 53/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5179\n",
      "Epoch 53: val_loss did not improve from 0.69109\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 54/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5188\n",
      "Epoch 54: val_loss improved from 0.69109 to 0.69108, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 55/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5182\n",
      "Epoch 55: val_loss did not improve from 0.69108\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 56/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.6910 - accuracy: 0.5202\n",
      "Epoch 56: val_loss did not improve from 0.69108\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 57/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.6913 - accuracy: 0.5171\n",
      "Epoch 57: val_loss did not improve from 0.69108\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 58/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5182\n",
      "Epoch 58: val_loss did not improve from 0.69108\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 59/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.6913 - accuracy: 0.5179\n",
      "Epoch 59: val_loss did not improve from 0.69108\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "Epoch 60/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.6913 - accuracy: 0.5174\n",
      "Epoch 60: val_loss did not improve from 0.69108\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.6911 - accuracy: 0.5167\n",
      "Processing feature: student\n",
      "data frame x has shape: (8516, 1)\n",
      "Epoch 1/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.6893 - accuracy: 0.5096\n",
      "Epoch 1: val_loss improved from inf to 0.68697, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6893 - accuracy: 0.5098 - val_loss: 0.6870 - val_accuracy: 0.5238\n",
      "Epoch 2/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.6889 - accuracy: 0.5158\n",
      "Epoch 2: val_loss improved from 0.68697 to 0.68647, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6889 - accuracy: 0.5165 - val_loss: 0.6865 - val_accuracy: 0.5238\n",
      "Epoch 3/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.6887 - accuracy: 0.5130\n",
      "Epoch 3: val_loss improved from 0.68647 to 0.68610, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6887 - accuracy: 0.5128 - val_loss: 0.6861 - val_accuracy: 0.5238\n",
      "Epoch 4/60\n",
      "650/682 [===========================>..] - ETA: 0s - loss: 0.6888 - accuracy: 0.5158\n",
      "Epoch 4: val_loss improved from 0.68610 to 0.68582, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6886 - accuracy: 0.5165 - val_loss: 0.6858 - val_accuracy: 0.5238\n",
      "Epoch 5/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.6885 - accuracy: 0.5170\n",
      "Epoch 5: val_loss improved from 0.68582 to 0.68562, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6885 - accuracy: 0.5165 - val_loss: 0.6856 - val_accuracy: 0.5238\n",
      "Epoch 6/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.6885 - accuracy: 0.5161\n",
      "Epoch 6: val_loss improved from 0.68562 to 0.68544, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6884 - accuracy: 0.5165 - val_loss: 0.6854 - val_accuracy: 0.5238\n",
      "Epoch 7/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.6883 - accuracy: 0.5180\n",
      "Epoch 7: val_loss improved from 0.68544 to 0.68530, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6883 - accuracy: 0.5165 - val_loss: 0.6853 - val_accuracy: 0.5238\n",
      "Epoch 8/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.6883 - accuracy: 0.5153\n",
      "Epoch 8: val_loss improved from 0.68530 to 0.68520, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6883 - accuracy: 0.5165 - val_loss: 0.6852 - val_accuracy: 0.5238\n",
      "Epoch 9/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.6883 - accuracy: 0.5175\n",
      "Epoch 9: val_loss improved from 0.68520 to 0.68511, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6883 - accuracy: 0.5165 - val_loss: 0.6851 - val_accuracy: 0.5238\n",
      "Epoch 10/60\n",
      "654/682 [===========================>..] - ETA: 0s - loss: 0.6883 - accuracy: 0.5128\n",
      "Epoch 10: val_loss improved from 0.68511 to 0.68504, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6850 - val_accuracy: 0.5238\n",
      "Epoch 11/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.6882 - accuracy: 0.5164\n",
      "Epoch 11: val_loss improved from 0.68504 to 0.68499, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6850 - val_accuracy: 0.5238\n",
      "Epoch 12/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.6879 - accuracy: 0.5157\n",
      "Epoch 12: val_loss improved from 0.68499 to 0.68495, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6850 - val_accuracy: 0.5238\n",
      "Epoch 13/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.6882 - accuracy: 0.5165\n",
      "Epoch 13: val_loss improved from 0.68495 to 0.68490, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6849 - val_accuracy: 0.5238\n",
      "Epoch 14/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.6882 - accuracy: 0.5170\n",
      "Epoch 14: val_loss improved from 0.68490 to 0.68486, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6849 - val_accuracy: 0.5238\n",
      "Epoch 15/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.6883 - accuracy: 0.5162\n",
      "Epoch 15: val_loss improved from 0.68486 to 0.68486, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6849 - val_accuracy: 0.5238\n",
      "Epoch 16/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.6883 - accuracy: 0.5170\n",
      "Epoch 16: val_loss improved from 0.68486 to 0.68483, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6848 - val_accuracy: 0.5238\n",
      "Epoch 17/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.6883 - accuracy: 0.5161\n",
      "Epoch 17: val_loss improved from 0.68483 to 0.68480, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6848 - val_accuracy: 0.5238\n",
      "Epoch 18/60\n",
      "654/682 [===========================>..] - ETA: 0s - loss: 0.6886 - accuracy: 0.5156\n",
      "Epoch 18: val_loss improved from 0.68480 to 0.68480, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6848 - val_accuracy: 0.5238\n",
      "Epoch 19/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.6882 - accuracy: 0.5161\n",
      "Epoch 19: val_loss improved from 0.68480 to 0.68480, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6848 - val_accuracy: 0.5238\n",
      "Epoch 20/60\n",
      "653/682 [===========================>..] - ETA: 0s - loss: 0.6881 - accuracy: 0.5173\n",
      "Epoch 20: val_loss improved from 0.68480 to 0.68477, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6848 - val_accuracy: 0.5238\n",
      "Epoch 21/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.6883 - accuracy: 0.5176\n",
      "Epoch 21: val_loss improved from 0.68477 to 0.68476, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6848 - val_accuracy: 0.5238\n",
      "Epoch 22/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.6882 - accuracy: 0.5176\n",
      "Epoch 22: val_loss improved from 0.68476 to 0.68474, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238\n",
      "Epoch 23/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.6882 - accuracy: 0.5167\n",
      "Epoch 23: val_loss improved from 0.68474 to 0.68474, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238\n",
      "Epoch 24/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.6884 - accuracy: 0.5148\n",
      "Epoch 24: val_loss did not improve from 0.68474\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238\n",
      "Epoch 25/60\n",
      "652/682 [===========================>..] - ETA: 0s - loss: 0.6885 - accuracy: 0.5164\n",
      "Epoch 25: val_loss did not improve from 0.68474\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238\n",
      "Epoch 26/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.6880 - accuracy: 0.5175\n",
      "Epoch 26: val_loss did not improve from 0.68474\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238\n",
      "Epoch 27/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.6883 - accuracy: 0.5168\n",
      "Epoch 27: val_loss improved from 0.68474 to 0.68473, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238\n",
      "Epoch 28/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.6882 - accuracy: 0.5156\n",
      "Epoch 28: val_loss did not improve from 0.68473\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238\n",
      "Epoch 29/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.6882 - accuracy: 0.5158\n",
      "Epoch 29: val_loss did not improve from 0.68473\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238\n",
      "Epoch 30/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.6882 - accuracy: 0.5160\n",
      "Epoch 30: val_loss improved from 0.68473 to 0.68472, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238\n",
      "Epoch 31/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.6879 - accuracy: 0.5177\n",
      "Epoch 31: val_loss improved from 0.68472 to 0.68471, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238\n",
      "Epoch 32/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.6881 - accuracy: 0.5114\n",
      "Epoch 32: val_loss did not improve from 0.68471\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6881 - accuracy: 0.5114 - val_loss: 0.6847 - val_accuracy: 0.5238\n",
      "Epoch 33/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.6882 - accuracy: 0.5170\n",
      "Epoch 33: val_loss did not improve from 0.68471\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238\n",
      "Epoch 34/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.6882 - accuracy: 0.5171\n",
      "Epoch 34: val_loss did not improve from 0.68471\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238\n",
      "Epoch 35/60\n",
      "654/682 [===========================>..] - ETA: 0s - loss: 0.6885 - accuracy: 0.5135\n",
      "Epoch 35: val_loss did not improve from 0.68471\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238\n",
      "Epoch 36/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.6880 - accuracy: 0.5158\n",
      "Epoch 36: val_loss did not improve from 0.68471\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238\n",
      "Epoch 37/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.6882 - accuracy: 0.5160\n",
      "Epoch 37: val_loss did not improve from 0.68471\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6848 - val_accuracy: 0.5238\n",
      "Epoch 38/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.6883 - accuracy: 0.5153\n",
      "Epoch 38: val_loss did not improve from 0.68471\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238\n",
      "Epoch 39/60\n",
      "653/682 [===========================>..] - ETA: 0s - loss: 0.6885 - accuracy: 0.5167\n",
      "Epoch 39: val_loss did not improve from 0.68471\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238\n",
      "Epoch 40/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.6883 - accuracy: 0.5151\n",
      "Epoch 40: val_loss did not improve from 0.68471\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238\n",
      "Epoch 41/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.6882 - accuracy: 0.5176\n",
      "Epoch 41: val_loss did not improve from 0.68471\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238\n",
      "Epoch 42/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.6883 - accuracy: 0.5162\n",
      "Epoch 42: val_loss did not improve from 0.68471\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238\n",
      "Epoch 43/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.6883 - accuracy: 0.5171\n",
      "Epoch 43: val_loss did not improve from 0.68471\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238\n",
      "Epoch 44/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.6881 - accuracy: 0.5193\n",
      "Epoch 44: val_loss did not improve from 0.68471\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238\n",
      "Epoch 45/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.6882 - accuracy: 0.5156\n",
      "Epoch 45: val_loss did not improve from 0.68471\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238\n",
      "Epoch 46/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.6877 - accuracy: 0.5195\n",
      "Epoch 46: val_loss improved from 0.68471 to 0.68470, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238\n",
      "Epoch 47/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.6883 - accuracy: 0.5158\n",
      "Epoch 47: val_loss did not improve from 0.68470\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238\n",
      "Epoch 48/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.6884 - accuracy: 0.5169\n",
      "Epoch 48: val_loss did not improve from 0.68470\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238\n",
      "Epoch 49/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.6881 - accuracy: 0.5169\n",
      "Epoch 49: val_loss did not improve from 0.68470\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238\n",
      "Epoch 50/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.6881 - accuracy: 0.5150\n",
      "Epoch 50: val_loss did not improve from 0.68470\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238\n",
      "Epoch 51/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.6882 - accuracy: 0.5163\n",
      "Epoch 51: val_loss did not improve from 0.68470\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238\n",
      "Epoch 52/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.6880 - accuracy: 0.5160\n",
      "Epoch 52: val_loss did not improve from 0.68470\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238\n",
      "Epoch 53/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.6885 - accuracy: 0.5154\n",
      "Epoch 53: val_loss did not improve from 0.68470\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238\n",
      "Epoch 54/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.6879 - accuracy: 0.5168\n",
      "Epoch 54: val_loss did not improve from 0.68470\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238\n",
      "Epoch 55/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.6878 - accuracy: 0.5173\n",
      "Epoch 55: val_loss did not improve from 0.68470\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238\n",
      "Epoch 56/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.6880 - accuracy: 0.5158\n",
      "Epoch 56: val_loss did not improve from 0.68470\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238\n",
      "Epoch 57/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.6886 - accuracy: 0.5144\n",
      "Epoch 57: val_loss did not improve from 0.68470\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238\n",
      "Epoch 58/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.6881 - accuracy: 0.5161\n",
      "Epoch 58: val_loss improved from 0.68470 to 0.68470, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238\n",
      "Epoch 59/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.6882 - accuracy: 0.5168\n",
      "Epoch 59: val_loss improved from 0.68470 to 0.68470, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238\n",
      "Epoch 60/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.6885 - accuracy: 0.5145\n",
      "Epoch 60: val_loss did not improve from 0.68470\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.6847 - accuracy: 0.5238\n",
      "Processing feature: technician\n",
      "data frame x has shape: (8516, 1)\n",
      "Epoch 1/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.6991 - accuracy: 0.4949\n",
      "Epoch 1: val_loss improved from inf to 0.69570, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 3ms/step - loss: 0.6991 - accuracy: 0.4949 - val_loss: 0.6957 - val_accuracy: 0.5015\n",
      "Epoch 2/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.6960 - accuracy: 0.4939\n",
      "Epoch 2: val_loss improved from 0.69570 to 0.69416, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6960 - accuracy: 0.4942 - val_loss: 0.6942 - val_accuracy: 0.5015\n",
      "Epoch 3/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.6945 - accuracy: 0.4941\n",
      "Epoch 3: val_loss improved from 0.69416 to 0.69351, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6946 - accuracy: 0.4942 - val_loss: 0.6935 - val_accuracy: 0.5015\n",
      "Epoch 4/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.6938 - accuracy: 0.4962\n",
      "Epoch 4: val_loss improved from 0.69351 to 0.69326, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6939 - accuracy: 0.4942 - val_loss: 0.6933 - val_accuracy: 0.5015\n",
      "Epoch 5/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.6935 - accuracy: 0.4942\n",
      "Epoch 5: val_loss improved from 0.69326 to 0.69316, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6935 - accuracy: 0.4942 - val_loss: 0.6932 - val_accuracy: 0.5015\n",
      "Epoch 6/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.6933 - accuracy: 0.4955\n",
      "Epoch 6: val_loss improved from 0.69316 to 0.69315, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6933 - accuracy: 0.4960 - val_loss: 0.6931 - val_accuracy: 0.5015\n",
      "Epoch 7/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4888\n",
      "Epoch 7: val_loss did not improve from 0.69315\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4889 - val_loss: 0.6932 - val_accuracy: 0.5009\n",
      "Epoch 8/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4981\n",
      "Epoch 8: val_loss did not improve from 0.69315\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4970 - val_loss: 0.6932 - val_accuracy: 0.4985\n",
      "Epoch 9/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.5046\n",
      "Epoch 9: val_loss did not improve from 0.69315\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6932 - val_accuracy: 0.4985\n",
      "Epoch 10/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5073\n",
      "Epoch 10: val_loss did not improve from 0.69315\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5065 - val_loss: 0.6933 - val_accuracy: 0.5009\n",
      "Epoch 11/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.4939\n",
      "Epoch 11: val_loss did not improve from 0.69315\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4954 - val_loss: 0.6933 - val_accuracy: 0.4985\n",
      "Epoch 12/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5056\n",
      "Epoch 12: val_loss did not improve from 0.69315\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5058 - val_loss: 0.6933 - val_accuracy: 0.4985\n",
      "Epoch 13/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.6931 - accuracy: 0.5059\n",
      "Epoch 13: val_loss did not improve from 0.69315\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5059 - val_loss: 0.6933 - val_accuracy: 0.4985\n",
      "Epoch 14/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5063\n",
      "Epoch 14: val_loss did not improve from 0.69315\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6931 - accuracy: 0.5058 - val_loss: 0.6933 - val_accuracy: 0.4985\n",
      "Epoch 15/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4987\n",
      "Epoch 15: val_loss did not improve from 0.69315\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4985 - val_loss: 0.6933 - val_accuracy: 0.4985\n",
      "Epoch 16/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5028\n",
      "Epoch 16: val_loss did not improve from 0.69315\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5036 - val_loss: 0.6933 - val_accuracy: 0.4985\n",
      "Epoch 17/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5057\n",
      "Epoch 17: val_loss did not improve from 0.69315\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5058 - val_loss: 0.6933 - val_accuracy: 0.4985\n",
      "Epoch 18/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5058\n",
      "Epoch 18: val_loss did not improve from 0.69315\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5058 - val_loss: 0.6933 - val_accuracy: 0.4985\n",
      "Epoch 19/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5057\n",
      "Epoch 19: val_loss did not improve from 0.69315\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5058 - val_loss: 0.6933 - val_accuracy: 0.4985\n",
      "Epoch 20/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5061\n",
      "Epoch 20: val_loss did not improve from 0.69315\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5058 - val_loss: 0.6933 - val_accuracy: 0.4985\n",
      "Epoch 21/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5059\n",
      "Epoch 21: val_loss did not improve from 0.69315\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5058 - val_loss: 0.6933 - val_accuracy: 0.4985\n",
      "Epoch 22/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5064\n",
      "Epoch 22: val_loss did not improve from 0.69315\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5058 - val_loss: 0.6933 - val_accuracy: 0.4985\n",
      "Epoch 23/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.5059\n",
      "Epoch 23: val_loss did not improve from 0.69315\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5058 - val_loss: 0.6933 - val_accuracy: 0.4985\n",
      "Epoch 24/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5061\n",
      "Epoch 24: val_loss did not improve from 0.69315\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5058 - val_loss: 0.6933 - val_accuracy: 0.4985\n",
      "Epoch 25/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5000\n",
      "Epoch 25: val_loss did not improve from 0.69315\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4996 - val_loss: 0.6933 - val_accuracy: 0.4985\n",
      "Epoch 26/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5015\n",
      "Epoch 26: val_loss did not improve from 0.69315\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5012 - val_loss: 0.6933 - val_accuracy: 0.4985\n",
      "Epoch 26: early stopping\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5015\n",
      "Processing feature: unemployed\n",
      "data frame x has shape: (8516, 1)\n",
      "Epoch 1/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.6954 - accuracy: 0.4960\n",
      "Epoch 1: val_loss improved from inf to 0.69384, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6954 - accuracy: 0.4961 - val_loss: 0.6938 - val_accuracy: 0.4979\n",
      "Epoch 2/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.6946 - accuracy: 0.4964\n",
      "Epoch 2: val_loss improved from 0.69384 to 0.69356, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6946 - accuracy: 0.4955 - val_loss: 0.6936 - val_accuracy: 0.4979\n",
      "Epoch 3/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.6940 - accuracy: 0.4880\n",
      "Epoch 3: val_loss improved from 0.69356 to 0.69337, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6940 - accuracy: 0.4885 - val_loss: 0.6934 - val_accuracy: 0.4979\n",
      "Epoch 4/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.6937 - accuracy: 0.4938\n",
      "Epoch 4: val_loss improved from 0.69337 to 0.69322, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6937 - accuracy: 0.4946 - val_loss: 0.6932 - val_accuracy: 0.4979\n",
      "Epoch 5/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.6934 - accuracy: 0.4921\n",
      "Epoch 5: val_loss improved from 0.69322 to 0.69315, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6934 - accuracy: 0.4921 - val_loss: 0.6932 - val_accuracy: 0.5009\n",
      "Epoch 6/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.6932 - accuracy: 0.4968\n",
      "Epoch 6: val_loss improved from 0.69315 to 0.69312, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4963 - val_loss: 0.6931 - val_accuracy: 0.5021\n",
      "Epoch 7/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.6930 - accuracy: 0.4854\n",
      "Epoch 7: val_loss improved from 0.69312 to 0.69312, saving model to feature_best.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.4838 - val_loss: 0.6931 - val_accuracy: 0.4991\n",
      "Epoch 8/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.6929 - accuracy: 0.4945\n",
      "Epoch 8: val_loss did not improve from 0.69312\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6929 - accuracy: 0.4957 - val_loss: 0.6931 - val_accuracy: 0.5021\n",
      "Epoch 9/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.6929 - accuracy: 0.5014\n",
      "Epoch 9: val_loss did not improve from 0.69312\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6929 - accuracy: 0.5012 - val_loss: 0.6931 - val_accuracy: 0.5021\n",
      "Epoch 10/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.6928 - accuracy: 0.5033\n",
      "Epoch 10: val_loss did not improve from 0.69312\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6928 - accuracy: 0.5029 - val_loss: 0.6932 - val_accuracy: 0.5021\n",
      "Epoch 11/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.6928 - accuracy: 0.4976\n",
      "Epoch 11: val_loss did not improve from 0.69312\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6928 - accuracy: 0.4971 - val_loss: 0.6932 - val_accuracy: 0.4991\n",
      "Epoch 12/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.6928 - accuracy: 0.5002\n",
      "Epoch 12: val_loss did not improve from 0.69312\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6927 - accuracy: 0.5001 - val_loss: 0.6932 - val_accuracy: 0.5021\n",
      "Epoch 13/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.6927 - accuracy: 0.5031\n",
      "Epoch 13: val_loss did not improve from 0.69312\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6927 - accuracy: 0.5024 - val_loss: 0.6932 - val_accuracy: 0.4991\n",
      "Epoch 14/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.6927 - accuracy: 0.4999\n",
      "Epoch 14: val_loss did not improve from 0.69312\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6927 - accuracy: 0.4998 - val_loss: 0.6932 - val_accuracy: 0.5021\n",
      "Epoch 15/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.6926 - accuracy: 0.4986\n",
      "Epoch 15: val_loss did not improve from 0.69312\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6927 - accuracy: 0.5001 - val_loss: 0.6932 - val_accuracy: 0.5021\n",
      "Epoch 16/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.6927 - accuracy: 0.5053\n",
      "Epoch 16: val_loss did not improve from 0.69312\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6927 - accuracy: 0.5054 - val_loss: 0.6933 - val_accuracy: 0.5021\n",
      "Epoch 17/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.6927 - accuracy: 0.5058\n",
      "Epoch 17: val_loss did not improve from 0.69312\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6927 - accuracy: 0.5054 - val_loss: 0.6933 - val_accuracy: 0.5021\n",
      "Epoch 18/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.6927 - accuracy: 0.4972\n",
      "Epoch 18: val_loss did not improve from 0.69312\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6927 - accuracy: 0.4963 - val_loss: 0.6933 - val_accuracy: 0.5021\n",
      "Epoch 19/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.6927 - accuracy: 0.5008\n",
      "Epoch 19: val_loss did not improve from 0.69312\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6927 - accuracy: 0.4999 - val_loss: 0.6933 - val_accuracy: 0.5021\n",
      "Epoch 20/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.6927 - accuracy: 0.5021\n",
      "Epoch 20: val_loss did not improve from 0.69312\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6926 - accuracy: 0.5032 - val_loss: 0.6933 - val_accuracy: 0.5021\n",
      "Epoch 21/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.6927 - accuracy: 0.5015\n",
      "Epoch 21: val_loss did not improve from 0.69312\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6927 - accuracy: 0.5018 - val_loss: 0.6933 - val_accuracy: 0.5021\n",
      "Epoch 22/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.6927 - accuracy: 0.5051\n",
      "Epoch 22: val_loss did not improve from 0.69312\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6927 - accuracy: 0.5058 - val_loss: 0.6933 - val_accuracy: 0.5021\n",
      "Epoch 23/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.6927 - accuracy: 0.5052\n",
      "Epoch 23: val_loss did not improve from 0.69312\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6927 - accuracy: 0.5054 - val_loss: 0.6933 - val_accuracy: 0.5021\n",
      "Epoch 24/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.6926 - accuracy: 0.5027\n",
      "Epoch 24: val_loss did not improve from 0.69312\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6927 - accuracy: 0.5027 - val_loss: 0.6933 - val_accuracy: 0.5021\n",
      "Epoch 25/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.6927 - accuracy: 0.5053\n",
      "Epoch 25: val_loss did not improve from 0.69312\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6927 - accuracy: 0.5054 - val_loss: 0.6933 - val_accuracy: 0.5021\n",
      "Epoch 26/60\n",
      "649/682 [===========================>..] - ETA: 0s - loss: 0.6927 - accuracy: 0.4985\n",
      "Epoch 26: val_loss did not improve from 0.69312\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6927 - accuracy: 0.4992 - val_loss: 0.6933 - val_accuracy: 0.5021\n",
      "Epoch 27/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.6926 - accuracy: 0.4979\n",
      "Epoch 27: val_loss did not improve from 0.69312\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6927 - accuracy: 0.4970 - val_loss: 0.6933 - val_accuracy: 0.5021\n",
      "Epoch 27: early stopping\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.4991\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame(columns=['Feature', 'Validation_Accuracy'])\n",
    "X = banking_data.drop(['output'], axis = 'columns')\n",
    "\n",
    "for feature in X.columns:\n",
    "    print(f\"Processing feature: {feature}\")\n",
    "    feature_X = banking_data.loc[:, [feature]]\n",
    "    print(f\"data frame x has shape: {feature_X.shape}\")\n",
    "    \n",
    "    # Split x into validation (20%) and training (80%)\n",
    "    X_train = feature_X.iloc[index_20:, :]\n",
    "    X_test = feature_X.iloc[:index_20, :]\n",
    "    min = X_train.min(axis = 0) \n",
    "    max = X_train.max(axis = 0) \n",
    "    X_train = (X_train - min) / (max - min)\n",
    "    X_test = (X_test - min) / (max - min)\n",
    "\n",
    "    feature_regression_model = Sequential()\n",
    "    feature_regression_model.add(Dense(1, input_dim = len(X_train.columns), activation='sigmoid'))\n",
    "\n",
    "    feature_regression_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    callback_a = ModelCheckpoint(filepath = 'feature_best.hdf5', monitor='val_loss', save_best_only = True, save_weights_only = True, verbose = 1)\n",
    "    callback_b = EarlyStopping(monitor='val_loss', mode='min', patience=20, verbose=1)\n",
    "    history_regression_feature = feature_regression_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=60, batch_size=10, callbacks = [callback_a, callback_b])\n",
    "    feature_regression_model.load_weights('feature_best.hdf5')\n",
    "    regression_model_scores =  feature_regression_model.evaluate(X_test, y_test)\n",
    "    accuracy = (regression_model_scores[1]*100)\n",
    "    results.loc[len(results.index)] = [feature, accuracy] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26d9207c-db22-4345-9415-028986492da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of            Feature  Validation_Accuracy\n",
       "0              age            44.274810\n",
       "1        education            55.901349\n",
       "2          housing            50.734001\n",
       "3             loan            50.205517\n",
       "4         campaign            53.082794\n",
       "5            pdays            59.072226\n",
       "6         previous            59.718144\n",
       "7         poutcome            58.719903\n",
       "8     emp.var.rate            71.579564\n",
       "9   cons.price.idx            57.839108\n",
       "10   cons.conf.idx            54.198474\n",
       "11       euribor3m            71.579564\n",
       "12        divorced            50.675279\n",
       "13         married            53.200233\n",
       "14          single            53.963596\n",
       "15          admin.            53.258955\n",
       "16     blue-collar            55.314153\n",
       "17    entrepreneur            50.381678\n",
       "18       housemaid            49.970639\n",
       "19      management            50.088078\n",
       "20         retired            51.790959\n",
       "21   self-employed            50.440401\n",
       "22        services            51.673520\n",
       "23         student            52.378154\n",
       "24      technician            50.146800\n",
       "25      unemployed            49.911919>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " results.head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1546b498-4592-40ee-ae66-ea81778c4298",
   "metadata": {},
   "source": [
    "#### 4.3 Graph the Accuracy of the Model for each Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92d6e224-d9a4-4057-a33a-3c5abf9a5a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the accuracy by feature bar graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(results['Feature'], results['Validation_Accuracy'])\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Validation Accuracy (%)')\n",
    "plt.title('Validation Accuracy for Each Type')\n",
    "plt.ylim(0, 100)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753b8c67-d576-425c-9b8b-a78f503e1058",
   "metadata": {},
   "source": [
    "#### 4.4 Sort the Features by Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "211f4f9b-be7a-492b-8fa7-dce347109fd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = results.sort_values(by='Validation_Accuracy')\n",
    "\n",
    "# plot the sorted result dataframe. \n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(results['Feature'], results['Validation_Accuracy'])\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Validation Accuracy (%)')\n",
    "plt.title('Validation Accuracy for Each Type')\n",
    "plt.ylim(0, 100)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332bb54d-05c6-4943-a7f8-97c60cd80e57",
   "metadata": {},
   "source": [
    "#### 4.5 Train New Models and Dropping Features to Determine which Features are Best\n",
    "To determine if our model would have performed better without specific features, I will build another 26 models. In each model I will remove a feature that I found earlier had low validation accuracy and led to poor predictions to see if I had removed specific features if the model would have performed better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755b0166-a787-4995-a080-ddf09045a04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drop Feature age\n",
      "data frame x has shape: (8516, 25)\n",
      "['education', 'housing', 'loan', 'campaign', 'pdays', 'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'divorced', 'married', 'single', 'admin.', 'blue-collar', 'entrepreneur', 'housemaid', 'management', 'retired', 'self-employed', 'services', 'student', 'technician', 'unemployed']\n",
      "Epoch 1/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.6683 - accuracy: 0.5983\n",
      "Epoch 1: val_loss improved from inf to 0.63109, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 3ms/step - loss: 0.6672 - accuracy: 0.6002 - val_loss: 0.6311 - val_accuracy: 0.6788\n",
      "Epoch 2/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.6162 - accuracy: 0.6894\n",
      "Epoch 2: val_loss improved from 0.63109 to 0.59908, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6164 - accuracy: 0.6890 - val_loss: 0.5991 - val_accuracy: 0.7170\n",
      "Epoch 3/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.5948 - accuracy: 0.7057\n",
      "Epoch 3: val_loss improved from 0.59908 to 0.58444, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5947 - accuracy: 0.7059 - val_loss: 0.5844 - val_accuracy: 0.7246\n",
      "Epoch 4/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.5848 - accuracy: 0.7078\n",
      "Epoch 4: val_loss improved from 0.58444 to 0.57717, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5842 - accuracy: 0.7085 - val_loss: 0.5772 - val_accuracy: 0.7264\n",
      "Epoch 5/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.5788 - accuracy: 0.7103\n",
      "Epoch 5: val_loss improved from 0.57717 to 0.57330, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5784 - accuracy: 0.7106 - val_loss: 0.5733 - val_accuracy: 0.7258\n",
      "Epoch 6/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.5743 - accuracy: 0.7133\n",
      "Epoch 6: val_loss improved from 0.57330 to 0.57123, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5751 - accuracy: 0.7119 - val_loss: 0.5712 - val_accuracy: 0.7187\n",
      "Epoch 7/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.5720 - accuracy: 0.7121\n",
      "Epoch 7: val_loss improved from 0.57123 to 0.57002, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5730 - accuracy: 0.7116 - val_loss: 0.5700 - val_accuracy: 0.7252\n",
      "Epoch 8/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.5720 - accuracy: 0.7126\n",
      "Epoch 8: val_loss improved from 0.57002 to 0.56907, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5717 - accuracy: 0.7125 - val_loss: 0.5691 - val_accuracy: 0.7176\n",
      "Epoch 9/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.5706 - accuracy: 0.7120\n",
      "Epoch 9: val_loss improved from 0.56907 to 0.56867, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5706 - accuracy: 0.7120 - val_loss: 0.5687 - val_accuracy: 0.7158\n",
      "Epoch 10/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.5705 - accuracy: 0.7098\n",
      "Epoch 10: val_loss improved from 0.56867 to 0.56761, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5698 - accuracy: 0.7104 - val_loss: 0.5676 - val_accuracy: 0.7234\n",
      "Epoch 11/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.5679 - accuracy: 0.7131\n",
      "Epoch 11: val_loss improved from 0.56761 to 0.56746, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5691 - accuracy: 0.7117 - val_loss: 0.5675 - val_accuracy: 0.7217\n",
      "Epoch 12/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.5685 - accuracy: 0.7128\n",
      "Epoch 12: val_loss improved from 0.56746 to 0.56700, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5685 - accuracy: 0.7130 - val_loss: 0.5670 - val_accuracy: 0.7205\n",
      "Epoch 13/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.5681 - accuracy: 0.7117\n",
      "Epoch 13: val_loss improved from 0.56700 to 0.56664, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5680 - accuracy: 0.7119 - val_loss: 0.5666 - val_accuracy: 0.7223\n",
      "Epoch 14/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.5674 - accuracy: 0.7126\n",
      "Epoch 14: val_loss improved from 0.56664 to 0.56619, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5676 - accuracy: 0.7123 - val_loss: 0.5662 - val_accuracy: 0.7234\n",
      "Epoch 15/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.5665 - accuracy: 0.7121\n",
      "Epoch 15: val_loss improved from 0.56619 to 0.56604, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5672 - accuracy: 0.7123 - val_loss: 0.5660 - val_accuracy: 0.7217\n",
      "Epoch 16/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.5669 - accuracy: 0.7141\n",
      "Epoch 16: val_loss improved from 0.56604 to 0.56579, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5669 - accuracy: 0.7141 - val_loss: 0.5658 - val_accuracy: 0.7217\n",
      "Epoch 17/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.5666 - accuracy: 0.7138\n",
      "Epoch 17: val_loss improved from 0.56579 to 0.56539, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5664 - accuracy: 0.7141 - val_loss: 0.5654 - val_accuracy: 0.7211\n",
      "Epoch 18/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.5651 - accuracy: 0.7150\n",
      "Epoch 18: val_loss improved from 0.56539 to 0.56504, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5662 - accuracy: 0.7141 - val_loss: 0.5650 - val_accuracy: 0.7211\n",
      "Epoch 19/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.5664 - accuracy: 0.7132\n",
      "Epoch 19: val_loss improved from 0.56504 to 0.56497, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5658 - accuracy: 0.7138 - val_loss: 0.5650 - val_accuracy: 0.7205\n",
      "Epoch 20/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.5649 - accuracy: 0.7170\n",
      "Epoch 20: val_loss did not improve from 0.56497\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5652 - accuracy: 0.7172 - val_loss: 0.5650 - val_accuracy: 0.7199\n",
      "Epoch 21/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.5654 - accuracy: 0.7127\n",
      "Epoch 21: val_loss improved from 0.56497 to 0.56478, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5653 - accuracy: 0.7129 - val_loss: 0.5648 - val_accuracy: 0.7234\n",
      "Epoch 22/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.5650 - accuracy: 0.7160\n",
      "Epoch 22: val_loss improved from 0.56478 to 0.56418, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5650 - accuracy: 0.7160 - val_loss: 0.5642 - val_accuracy: 0.7217\n",
      "Epoch 23/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.5644 - accuracy: 0.7165\n",
      "Epoch 23: val_loss did not improve from 0.56418\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5648 - accuracy: 0.7163 - val_loss: 0.5643 - val_accuracy: 0.7217\n",
      "Epoch 24/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.5646 - accuracy: 0.7175\n",
      "Epoch 24: val_loss improved from 0.56418 to 0.56414, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5645 - accuracy: 0.7175 - val_loss: 0.5641 - val_accuracy: 0.7217\n",
      "Epoch 25/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.5643 - accuracy: 0.7168\n",
      "Epoch 25: val_loss improved from 0.56414 to 0.56378, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5644 - accuracy: 0.7166 - val_loss: 0.5638 - val_accuracy: 0.7228\n",
      "Epoch 26/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.5642 - accuracy: 0.7166\n",
      "Epoch 26: val_loss improved from 0.56378 to 0.56359, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5643 - accuracy: 0.7167 - val_loss: 0.5636 - val_accuracy: 0.7228\n",
      "Epoch 27/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.5639 - accuracy: 0.7175\n",
      "Epoch 27: val_loss improved from 0.56359 to 0.56340, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5641 - accuracy: 0.7177 - val_loss: 0.5634 - val_accuracy: 0.7205\n",
      "Epoch 28/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.5634 - accuracy: 0.7185\n",
      "Epoch 28: val_loss improved from 0.56340 to 0.56332, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5637 - accuracy: 0.7186 - val_loss: 0.5633 - val_accuracy: 0.7228\n",
      "Epoch 29/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.5638 - accuracy: 0.7175\n",
      "Epoch 29: val_loss improved from 0.56332 to 0.56319, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5637 - accuracy: 0.7179 - val_loss: 0.5632 - val_accuracy: 0.7228\n",
      "Epoch 30/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.5645 - accuracy: 0.7185\n",
      "Epoch 30: val_loss improved from 0.56319 to 0.56311, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5633 - accuracy: 0.7194 - val_loss: 0.5631 - val_accuracy: 0.7228\n",
      "Epoch 31/60\n",
      "652/682 [===========================>..] - ETA: 0s - loss: 0.5627 - accuracy: 0.7179\n",
      "Epoch 31: val_loss did not improve from 0.56311\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5635 - accuracy: 0.7172 - val_loss: 0.5633 - val_accuracy: 0.7211\n",
      "Epoch 32/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.5632 - accuracy: 0.7202\n",
      "Epoch 32: val_loss improved from 0.56311 to 0.56307, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5632 - accuracy: 0.7202 - val_loss: 0.5631 - val_accuracy: 0.7228\n",
      "Epoch 33/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.5630 - accuracy: 0.7185\n",
      "Epoch 33: val_loss improved from 0.56307 to 0.56302, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5630 - accuracy: 0.7191 - val_loss: 0.5630 - val_accuracy: 0.7228\n",
      "Epoch 34/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.5628 - accuracy: 0.7194\n",
      "Epoch 34: val_loss did not improve from 0.56302\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5630 - accuracy: 0.7191 - val_loss: 0.5631 - val_accuracy: 0.7228\n",
      "Epoch 35/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.5628 - accuracy: 0.7195\n",
      "Epoch 35: val_loss improved from 0.56302 to 0.56286, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5629 - accuracy: 0.7191 - val_loss: 0.5629 - val_accuracy: 0.7234\n",
      "Epoch 36/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.5630 - accuracy: 0.7184\n",
      "Epoch 36: val_loss improved from 0.56286 to 0.56254, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5628 - accuracy: 0.7185 - val_loss: 0.5625 - val_accuracy: 0.7199\n",
      "Epoch 37/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.5621 - accuracy: 0.7216\n",
      "Epoch 37: val_loss improved from 0.56254 to 0.56251, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5629 - accuracy: 0.7208 - val_loss: 0.5625 - val_accuracy: 0.7217\n",
      "Epoch 38/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.5631 - accuracy: 0.7194\n",
      "Epoch 38: val_loss improved from 0.56251 to 0.56238, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5628 - accuracy: 0.7197 - val_loss: 0.5624 - val_accuracy: 0.7211\n",
      "Epoch 39/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.5620 - accuracy: 0.7180\n",
      "Epoch 39: val_loss did not improve from 0.56238\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5623 - accuracy: 0.7179 - val_loss: 0.5628 - val_accuracy: 0.7187\n",
      "Epoch 40/60\n",
      "651/682 [===========================>..] - ETA: 0s - loss: 0.5635 - accuracy: 0.7194\n",
      "Epoch 40: val_loss did not improve from 0.56238\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5624 - accuracy: 0.7201 - val_loss: 0.5628 - val_accuracy: 0.7211\n",
      "Epoch 41/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.5631 - accuracy: 0.7191\n",
      "Epoch 41: val_loss improved from 0.56238 to 0.56223, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5626 - accuracy: 0.7191 - val_loss: 0.5622 - val_accuracy: 0.7217\n",
      "Epoch 42/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.5622 - accuracy: 0.7201\n",
      "Epoch 42: val_loss improved from 0.56223 to 0.56215, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5624 - accuracy: 0.7199 - val_loss: 0.5622 - val_accuracy: 0.7211\n",
      "Epoch 43/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.5624 - accuracy: 0.7206\n",
      "Epoch 43: val_loss improved from 0.56215 to 0.56197, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5624 - accuracy: 0.7204 - val_loss: 0.5620 - val_accuracy: 0.7211\n",
      "Epoch 44/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.5631 - accuracy: 0.7207\n",
      "Epoch 44: val_loss did not improve from 0.56197\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5623 - accuracy: 0.7210 - val_loss: 0.5621 - val_accuracy: 0.7199\n",
      "Epoch 45/60\n",
      "653/682 [===========================>..] - ETA: 0s - loss: 0.5631 - accuracy: 0.7194\n",
      "Epoch 45: val_loss did not improve from 0.56197\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5620 - accuracy: 0.7210 - val_loss: 0.5631 - val_accuracy: 0.7205\n",
      "Epoch 46/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.5623 - accuracy: 0.7215\n",
      "Epoch 46: val_loss did not improve from 0.56197\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5623 - accuracy: 0.7216 - val_loss: 0.5621 - val_accuracy: 0.7193\n",
      "Epoch 47/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.5615 - accuracy: 0.7200\n",
      "Epoch 47: val_loss did not improve from 0.56197\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5619 - accuracy: 0.7198 - val_loss: 0.5621 - val_accuracy: 0.7205\n",
      "Epoch 48/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.5630 - accuracy: 0.7196\n",
      "Epoch 48: val_loss improved from 0.56197 to 0.56195, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5623 - accuracy: 0.7205 - val_loss: 0.5620 - val_accuracy: 0.7211\n",
      "Epoch 49/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.5609 - accuracy: 0.7220\n",
      "Epoch 49: val_loss improved from 0.56195 to 0.56180, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5621 - accuracy: 0.7207 - val_loss: 0.5618 - val_accuracy: 0.7211\n",
      "Epoch 50/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.5621 - accuracy: 0.7215\n",
      "Epoch 50: val_loss did not improve from 0.56180\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5621 - accuracy: 0.7217 - val_loss: 0.5618 - val_accuracy: 0.7193\n",
      "Epoch 51/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.5631 - accuracy: 0.7210\n",
      "Epoch 51: val_loss improved from 0.56180 to 0.56172, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5621 - accuracy: 0.7211 - val_loss: 0.5617 - val_accuracy: 0.7217\n",
      "Epoch 52/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.5619 - accuracy: 0.7206\n",
      "Epoch 52: val_loss did not improve from 0.56172\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5618 - accuracy: 0.7210 - val_loss: 0.5618 - val_accuracy: 0.7181\n",
      "Epoch 53/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.5620 - accuracy: 0.7216\n",
      "Epoch 53: val_loss did not improve from 0.56172\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5619 - accuracy: 0.7214 - val_loss: 0.5619 - val_accuracy: 0.7181\n",
      "Epoch 54/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.5620 - accuracy: 0.7224\n",
      "Epoch 54: val_loss improved from 0.56172 to 0.56161, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5620 - accuracy: 0.7221 - val_loss: 0.5616 - val_accuracy: 0.7223\n",
      "Epoch 55/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.5616 - accuracy: 0.7215\n",
      "Epoch 55: val_loss did not improve from 0.56161\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5619 - accuracy: 0.7213 - val_loss: 0.5618 - val_accuracy: 0.7181\n",
      "Epoch 56/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.5612 - accuracy: 0.7223\n",
      "Epoch 56: val_loss improved from 0.56161 to 0.56154, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5619 - accuracy: 0.7217 - val_loss: 0.5615 - val_accuracy: 0.7217\n",
      "Epoch 57/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.5621 - accuracy: 0.7213\n",
      "Epoch 57: val_loss did not improve from 0.56154\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5618 - accuracy: 0.7210 - val_loss: 0.5618 - val_accuracy: 0.7181\n",
      "Epoch 58/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.5618 - accuracy: 0.7231\n",
      "Epoch 58: val_loss did not improve from 0.56154\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5618 - accuracy: 0.7230 - val_loss: 0.5616 - val_accuracy: 0.7199\n",
      "Epoch 59/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.5611 - accuracy: 0.7215\n",
      "Epoch 59: val_loss improved from 0.56154 to 0.56150, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5616 - accuracy: 0.7211 - val_loss: 0.5615 - val_accuracy: 0.7211\n",
      "Epoch 60/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.5618 - accuracy: 0.7213\n",
      "Epoch 60: val_loss did not improve from 0.56150\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7213 - val_loss: 0.5618 - val_accuracy: 0.7193\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.5615 - accuracy: 0.7211\n",
      "Drop Feature unemployed\n",
      "data frame x has shape: (8516, 24)\n",
      "['education', 'housing', 'loan', 'campaign', 'pdays', 'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'divorced', 'married', 'single', 'admin.', 'blue-collar', 'entrepreneur', 'housemaid', 'management', 'retired', 'self-employed', 'services', 'student', 'technician']\n",
      "Epoch 1/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.6516 - accuracy: 0.6304\n",
      "Epoch 1: val_loss improved from inf to 0.62058, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6511 - accuracy: 0.6313 - val_loss: 0.6206 - val_accuracy: 0.6635\n",
      "Epoch 2/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.6053 - accuracy: 0.6971\n",
      "Epoch 2: val_loss improved from 0.62058 to 0.59078, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6056 - accuracy: 0.6972 - val_loss: 0.5908 - val_accuracy: 0.7146\n",
      "Epoch 3/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.5864 - accuracy: 0.7159\n",
      "Epoch 3: val_loss improved from 0.59078 to 0.57843, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5863 - accuracy: 0.7150 - val_loss: 0.5784 - val_accuracy: 0.7211\n",
      "Epoch 4/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.5785 - accuracy: 0.7171\n",
      "Epoch 4: val_loss improved from 0.57843 to 0.57286, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5775 - accuracy: 0.7180 - val_loss: 0.5729 - val_accuracy: 0.7252\n",
      "Epoch 5/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.5724 - accuracy: 0.7174\n",
      "Epoch 5: val_loss improved from 0.57286 to 0.57008, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5730 - accuracy: 0.7163 - val_loss: 0.5701 - val_accuracy: 0.7170\n",
      "Epoch 6/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.5709 - accuracy: 0.7167\n",
      "Epoch 6: val_loss improved from 0.57008 to 0.56827, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5708 - accuracy: 0.7170 - val_loss: 0.5683 - val_accuracy: 0.7205\n",
      "Epoch 7/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.5687 - accuracy: 0.7186\n",
      "Epoch 7: val_loss improved from 0.56827 to 0.56724, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5692 - accuracy: 0.7182 - val_loss: 0.5672 - val_accuracy: 0.7176\n",
      "Epoch 8/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.5687 - accuracy: 0.7158\n",
      "Epoch 8: val_loss improved from 0.56724 to 0.56664, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5682 - accuracy: 0.7160 - val_loss: 0.5666 - val_accuracy: 0.7187\n",
      "Epoch 9/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.5672 - accuracy: 0.7169\n",
      "Epoch 9: val_loss did not improve from 0.56664\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5674 - accuracy: 0.7161 - val_loss: 0.5668 - val_accuracy: 0.7234\n",
      "Epoch 10/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.5663 - accuracy: 0.7175\n",
      "Epoch 10: val_loss improved from 0.56664 to 0.56609, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5670 - accuracy: 0.7169 - val_loss: 0.5661 - val_accuracy: 0.7146\n",
      "Epoch 11/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.5668 - accuracy: 0.7139\n",
      "Epoch 11: val_loss improved from 0.56609 to 0.56553, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5665 - accuracy: 0.7153 - val_loss: 0.5655 - val_accuracy: 0.7181\n",
      "Epoch 12/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.5662 - accuracy: 0.7151\n",
      "Epoch 12: val_loss did not improve from 0.56553\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5659 - accuracy: 0.7155 - val_loss: 0.5655 - val_accuracy: 0.7152\n",
      "Epoch 13/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.5658 - accuracy: 0.7159\n",
      "Epoch 13: val_loss improved from 0.56553 to 0.56482, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5658 - accuracy: 0.7155 - val_loss: 0.5648 - val_accuracy: 0.7217\n",
      "Epoch 14/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.5651 - accuracy: 0.7149\n",
      "Epoch 14: val_loss did not improve from 0.56482\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5651 - accuracy: 0.7148 - val_loss: 0.5660 - val_accuracy: 0.7199\n",
      "Epoch 15/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.5661 - accuracy: 0.7164\n",
      "Epoch 15: val_loss improved from 0.56482 to 0.56452, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5652 - accuracy: 0.7173 - val_loss: 0.5645 - val_accuracy: 0.7199\n",
      "Epoch 16/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.5641 - accuracy: 0.7162\n",
      "Epoch 16: val_loss improved from 0.56452 to 0.56417, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5650 - accuracy: 0.7157 - val_loss: 0.5642 - val_accuracy: 0.7223\n",
      "Epoch 17/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.5654 - accuracy: 0.7169\n",
      "Epoch 17: val_loss improved from 0.56417 to 0.56396, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5647 - accuracy: 0.7176 - val_loss: 0.5640 - val_accuracy: 0.7228\n",
      "Epoch 18/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.5649 - accuracy: 0.7170\n",
      "Epoch 18: val_loss did not improve from 0.56396\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5644 - accuracy: 0.7167 - val_loss: 0.5640 - val_accuracy: 0.7205\n",
      "Epoch 19/60\n",
      "654/682 [===========================>..] - ETA: 0s - loss: 0.5651 - accuracy: 0.7171\n",
      "Epoch 19: val_loss improved from 0.56396 to 0.56366, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5642 - accuracy: 0.7169 - val_loss: 0.5637 - val_accuracy: 0.7223\n",
      "Epoch 20/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.5644 - accuracy: 0.7175\n",
      "Epoch 20: val_loss improved from 0.56366 to 0.56353, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5641 - accuracy: 0.7176 - val_loss: 0.5635 - val_accuracy: 0.7205\n",
      "Epoch 21/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.5633 - accuracy: 0.7177\n",
      "Epoch 21: val_loss did not improve from 0.56353\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5636 - accuracy: 0.7175 - val_loss: 0.5637 - val_accuracy: 0.7205\n",
      "Epoch 22/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.5639 - accuracy: 0.7185\n",
      "Epoch 22: val_loss improved from 0.56353 to 0.56329, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5638 - accuracy: 0.7186 - val_loss: 0.5633 - val_accuracy: 0.7211\n",
      "Epoch 23/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.5644 - accuracy: 0.7173\n",
      "Epoch 23: val_loss improved from 0.56329 to 0.56324, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5636 - accuracy: 0.7180 - val_loss: 0.5632 - val_accuracy: 0.7205\n",
      "Epoch 24/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.5633 - accuracy: 0.7201\n",
      "Epoch 24: val_loss improved from 0.56324 to 0.56310, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5633 - accuracy: 0.7197 - val_loss: 0.5631 - val_accuracy: 0.7193\n",
      "Epoch 25/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.5632 - accuracy: 0.7190\n",
      "Epoch 25: val_loss improved from 0.56310 to 0.56291, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5633 - accuracy: 0.7191 - val_loss: 0.5629 - val_accuracy: 0.7205\n",
      "Epoch 26/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.5640 - accuracy: 0.7196\n",
      "Epoch 26: val_loss improved from 0.56291 to 0.56273, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5632 - accuracy: 0.7207 - val_loss: 0.5627 - val_accuracy: 0.7217\n",
      "Epoch 27/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.5627 - accuracy: 0.7208\n",
      "Epoch 27: val_loss did not improve from 0.56273\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5629 - accuracy: 0.7205 - val_loss: 0.5629 - val_accuracy: 0.7228\n",
      "Epoch 28/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.5622 - accuracy: 0.7196\n",
      "Epoch 28: val_loss improved from 0.56273 to 0.56263, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5631 - accuracy: 0.7191 - val_loss: 0.5626 - val_accuracy: 0.7217\n",
      "Epoch 29/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.5629 - accuracy: 0.7200\n",
      "Epoch 29: val_loss did not improve from 0.56263\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5628 - accuracy: 0.7201 - val_loss: 0.5629 - val_accuracy: 0.7217\n",
      "Epoch 30/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.5618 - accuracy: 0.7212\n",
      "Epoch 30: val_loss improved from 0.56263 to 0.56241, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5627 - accuracy: 0.7204 - val_loss: 0.5624 - val_accuracy: 0.7199\n",
      "Epoch 31/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.5625 - accuracy: 0.7182\n",
      "Epoch 31: val_loss did not improve from 0.56241\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5625 - accuracy: 0.7182 - val_loss: 0.5630 - val_accuracy: 0.7211\n",
      "Epoch 32/60\n",
      "649/682 [===========================>..] - ETA: 0s - loss: 0.5639 - accuracy: 0.7168\n",
      "Epoch 32: val_loss did not improve from 0.56241\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5624 - accuracy: 0.7191 - val_loss: 0.5628 - val_accuracy: 0.7199\n",
      "Epoch 33/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.5623 - accuracy: 0.7205\n",
      "Epoch 33: val_loss improved from 0.56241 to 0.56214, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5626 - accuracy: 0.7201 - val_loss: 0.5621 - val_accuracy: 0.7223\n",
      "Epoch 34/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.5622 - accuracy: 0.7196\n",
      "Epoch 34: val_loss improved from 0.56214 to 0.56193, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5625 - accuracy: 0.7199 - val_loss: 0.5619 - val_accuracy: 0.7211\n",
      "Epoch 35/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.5623 - accuracy: 0.7195\n",
      "Epoch 35: val_loss did not improve from 0.56193\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5623 - accuracy: 0.7195 - val_loss: 0.5626 - val_accuracy: 0.7181\n",
      "Epoch 36/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.5639 - accuracy: 0.7185\n",
      "Epoch 36: val_loss improved from 0.56193 to 0.56179, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5625 - accuracy: 0.7197 - val_loss: 0.5618 - val_accuracy: 0.7187\n",
      "Epoch 37/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.5635 - accuracy: 0.7195\n",
      "Epoch 37: val_loss did not improve from 0.56179\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5623 - accuracy: 0.7199 - val_loss: 0.5619 - val_accuracy: 0.7193\n",
      "Epoch 38/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.5621 - accuracy: 0.7201\n",
      "Epoch 38: val_loss did not improve from 0.56179\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5621 - accuracy: 0.7204 - val_loss: 0.5618 - val_accuracy: 0.7211\n",
      "Epoch 39/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.5608 - accuracy: 0.7222\n",
      "Epoch 39: val_loss improved from 0.56179 to 0.56177, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5622 - accuracy: 0.7210 - val_loss: 0.5618 - val_accuracy: 0.7199\n",
      "Epoch 40/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.5612 - accuracy: 0.7222\n",
      "Epoch 40: val_loss did not improve from 0.56177\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5622 - accuracy: 0.7213 - val_loss: 0.5618 - val_accuracy: 0.7211\n",
      "Epoch 41/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.5629 - accuracy: 0.7202\n",
      "Epoch 41: val_loss did not improve from 0.56177\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5620 - accuracy: 0.7202 - val_loss: 0.5619 - val_accuracy: 0.7199\n",
      "Epoch 42/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.5622 - accuracy: 0.7216\n",
      "Epoch 42: val_loss did not improve from 0.56177\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5620 - accuracy: 0.7217 - val_loss: 0.5620 - val_accuracy: 0.7199\n",
      "Epoch 43/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.5617 - accuracy: 0.7207\n",
      "Epoch 43: val_loss improved from 0.56177 to 0.56155, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5620 - accuracy: 0.7204 - val_loss: 0.5615 - val_accuracy: 0.7205\n",
      "Epoch 44/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.5613 - accuracy: 0.7216\n",
      "Epoch 44: val_loss did not improve from 0.56155\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5619 - accuracy: 0.7213 - val_loss: 0.5616 - val_accuracy: 0.7211\n",
      "Epoch 45/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.5622 - accuracy: 0.7205\n",
      "Epoch 45: val_loss did not improve from 0.56155\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5618 - accuracy: 0.7205 - val_loss: 0.5616 - val_accuracy: 0.7193\n",
      "Epoch 46/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.5605 - accuracy: 0.7208\n",
      "Epoch 46: val_loss did not improve from 0.56155\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5619 - accuracy: 0.7192 - val_loss: 0.5617 - val_accuracy: 0.7199\n",
      "Epoch 47/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.5615 - accuracy: 0.7198\n",
      "Epoch 47: val_loss improved from 0.56155 to 0.56150, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5618 - accuracy: 0.7197 - val_loss: 0.5615 - val_accuracy: 0.7223\n",
      "Epoch 48/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.5619 - accuracy: 0.7225\n",
      "Epoch 48: val_loss improved from 0.56150 to 0.56143, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5618 - accuracy: 0.7221 - val_loss: 0.5614 - val_accuracy: 0.7217\n",
      "Epoch 49/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.5627 - accuracy: 0.7184\n",
      "Epoch 49: val_loss did not improve from 0.56143\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7194 - val_loss: 0.5615 - val_accuracy: 0.7205\n",
      "Epoch 50/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.5601 - accuracy: 0.7231\n",
      "Epoch 50: val_loss improved from 0.56143 to 0.56142, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5617 - accuracy: 0.7207 - val_loss: 0.5614 - val_accuracy: 0.7199\n",
      "Epoch 51/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.5618 - accuracy: 0.7207\n",
      "Epoch 51: val_loss did not improve from 0.56142\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7205 - val_loss: 0.5615 - val_accuracy: 0.7199\n",
      "Epoch 52/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.5617 - accuracy: 0.7220\n",
      "Epoch 52: val_loss improved from 0.56142 to 0.56135, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7220 - val_loss: 0.5613 - val_accuracy: 0.7217\n",
      "Epoch 53/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.5612 - accuracy: 0.7226\n",
      "Epoch 53: val_loss did not improve from 0.56135\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5616 - accuracy: 0.7220 - val_loss: 0.5616 - val_accuracy: 0.7211\n",
      "Epoch 54/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.5609 - accuracy: 0.7225\n",
      "Epoch 54: val_loss did not improve from 0.56135\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7214 - val_loss: 0.5615 - val_accuracy: 0.7187\n",
      "Epoch 55/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.5614 - accuracy: 0.7215\n",
      "Epoch 55: val_loss did not improve from 0.56135\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5616 - accuracy: 0.7210 - val_loss: 0.5615 - val_accuracy: 0.7217\n",
      "Epoch 56/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.5625 - accuracy: 0.7189\n",
      "Epoch 56: val_loss did not improve from 0.56135\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5617 - accuracy: 0.7197 - val_loss: 0.5614 - val_accuracy: 0.7211\n",
      "Epoch 57/60\n",
      "653/682 [===========================>..] - ETA: 0s - loss: 0.5583 - accuracy: 0.7243\n",
      "Epoch 57: val_loss did not improve from 0.56135\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5616 - accuracy: 0.7214 - val_loss: 0.5614 - val_accuracy: 0.7217\n",
      "Epoch 58/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.5619 - accuracy: 0.7213\n",
      "Epoch 58: val_loss did not improve from 0.56135\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5615 - accuracy: 0.7211 - val_loss: 0.5614 - val_accuracy: 0.7205\n",
      "Epoch 59/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.5624 - accuracy: 0.7201\n",
      "Epoch 59: val_loss improved from 0.56135 to 0.56125, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5615 - accuracy: 0.7213 - val_loss: 0.5613 - val_accuracy: 0.7217\n",
      "Epoch 60/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.5604 - accuracy: 0.7228\n",
      "Epoch 60: val_loss did not improve from 0.56125\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5614 - accuracy: 0.7217 - val_loss: 0.5616 - val_accuracy: 0.7193\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.5613 - accuracy: 0.7217\n",
      "Drop Feature housemaid\n",
      "data frame x has shape: (8516, 23)\n",
      "['education', 'housing', 'loan', 'campaign', 'pdays', 'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'divorced', 'married', 'single', 'admin.', 'blue-collar', 'entrepreneur', 'management', 'retired', 'self-employed', 'services', 'student', 'technician']\n",
      "Epoch 1/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.6547 - accuracy: 0.6417\n",
      "Epoch 1: val_loss improved from inf to 0.62072, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 3ms/step - loss: 0.6546 - accuracy: 0.6423 - val_loss: 0.6207 - val_accuracy: 0.7041\n",
      "Epoch 2/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.6063 - accuracy: 0.7132\n",
      "Epoch 2: val_loss improved from 0.62072 to 0.59093, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6062 - accuracy: 0.7130 - val_loss: 0.5909 - val_accuracy: 0.7187\n",
      "Epoch 3/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.5873 - accuracy: 0.7176\n",
      "Epoch 3: val_loss improved from 0.59093 to 0.57872, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5871 - accuracy: 0.7176 - val_loss: 0.5787 - val_accuracy: 0.7199\n",
      "Epoch 4/60\n",
      "648/682 [===========================>..] - ETA: 0s - loss: 0.5784 - accuracy: 0.7173\n",
      "Epoch 4: val_loss improved from 0.57872 to 0.57289, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5786 - accuracy: 0.7179 - val_loss: 0.5729 - val_accuracy: 0.7234\n",
      "Epoch 5/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.5725 - accuracy: 0.7156\n",
      "Epoch 5: val_loss improved from 0.57289 to 0.57008, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5744 - accuracy: 0.7145 - val_loss: 0.5701 - val_accuracy: 0.7234\n",
      "Epoch 6/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.5721 - accuracy: 0.7139\n",
      "Epoch 6: val_loss improved from 0.57008 to 0.56897, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5718 - accuracy: 0.7144 - val_loss: 0.5690 - val_accuracy: 0.7240\n",
      "Epoch 7/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.5696 - accuracy: 0.7164\n",
      "Epoch 7: val_loss improved from 0.56897 to 0.56762, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5703 - accuracy: 0.7160 - val_loss: 0.5676 - val_accuracy: 0.7211\n",
      "Epoch 8/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.5684 - accuracy: 0.7152\n",
      "Epoch 8: val_loss improved from 0.56762 to 0.56695, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5693 - accuracy: 0.7147 - val_loss: 0.5669 - val_accuracy: 0.7223\n",
      "Epoch 9/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.5681 - accuracy: 0.7147\n",
      "Epoch 9: val_loss improved from 0.56695 to 0.56652, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5685 - accuracy: 0.7139 - val_loss: 0.5665 - val_accuracy: 0.7205\n",
      "Epoch 10/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.5676 - accuracy: 0.7131\n",
      "Epoch 10: val_loss improved from 0.56652 to 0.56618, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5679 - accuracy: 0.7128 - val_loss: 0.5662 - val_accuracy: 0.7181\n",
      "Epoch 11/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.5667 - accuracy: 0.7140\n",
      "Epoch 11: val_loss improved from 0.56618 to 0.56604, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5673 - accuracy: 0.7132 - val_loss: 0.5660 - val_accuracy: 0.7181\n",
      "Epoch 12/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.5666 - accuracy: 0.7128\n",
      "Epoch 12: val_loss improved from 0.56604 to 0.56554, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5668 - accuracy: 0.7122 - val_loss: 0.5655 - val_accuracy: 0.7217\n",
      "Epoch 13/60\n",
      "654/682 [===========================>..] - ETA: 0s - loss: 0.5677 - accuracy: 0.7102\n",
      "Epoch 13: val_loss improved from 0.56554 to 0.56521, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5664 - accuracy: 0.7116 - val_loss: 0.5652 - val_accuracy: 0.7181\n",
      "Epoch 14/60\n",
      "654/682 [===========================>..] - ETA: 0s - loss: 0.5655 - accuracy: 0.7145\n",
      "Epoch 14: val_loss improved from 0.56521 to 0.56494, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5660 - accuracy: 0.7141 - val_loss: 0.5649 - val_accuracy: 0.7205\n",
      "Epoch 15/60\n",
      "653/682 [===========================>..] - ETA: 0s - loss: 0.5641 - accuracy: 0.7141\n",
      "Epoch 15: val_loss improved from 0.56494 to 0.56471, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5658 - accuracy: 0.7119 - val_loss: 0.5647 - val_accuracy: 0.7199\n",
      "Epoch 16/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.5650 - accuracy: 0.7142\n",
      "Epoch 16: val_loss improved from 0.56471 to 0.56451, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5654 - accuracy: 0.7139 - val_loss: 0.5645 - val_accuracy: 0.7199\n",
      "Epoch 17/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.5651 - accuracy: 0.7120\n",
      "Epoch 17: val_loss did not improve from 0.56451\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5650 - accuracy: 0.7120 - val_loss: 0.5647 - val_accuracy: 0.7240\n",
      "Epoch 18/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.5645 - accuracy: 0.7149\n",
      "Epoch 18: val_loss improved from 0.56451 to 0.56409, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5649 - accuracy: 0.7150 - val_loss: 0.5641 - val_accuracy: 0.7228\n",
      "Epoch 19/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.5648 - accuracy: 0.7157\n",
      "Epoch 19: val_loss improved from 0.56409 to 0.56397, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5645 - accuracy: 0.7163 - val_loss: 0.5640 - val_accuracy: 0.7211\n",
      "Epoch 20/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.5645 - accuracy: 0.7164\n",
      "Epoch 20: val_loss improved from 0.56397 to 0.56368, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5644 - accuracy: 0.7166 - val_loss: 0.5637 - val_accuracy: 0.7217\n",
      "Epoch 21/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.5638 - accuracy: 0.7175\n",
      "Epoch 21: val_loss improved from 0.56368 to 0.56355, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5642 - accuracy: 0.7175 - val_loss: 0.5636 - val_accuracy: 0.7211\n",
      "Epoch 22/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.5645 - accuracy: 0.7140\n",
      "Epoch 22: val_loss improved from 0.56355 to 0.56339, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5640 - accuracy: 0.7157 - val_loss: 0.5634 - val_accuracy: 0.7223\n",
      "Epoch 23/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.5634 - accuracy: 0.7177\n",
      "Epoch 23: val_loss improved from 0.56339 to 0.56326, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5637 - accuracy: 0.7175 - val_loss: 0.5633 - val_accuracy: 0.7223\n",
      "Epoch 24/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.5637 - accuracy: 0.7184\n",
      "Epoch 24: val_loss improved from 0.56326 to 0.56308, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5637 - accuracy: 0.7185 - val_loss: 0.5631 - val_accuracy: 0.7217\n",
      "Epoch 25/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.5649 - accuracy: 0.7156\n",
      "Epoch 25: val_loss improved from 0.56308 to 0.56301, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5635 - accuracy: 0.7170 - val_loss: 0.5630 - val_accuracy: 0.7217\n",
      "Epoch 26/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.5632 - accuracy: 0.7181\n",
      "Epoch 26: val_loss improved from 0.56301 to 0.56294, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5634 - accuracy: 0.7179 - val_loss: 0.5629 - val_accuracy: 0.7217\n",
      "Epoch 27/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.5635 - accuracy: 0.7176\n",
      "Epoch 27: val_loss did not improve from 0.56294\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5632 - accuracy: 0.7176 - val_loss: 0.5631 - val_accuracy: 0.7205\n",
      "Epoch 28/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.5627 - accuracy: 0.7204\n",
      "Epoch 28: val_loss improved from 0.56294 to 0.56273, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5630 - accuracy: 0.7195 - val_loss: 0.5627 - val_accuracy: 0.7211\n",
      "Epoch 29/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.5642 - accuracy: 0.7170\n",
      "Epoch 29: val_loss improved from 0.56273 to 0.56269, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5629 - accuracy: 0.7183 - val_loss: 0.5627 - val_accuracy: 0.7217\n",
      "Epoch 30/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.5631 - accuracy: 0.7193\n",
      "Epoch 30: val_loss did not improve from 0.56269\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5628 - accuracy: 0.7199 - val_loss: 0.5627 - val_accuracy: 0.7223\n",
      "Epoch 31/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.5642 - accuracy: 0.7172\n",
      "Epoch 31: val_loss improved from 0.56269 to 0.56252, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5627 - accuracy: 0.7183 - val_loss: 0.5625 - val_accuracy: 0.7205\n",
      "Epoch 32/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.5624 - accuracy: 0.7186\n",
      "Epoch 32: val_loss did not improve from 0.56252\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5625 - accuracy: 0.7185 - val_loss: 0.5631 - val_accuracy: 0.7228\n",
      "Epoch 33/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.5613 - accuracy: 0.7212\n",
      "Epoch 33: val_loss improved from 0.56252 to 0.56232, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5626 - accuracy: 0.7198 - val_loss: 0.5623 - val_accuracy: 0.7217\n",
      "Epoch 34/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.5619 - accuracy: 0.7188\n",
      "Epoch 34: val_loss did not improve from 0.56232\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5625 - accuracy: 0.7182 - val_loss: 0.5623 - val_accuracy: 0.7193\n",
      "Epoch 35/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.5615 - accuracy: 0.7206\n",
      "Epoch 35: val_loss improved from 0.56232 to 0.56204, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5623 - accuracy: 0.7201 - val_loss: 0.5620 - val_accuracy: 0.7228\n",
      "Epoch 36/60\n",
      "651/682 [===========================>..] - ETA: 0s - loss: 0.5622 - accuracy: 0.7203\n",
      "Epoch 36: val_loss did not improve from 0.56204\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5622 - accuracy: 0.7211 - val_loss: 0.5622 - val_accuracy: 0.7205\n",
      "Epoch 37/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.5619 - accuracy: 0.7197\n",
      "Epoch 37: val_loss did not improve from 0.56204\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5622 - accuracy: 0.7195 - val_loss: 0.5627 - val_accuracy: 0.7187\n",
      "Epoch 38/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.5620 - accuracy: 0.7201\n",
      "Epoch 38: val_loss improved from 0.56204 to 0.56193, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5620 - accuracy: 0.7201 - val_loss: 0.5619 - val_accuracy: 0.7234\n",
      "Epoch 39/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.5630 - accuracy: 0.7194\n",
      "Epoch 39: val_loss improved from 0.56193 to 0.56193, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5621 - accuracy: 0.7201 - val_loss: 0.5619 - val_accuracy: 0.7211\n",
      "Epoch 40/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.5598 - accuracy: 0.7218\n",
      "Epoch 40: val_loss improved from 0.56193 to 0.56185, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5619 - accuracy: 0.7208 - val_loss: 0.5619 - val_accuracy: 0.7211\n",
      "Epoch 41/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.5599 - accuracy: 0.7231\n",
      "Epoch 41: val_loss improved from 0.56185 to 0.56184, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5620 - accuracy: 0.7208 - val_loss: 0.5618 - val_accuracy: 0.7223\n",
      "Epoch 42/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.5625 - accuracy: 0.7205\n",
      "Epoch 42: val_loss did not improve from 0.56184\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5619 - accuracy: 0.7213 - val_loss: 0.5619 - val_accuracy: 0.7211\n",
      "Epoch 43/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.5605 - accuracy: 0.7215\n",
      "Epoch 43: val_loss did not improve from 0.56184\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5618 - accuracy: 0.7208 - val_loss: 0.5620 - val_accuracy: 0.7205\n",
      "Epoch 44/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.5619 - accuracy: 0.7188\n",
      "Epoch 44: val_loss did not improve from 0.56184\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7191 - val_loss: 0.5619 - val_accuracy: 0.7199\n",
      "Epoch 45/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.5623 - accuracy: 0.7195\n",
      "Epoch 45: val_loss improved from 0.56184 to 0.56151, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7199 - val_loss: 0.5615 - val_accuracy: 0.7234\n",
      "Epoch 46/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.5611 - accuracy: 0.7192\n",
      "Epoch 46: val_loss did not improve from 0.56151\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5616 - accuracy: 0.7189 - val_loss: 0.5626 - val_accuracy: 0.7205\n",
      "Epoch 47/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.5620 - accuracy: 0.7201\n",
      "Epoch 47: val_loss did not improve from 0.56151\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5616 - accuracy: 0.7208 - val_loss: 0.5626 - val_accuracy: 0.7228\n",
      "Epoch 48/60\n",
      "652/682 [===========================>..] - ETA: 0s - loss: 0.5607 - accuracy: 0.7210\n",
      "Epoch 48: val_loss improved from 0.56151 to 0.56134, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5615 - accuracy: 0.7205 - val_loss: 0.5613 - val_accuracy: 0.7223\n",
      "Epoch 49/60\n",
      "654/682 [===========================>..] - ETA: 0s - loss: 0.5625 - accuracy: 0.7206\n",
      "Epoch 49: val_loss did not improve from 0.56134\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7211 - val_loss: 0.5614 - val_accuracy: 0.7246\n",
      "Epoch 50/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.5611 - accuracy: 0.7206\n",
      "Epoch 50: val_loss did not improve from 0.56134\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5614 - accuracy: 0.7208 - val_loss: 0.5616 - val_accuracy: 0.7199\n",
      "Epoch 51/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.5619 - accuracy: 0.7201\n",
      "Epoch 51: val_loss did not improve from 0.56134\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5616 - accuracy: 0.7195 - val_loss: 0.5615 - val_accuracy: 0.7234\n",
      "Epoch 52/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.5629 - accuracy: 0.7207\n",
      "Epoch 52: val_loss did not improve from 0.56134\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5614 - accuracy: 0.7219 - val_loss: 0.5617 - val_accuracy: 0.7205\n",
      "Epoch 53/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.5608 - accuracy: 0.7222\n",
      "Epoch 53: val_loss did not improve from 0.56134\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5615 - accuracy: 0.7220 - val_loss: 0.5615 - val_accuracy: 0.7211\n",
      "Epoch 54/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.5606 - accuracy: 0.7211\n",
      "Epoch 54: val_loss improved from 0.56134 to 0.56130, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5615 - accuracy: 0.7210 - val_loss: 0.5613 - val_accuracy: 0.7217\n",
      "Epoch 55/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.5611 - accuracy: 0.7220\n",
      "Epoch 55: val_loss did not improve from 0.56130\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5615 - accuracy: 0.7211 - val_loss: 0.5616 - val_accuracy: 0.7187\n",
      "Epoch 56/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.5606 - accuracy: 0.7225\n",
      "Epoch 56: val_loss did not improve from 0.56130\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5615 - accuracy: 0.7226 - val_loss: 0.5613 - val_accuracy: 0.7217\n",
      "Epoch 57/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.5614 - accuracy: 0.7217\n",
      "Epoch 57: val_loss improved from 0.56130 to 0.56129, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5613 - accuracy: 0.7217 - val_loss: 0.5613 - val_accuracy: 0.7217\n",
      "Epoch 58/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.5612 - accuracy: 0.7217\n",
      "Epoch 58: val_loss improved from 0.56129 to 0.56122, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5614 - accuracy: 0.7216 - val_loss: 0.5612 - val_accuracy: 0.7211\n",
      "Epoch 59/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.5619 - accuracy: 0.7223\n",
      "Epoch 59: val_loss did not improve from 0.56122\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5613 - accuracy: 0.7232 - val_loss: 0.5613 - val_accuracy: 0.7234\n",
      "Epoch 60/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.5615 - accuracy: 0.7218\n",
      "Epoch 60: val_loss did not improve from 0.56122\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5614 - accuracy: 0.7220 - val_loss: 0.5613 - val_accuracy: 0.7211\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.5612 - accuracy: 0.7211\n",
      "Drop Feature management\n",
      "data frame x has shape: (8516, 22)\n",
      "['education', 'housing', 'loan', 'campaign', 'pdays', 'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'divorced', 'married', 'single', 'admin.', 'blue-collar', 'entrepreneur', 'retired', 'self-employed', 'services', 'student', 'technician']\n",
      "Epoch 1/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.6848 - accuracy: 0.5650\n",
      "Epoch 1: val_loss improved from inf to 0.64007, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 3ms/step - loss: 0.6847 - accuracy: 0.5652 - val_loss: 0.6401 - val_accuracy: 0.6371\n",
      "Epoch 2/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.6248 - accuracy: 0.6762\n",
      "Epoch 2: val_loss improved from 0.64007 to 0.60333, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6248 - accuracy: 0.6759 - val_loss: 0.6033 - val_accuracy: 0.7011\n",
      "Epoch 3/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.5986 - accuracy: 0.7037\n",
      "Epoch 3: val_loss improved from 0.60333 to 0.58700, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5989 - accuracy: 0.7034 - val_loss: 0.5870 - val_accuracy: 0.7199\n",
      "Epoch 4/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.5870 - accuracy: 0.7104\n",
      "Epoch 4: val_loss improved from 0.58700 to 0.57950, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5868 - accuracy: 0.7101 - val_loss: 0.5795 - val_accuracy: 0.7211\n",
      "Epoch 5/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.5802 - accuracy: 0.7107\n",
      "Epoch 5: val_loss improved from 0.57950 to 0.57519, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5804 - accuracy: 0.7101 - val_loss: 0.5752 - val_accuracy: 0.7211\n",
      "Epoch 6/60\n",
      "654/682 [===========================>..] - ETA: 0s - loss: 0.5775 - accuracy: 0.7099\n",
      "Epoch 6: val_loss improved from 0.57519 to 0.57299, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5771 - accuracy: 0.7101 - val_loss: 0.5730 - val_accuracy: 0.7228\n",
      "Epoch 7/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.5752 - accuracy: 0.7110\n",
      "Epoch 7: val_loss improved from 0.57299 to 0.57147, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5750 - accuracy: 0.7108 - val_loss: 0.5715 - val_accuracy: 0.7234\n",
      "Epoch 8/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.5723 - accuracy: 0.7126\n",
      "Epoch 8: val_loss improved from 0.57147 to 0.57083, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5733 - accuracy: 0.7113 - val_loss: 0.5708 - val_accuracy: 0.7211\n",
      "Epoch 9/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.5719 - accuracy: 0.7110\n",
      "Epoch 9: val_loss improved from 0.57083 to 0.56974, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5722 - accuracy: 0.7108 - val_loss: 0.5697 - val_accuracy: 0.7228\n",
      "Epoch 10/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.5715 - accuracy: 0.7123\n",
      "Epoch 10: val_loss improved from 0.56974 to 0.56909, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5712 - accuracy: 0.7125 - val_loss: 0.5691 - val_accuracy: 0.7228\n",
      "Epoch 11/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.5699 - accuracy: 0.7132\n",
      "Epoch 11: val_loss improved from 0.56909 to 0.56856, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5705 - accuracy: 0.7123 - val_loss: 0.5686 - val_accuracy: 0.7217\n",
      "Epoch 12/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.5690 - accuracy: 0.7134\n",
      "Epoch 12: val_loss improved from 0.56856 to 0.56813, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5697 - accuracy: 0.7125 - val_loss: 0.5681 - val_accuracy: 0.7193\n",
      "Epoch 13/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.5702 - accuracy: 0.7122\n",
      "Epoch 13: val_loss did not improve from 0.56813\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5689 - accuracy: 0.7128 - val_loss: 0.5681 - val_accuracy: 0.7228\n",
      "Epoch 14/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.5688 - accuracy: 0.7134\n",
      "Epoch 14: val_loss improved from 0.56813 to 0.56722, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5687 - accuracy: 0.7138 - val_loss: 0.5672 - val_accuracy: 0.7223\n",
      "Epoch 15/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.5692 - accuracy: 0.7123\n",
      "Epoch 15: val_loss improved from 0.56722 to 0.56688, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5681 - accuracy: 0.7135 - val_loss: 0.5669 - val_accuracy: 0.7211\n",
      "Epoch 16/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.5679 - accuracy: 0.7112\n",
      "Epoch 16: val_loss improved from 0.56688 to 0.56662, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5676 - accuracy: 0.7119 - val_loss: 0.5666 - val_accuracy: 0.7199\n",
      "Epoch 17/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.5652 - accuracy: 0.7150\n",
      "Epoch 17: val_loss improved from 0.56662 to 0.56606, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5672 - accuracy: 0.7130 - val_loss: 0.5661 - val_accuracy: 0.7228\n",
      "Epoch 18/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.5670 - accuracy: 0.7146\n",
      "Epoch 18: val_loss did not improve from 0.56606\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5666 - accuracy: 0.7142 - val_loss: 0.5662 - val_accuracy: 0.7170\n",
      "Epoch 19/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.5668 - accuracy: 0.7142\n",
      "Epoch 19: val_loss improved from 0.56606 to 0.56541, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5666 - accuracy: 0.7151 - val_loss: 0.5654 - val_accuracy: 0.7234\n",
      "Epoch 20/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.5661 - accuracy: 0.7137\n",
      "Epoch 20: val_loss improved from 0.56541 to 0.56509, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5661 - accuracy: 0.7136 - val_loss: 0.5651 - val_accuracy: 0.7246\n",
      "Epoch 21/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.5669 - accuracy: 0.7138\n",
      "Epoch 21: val_loss did not improve from 0.56509\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5656 - accuracy: 0.7153 - val_loss: 0.5657 - val_accuracy: 0.7234\n",
      "Epoch 22/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.5640 - accuracy: 0.7180\n",
      "Epoch 22: val_loss improved from 0.56509 to 0.56481, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5656 - accuracy: 0.7166 - val_loss: 0.5648 - val_accuracy: 0.7211\n",
      "Epoch 23/60\n",
      "654/682 [===========================>..] - ETA: 0s - loss: 0.5676 - accuracy: 0.7115\n",
      "Epoch 23: val_loss improved from 0.56481 to 0.56463, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5652 - accuracy: 0.7139 - val_loss: 0.5646 - val_accuracy: 0.7234\n",
      "Epoch 24/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.5643 - accuracy: 0.7156\n",
      "Epoch 24: val_loss improved from 0.56463 to 0.56428, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5650 - accuracy: 0.7151 - val_loss: 0.5643 - val_accuracy: 0.7217\n",
      "Epoch 25/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.5647 - accuracy: 0.7182\n",
      "Epoch 25: val_loss improved from 0.56428 to 0.56415, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5647 - accuracy: 0.7180 - val_loss: 0.5641 - val_accuracy: 0.7217\n",
      "Epoch 26/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.5629 - accuracy: 0.7183\n",
      "Epoch 26: val_loss improved from 0.56415 to 0.56410, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5644 - accuracy: 0.7167 - val_loss: 0.5641 - val_accuracy: 0.7228\n",
      "Epoch 27/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.5645 - accuracy: 0.7192\n",
      "Epoch 27: val_loss improved from 0.56410 to 0.56385, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5641 - accuracy: 0.7189 - val_loss: 0.5638 - val_accuracy: 0.7223\n",
      "Epoch 28/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.5640 - accuracy: 0.7173\n",
      "Epoch 28: val_loss improved from 0.56385 to 0.56354, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5642 - accuracy: 0.7172 - val_loss: 0.5635 - val_accuracy: 0.7234\n",
      "Epoch 29/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.5644 - accuracy: 0.7173\n",
      "Epoch 29: val_loss did not improve from 0.56354\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5638 - accuracy: 0.7179 - val_loss: 0.5636 - val_accuracy: 0.7234\n",
      "Epoch 30/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.5634 - accuracy: 0.7166\n",
      "Epoch 30: val_loss improved from 0.56354 to 0.56330, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5635 - accuracy: 0.7163 - val_loss: 0.5633 - val_accuracy: 0.7240\n",
      "Epoch 31/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.5632 - accuracy: 0.7167\n",
      "Epoch 31: val_loss improved from 0.56330 to 0.56311, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5635 - accuracy: 0.7167 - val_loss: 0.5631 - val_accuracy: 0.7217\n",
      "Epoch 32/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.5638 - accuracy: 0.7181\n",
      "Epoch 32: val_loss improved from 0.56311 to 0.56293, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5634 - accuracy: 0.7182 - val_loss: 0.5629 - val_accuracy: 0.7223\n",
      "Epoch 33/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.5637 - accuracy: 0.7175\n",
      "Epoch 33: val_loss improved from 0.56293 to 0.56283, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5634 - accuracy: 0.7185 - val_loss: 0.5628 - val_accuracy: 0.7228\n",
      "Epoch 34/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.5627 - accuracy: 0.7204\n",
      "Epoch 34: val_loss did not improve from 0.56283\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5630 - accuracy: 0.7192 - val_loss: 0.5629 - val_accuracy: 0.7228\n",
      "Epoch 35/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.5617 - accuracy: 0.7202\n",
      "Epoch 35: val_loss improved from 0.56283 to 0.56274, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5630 - accuracy: 0.7186 - val_loss: 0.5627 - val_accuracy: 0.7228\n",
      "Epoch 36/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.5613 - accuracy: 0.7187\n",
      "Epoch 36: val_loss improved from 0.56274 to 0.56272, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5629 - accuracy: 0.7177 - val_loss: 0.5627 - val_accuracy: 0.7234\n",
      "Epoch 37/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.5627 - accuracy: 0.7183\n",
      "Epoch 37: val_loss improved from 0.56272 to 0.56254, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5629 - accuracy: 0.7180 - val_loss: 0.5625 - val_accuracy: 0.7217\n",
      "Epoch 38/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.5634 - accuracy: 0.7187\n",
      "Epoch 38: val_loss improved from 0.56254 to 0.56246, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5628 - accuracy: 0.7189 - val_loss: 0.5625 - val_accuracy: 0.7217\n",
      "Epoch 39/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.5632 - accuracy: 0.7192\n",
      "Epoch 39: val_loss did not improve from 0.56246\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5626 - accuracy: 0.7194 - val_loss: 0.5625 - val_accuracy: 0.7199\n",
      "Epoch 40/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.5627 - accuracy: 0.7191\n",
      "Epoch 40: val_loss improved from 0.56246 to 0.56227, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5625 - accuracy: 0.7199 - val_loss: 0.5623 - val_accuracy: 0.7211\n",
      "Epoch 41/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.5619 - accuracy: 0.7197\n",
      "Epoch 41: val_loss did not improve from 0.56227\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5624 - accuracy: 0.7186 - val_loss: 0.5626 - val_accuracy: 0.7205\n",
      "Epoch 42/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.5632 - accuracy: 0.7190\n",
      "Epoch 42: val_loss improved from 0.56227 to 0.56209, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5624 - accuracy: 0.7201 - val_loss: 0.5621 - val_accuracy: 0.7211\n",
      "Epoch 43/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.5621 - accuracy: 0.7203\n",
      "Epoch 43: val_loss improved from 0.56209 to 0.56200, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5623 - accuracy: 0.7201 - val_loss: 0.5620 - val_accuracy: 0.7211\n",
      "Epoch 44/60\n",
      "649/682 [===========================>..] - ETA: 0s - loss: 0.5641 - accuracy: 0.7191\n",
      "Epoch 44: val_loss did not improve from 0.56200\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5622 - accuracy: 0.7207 - val_loss: 0.5620 - val_accuracy: 0.7205\n",
      "Epoch 45/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.5631 - accuracy: 0.7183\n",
      "Epoch 45: val_loss did not improve from 0.56200\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5623 - accuracy: 0.7195 - val_loss: 0.5621 - val_accuracy: 0.7217\n",
      "Epoch 46/60\n",
      "652/682 [===========================>..] - ETA: 0s - loss: 0.5609 - accuracy: 0.7196\n",
      "Epoch 46: val_loss improved from 0.56200 to 0.56174, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5621 - accuracy: 0.7192 - val_loss: 0.5617 - val_accuracy: 0.7228\n",
      "Epoch 47/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.5621 - accuracy: 0.7191\n",
      "Epoch 47: val_loss did not improve from 0.56174\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5621 - accuracy: 0.7194 - val_loss: 0.5619 - val_accuracy: 0.7223\n",
      "Epoch 48/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.5623 - accuracy: 0.7202\n",
      "Epoch 48: val_loss did not improve from 0.56174\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5618 - accuracy: 0.7205 - val_loss: 0.5622 - val_accuracy: 0.7217\n",
      "Epoch 49/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.5615 - accuracy: 0.7211\n",
      "Epoch 49: val_loss improved from 0.56174 to 0.56157, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5618 - accuracy: 0.7202 - val_loss: 0.5616 - val_accuracy: 0.7211\n",
      "Epoch 50/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.5620 - accuracy: 0.7201\n",
      "Epoch 50: val_loss did not improve from 0.56157\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5620 - accuracy: 0.7201 - val_loss: 0.5617 - val_accuracy: 0.7205\n",
      "Epoch 51/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.5628 - accuracy: 0.7189\n",
      "Epoch 51: val_loss did not improve from 0.56157\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5618 - accuracy: 0.7201 - val_loss: 0.5620 - val_accuracy: 0.7193\n",
      "Epoch 52/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.5606 - accuracy: 0.7227\n",
      "Epoch 52: val_loss did not improve from 0.56157\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5618 - accuracy: 0.7219 - val_loss: 0.5618 - val_accuracy: 0.7205\n",
      "Epoch 53/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.5626 - accuracy: 0.7199\n",
      "Epoch 53: val_loss did not improve from 0.56157\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5618 - accuracy: 0.7207 - val_loss: 0.5617 - val_accuracy: 0.7211\n",
      "Epoch 54/60\n",
      "654/682 [===========================>..] - ETA: 0s - loss: 0.5621 - accuracy: 0.7206\n",
      "Epoch 54: val_loss did not improve from 0.56157\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5617 - accuracy: 0.7210 - val_loss: 0.5617 - val_accuracy: 0.7211\n",
      "Epoch 55/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.5615 - accuracy: 0.7200\n",
      "Epoch 55: val_loss did not improve from 0.56157\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5617 - accuracy: 0.7199 - val_loss: 0.5617 - val_accuracy: 0.7199\n",
      "Epoch 56/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.5604 - accuracy: 0.7217\n",
      "Epoch 56: val_loss did not improve from 0.56157\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7211 - val_loss: 0.5616 - val_accuracy: 0.7234\n",
      "Epoch 57/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.5611 - accuracy: 0.7202\n",
      "Epoch 57: val_loss did not improve from 0.56157\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5616 - accuracy: 0.7199 - val_loss: 0.5617 - val_accuracy: 0.7199\n",
      "Epoch 58/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.5616 - accuracy: 0.7218\n",
      "Epoch 58: val_loss improved from 0.56157 to 0.56145, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5616 - accuracy: 0.7217 - val_loss: 0.5615 - val_accuracy: 0.7211\n",
      "Epoch 59/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.5619 - accuracy: 0.7209\n",
      "Epoch 59: val_loss did not improve from 0.56145\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5616 - accuracy: 0.7211 - val_loss: 0.5617 - val_accuracy: 0.7199\n",
      "Epoch 60/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.5621 - accuracy: 0.7213\n",
      "Epoch 60: val_loss did not improve from 0.56145\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5615 - accuracy: 0.7224 - val_loss: 0.5615 - val_accuracy: 0.7211\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.5615 - accuracy: 0.7211\n",
      "Drop Feature technician\n",
      "data frame x has shape: (8516, 21)\n",
      "['education', 'housing', 'loan', 'campaign', 'pdays', 'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'divorced', 'married', 'single', 'admin.', 'blue-collar', 'entrepreneur', 'retired', 'self-employed', 'services', 'student']\n",
      "Epoch 1/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.6753 - accuracy: 0.5550\n",
      "Epoch 1: val_loss improved from inf to 0.63657, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 3ms/step - loss: 0.6750 - accuracy: 0.5566 - val_loss: 0.6366 - val_accuracy: 0.6494\n",
      "Epoch 2/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.6239 - accuracy: 0.6598\n",
      "Epoch 2: val_loss improved from 0.63657 to 0.60533, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6227 - accuracy: 0.6624 - val_loss: 0.6053 - val_accuracy: 0.6782\n",
      "Epoch 3/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.6003 - accuracy: 0.6914\n",
      "Epoch 3: val_loss improved from 0.60533 to 0.58878, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6000 - accuracy: 0.6916 - val_loss: 0.5888 - val_accuracy: 0.7070\n",
      "Epoch 4/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.5886 - accuracy: 0.7053\n",
      "Epoch 4: val_loss improved from 0.58878 to 0.58063, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5881 - accuracy: 0.7063 - val_loss: 0.5806 - val_accuracy: 0.7146\n",
      "Epoch 5/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.5815 - accuracy: 0.7129\n",
      "Epoch 5: val_loss improved from 0.58063 to 0.57582, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5814 - accuracy: 0.7128 - val_loss: 0.5758 - val_accuracy: 0.7181\n",
      "Epoch 6/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.5778 - accuracy: 0.7156\n",
      "Epoch 6: val_loss improved from 0.57582 to 0.57310, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5774 - accuracy: 0.7151 - val_loss: 0.5731 - val_accuracy: 0.7170\n",
      "Epoch 7/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.5748 - accuracy: 0.7145\n",
      "Epoch 7: val_loss improved from 0.57310 to 0.57160, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5748 - accuracy: 0.7147 - val_loss: 0.5716 - val_accuracy: 0.7193\n",
      "Epoch 8/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.5730 - accuracy: 0.7154\n",
      "Epoch 8: val_loss improved from 0.57160 to 0.56993, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5730 - accuracy: 0.7151 - val_loss: 0.5699 - val_accuracy: 0.7217\n",
      "Epoch 9/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.5732 - accuracy: 0.7159\n",
      "Epoch 9: val_loss improved from 0.56993 to 0.56900, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5718 - accuracy: 0.7170 - val_loss: 0.5690 - val_accuracy: 0.7205\n",
      "Epoch 10/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.5681 - accuracy: 0.7169\n",
      "Epoch 10: val_loss improved from 0.56900 to 0.56840, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5707 - accuracy: 0.7145 - val_loss: 0.5684 - val_accuracy: 0.7240\n",
      "Epoch 11/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.5693 - accuracy: 0.7152\n",
      "Epoch 11: val_loss improved from 0.56840 to 0.56809, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5697 - accuracy: 0.7144 - val_loss: 0.5681 - val_accuracy: 0.7176\n",
      "Epoch 12/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.5692 - accuracy: 0.7150\n",
      "Epoch 12: val_loss improved from 0.56809 to 0.56752, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5692 - accuracy: 0.7150 - val_loss: 0.5675 - val_accuracy: 0.7187\n",
      "Epoch 13/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.5692 - accuracy: 0.7151\n",
      "Epoch 13: val_loss improved from 0.56752 to 0.56676, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5685 - accuracy: 0.7157 - val_loss: 0.5668 - val_accuracy: 0.7211\n",
      "Epoch 14/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.5668 - accuracy: 0.7153\n",
      "Epoch 14: val_loss improved from 0.56676 to 0.56637, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5680 - accuracy: 0.7130 - val_loss: 0.5664 - val_accuracy: 0.7228\n",
      "Epoch 15/60\n",
      "654/682 [===========================>..] - ETA: 0s - loss: 0.5691 - accuracy: 0.7131\n",
      "Epoch 15: val_loss improved from 0.56637 to 0.56605, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5676 - accuracy: 0.7147 - val_loss: 0.5661 - val_accuracy: 0.7205\n",
      "Epoch 16/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.5682 - accuracy: 0.7145\n",
      "Epoch 16: val_loss improved from 0.56605 to 0.56594, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5670 - accuracy: 0.7153 - val_loss: 0.5659 - val_accuracy: 0.7240\n",
      "Epoch 17/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.5668 - accuracy: 0.7183\n",
      "Epoch 17: val_loss improved from 0.56594 to 0.56544, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5667 - accuracy: 0.7172 - val_loss: 0.5654 - val_accuracy: 0.7223\n",
      "Epoch 18/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.5663 - accuracy: 0.7177\n",
      "Epoch 18: val_loss improved from 0.56544 to 0.56502, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5664 - accuracy: 0.7175 - val_loss: 0.5650 - val_accuracy: 0.7217\n",
      "Epoch 19/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.5659 - accuracy: 0.7149\n",
      "Epoch 19: val_loss did not improve from 0.56502\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5660 - accuracy: 0.7147 - val_loss: 0.5653 - val_accuracy: 0.7246\n",
      "Epoch 20/60\n",
      "653/682 [===========================>..] - ETA: 0s - loss: 0.5647 - accuracy: 0.7168\n",
      "Epoch 20: val_loss improved from 0.56502 to 0.56456, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5658 - accuracy: 0.7161 - val_loss: 0.5646 - val_accuracy: 0.7228\n",
      "Epoch 21/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.5650 - accuracy: 0.7184\n",
      "Epoch 21: val_loss did not improve from 0.56456\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5654 - accuracy: 0.7176 - val_loss: 0.5647 - val_accuracy: 0.7193\n",
      "Epoch 22/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.5654 - accuracy: 0.7153\n",
      "Epoch 22: val_loss did not improve from 0.56456\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5650 - accuracy: 0.7155 - val_loss: 0.5649 - val_accuracy: 0.7228\n",
      "Epoch 23/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.5651 - accuracy: 0.7175\n",
      "Epoch 23: val_loss improved from 0.56456 to 0.56397, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5648 - accuracy: 0.7180 - val_loss: 0.5640 - val_accuracy: 0.7234\n",
      "Epoch 24/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.5642 - accuracy: 0.7150\n",
      "Epoch 24: val_loss improved from 0.56397 to 0.56382, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5646 - accuracy: 0.7151 - val_loss: 0.5638 - val_accuracy: 0.7217\n",
      "Epoch 25/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.5634 - accuracy: 0.7171\n",
      "Epoch 25: val_loss improved from 0.56382 to 0.56359, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5645 - accuracy: 0.7167 - val_loss: 0.5636 - val_accuracy: 0.7228\n",
      "Epoch 26/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.5630 - accuracy: 0.7195\n",
      "Epoch 26: val_loss improved from 0.56359 to 0.56341, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5642 - accuracy: 0.7186 - val_loss: 0.5634 - val_accuracy: 0.7246\n",
      "Epoch 27/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.5638 - accuracy: 0.7169\n",
      "Epoch 27: val_loss improved from 0.56341 to 0.56330, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5640 - accuracy: 0.7163 - val_loss: 0.5633 - val_accuracy: 0.7217\n",
      "Epoch 28/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.5638 - accuracy: 0.7190\n",
      "Epoch 28: val_loss did not improve from 0.56330\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5638 - accuracy: 0.7186 - val_loss: 0.5634 - val_accuracy: 0.7234\n",
      "Epoch 29/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.5638 - accuracy: 0.7187\n",
      "Epoch 29: val_loss improved from 0.56330 to 0.56305, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5637 - accuracy: 0.7188 - val_loss: 0.5631 - val_accuracy: 0.7217\n",
      "Epoch 30/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.5634 - accuracy: 0.7185\n",
      "Epoch 30: val_loss improved from 0.56305 to 0.56290, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5635 - accuracy: 0.7176 - val_loss: 0.5629 - val_accuracy: 0.7223\n",
      "Epoch 31/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.5636 - accuracy: 0.7195\n",
      "Epoch 31: val_loss did not improve from 0.56290\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5633 - accuracy: 0.7198 - val_loss: 0.5631 - val_accuracy: 0.7234\n",
      "Epoch 32/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.5636 - accuracy: 0.7183\n",
      "Epoch 32: val_loss improved from 0.56290 to 0.56278, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5634 - accuracy: 0.7185 - val_loss: 0.5628 - val_accuracy: 0.7228\n",
      "Epoch 33/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.5630 - accuracy: 0.7195\n",
      "Epoch 33: val_loss improved from 0.56278 to 0.56266, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5631 - accuracy: 0.7191 - val_loss: 0.5627 - val_accuracy: 0.7217\n",
      "Epoch 34/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.5629 - accuracy: 0.7192\n",
      "Epoch 34: val_loss did not improve from 0.56266\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5629 - accuracy: 0.7192 - val_loss: 0.5627 - val_accuracy: 0.7217\n",
      "Epoch 35/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.5639 - accuracy: 0.7206\n",
      "Epoch 35: val_loss did not improve from 0.56266\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5628 - accuracy: 0.7214 - val_loss: 0.5627 - val_accuracy: 0.7240\n",
      "Epoch 36/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.5631 - accuracy: 0.7183\n",
      "Epoch 36: val_loss improved from 0.56266 to 0.56265, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5627 - accuracy: 0.7186 - val_loss: 0.5627 - val_accuracy: 0.7240\n",
      "Epoch 37/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.5620 - accuracy: 0.7187\n",
      "Epoch 37: val_loss improved from 0.56265 to 0.56236, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5626 - accuracy: 0.7183 - val_loss: 0.5624 - val_accuracy: 0.7228\n",
      "Epoch 38/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.5623 - accuracy: 0.7196\n",
      "Epoch 38: val_loss improved from 0.56236 to 0.56216, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5627 - accuracy: 0.7191 - val_loss: 0.5622 - val_accuracy: 0.7199\n",
      "Epoch 39/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.5621 - accuracy: 0.7199\n",
      "Epoch 39: val_loss did not improve from 0.56216\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5623 - accuracy: 0.7199 - val_loss: 0.5625 - val_accuracy: 0.7217\n",
      "Epoch 40/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.5621 - accuracy: 0.7217\n",
      "Epoch 40: val_loss did not improve from 0.56216\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5622 - accuracy: 0.7217 - val_loss: 0.5624 - val_accuracy: 0.7193\n",
      "Epoch 41/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.5610 - accuracy: 0.7209\n",
      "Epoch 41: val_loss improved from 0.56216 to 0.56215, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5624 - accuracy: 0.7194 - val_loss: 0.5622 - val_accuracy: 0.7211\n",
      "Epoch 42/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.5626 - accuracy: 0.7197\n",
      "Epoch 42: val_loss did not improve from 0.56215\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5623 - accuracy: 0.7199 - val_loss: 0.5624 - val_accuracy: 0.7187\n",
      "Epoch 43/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.5616 - accuracy: 0.7206\n",
      "Epoch 43: val_loss improved from 0.56215 to 0.56177, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5621 - accuracy: 0.7202 - val_loss: 0.5618 - val_accuracy: 0.7205\n",
      "Epoch 44/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.5618 - accuracy: 0.7204\n",
      "Epoch 44: val_loss improved from 0.56177 to 0.56172, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5621 - accuracy: 0.7204 - val_loss: 0.5617 - val_accuracy: 0.7205\n",
      "Epoch 45/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.5617 - accuracy: 0.7204\n",
      "Epoch 45: val_loss did not improve from 0.56172\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5619 - accuracy: 0.7202 - val_loss: 0.5623 - val_accuracy: 0.7211\n",
      "Epoch 46/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.5627 - accuracy: 0.7192\n",
      "Epoch 46: val_loss improved from 0.56172 to 0.56168, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5621 - accuracy: 0.7204 - val_loss: 0.5617 - val_accuracy: 0.7205\n",
      "Epoch 47/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.5619 - accuracy: 0.7208\n",
      "Epoch 47: val_loss did not improve from 0.56168\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5619 - accuracy: 0.7207 - val_loss: 0.5620 - val_accuracy: 0.7217\n",
      "Epoch 48/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.5628 - accuracy: 0.7200\n",
      "Epoch 48: val_loss did not improve from 0.56168\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5619 - accuracy: 0.7202 - val_loss: 0.5617 - val_accuracy: 0.7223\n",
      "Epoch 49/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.5619 - accuracy: 0.7209\n",
      "Epoch 49: val_loss improved from 0.56168 to 0.56165, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5619 - accuracy: 0.7205 - val_loss: 0.5616 - val_accuracy: 0.7211\n",
      "Epoch 50/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.5603 - accuracy: 0.7216\n",
      "Epoch 50: val_loss improved from 0.56165 to 0.56150, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5618 - accuracy: 0.7205 - val_loss: 0.5615 - val_accuracy: 0.7217\n",
      "Epoch 51/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.5622 - accuracy: 0.7223\n",
      "Epoch 51: val_loss did not improve from 0.56150\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7220 - val_loss: 0.5616 - val_accuracy: 0.7211\n",
      "Epoch 52/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.5623 - accuracy: 0.7224\n",
      "Epoch 52: val_loss did not improve from 0.56150\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7229 - val_loss: 0.5616 - val_accuracy: 0.7211\n",
      "Epoch 53/60\n",
      "651/682 [===========================>..] - ETA: 0s - loss: 0.5611 - accuracy: 0.7203\n",
      "Epoch 53: val_loss improved from 0.56150 to 0.56140, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7208 - val_loss: 0.5614 - val_accuracy: 0.7211\n",
      "Epoch 54/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.5620 - accuracy: 0.7216\n",
      "Epoch 54: val_loss did not improve from 0.56140\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7211 - val_loss: 0.5616 - val_accuracy: 0.7223\n",
      "Epoch 55/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.5619 - accuracy: 0.7199\n",
      "Epoch 55: val_loss did not improve from 0.56140\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7204 - val_loss: 0.5615 - val_accuracy: 0.7211\n",
      "Epoch 56/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.5616 - accuracy: 0.7198\n",
      "Epoch 56: val_loss did not improve from 0.56140\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7208 - val_loss: 0.5620 - val_accuracy: 0.7187\n",
      "Epoch 57/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.5623 - accuracy: 0.7206\n",
      "Epoch 57: val_loss improved from 0.56140 to 0.56135, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7214 - val_loss: 0.5614 - val_accuracy: 0.7211\n",
      "Epoch 58/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.5629 - accuracy: 0.7206\n",
      "Epoch 58: val_loss did not improve from 0.56135\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5616 - accuracy: 0.7217 - val_loss: 0.5615 - val_accuracy: 0.7205\n",
      "Epoch 59/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.5617 - accuracy: 0.7212\n",
      "Epoch 59: val_loss did not improve from 0.56135\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5615 - accuracy: 0.7210 - val_loss: 0.5615 - val_accuracy: 0.7223\n",
      "Epoch 60/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.5609 - accuracy: 0.7234\n",
      "Epoch 60: val_loss did not improve from 0.56135\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5615 - accuracy: 0.7226 - val_loss: 0.5614 - val_accuracy: 0.7217\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.5614 - accuracy: 0.7211\n",
      "Drop Feature loan\n",
      "data frame x has shape: (8516, 20)\n",
      "['education', 'housing', 'campaign', 'pdays', 'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'divorced', 'married', 'single', 'admin.', 'blue-collar', 'entrepreneur', 'retired', 'self-employed', 'services', 'student']\n",
      "Epoch 1/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.7084 - accuracy: 0.5214\n",
      "Epoch 1: val_loss improved from inf to 0.65460, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 3ms/step - loss: 0.7081 - accuracy: 0.5219 - val_loss: 0.6546 - val_accuracy: 0.6207\n",
      "Epoch 2/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.6408 - accuracy: 0.6427\n",
      "Epoch 2: val_loss improved from 0.65460 to 0.61211, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6407 - accuracy: 0.6429 - val_loss: 0.6121 - val_accuracy: 0.6970\n",
      "Epoch 3/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.6092 - accuracy: 0.6894\n",
      "Epoch 3: val_loss improved from 0.61211 to 0.59192, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6091 - accuracy: 0.6896 - val_loss: 0.5919 - val_accuracy: 0.7111\n",
      "Epoch 4/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.5933 - accuracy: 0.7029\n",
      "Epoch 4: val_loss improved from 0.59192 to 0.58218, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5933 - accuracy: 0.7029 - val_loss: 0.5822 - val_accuracy: 0.7199\n",
      "Epoch 5/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.5842 - accuracy: 0.7090\n",
      "Epoch 5: val_loss improved from 0.58218 to 0.57728, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5850 - accuracy: 0.7081 - val_loss: 0.5773 - val_accuracy: 0.7181\n",
      "Epoch 6/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.5801 - accuracy: 0.7098\n",
      "Epoch 6: val_loss improved from 0.57728 to 0.57494, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5802 - accuracy: 0.7098 - val_loss: 0.5749 - val_accuracy: 0.7146\n",
      "Epoch 7/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.5779 - accuracy: 0.7107\n",
      "Epoch 7: val_loss improved from 0.57494 to 0.57261, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5774 - accuracy: 0.7107 - val_loss: 0.5726 - val_accuracy: 0.7193\n",
      "Epoch 8/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.5751 - accuracy: 0.7097\n",
      "Epoch 8: val_loss improved from 0.57261 to 0.57142, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5753 - accuracy: 0.7097 - val_loss: 0.5714 - val_accuracy: 0.7199\n",
      "Epoch 9/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.5745 - accuracy: 0.7128\n",
      "Epoch 9: val_loss improved from 0.57142 to 0.57054, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5740 - accuracy: 0.7123 - val_loss: 0.5705 - val_accuracy: 0.7223\n",
      "Epoch 10/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.5729 - accuracy: 0.7112\n",
      "Epoch 10: val_loss improved from 0.57054 to 0.56970, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5728 - accuracy: 0.7104 - val_loss: 0.5697 - val_accuracy: 0.7205\n",
      "Epoch 11/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.5713 - accuracy: 0.7122\n",
      "Epoch 11: val_loss improved from 0.56970 to 0.56915, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5717 - accuracy: 0.7119 - val_loss: 0.5692 - val_accuracy: 0.7228\n",
      "Epoch 12/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.5703 - accuracy: 0.7123\n",
      "Epoch 12: val_loss improved from 0.56915 to 0.56880, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5710 - accuracy: 0.7114 - val_loss: 0.5688 - val_accuracy: 0.7152\n",
      "Epoch 13/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.5711 - accuracy: 0.7113\n",
      "Epoch 13: val_loss improved from 0.56880 to 0.56806, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5704 - accuracy: 0.7114 - val_loss: 0.5681 - val_accuracy: 0.7193\n",
      "Epoch 14/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.5699 - accuracy: 0.7120\n",
      "Epoch 14: val_loss improved from 0.56806 to 0.56760, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5697 - accuracy: 0.7122 - val_loss: 0.5676 - val_accuracy: 0.7158\n",
      "Epoch 15/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.5683 - accuracy: 0.7123\n",
      "Epoch 15: val_loss improved from 0.56760 to 0.56727, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5691 - accuracy: 0.7116 - val_loss: 0.5673 - val_accuracy: 0.7158\n",
      "Epoch 16/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.5678 - accuracy: 0.7126\n",
      "Epoch 16: val_loss improved from 0.56727 to 0.56678, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5684 - accuracy: 0.7117 - val_loss: 0.5668 - val_accuracy: 0.7176\n",
      "Epoch 17/60\n",
      "652/682 [===========================>..] - ETA: 0s - loss: 0.5683 - accuracy: 0.7144\n",
      "Epoch 17: val_loss did not improve from 0.56678\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5678 - accuracy: 0.7136 - val_loss: 0.5669 - val_accuracy: 0.7152\n",
      "Epoch 18/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.5665 - accuracy: 0.7138\n",
      "Epoch 18: val_loss improved from 0.56678 to 0.56604, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5677 - accuracy: 0.7120 - val_loss: 0.5660 - val_accuracy: 0.7211\n",
      "Epoch 19/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.5680 - accuracy: 0.7117\n",
      "Epoch 19: val_loss improved from 0.56604 to 0.56568, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5672 - accuracy: 0.7123 - val_loss: 0.5657 - val_accuracy: 0.7199\n",
      "Epoch 20/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.5662 - accuracy: 0.7127\n",
      "Epoch 20: val_loss improved from 0.56568 to 0.56548, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5668 - accuracy: 0.7128 - val_loss: 0.5655 - val_accuracy: 0.7228\n",
      "Epoch 21/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.5655 - accuracy: 0.7148\n",
      "Epoch 21: val_loss improved from 0.56548 to 0.56533, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5663 - accuracy: 0.7138 - val_loss: 0.5653 - val_accuracy: 0.7170\n",
      "Epoch 22/60\n",
      "653/682 [===========================>..] - ETA: 0s - loss: 0.5683 - accuracy: 0.7106\n",
      "Epoch 22: val_loss improved from 0.56533 to 0.56490, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5660 - accuracy: 0.7126 - val_loss: 0.5649 - val_accuracy: 0.7217\n",
      "Epoch 23/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.5663 - accuracy: 0.7137\n",
      "Epoch 23: val_loss did not improve from 0.56490\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5656 - accuracy: 0.7144 - val_loss: 0.5652 - val_accuracy: 0.7211\n",
      "Epoch 24/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.5654 - accuracy: 0.7158\n",
      "Epoch 24: val_loss improved from 0.56490 to 0.56446, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5656 - accuracy: 0.7155 - val_loss: 0.5645 - val_accuracy: 0.7199\n",
      "Epoch 25/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.5652 - accuracy: 0.7134\n",
      "Epoch 25: val_loss improved from 0.56446 to 0.56418, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5652 - accuracy: 0.7133 - val_loss: 0.5642 - val_accuracy: 0.7199\n",
      "Epoch 26/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.5651 - accuracy: 0.7160\n",
      "Epoch 26: val_loss improved from 0.56418 to 0.56412, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5648 - accuracy: 0.7154 - val_loss: 0.5641 - val_accuracy: 0.7228\n",
      "Epoch 27/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.5652 - accuracy: 0.7164\n",
      "Epoch 27: val_loss improved from 0.56412 to 0.56377, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5648 - accuracy: 0.7164 - val_loss: 0.5638 - val_accuracy: 0.7193\n",
      "Epoch 28/60\n",
      "654/682 [===========================>..] - ETA: 0s - loss: 0.5663 - accuracy: 0.7138\n",
      "Epoch 28: val_loss improved from 0.56377 to 0.56370, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5644 - accuracy: 0.7153 - val_loss: 0.5637 - val_accuracy: 0.7234\n",
      "Epoch 29/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.5640 - accuracy: 0.7186\n",
      "Epoch 29: val_loss improved from 0.56370 to 0.56343, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5641 - accuracy: 0.7182 - val_loss: 0.5634 - val_accuracy: 0.7181\n",
      "Epoch 30/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.5643 - accuracy: 0.7160\n",
      "Epoch 30: val_loss improved from 0.56343 to 0.56325, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5641 - accuracy: 0.7161 - val_loss: 0.5632 - val_accuracy: 0.7199\n",
      "Epoch 31/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.5636 - accuracy: 0.7179\n",
      "Epoch 31: val_loss did not improve from 0.56325\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5637 - accuracy: 0.7180 - val_loss: 0.5636 - val_accuracy: 0.7205\n",
      "Epoch 32/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.5642 - accuracy: 0.7168\n",
      "Epoch 32: val_loss improved from 0.56325 to 0.56308, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5639 - accuracy: 0.7170 - val_loss: 0.5631 - val_accuracy: 0.7199\n",
      "Epoch 33/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.5621 - accuracy: 0.7191\n",
      "Epoch 33: val_loss did not improve from 0.56308\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5635 - accuracy: 0.7179 - val_loss: 0.5633 - val_accuracy: 0.7217\n",
      "Epoch 34/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.5632 - accuracy: 0.7182\n",
      "Epoch 34: val_loss improved from 0.56308 to 0.56288, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5635 - accuracy: 0.7175 - val_loss: 0.5629 - val_accuracy: 0.7205\n",
      "Epoch 35/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.5632 - accuracy: 0.7191\n",
      "Epoch 35: val_loss improved from 0.56288 to 0.56274, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5633 - accuracy: 0.7186 - val_loss: 0.5627 - val_accuracy: 0.7205\n",
      "Epoch 36/60\n",
      "653/682 [===========================>..] - ETA: 0s - loss: 0.5645 - accuracy: 0.7159\n",
      "Epoch 36: val_loss improved from 0.56274 to 0.56267, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5632 - accuracy: 0.7175 - val_loss: 0.5627 - val_accuracy: 0.7199\n",
      "Epoch 37/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.5633 - accuracy: 0.7175\n",
      "Epoch 37: val_loss improved from 0.56267 to 0.56259, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5630 - accuracy: 0.7179 - val_loss: 0.5626 - val_accuracy: 0.7223\n",
      "Epoch 38/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.5620 - accuracy: 0.7210\n",
      "Epoch 38: val_loss improved from 0.56259 to 0.56244, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5629 - accuracy: 0.7204 - val_loss: 0.5624 - val_accuracy: 0.7223\n",
      "Epoch 39/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.5623 - accuracy: 0.7196\n",
      "Epoch 39: val_loss improved from 0.56244 to 0.56208, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5627 - accuracy: 0.7189 - val_loss: 0.5621 - val_accuracy: 0.7217\n",
      "Epoch 40/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.5619 - accuracy: 0.7186\n",
      "Epoch 40: val_loss did not improve from 0.56208\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5628 - accuracy: 0.7185 - val_loss: 0.5624 - val_accuracy: 0.7187\n",
      "Epoch 41/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.5625 - accuracy: 0.7191\n",
      "Epoch 41: val_loss did not improve from 0.56208\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5626 - accuracy: 0.7191 - val_loss: 0.5621 - val_accuracy: 0.7217\n",
      "Epoch 42/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.5624 - accuracy: 0.7190\n",
      "Epoch 42: val_loss did not improve from 0.56208\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5624 - accuracy: 0.7186 - val_loss: 0.5621 - val_accuracy: 0.7205\n",
      "Epoch 43/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.5632 - accuracy: 0.7185\n",
      "Epoch 43: val_loss did not improve from 0.56208\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5625 - accuracy: 0.7189 - val_loss: 0.5622 - val_accuracy: 0.7205\n",
      "Epoch 44/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.5627 - accuracy: 0.7192\n",
      "Epoch 44: val_loss improved from 0.56208 to 0.56189, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5623 - accuracy: 0.7199 - val_loss: 0.5619 - val_accuracy: 0.7211\n",
      "Epoch 45/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.5619 - accuracy: 0.7213\n",
      "Epoch 45: val_loss did not improve from 0.56189\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5621 - accuracy: 0.7205 - val_loss: 0.5619 - val_accuracy: 0.7199\n",
      "Epoch 46/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.5628 - accuracy: 0.7186\n",
      "Epoch 46: val_loss improved from 0.56189 to 0.56178, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5623 - accuracy: 0.7189 - val_loss: 0.5618 - val_accuracy: 0.7211\n",
      "Epoch 47/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.5622 - accuracy: 0.7199\n",
      "Epoch 47: val_loss improved from 0.56178 to 0.56169, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5621 - accuracy: 0.7197 - val_loss: 0.5617 - val_accuracy: 0.7228\n",
      "Epoch 48/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.5617 - accuracy: 0.7193\n",
      "Epoch 48: val_loss did not improve from 0.56169\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5622 - accuracy: 0.7192 - val_loss: 0.5618 - val_accuracy: 0.7193\n",
      "Epoch 49/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.5625 - accuracy: 0.7195\n",
      "Epoch 49: val_loss did not improve from 0.56169\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5620 - accuracy: 0.7191 - val_loss: 0.5618 - val_accuracy: 0.7205\n",
      "Epoch 50/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.5617 - accuracy: 0.7202\n",
      "Epoch 50: val_loss did not improve from 0.56169\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5619 - accuracy: 0.7205 - val_loss: 0.5620 - val_accuracy: 0.7223\n",
      "Epoch 51/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.5627 - accuracy: 0.7200\n",
      "Epoch 51: val_loss improved from 0.56169 to 0.56156, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5621 - accuracy: 0.7199 - val_loss: 0.5616 - val_accuracy: 0.7205\n",
      "Epoch 52/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.5620 - accuracy: 0.7189\n",
      "Epoch 52: val_loss did not improve from 0.56156\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7197 - val_loss: 0.5623 - val_accuracy: 0.7211\n",
      "Epoch 53/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.5611 - accuracy: 0.7222\n",
      "Epoch 53: val_loss improved from 0.56156 to 0.56155, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5619 - accuracy: 0.7214 - val_loss: 0.5615 - val_accuracy: 0.7205\n",
      "Epoch 54/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.5609 - accuracy: 0.7204\n",
      "Epoch 54: val_loss did not improve from 0.56155\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7197 - val_loss: 0.5617 - val_accuracy: 0.7217\n",
      "Epoch 55/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.5619 - accuracy: 0.7193\n",
      "Epoch 55: val_loss did not improve from 0.56155\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7199 - val_loss: 0.5616 - val_accuracy: 0.7205\n",
      "Epoch 56/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.5619 - accuracy: 0.7198\n",
      "Epoch 56: val_loss improved from 0.56155 to 0.56150, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5618 - accuracy: 0.7199 - val_loss: 0.5615 - val_accuracy: 0.7205\n",
      "Epoch 57/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.5612 - accuracy: 0.7214\n",
      "Epoch 57: val_loss improved from 0.56150 to 0.56141, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5616 - accuracy: 0.7208 - val_loss: 0.5614 - val_accuracy: 0.7193\n",
      "Epoch 58/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.5618 - accuracy: 0.7201\n",
      "Epoch 58: val_loss did not improve from 0.56141\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7202 - val_loss: 0.5615 - val_accuracy: 0.7187\n",
      "Epoch 59/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.5616 - accuracy: 0.7217\n",
      "Epoch 59: val_loss did not improve from 0.56141\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5616 - accuracy: 0.7217 - val_loss: 0.5617 - val_accuracy: 0.7193\n",
      "Epoch 60/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.5611 - accuracy: 0.7210\n",
      "Epoch 60: val_loss did not improve from 0.56141\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5616 - accuracy: 0.7207 - val_loss: 0.5617 - val_accuracy: 0.7187\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.5614 - accuracy: 0.7193\n",
      "Drop Feature entrepreneur\n",
      "data frame x has shape: (8516, 19)\n",
      "['education', 'housing', 'campaign', 'pdays', 'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'divorced', 'married', 'single', 'admin.', 'blue-collar', 'retired', 'self-employed', 'services', 'student']\n",
      "Epoch 1/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.7075 - accuracy: 0.5178\n",
      "Epoch 1: val_loss improved from inf to 0.65347, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 3ms/step - loss: 0.7056 - accuracy: 0.5214 - val_loss: 0.6535 - val_accuracy: 0.6301\n",
      "Epoch 2/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.6346 - accuracy: 0.6581\n",
      "Epoch 2: val_loss improved from 0.65347 to 0.61037, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6346 - accuracy: 0.6582 - val_loss: 0.6104 - val_accuracy: 0.7129\n",
      "Epoch 3/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.6054 - accuracy: 0.6961\n",
      "Epoch 3: val_loss improved from 0.61037 to 0.59064, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6046 - accuracy: 0.6968 - val_loss: 0.5906 - val_accuracy: 0.7223\n",
      "Epoch 4/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.5904 - accuracy: 0.7072\n",
      "Epoch 4: val_loss improved from 0.59064 to 0.58080, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5902 - accuracy: 0.7079 - val_loss: 0.5808 - val_accuracy: 0.7240\n",
      "Epoch 5/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.5820 - accuracy: 0.7105\n",
      "Epoch 5: val_loss improved from 0.58080 to 0.57592, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5825 - accuracy: 0.7104 - val_loss: 0.5759 - val_accuracy: 0.7252\n",
      "Epoch 6/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.5791 - accuracy: 0.7080\n",
      "Epoch 6: val_loss improved from 0.57592 to 0.57298, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5782 - accuracy: 0.7100 - val_loss: 0.5730 - val_accuracy: 0.7264\n",
      "Epoch 7/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.5756 - accuracy: 0.7122\n",
      "Epoch 7: val_loss improved from 0.57298 to 0.57192, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5752 - accuracy: 0.7126 - val_loss: 0.5719 - val_accuracy: 0.7205\n",
      "Epoch 8/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.5737 - accuracy: 0.7118\n",
      "Epoch 8: val_loss improved from 0.57192 to 0.57011, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5737 - accuracy: 0.7119 - val_loss: 0.5701 - val_accuracy: 0.7228\n",
      "Epoch 9/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.5740 - accuracy: 0.7107\n",
      "Epoch 9: val_loss improved from 0.57011 to 0.56920, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5723 - accuracy: 0.7123 - val_loss: 0.5692 - val_accuracy: 0.7223\n",
      "Epoch 10/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.5715 - accuracy: 0.7129\n",
      "Epoch 10: val_loss improved from 0.56920 to 0.56861, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5712 - accuracy: 0.7129 - val_loss: 0.5686 - val_accuracy: 0.7217\n",
      "Epoch 11/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.5695 - accuracy: 0.7140\n",
      "Epoch 11: val_loss improved from 0.56861 to 0.56844, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5703 - accuracy: 0.7129 - val_loss: 0.5684 - val_accuracy: 0.7252\n",
      "Epoch 12/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.5691 - accuracy: 0.7145\n",
      "Epoch 12: val_loss improved from 0.56844 to 0.56762, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5696 - accuracy: 0.7138 - val_loss: 0.5676 - val_accuracy: 0.7181\n",
      "Epoch 13/60\n",
      "653/682 [===========================>..] - ETA: 0s - loss: 0.5677 - accuracy: 0.7156\n",
      "Epoch 13: val_loss improved from 0.56762 to 0.56710, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5691 - accuracy: 0.7145 - val_loss: 0.5671 - val_accuracy: 0.7211\n",
      "Epoch 14/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.5692 - accuracy: 0.7123\n",
      "Epoch 14: val_loss improved from 0.56710 to 0.56683, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5684 - accuracy: 0.7128 - val_loss: 0.5668 - val_accuracy: 0.7234\n",
      "Epoch 15/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.5677 - accuracy: 0.7145\n",
      "Epoch 15: val_loss improved from 0.56683 to 0.56638, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5679 - accuracy: 0.7141 - val_loss: 0.5664 - val_accuracy: 0.7187\n",
      "Epoch 16/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.5672 - accuracy: 0.7129\n",
      "Epoch 16: val_loss improved from 0.56638 to 0.56611, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5674 - accuracy: 0.7132 - val_loss: 0.5661 - val_accuracy: 0.7187\n",
      "Epoch 17/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.5675 - accuracy: 0.7132\n",
      "Epoch 17: val_loss improved from 0.56611 to 0.56592, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5670 - accuracy: 0.7135 - val_loss: 0.5659 - val_accuracy: 0.7176\n",
      "Epoch 18/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.5656 - accuracy: 0.7133\n",
      "Epoch 18: val_loss improved from 0.56592 to 0.56542, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5666 - accuracy: 0.7126 - val_loss: 0.5654 - val_accuracy: 0.7181\n",
      "Epoch 19/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.5665 - accuracy: 0.7127\n",
      "Epoch 19: val_loss improved from 0.56542 to 0.56505, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5663 - accuracy: 0.7129 - val_loss: 0.5650 - val_accuracy: 0.7211\n",
      "Epoch 20/60\n",
      "653/682 [===========================>..] - ETA: 0s - loss: 0.5651 - accuracy: 0.7139\n",
      "Epoch 20: val_loss improved from 0.56505 to 0.56497, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5659 - accuracy: 0.7133 - val_loss: 0.5650 - val_accuracy: 0.7205\n",
      "Epoch 21/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.5653 - accuracy: 0.7151\n",
      "Epoch 21: val_loss did not improve from 0.56497\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5653 - accuracy: 0.7154 - val_loss: 0.5650 - val_accuracy: 0.7240\n",
      "Epoch 22/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.5652 - accuracy: 0.7124\n",
      "Epoch 22: val_loss improved from 0.56497 to 0.56466, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5653 - accuracy: 0.7125 - val_loss: 0.5647 - val_accuracy: 0.7228\n",
      "Epoch 23/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.5650 - accuracy: 0.7147\n",
      "Epoch 23: val_loss improved from 0.56466 to 0.56426, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5650 - accuracy: 0.7147 - val_loss: 0.5643 - val_accuracy: 0.7193\n",
      "Epoch 24/60\n",
      "654/682 [===========================>..] - ETA: 0s - loss: 0.5638 - accuracy: 0.7148\n",
      "Epoch 24: val_loss improved from 0.56426 to 0.56422, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5648 - accuracy: 0.7136 - val_loss: 0.5642 - val_accuracy: 0.7211\n",
      "Epoch 25/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.5647 - accuracy: 0.7155\n",
      "Epoch 25: val_loss improved from 0.56422 to 0.56378, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5646 - accuracy: 0.7154 - val_loss: 0.5638 - val_accuracy: 0.7193\n",
      "Epoch 26/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.5634 - accuracy: 0.7167\n",
      "Epoch 26: val_loss improved from 0.56378 to 0.56360, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5645 - accuracy: 0.7161 - val_loss: 0.5636 - val_accuracy: 0.7217\n",
      "Epoch 27/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.5639 - accuracy: 0.7188\n",
      "Epoch 27: val_loss did not improve from 0.56360\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5640 - accuracy: 0.7186 - val_loss: 0.5640 - val_accuracy: 0.7187\n",
      "Epoch 28/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.5648 - accuracy: 0.7156\n",
      "Epoch 28: val_loss improved from 0.56360 to 0.56332, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5643 - accuracy: 0.7163 - val_loss: 0.5633 - val_accuracy: 0.7205\n",
      "Epoch 29/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.5638 - accuracy: 0.7165\n",
      "Epoch 29: val_loss improved from 0.56332 to 0.56304, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5639 - accuracy: 0.7163 - val_loss: 0.5630 - val_accuracy: 0.7205\n",
      "Epoch 30/60\n",
      "652/682 [===========================>..] - ETA: 0s - loss: 0.5625 - accuracy: 0.7170\n",
      "Epoch 30: val_loss did not improve from 0.56304\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5631 - accuracy: 0.7175 - val_loss: 0.5642 - val_accuracy: 0.7258\n",
      "Epoch 31/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.5646 - accuracy: 0.7190\n",
      "Epoch 31: val_loss improved from 0.56304 to 0.56275, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5639 - accuracy: 0.7197 - val_loss: 0.5627 - val_accuracy: 0.7211\n",
      "Epoch 32/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.5637 - accuracy: 0.7184\n",
      "Epoch 32: val_loss improved from 0.56275 to 0.56265, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5634 - accuracy: 0.7191 - val_loss: 0.5626 - val_accuracy: 0.7217\n",
      "Epoch 33/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.5633 - accuracy: 0.7181\n",
      "Epoch 33: val_loss improved from 0.56265 to 0.56263, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5633 - accuracy: 0.7180 - val_loss: 0.5626 - val_accuracy: 0.7211\n",
      "Epoch 34/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.5630 - accuracy: 0.7195\n",
      "Epoch 34: val_loss did not improve from 0.56263\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5630 - accuracy: 0.7194 - val_loss: 0.5628 - val_accuracy: 0.7223\n",
      "Epoch 35/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.5645 - accuracy: 0.7182\n",
      "Epoch 35: val_loss improved from 0.56263 to 0.56232, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5631 - accuracy: 0.7189 - val_loss: 0.5623 - val_accuracy: 0.7217\n",
      "Epoch 36/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.5630 - accuracy: 0.7192\n",
      "Epoch 36: val_loss improved from 0.56232 to 0.56225, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5630 - accuracy: 0.7197 - val_loss: 0.5623 - val_accuracy: 0.7223\n",
      "Epoch 37/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.5633 - accuracy: 0.7190\n",
      "Epoch 37: val_loss did not improve from 0.56225\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5627 - accuracy: 0.7198 - val_loss: 0.5630 - val_accuracy: 0.7193\n",
      "Epoch 38/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.5626 - accuracy: 0.7165\n",
      "Epoch 38: val_loss did not improve from 0.56225\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5626 - accuracy: 0.7172 - val_loss: 0.5626 - val_accuracy: 0.7223\n",
      "Epoch 39/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.5615 - accuracy: 0.7200\n",
      "Epoch 39: val_loss did not improve from 0.56225\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5625 - accuracy: 0.7185 - val_loss: 0.5627 - val_accuracy: 0.7234\n",
      "Epoch 40/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.5615 - accuracy: 0.7205\n",
      "Epoch 40: val_loss improved from 0.56225 to 0.56215, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5626 - accuracy: 0.7198 - val_loss: 0.5621 - val_accuracy: 0.7223\n",
      "Epoch 41/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.5620 - accuracy: 0.7204\n",
      "Epoch 41: val_loss did not improve from 0.56215\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5625 - accuracy: 0.7195 - val_loss: 0.5622 - val_accuracy: 0.7199\n",
      "Epoch 42/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.5631 - accuracy: 0.7174\n",
      "Epoch 42: val_loss improved from 0.56215 to 0.56194, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5624 - accuracy: 0.7182 - val_loss: 0.5619 - val_accuracy: 0.7223\n",
      "Epoch 43/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.5618 - accuracy: 0.7197\n",
      "Epoch 43: val_loss improved from 0.56194 to 0.56190, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5623 - accuracy: 0.7195 - val_loss: 0.5619 - val_accuracy: 0.7205\n",
      "Epoch 44/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.5616 - accuracy: 0.7196\n",
      "Epoch 44: val_loss improved from 0.56190 to 0.56188, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5620 - accuracy: 0.7191 - val_loss: 0.5619 - val_accuracy: 0.7211\n",
      "Epoch 45/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.5617 - accuracy: 0.7190\n",
      "Epoch 45: val_loss did not improve from 0.56188\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5621 - accuracy: 0.7186 - val_loss: 0.5620 - val_accuracy: 0.7187\n",
      "Epoch 46/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.5635 - accuracy: 0.7194\n",
      "Epoch 46: val_loss improved from 0.56188 to 0.56179, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5620 - accuracy: 0.7205 - val_loss: 0.5618 - val_accuracy: 0.7199\n",
      "Epoch 47/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.5622 - accuracy: 0.7191\n",
      "Epoch 47: val_loss improved from 0.56179 to 0.56173, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5621 - accuracy: 0.7195 - val_loss: 0.5617 - val_accuracy: 0.7217\n",
      "Epoch 48/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.5623 - accuracy: 0.7187\n",
      "Epoch 48: val_loss improved from 0.56173 to 0.56161, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5620 - accuracy: 0.7192 - val_loss: 0.5616 - val_accuracy: 0.7223\n",
      "Epoch 49/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.5614 - accuracy: 0.7209\n",
      "Epoch 49: val_loss improved from 0.56161 to 0.56159, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5619 - accuracy: 0.7201 - val_loss: 0.5616 - val_accuracy: 0.7211\n",
      "Epoch 50/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.5642 - accuracy: 0.7179\n",
      "Epoch 50: val_loss improved from 0.56159 to 0.56155, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5619 - accuracy: 0.7199 - val_loss: 0.5615 - val_accuracy: 0.7234\n",
      "Epoch 51/60\n",
      "648/682 [===========================>..] - ETA: 0s - loss: 0.5609 - accuracy: 0.7196\n",
      "Epoch 51: val_loss did not improve from 0.56155\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5619 - accuracy: 0.7198 - val_loss: 0.5616 - val_accuracy: 0.7211\n",
      "Epoch 52/60\n",
      "652/682 [===========================>..] - ETA: 0s - loss: 0.5627 - accuracy: 0.7198\n",
      "Epoch 52: val_loss did not improve from 0.56155\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5618 - accuracy: 0.7204 - val_loss: 0.5617 - val_accuracy: 0.7187\n",
      "Epoch 53/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.5623 - accuracy: 0.7210\n",
      "Epoch 53: val_loss improved from 0.56155 to 0.56146, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5616 - accuracy: 0.7216 - val_loss: 0.5615 - val_accuracy: 0.7205\n",
      "Epoch 54/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.5625 - accuracy: 0.7192\n",
      "Epoch 54: val_loss improved from 0.56146 to 0.56139, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7199 - val_loss: 0.5614 - val_accuracy: 0.7223\n",
      "Epoch 55/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.5619 - accuracy: 0.7221\n",
      "Epoch 55: val_loss did not improve from 0.56139\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7217 - val_loss: 0.5618 - val_accuracy: 0.7205\n",
      "Epoch 56/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.5631 - accuracy: 0.7194\n",
      "Epoch 56: val_loss improved from 0.56139 to 0.56138, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5616 - accuracy: 0.7207 - val_loss: 0.5614 - val_accuracy: 0.7199\n",
      "Epoch 57/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.5619 - accuracy: 0.7199\n",
      "Epoch 57: val_loss improved from 0.56138 to 0.56126, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5616 - accuracy: 0.7202 - val_loss: 0.5613 - val_accuracy: 0.7187\n",
      "Epoch 58/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.5617 - accuracy: 0.7224\n",
      "Epoch 58: val_loss did not improve from 0.56126\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5617 - accuracy: 0.7224 - val_loss: 0.5613 - val_accuracy: 0.7240\n",
      "Epoch 59/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.5627 - accuracy: 0.7195\n",
      "Epoch 59: val_loss improved from 0.56126 to 0.56122, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5616 - accuracy: 0.7207 - val_loss: 0.5612 - val_accuracy: 0.7205\n",
      "Epoch 60/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.5607 - accuracy: 0.7215\n",
      "Epoch 60: val_loss did not improve from 0.56122\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5615 - accuracy: 0.7205 - val_loss: 0.5614 - val_accuracy: 0.7193\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.5612 - accuracy: 0.7205\n",
      "Drop Feature self-employed\n",
      "data frame x has shape: (8516, 18)\n",
      "['education', 'housing', 'campaign', 'pdays', 'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'divorced', 'married', 'single', 'admin.', 'blue-collar', 'retired', 'services', 'student']\n",
      "Epoch 1/60\n",
      "652/682 [===========================>..] - ETA: 0s - loss: 0.6512 - accuracy: 0.6426\n",
      "Epoch 1: val_loss improved from inf to 0.61886, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 3ms/step - loss: 0.6497 - accuracy: 0.6454 - val_loss: 0.6189 - val_accuracy: 0.7129\n",
      "Epoch 2/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.6063 - accuracy: 0.7068\n",
      "Epoch 2: val_loss improved from 0.61886 to 0.59236, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6068 - accuracy: 0.7060 - val_loss: 0.5924 - val_accuracy: 0.7234\n",
      "Epoch 3/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.5895 - accuracy: 0.7085\n",
      "Epoch 3: val_loss improved from 0.59236 to 0.58149, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5895 - accuracy: 0.7085 - val_loss: 0.5815 - val_accuracy: 0.7211\n",
      "Epoch 4/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.5815 - accuracy: 0.7108\n",
      "Epoch 4: val_loss improved from 0.58149 to 0.57590, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5812 - accuracy: 0.7106 - val_loss: 0.5759 - val_accuracy: 0.7187\n",
      "Epoch 5/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.5772 - accuracy: 0.7104\n",
      "Epoch 5: val_loss improved from 0.57590 to 0.57290, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5769 - accuracy: 0.7106 - val_loss: 0.5729 - val_accuracy: 0.7176\n",
      "Epoch 6/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.5735 - accuracy: 0.7120\n",
      "Epoch 6: val_loss improved from 0.57290 to 0.57114, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5743 - accuracy: 0.7116 - val_loss: 0.5711 - val_accuracy: 0.7176\n",
      "Epoch 7/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.5726 - accuracy: 0.7118\n",
      "Epoch 7: val_loss improved from 0.57114 to 0.57002, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5726 - accuracy: 0.7114 - val_loss: 0.5700 - val_accuracy: 0.7176\n",
      "Epoch 8/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.5710 - accuracy: 0.7121\n",
      "Epoch 8: val_loss improved from 0.57002 to 0.56904, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5714 - accuracy: 0.7113 - val_loss: 0.5690 - val_accuracy: 0.7176\n",
      "Epoch 9/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.5722 - accuracy: 0.7092\n",
      "Epoch 9: val_loss improved from 0.56904 to 0.56854, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5705 - accuracy: 0.7111 - val_loss: 0.5685 - val_accuracy: 0.7170\n",
      "Epoch 10/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.5698 - accuracy: 0.7115\n",
      "Epoch 10: val_loss improved from 0.56854 to 0.56807, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5696 - accuracy: 0.7116 - val_loss: 0.5681 - val_accuracy: 0.7146\n",
      "Epoch 11/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.5684 - accuracy: 0.7116\n",
      "Epoch 11: val_loss improved from 0.56807 to 0.56784, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5689 - accuracy: 0.7111 - val_loss: 0.5678 - val_accuracy: 0.7152\n",
      "Epoch 12/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.5691 - accuracy: 0.7113\n",
      "Epoch 12: val_loss improved from 0.56784 to 0.56718, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5685 - accuracy: 0.7120 - val_loss: 0.5672 - val_accuracy: 0.7164\n",
      "Epoch 13/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.5680 - accuracy: 0.7111\n",
      "Epoch 13: val_loss improved from 0.56718 to 0.56705, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5680 - accuracy: 0.7111 - val_loss: 0.5671 - val_accuracy: 0.7170\n",
      "Epoch 14/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.5663 - accuracy: 0.7137\n",
      "Epoch 14: val_loss improved from 0.56705 to 0.56679, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5675 - accuracy: 0.7128 - val_loss: 0.5668 - val_accuracy: 0.7146\n",
      "Epoch 15/60\n",
      "652/682 [===========================>..] - ETA: 0s - loss: 0.5663 - accuracy: 0.7124\n",
      "Epoch 15: val_loss improved from 0.56679 to 0.56605, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5671 - accuracy: 0.7116 - val_loss: 0.5660 - val_accuracy: 0.7164\n",
      "Epoch 16/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.5676 - accuracy: 0.7110\n",
      "Epoch 16: val_loss did not improve from 0.56605\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5667 - accuracy: 0.7122 - val_loss: 0.5665 - val_accuracy: 0.7146\n",
      "Epoch 17/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.5663 - accuracy: 0.7117\n",
      "Epoch 17: val_loss did not improve from 0.56605\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5662 - accuracy: 0.7119 - val_loss: 0.5661 - val_accuracy: 0.7205\n",
      "Epoch 18/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.5675 - accuracy: 0.7129\n",
      "Epoch 18: val_loss improved from 0.56605 to 0.56516, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5662 - accuracy: 0.7138 - val_loss: 0.5652 - val_accuracy: 0.7170\n",
      "Epoch 19/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.5651 - accuracy: 0.7137\n",
      "Epoch 19: val_loss improved from 0.56516 to 0.56510, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5655 - accuracy: 0.7135 - val_loss: 0.5651 - val_accuracy: 0.7199\n",
      "Epoch 20/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.5654 - accuracy: 0.7131\n",
      "Epoch 20: val_loss improved from 0.56510 to 0.56474, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5655 - accuracy: 0.7128 - val_loss: 0.5647 - val_accuracy: 0.7199\n",
      "Epoch 21/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.5644 - accuracy: 0.7151\n",
      "Epoch 21: val_loss improved from 0.56474 to 0.56447, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5653 - accuracy: 0.7136 - val_loss: 0.5645 - val_accuracy: 0.7193\n",
      "Epoch 22/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.5640 - accuracy: 0.7155\n",
      "Epoch 22: val_loss improved from 0.56447 to 0.56423, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5650 - accuracy: 0.7141 - val_loss: 0.5642 - val_accuracy: 0.7181\n",
      "Epoch 23/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.5650 - accuracy: 0.7154\n",
      "Epoch 23: val_loss improved from 0.56423 to 0.56401, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5647 - accuracy: 0.7155 - val_loss: 0.5640 - val_accuracy: 0.7187\n",
      "Epoch 24/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.5636 - accuracy: 0.7159\n",
      "Epoch 24: val_loss improved from 0.56401 to 0.56374, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5645 - accuracy: 0.7148 - val_loss: 0.5637 - val_accuracy: 0.7187\n",
      "Epoch 25/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.5636 - accuracy: 0.7148\n",
      "Epoch 25: val_loss did not improve from 0.56374\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5642 - accuracy: 0.7147 - val_loss: 0.5641 - val_accuracy: 0.7217\n",
      "Epoch 26/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.5634 - accuracy: 0.7164\n",
      "Epoch 26: val_loss improved from 0.56374 to 0.56362, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5643 - accuracy: 0.7155 - val_loss: 0.5636 - val_accuracy: 0.7205\n",
      "Epoch 27/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.5628 - accuracy: 0.7174\n",
      "Epoch 27: val_loss improved from 0.56362 to 0.56333, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5639 - accuracy: 0.7175 - val_loss: 0.5633 - val_accuracy: 0.7181\n",
      "Epoch 28/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.5635 - accuracy: 0.7169\n",
      "Epoch 28: val_loss improved from 0.56333 to 0.56326, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5639 - accuracy: 0.7166 - val_loss: 0.5633 - val_accuracy: 0.7211\n",
      "Epoch 29/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.5634 - accuracy: 0.7170\n",
      "Epoch 29: val_loss improved from 0.56326 to 0.56306, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5636 - accuracy: 0.7169 - val_loss: 0.5631 - val_accuracy: 0.7187\n",
      "Epoch 30/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.5635 - accuracy: 0.7179\n",
      "Epoch 30: val_loss improved from 0.56306 to 0.56296, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5635 - accuracy: 0.7179 - val_loss: 0.5630 - val_accuracy: 0.7199\n",
      "Epoch 31/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.5617 - accuracy: 0.7177\n",
      "Epoch 31: val_loss did not improve from 0.56296\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5634 - accuracy: 0.7166 - val_loss: 0.5630 - val_accuracy: 0.7187\n",
      "Epoch 32/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.5633 - accuracy: 0.7192\n",
      "Epoch 32: val_loss improved from 0.56296 to 0.56278, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5633 - accuracy: 0.7192 - val_loss: 0.5628 - val_accuracy: 0.7187\n",
      "Epoch 33/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.5626 - accuracy: 0.7188\n",
      "Epoch 33: val_loss improved from 0.56278 to 0.56270, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5632 - accuracy: 0.7180 - val_loss: 0.5627 - val_accuracy: 0.7193\n",
      "Epoch 34/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.5634 - accuracy: 0.7175\n",
      "Epoch 34: val_loss improved from 0.56270 to 0.56246, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5630 - accuracy: 0.7173 - val_loss: 0.5625 - val_accuracy: 0.7205\n",
      "Epoch 35/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.5629 - accuracy: 0.7190\n",
      "Epoch 35: val_loss did not improve from 0.56246\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5628 - accuracy: 0.7191 - val_loss: 0.5625 - val_accuracy: 0.7199\n",
      "Epoch 36/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.5638 - accuracy: 0.7179\n",
      "Epoch 36: val_loss did not improve from 0.56246\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5627 - accuracy: 0.7191 - val_loss: 0.5625 - val_accuracy: 0.7228\n",
      "Epoch 37/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.5612 - accuracy: 0.7213\n",
      "Epoch 37: val_loss improved from 0.56246 to 0.56235, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5623 - accuracy: 0.7204 - val_loss: 0.5624 - val_accuracy: 0.7193\n",
      "Epoch 38/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.5624 - accuracy: 0.7184\n",
      "Epoch 38: val_loss did not improve from 0.56235\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5624 - accuracy: 0.7180 - val_loss: 0.5629 - val_accuracy: 0.7223\n",
      "Epoch 39/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.5620 - accuracy: 0.7187\n",
      "Epoch 39: val_loss improved from 0.56235 to 0.56226, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5626 - accuracy: 0.7182 - val_loss: 0.5623 - val_accuracy: 0.7193\n",
      "Epoch 40/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.5632 - accuracy: 0.7171\n",
      "Epoch 40: val_loss improved from 0.56226 to 0.56203, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5625 - accuracy: 0.7183 - val_loss: 0.5620 - val_accuracy: 0.7199\n",
      "Epoch 41/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.5620 - accuracy: 0.7185\n",
      "Epoch 41: val_loss did not improve from 0.56203\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5623 - accuracy: 0.7183 - val_loss: 0.5620 - val_accuracy: 0.7193\n",
      "Epoch 42/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.5626 - accuracy: 0.7189\n",
      "Epoch 42: val_loss improved from 0.56203 to 0.56186, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5623 - accuracy: 0.7191 - val_loss: 0.5619 - val_accuracy: 0.7240\n",
      "Epoch 43/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.5624 - accuracy: 0.7172\n",
      "Epoch 43: val_loss did not improve from 0.56186\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5621 - accuracy: 0.7182 - val_loss: 0.5623 - val_accuracy: 0.7205\n",
      "Epoch 44/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.5620 - accuracy: 0.7208\n",
      "Epoch 44: val_loss improved from 0.56186 to 0.56178, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5622 - accuracy: 0.7197 - val_loss: 0.5618 - val_accuracy: 0.7217\n",
      "Epoch 45/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.5621 - accuracy: 0.7195\n",
      "Epoch 45: val_loss did not improve from 0.56178\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5619 - accuracy: 0.7199 - val_loss: 0.5619 - val_accuracy: 0.7193\n",
      "Epoch 46/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.5621 - accuracy: 0.7189\n",
      "Epoch 46: val_loss improved from 0.56178 to 0.56174, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5620 - accuracy: 0.7192 - val_loss: 0.5617 - val_accuracy: 0.7211\n",
      "Epoch 47/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.5614 - accuracy: 0.7198\n",
      "Epoch 47: val_loss improved from 0.56174 to 0.56167, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5620 - accuracy: 0.7188 - val_loss: 0.5617 - val_accuracy: 0.7228\n",
      "Epoch 48/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.5627 - accuracy: 0.7193\n",
      "Epoch 48: val_loss did not improve from 0.56167\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5619 - accuracy: 0.7207 - val_loss: 0.5617 - val_accuracy: 0.7205\n",
      "Epoch 49/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.5624 - accuracy: 0.7202\n",
      "Epoch 49: val_loss did not improve from 0.56167\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7202 - val_loss: 0.5622 - val_accuracy: 0.7240\n",
      "Epoch 50/60\n",
      "649/682 [===========================>..] - ETA: 0s - loss: 0.5618 - accuracy: 0.7216\n",
      "Epoch 50: val_loss improved from 0.56167 to 0.56156, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5619 - accuracy: 0.7210 - val_loss: 0.5616 - val_accuracy: 0.7234\n",
      "Epoch 51/60\n",
      "650/682 [===========================>..] - ETA: 0s - loss: 0.5624 - accuracy: 0.7200\n",
      "Epoch 51: val_loss did not improve from 0.56156\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5618 - accuracy: 0.7198 - val_loss: 0.5617 - val_accuracy: 0.7199\n",
      "Epoch 52/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.5615 - accuracy: 0.7205\n",
      "Epoch 52: val_loss improved from 0.56156 to 0.56150, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5618 - accuracy: 0.7207 - val_loss: 0.5615 - val_accuracy: 0.7205\n",
      "Epoch 53/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.5618 - accuracy: 0.7218\n",
      "Epoch 53: val_loss did not improve from 0.56150\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5618 - accuracy: 0.7219 - val_loss: 0.5616 - val_accuracy: 0.7205\n",
      "Epoch 54/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.5618 - accuracy: 0.7201\n",
      "Epoch 54: val_loss did not improve from 0.56150\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5616 - accuracy: 0.7198 - val_loss: 0.5617 - val_accuracy: 0.7199\n",
      "Epoch 55/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.5621 - accuracy: 0.7202\n",
      "Epoch 55: val_loss improved from 0.56150 to 0.56146, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5616 - accuracy: 0.7214 - val_loss: 0.5615 - val_accuracy: 0.7205\n",
      "Epoch 56/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.5617 - accuracy: 0.7206\n",
      "Epoch 56: val_loss did not improve from 0.56146\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5616 - accuracy: 0.7207 - val_loss: 0.5615 - val_accuracy: 0.7211\n",
      "Epoch 57/60\n",
      "654/682 [===========================>..] - ETA: 0s - loss: 0.5628 - accuracy: 0.7176\n",
      "Epoch 57: val_loss improved from 0.56146 to 0.56136, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5615 - accuracy: 0.7189 - val_loss: 0.5614 - val_accuracy: 0.7228\n",
      "Epoch 58/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.5600 - accuracy: 0.7207\n",
      "Epoch 58: val_loss did not improve from 0.56136\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5615 - accuracy: 0.7194 - val_loss: 0.5615 - val_accuracy: 0.7205\n",
      "Epoch 59/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.5618 - accuracy: 0.7202\n",
      "Epoch 59: val_loss did not improve from 0.56136\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5615 - accuracy: 0.7213 - val_loss: 0.5614 - val_accuracy: 0.7205\n",
      "Epoch 60/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.5605 - accuracy: 0.7217\n",
      "Epoch 60: val_loss improved from 0.56136 to 0.56133, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5614 - accuracy: 0.7210 - val_loss: 0.5613 - val_accuracy: 0.7211\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.5613 - accuracy: 0.7211\n",
      "Drop Feature divorced\n",
      "data frame x has shape: (8516, 17)\n",
      "['education', 'housing', 'campaign', 'pdays', 'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'married', 'single', 'admin.', 'blue-collar', 'retired', 'services', 'student']\n",
      "Epoch 1/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.7100 - accuracy: 0.5559\n",
      "Epoch 1: val_loss improved from inf to 0.64562, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.7076 - accuracy: 0.5591 - val_loss: 0.6456 - val_accuracy: 0.6424\n",
      "Epoch 2/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.6220 - accuracy: 0.6774\n",
      "Epoch 2: val_loss improved from 0.64562 to 0.60156, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6211 - accuracy: 0.6786 - val_loss: 0.6016 - val_accuracy: 0.6958\n",
      "Epoch 3/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.5932 - accuracy: 0.7036\n",
      "Epoch 3: val_loss improved from 0.60156 to 0.58359, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5929 - accuracy: 0.7044 - val_loss: 0.5836 - val_accuracy: 0.7082\n",
      "Epoch 4/60\n",
      "653/682 [===========================>..] - ETA: 0s - loss: 0.5807 - accuracy: 0.7086\n",
      "Epoch 4: val_loss improved from 0.58359 to 0.57586, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5813 - accuracy: 0.7082 - val_loss: 0.5759 - val_accuracy: 0.7111\n",
      "Epoch 5/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.5759 - accuracy: 0.7109\n",
      "Epoch 5: val_loss improved from 0.57586 to 0.57193, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5759 - accuracy: 0.7110 - val_loss: 0.5719 - val_accuracy: 0.7164\n",
      "Epoch 6/60\n",
      "653/682 [===========================>..] - ETA: 0s - loss: 0.5729 - accuracy: 0.7119\n",
      "Epoch 6: val_loss improved from 0.57193 to 0.56999, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5731 - accuracy: 0.7111 - val_loss: 0.5700 - val_accuracy: 0.7164\n",
      "Epoch 7/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.5715 - accuracy: 0.7116\n",
      "Epoch 7: val_loss improved from 0.56999 to 0.56936, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5711 - accuracy: 0.7120 - val_loss: 0.5694 - val_accuracy: 0.7140\n",
      "Epoch 8/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.5700 - accuracy: 0.7121\n",
      "Epoch 8: val_loss improved from 0.56936 to 0.56803, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5701 - accuracy: 0.7123 - val_loss: 0.5680 - val_accuracy: 0.7205\n",
      "Epoch 9/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.5692 - accuracy: 0.7139\n",
      "Epoch 9: val_loss improved from 0.56803 to 0.56725, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5691 - accuracy: 0.7135 - val_loss: 0.5672 - val_accuracy: 0.7205\n",
      "Epoch 10/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.5693 - accuracy: 0.7119\n",
      "Epoch 10: val_loss improved from 0.56725 to 0.56690, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5686 - accuracy: 0.7129 - val_loss: 0.5669 - val_accuracy: 0.7211\n",
      "Epoch 11/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.5681 - accuracy: 0.7136\n",
      "Epoch 11: val_loss improved from 0.56690 to 0.56658, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5679 - accuracy: 0.7139 - val_loss: 0.5666 - val_accuracy: 0.7211\n",
      "Epoch 12/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.5673 - accuracy: 0.7126\n",
      "Epoch 12: val_loss improved from 0.56658 to 0.56641, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5674 - accuracy: 0.7136 - val_loss: 0.5664 - val_accuracy: 0.7234\n",
      "Epoch 13/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.5670 - accuracy: 0.7145\n",
      "Epoch 13: val_loss improved from 0.56641 to 0.56569, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5671 - accuracy: 0.7145 - val_loss: 0.5657 - val_accuracy: 0.7199\n",
      "Epoch 14/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.5665 - accuracy: 0.7126\n",
      "Epoch 14: val_loss improved from 0.56569 to 0.56540, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5665 - accuracy: 0.7119 - val_loss: 0.5654 - val_accuracy: 0.7187\n",
      "Epoch 15/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.5662 - accuracy: 0.7149\n",
      "Epoch 15: val_loss did not improve from 0.56540\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5660 - accuracy: 0.7150 - val_loss: 0.5657 - val_accuracy: 0.7234\n",
      "Epoch 16/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.5650 - accuracy: 0.7180\n",
      "Epoch 16: val_loss improved from 0.56540 to 0.56481, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5658 - accuracy: 0.7172 - val_loss: 0.5648 - val_accuracy: 0.7217\n",
      "Epoch 17/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.5652 - accuracy: 0.7151\n",
      "Epoch 17: val_loss improved from 0.56481 to 0.56454, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5652 - accuracy: 0.7151 - val_loss: 0.5645 - val_accuracy: 0.7211\n",
      "Epoch 18/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.5635 - accuracy: 0.7182\n",
      "Epoch 18: val_loss improved from 0.56454 to 0.56432, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5651 - accuracy: 0.7164 - val_loss: 0.5643 - val_accuracy: 0.7211\n",
      "Epoch 19/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.5655 - accuracy: 0.7156\n",
      "Epoch 19: val_loss improved from 0.56432 to 0.56399, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5648 - accuracy: 0.7161 - val_loss: 0.5640 - val_accuracy: 0.7223\n",
      "Epoch 20/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.5658 - accuracy: 0.7142\n",
      "Epoch 20: val_loss improved from 0.56399 to 0.56379, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5646 - accuracy: 0.7158 - val_loss: 0.5638 - val_accuracy: 0.7205\n",
      "Epoch 21/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.5641 - accuracy: 0.7161\n",
      "Epoch 21: val_loss improved from 0.56379 to 0.56367, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5645 - accuracy: 0.7164 - val_loss: 0.5637 - val_accuracy: 0.7211\n",
      "Epoch 22/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.5632 - accuracy: 0.7178\n",
      "Epoch 22: val_loss improved from 0.56367 to 0.56352, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5641 - accuracy: 0.7173 - val_loss: 0.5635 - val_accuracy: 0.7205\n",
      "Epoch 23/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.5634 - accuracy: 0.7192\n",
      "Epoch 23: val_loss improved from 0.56352 to 0.56339, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5639 - accuracy: 0.7182 - val_loss: 0.5634 - val_accuracy: 0.7217\n",
      "Epoch 24/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.5637 - accuracy: 0.7170\n",
      "Epoch 24: val_loss did not improve from 0.56339\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5636 - accuracy: 0.7176 - val_loss: 0.5637 - val_accuracy: 0.7199\n",
      "Epoch 25/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.5640 - accuracy: 0.7176\n",
      "Epoch 25: val_loss improved from 0.56339 to 0.56313, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5638 - accuracy: 0.7175 - val_loss: 0.5631 - val_accuracy: 0.7217\n",
      "Epoch 26/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.5634 - accuracy: 0.7177\n",
      "Epoch 26: val_loss improved from 0.56313 to 0.56286, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5635 - accuracy: 0.7176 - val_loss: 0.5629 - val_accuracy: 0.7223\n",
      "Epoch 27/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.5629 - accuracy: 0.7179\n",
      "Epoch 27: val_loss did not improve from 0.56286\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5632 - accuracy: 0.7186 - val_loss: 0.5629 - val_accuracy: 0.7211\n",
      "Epoch 28/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.5635 - accuracy: 0.7195\n",
      "Epoch 28: val_loss improved from 0.56286 to 0.56268, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5630 - accuracy: 0.7189 - val_loss: 0.5627 - val_accuracy: 0.7228\n",
      "Epoch 29/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.5623 - accuracy: 0.7215\n",
      "Epoch 29: val_loss did not improve from 0.56268\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5626 - accuracy: 0.7213 - val_loss: 0.5628 - val_accuracy: 0.7211\n",
      "Epoch 30/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.5627 - accuracy: 0.7175\n",
      "Epoch 30: val_loss improved from 0.56268 to 0.56245, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5630 - accuracy: 0.7176 - val_loss: 0.5625 - val_accuracy: 0.7205\n",
      "Epoch 31/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.5638 - accuracy: 0.7173\n",
      "Epoch 31: val_loss improved from 0.56245 to 0.56235, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5628 - accuracy: 0.7182 - val_loss: 0.5624 - val_accuracy: 0.7211\n",
      "Epoch 32/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.5629 - accuracy: 0.7188\n",
      "Epoch 32: val_loss did not improve from 0.56235\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5625 - accuracy: 0.7191 - val_loss: 0.5626 - val_accuracy: 0.7205\n",
      "Epoch 33/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.5637 - accuracy: 0.7177\n",
      "Epoch 33: val_loss improved from 0.56235 to 0.56223, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5625 - accuracy: 0.7188 - val_loss: 0.5622 - val_accuracy: 0.7217\n",
      "Epoch 34/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.5632 - accuracy: 0.7189\n",
      "Epoch 34: val_loss improved from 0.56223 to 0.56208, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5624 - accuracy: 0.7191 - val_loss: 0.5621 - val_accuracy: 0.7217\n",
      "Epoch 35/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.5619 - accuracy: 0.7201\n",
      "Epoch 35: val_loss improved from 0.56208 to 0.56203, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5624 - accuracy: 0.7197 - val_loss: 0.5620 - val_accuracy: 0.7187\n",
      "Epoch 36/60\n",
      "653/682 [===========================>..] - ETA: 0s - loss: 0.5623 - accuracy: 0.7187\n",
      "Epoch 36: val_loss did not improve from 0.56203\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5624 - accuracy: 0.7189 - val_loss: 0.5620 - val_accuracy: 0.7211\n",
      "Epoch 37/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.5630 - accuracy: 0.7198\n",
      "Epoch 37: val_loss improved from 0.56203 to 0.56177, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5622 - accuracy: 0.7207 - val_loss: 0.5618 - val_accuracy: 0.7199\n",
      "Epoch 38/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.5624 - accuracy: 0.7201\n",
      "Epoch 38: val_loss did not improve from 0.56177\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5622 - accuracy: 0.7204 - val_loss: 0.5619 - val_accuracy: 0.7199\n",
      "Epoch 39/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.5621 - accuracy: 0.7195\n",
      "Epoch 39: val_loss did not improve from 0.56177\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5622 - accuracy: 0.7195 - val_loss: 0.5618 - val_accuracy: 0.7199\n",
      "Epoch 40/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.5614 - accuracy: 0.7210\n",
      "Epoch 40: val_loss did not improve from 0.56177\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5619 - accuracy: 0.7204 - val_loss: 0.5623 - val_accuracy: 0.7217\n",
      "Epoch 41/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.5622 - accuracy: 0.7200\n",
      "Epoch 41: val_loss did not improve from 0.56177\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5619 - accuracy: 0.7204 - val_loss: 0.5619 - val_accuracy: 0.7217\n",
      "Epoch 42/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.5614 - accuracy: 0.7194\n",
      "Epoch 42: val_loss improved from 0.56177 to 0.56165, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5619 - accuracy: 0.7188 - val_loss: 0.5617 - val_accuracy: 0.7199\n",
      "Epoch 43/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.5609 - accuracy: 0.7220\n",
      "Epoch 43: val_loss did not improve from 0.56165\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5618 - accuracy: 0.7210 - val_loss: 0.5617 - val_accuracy: 0.7205\n",
      "Epoch 44/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.5626 - accuracy: 0.7192\n",
      "Epoch 44: val_loss did not improve from 0.56165\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5618 - accuracy: 0.7195 - val_loss: 0.5617 - val_accuracy: 0.7217\n",
      "Epoch 45/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.5613 - accuracy: 0.7187\n",
      "Epoch 45: val_loss improved from 0.56165 to 0.56161, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7191 - val_loss: 0.5616 - val_accuracy: 0.7187\n",
      "Epoch 46/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.5599 - accuracy: 0.7204\n",
      "Epoch 46: val_loss did not improve from 0.56161\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7199 - val_loss: 0.5617 - val_accuracy: 0.7199\n",
      "Epoch 47/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.5628 - accuracy: 0.7190\n",
      "Epoch 47: val_loss improved from 0.56161 to 0.56148, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7199 - val_loss: 0.5615 - val_accuracy: 0.7211\n",
      "Epoch 48/60\n",
      "653/682 [===========================>..] - ETA: 0s - loss: 0.5617 - accuracy: 0.7198\n",
      "Epoch 48: val_loss did not improve from 0.56148\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7195 - val_loss: 0.5617 - val_accuracy: 0.7223\n",
      "Epoch 49/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.5612 - accuracy: 0.7216\n",
      "Epoch 49: val_loss improved from 0.56148 to 0.56135, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5616 - accuracy: 0.7213 - val_loss: 0.5613 - val_accuracy: 0.7193\n",
      "Epoch 50/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.5631 - accuracy: 0.7162\n",
      "Epoch 50: val_loss did not improve from 0.56135\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5614 - accuracy: 0.7189 - val_loss: 0.5618 - val_accuracy: 0.7211\n",
      "Epoch 51/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.5613 - accuracy: 0.7215\n",
      "Epoch 51: val_loss did not improve from 0.56135\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5616 - accuracy: 0.7207 - val_loss: 0.5614 - val_accuracy: 0.7211\n",
      "Epoch 52/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.5618 - accuracy: 0.7202\n",
      "Epoch 52: val_loss did not improve from 0.56135\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5615 - accuracy: 0.7202 - val_loss: 0.5614 - val_accuracy: 0.7217\n",
      "Epoch 53/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.5618 - accuracy: 0.7209\n",
      "Epoch 53: val_loss did not improve from 0.56135\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5613 - accuracy: 0.7213 - val_loss: 0.5616 - val_accuracy: 0.7187\n",
      "Epoch 54/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.5615 - accuracy: 0.7209\n",
      "Epoch 54: val_loss did not improve from 0.56135\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5616 - accuracy: 0.7208 - val_loss: 0.5614 - val_accuracy: 0.7217\n",
      "Epoch 55/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.5611 - accuracy: 0.7218\n",
      "Epoch 55: val_loss did not improve from 0.56135\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5615 - accuracy: 0.7211 - val_loss: 0.5614 - val_accuracy: 0.7205\n",
      "Epoch 56/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.5598 - accuracy: 0.7226\n",
      "Epoch 56: val_loss did not improve from 0.56135\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5613 - accuracy: 0.7216 - val_loss: 0.5615 - val_accuracy: 0.7181\n",
      "Epoch 57/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.5636 - accuracy: 0.7177\n",
      "Epoch 57: val_loss improved from 0.56135 to 0.56117, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5616 - accuracy: 0.7198 - val_loss: 0.5612 - val_accuracy: 0.7199\n",
      "Epoch 58/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.5618 - accuracy: 0.7204\n",
      "Epoch 58: val_loss improved from 0.56117 to 0.56113, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5613 - accuracy: 0.7210 - val_loss: 0.5611 - val_accuracy: 0.7199\n",
      "Epoch 59/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.5615 - accuracy: 0.7206\n",
      "Epoch 59: val_loss did not improve from 0.56113\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5612 - accuracy: 0.7210 - val_loss: 0.5612 - val_accuracy: 0.7223\n",
      "Epoch 60/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.5607 - accuracy: 0.7215\n",
      "Epoch 60: val_loss did not improve from 0.56113\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5614 - accuracy: 0.7213 - val_loss: 0.5613 - val_accuracy: 0.7205\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.5611 - accuracy: 0.7199\n",
      "Drop Feature housing\n",
      "data frame x has shape: (8516, 16)\n",
      "['education', 'campaign', 'pdays', 'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'married', 'single', 'admin.', 'blue-collar', 'retired', 'services', 'student']\n",
      "Epoch 1/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.6776 - accuracy: 0.5875\n",
      "Epoch 1: val_loss improved from inf to 0.63762, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6769 - accuracy: 0.5889 - val_loss: 0.6376 - val_accuracy: 0.6835\n",
      "Epoch 2/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.6214 - accuracy: 0.6852\n",
      "Epoch 2: val_loss improved from 0.63762 to 0.60222, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6206 - accuracy: 0.6866 - val_loss: 0.6022 - val_accuracy: 0.6999\n",
      "Epoch 3/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.5970 - accuracy: 0.7014\n",
      "Epoch 3: val_loss improved from 0.60222 to 0.58648, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5971 - accuracy: 0.7015 - val_loss: 0.5865 - val_accuracy: 0.7140\n",
      "Epoch 4/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.5859 - accuracy: 0.7066\n",
      "Epoch 4: val_loss improved from 0.58648 to 0.57935, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5862 - accuracy: 0.7067 - val_loss: 0.5794 - val_accuracy: 0.7158\n",
      "Epoch 5/60\n",
      "653/682 [===========================>..] - ETA: 0s - loss: 0.5800 - accuracy: 0.7069\n",
      "Epoch 5: val_loss improved from 0.57935 to 0.57561, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5806 - accuracy: 0.7059 - val_loss: 0.5756 - val_accuracy: 0.7152\n",
      "Epoch 6/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.5752 - accuracy: 0.7100\n",
      "Epoch 6: val_loss improved from 0.57561 to 0.57329, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5773 - accuracy: 0.7081 - val_loss: 0.5733 - val_accuracy: 0.7193\n",
      "Epoch 7/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.5750 - accuracy: 0.7094\n",
      "Epoch 7: val_loss improved from 0.57329 to 0.57195, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5750 - accuracy: 0.7095 - val_loss: 0.5719 - val_accuracy: 0.7217\n",
      "Epoch 8/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.5735 - accuracy: 0.7104\n",
      "Epoch 8: val_loss improved from 0.57195 to 0.57096, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5735 - accuracy: 0.7100 - val_loss: 0.5710 - val_accuracy: 0.7152\n",
      "Epoch 9/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.5732 - accuracy: 0.7090\n",
      "Epoch 9: val_loss improved from 0.57096 to 0.56993, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5723 - accuracy: 0.7104 - val_loss: 0.5699 - val_accuracy: 0.7181\n",
      "Epoch 10/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.5699 - accuracy: 0.7104\n",
      "Epoch 10: val_loss improved from 0.56993 to 0.56914, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5714 - accuracy: 0.7103 - val_loss: 0.5691 - val_accuracy: 0.7176\n",
      "Epoch 11/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.5703 - accuracy: 0.7113\n",
      "Epoch 11: val_loss improved from 0.56914 to 0.56849, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5705 - accuracy: 0.7111 - val_loss: 0.5685 - val_accuracy: 0.7193\n",
      "Epoch 12/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.5698 - accuracy: 0.7111\n",
      "Epoch 12: val_loss improved from 0.56849 to 0.56843, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5697 - accuracy: 0.7104 - val_loss: 0.5684 - val_accuracy: 0.7164\n",
      "Epoch 13/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.5696 - accuracy: 0.7102\n",
      "Epoch 13: val_loss improved from 0.56843 to 0.56740, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5690 - accuracy: 0.7108 - val_loss: 0.5674 - val_accuracy: 0.7199\n",
      "Epoch 14/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.5689 - accuracy: 0.7119\n",
      "Epoch 14: val_loss improved from 0.56740 to 0.56698, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5684 - accuracy: 0.7120 - val_loss: 0.5670 - val_accuracy: 0.7199\n",
      "Epoch 15/60\n",
      "650/682 [===========================>..] - ETA: 0s - loss: 0.5680 - accuracy: 0.7131\n",
      "Epoch 15: val_loss improved from 0.56698 to 0.56675, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5676 - accuracy: 0.7132 - val_loss: 0.5667 - val_accuracy: 0.7187\n",
      "Epoch 16/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.5674 - accuracy: 0.7121\n",
      "Epoch 16: val_loss improved from 0.56675 to 0.56616, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5673 - accuracy: 0.7117 - val_loss: 0.5662 - val_accuracy: 0.7205\n",
      "Epoch 17/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.5673 - accuracy: 0.7123\n",
      "Epoch 17: val_loss improved from 0.56616 to 0.56578, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5668 - accuracy: 0.7126 - val_loss: 0.5658 - val_accuracy: 0.7205\n",
      "Epoch 18/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.5661 - accuracy: 0.7134\n",
      "Epoch 18: val_loss improved from 0.56578 to 0.56566, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5663 - accuracy: 0.7129 - val_loss: 0.5657 - val_accuracy: 0.7258\n",
      "Epoch 19/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.5655 - accuracy: 0.7144\n",
      "Epoch 19: val_loss improved from 0.56566 to 0.56555, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5659 - accuracy: 0.7142 - val_loss: 0.5655 - val_accuracy: 0.7264\n",
      "Epoch 20/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.5664 - accuracy: 0.7151\n",
      "Epoch 20: val_loss improved from 0.56555 to 0.56475, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5658 - accuracy: 0.7145 - val_loss: 0.5647 - val_accuracy: 0.7228\n",
      "Epoch 21/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.5655 - accuracy: 0.7153\n",
      "Epoch 21: val_loss improved from 0.56475 to 0.56452, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5655 - accuracy: 0.7150 - val_loss: 0.5645 - val_accuracy: 0.7252\n",
      "Epoch 22/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.5636 - accuracy: 0.7170\n",
      "Epoch 22: val_loss improved from 0.56452 to 0.56432, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5650 - accuracy: 0.7153 - val_loss: 0.5643 - val_accuracy: 0.7234\n",
      "Epoch 23/60\n",
      "652/682 [===========================>..] - ETA: 0s - loss: 0.5639 - accuracy: 0.7166\n",
      "Epoch 23: val_loss improved from 0.56432 to 0.56398, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5648 - accuracy: 0.7158 - val_loss: 0.5640 - val_accuracy: 0.7240\n",
      "Epoch 24/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.5646 - accuracy: 0.7173\n",
      "Epoch 24: val_loss did not improve from 0.56398\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5645 - accuracy: 0.7170 - val_loss: 0.5640 - val_accuracy: 0.7246\n",
      "Epoch 25/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.5651 - accuracy: 0.7185\n",
      "Epoch 25: val_loss improved from 0.56398 to 0.56391, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 3ms/step - loss: 0.5642 - accuracy: 0.7191 - val_loss: 0.5639 - val_accuracy: 0.7217\n",
      "Epoch 26/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.5628 - accuracy: 0.7179\n",
      "Epoch 26: val_loss improved from 0.56391 to 0.56336, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 3ms/step - loss: 0.5639 - accuracy: 0.7170 - val_loss: 0.5634 - val_accuracy: 0.7246\n",
      "Epoch 27/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.5639 - accuracy: 0.7194\n",
      "Epoch 27: val_loss improved from 0.56336 to 0.56321, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5639 - accuracy: 0.7189 - val_loss: 0.5632 - val_accuracy: 0.7246\n",
      "Epoch 28/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.5629 - accuracy: 0.7190\n",
      "Epoch 28: val_loss improved from 0.56321 to 0.56313, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5636 - accuracy: 0.7185 - val_loss: 0.5631 - val_accuracy: 0.7223\n",
      "Epoch 29/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.5626 - accuracy: 0.7183\n",
      "Epoch 29: val_loss improved from 0.56313 to 0.56296, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5633 - accuracy: 0.7182 - val_loss: 0.5630 - val_accuracy: 0.7223\n",
      "Epoch 30/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.5633 - accuracy: 0.7182\n",
      "Epoch 30: val_loss did not improve from 0.56296\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5633 - accuracy: 0.7177 - val_loss: 0.5630 - val_accuracy: 0.7252\n",
      "Epoch 31/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.5643 - accuracy: 0.7167\n",
      "Epoch 31: val_loss improved from 0.56296 to 0.56290, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5631 - accuracy: 0.7183 - val_loss: 0.5629 - val_accuracy: 0.7217\n",
      "Epoch 32/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.5626 - accuracy: 0.7181\n",
      "Epoch 32: val_loss improved from 0.56290 to 0.56263, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5630 - accuracy: 0.7177 - val_loss: 0.5626 - val_accuracy: 0.7246\n",
      "Epoch 33/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.5628 - accuracy: 0.7180\n",
      "Epoch 33: val_loss did not improve from 0.56263\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5626 - accuracy: 0.7185 - val_loss: 0.5629 - val_accuracy: 0.7193\n",
      "Epoch 34/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.5616 - accuracy: 0.7175\n",
      "Epoch 34: val_loss improved from 0.56263 to 0.56248, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5627 - accuracy: 0.7164 - val_loss: 0.5625 - val_accuracy: 0.7234\n",
      "Epoch 35/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.5633 - accuracy: 0.7174\n",
      "Epoch 35: val_loss improved from 0.56248 to 0.56238, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5626 - accuracy: 0.7176 - val_loss: 0.5624 - val_accuracy: 0.7211\n",
      "Epoch 36/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.5628 - accuracy: 0.7185\n",
      "Epoch 36: val_loss improved from 0.56238 to 0.56233, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5624 - accuracy: 0.7183 - val_loss: 0.5623 - val_accuracy: 0.7252\n",
      "Epoch 37/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.5640 - accuracy: 0.7186\n",
      "Epoch 37: val_loss improved from 0.56233 to 0.56208, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5624 - accuracy: 0.7199 - val_loss: 0.5621 - val_accuracy: 0.7211\n",
      "Epoch 38/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.5633 - accuracy: 0.7188\n",
      "Epoch 38: val_loss did not improve from 0.56208\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5623 - accuracy: 0.7191 - val_loss: 0.5621 - val_accuracy: 0.7217\n",
      "Epoch 39/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.5625 - accuracy: 0.7202\n",
      "Epoch 39: val_loss improved from 0.56208 to 0.56198, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5623 - accuracy: 0.7204 - val_loss: 0.5620 - val_accuracy: 0.7246\n",
      "Epoch 40/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.5613 - accuracy: 0.7208\n",
      "Epoch 40: val_loss did not improve from 0.56198\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5621 - accuracy: 0.7199 - val_loss: 0.5621 - val_accuracy: 0.7205\n",
      "Epoch 41/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.5615 - accuracy: 0.7191\n",
      "Epoch 41: val_loss did not improve from 0.56198\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5621 - accuracy: 0.7189 - val_loss: 0.5620 - val_accuracy: 0.7246\n",
      "Epoch 42/60\n",
      "650/682 [===========================>..] - ETA: 0s - loss: 0.5608 - accuracy: 0.7212\n",
      "Epoch 42: val_loss improved from 0.56198 to 0.56196, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5619 - accuracy: 0.7194 - val_loss: 0.5620 - val_accuracy: 0.7199\n",
      "Epoch 43/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.5616 - accuracy: 0.7212\n",
      "Epoch 43: val_loss did not improve from 0.56196\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5621 - accuracy: 0.7208 - val_loss: 0.5620 - val_accuracy: 0.7199\n",
      "Epoch 44/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.5614 - accuracy: 0.7210\n",
      "Epoch 44: val_loss improved from 0.56196 to 0.56165, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5618 - accuracy: 0.7208 - val_loss: 0.5616 - val_accuracy: 0.7240\n",
      "Epoch 45/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.5617 - accuracy: 0.7189\n",
      "Epoch 45: val_loss did not improve from 0.56165\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5618 - accuracy: 0.7183 - val_loss: 0.5617 - val_accuracy: 0.7240\n",
      "Epoch 46/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.5610 - accuracy: 0.7192\n",
      "Epoch 46: val_loss did not improve from 0.56165\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5616 - accuracy: 0.7188 - val_loss: 0.5619 - val_accuracy: 0.7193\n",
      "Epoch 47/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.5631 - accuracy: 0.7196\n",
      "Epoch 47: val_loss did not improve from 0.56165\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5618 - accuracy: 0.7208 - val_loss: 0.5617 - val_accuracy: 0.7199\n",
      "Epoch 48/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.5622 - accuracy: 0.7193\n",
      "Epoch 48: val_loss improved from 0.56165 to 0.56150, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5617 - accuracy: 0.7198 - val_loss: 0.5615 - val_accuracy: 0.7199\n",
      "Epoch 49/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.5623 - accuracy: 0.7182\n",
      "Epoch 49: val_loss improved from 0.56150 to 0.56142, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5615 - accuracy: 0.7194 - val_loss: 0.5614 - val_accuracy: 0.7217\n",
      "Epoch 50/60\n",
      "653/682 [===========================>..] - ETA: 0s - loss: 0.5612 - accuracy: 0.7182\n",
      "Epoch 50: val_loss did not improve from 0.56142\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5614 - accuracy: 0.7182 - val_loss: 0.5621 - val_accuracy: 0.7205\n",
      "Epoch 51/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.5622 - accuracy: 0.7188\n",
      "Epoch 51: val_loss did not improve from 0.56142\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5615 - accuracy: 0.7201 - val_loss: 0.5616 - val_accuracy: 0.7217\n",
      "Epoch 52/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.5613 - accuracy: 0.7204\n",
      "Epoch 52: val_loss did not improve from 0.56142\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5614 - accuracy: 0.7204 - val_loss: 0.5622 - val_accuracy: 0.7211\n",
      "Epoch 53/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.5628 - accuracy: 0.7187\n",
      "Epoch 53: val_loss improved from 0.56142 to 0.56138, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5615 - accuracy: 0.7211 - val_loss: 0.5614 - val_accuracy: 0.7223\n",
      "Epoch 54/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.5610 - accuracy: 0.7209\n",
      "Epoch 54: val_loss improved from 0.56138 to 0.56134, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5614 - accuracy: 0.7205 - val_loss: 0.5613 - val_accuracy: 0.7187\n",
      "Epoch 55/60\n",
      "650/682 [===========================>..] - ETA: 0s - loss: 0.5635 - accuracy: 0.7188\n",
      "Epoch 55: val_loss improved from 0.56134 to 0.56128, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5613 - accuracy: 0.7213 - val_loss: 0.5613 - val_accuracy: 0.7211\n",
      "Epoch 56/60\n",
      "653/682 [===========================>..] - ETA: 0s - loss: 0.5593 - accuracy: 0.7221\n",
      "Epoch 56: val_loss did not improve from 0.56128\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5614 - accuracy: 0.7205 - val_loss: 0.5613 - val_accuracy: 0.7223\n",
      "Epoch 57/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.5618 - accuracy: 0.7209\n",
      "Epoch 57: val_loss improved from 0.56128 to 0.56121, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5613 - accuracy: 0.7216 - val_loss: 0.5612 - val_accuracy: 0.7205\n",
      "Epoch 58/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.5613 - accuracy: 0.7212\n",
      "Epoch 58: val_loss did not improve from 0.56121\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5614 - accuracy: 0.7207 - val_loss: 0.5614 - val_accuracy: 0.7199\n",
      "Epoch 59/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.5612 - accuracy: 0.7213\n",
      "Epoch 59: val_loss did not improve from 0.56121\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5612 - accuracy: 0.7213 - val_loss: 0.5617 - val_accuracy: 0.7211\n",
      "Epoch 60/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.5618 - accuracy: 0.7202\n",
      "Epoch 60: val_loss did not improve from 0.56121\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5612 - accuracy: 0.7210 - val_loss: 0.5612 - val_accuracy: 0.7193\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.5612 - accuracy: 0.7205\n",
      "Drop Feature services\n",
      "data frame x has shape: (8516, 15)\n",
      "['education', 'campaign', 'pdays', 'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'married', 'single', 'admin.', 'blue-collar', 'retired', 'student']\n",
      "Epoch 1/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.6248 - accuracy: 0.6687\n",
      "Epoch 1: val_loss improved from inf to 0.60470, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 3ms/step - loss: 0.6249 - accuracy: 0.6687 - val_loss: 0.6047 - val_accuracy: 0.6882\n",
      "Epoch 2/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.5983 - accuracy: 0.7018\n",
      "Epoch 2: val_loss improved from 0.60470 to 0.58773, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5981 - accuracy: 0.7022 - val_loss: 0.5877 - val_accuracy: 0.7158\n",
      "Epoch 3/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.5853 - accuracy: 0.7081\n",
      "Epoch 3: val_loss improved from 0.58773 to 0.57973, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5857 - accuracy: 0.7078 - val_loss: 0.5797 - val_accuracy: 0.7228\n",
      "Epoch 4/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.5790 - accuracy: 0.7098\n",
      "Epoch 4: val_loss improved from 0.57973 to 0.57570, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5798 - accuracy: 0.7091 - val_loss: 0.5757 - val_accuracy: 0.7223\n",
      "Epoch 5/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.5760 - accuracy: 0.7100\n",
      "Epoch 5: val_loss improved from 0.57570 to 0.57321, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5763 - accuracy: 0.7103 - val_loss: 0.5732 - val_accuracy: 0.7252\n",
      "Epoch 6/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.5742 - accuracy: 0.7092\n",
      "Epoch 6: val_loss improved from 0.57321 to 0.57174, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5743 - accuracy: 0.7097 - val_loss: 0.5717 - val_accuracy: 0.7234\n",
      "Epoch 7/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.5748 - accuracy: 0.7075\n",
      "Epoch 7: val_loss improved from 0.57174 to 0.57083, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5728 - accuracy: 0.7092 - val_loss: 0.5708 - val_accuracy: 0.7211\n",
      "Epoch 8/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.5714 - accuracy: 0.7113\n",
      "Epoch 8: val_loss improved from 0.57083 to 0.56980, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5717 - accuracy: 0.7103 - val_loss: 0.5698 - val_accuracy: 0.7223\n",
      "Epoch 9/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.5708 - accuracy: 0.7116\n",
      "Epoch 9: val_loss improved from 0.56980 to 0.56920, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5709 - accuracy: 0.7119 - val_loss: 0.5692 - val_accuracy: 0.7176\n",
      "Epoch 10/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.5701 - accuracy: 0.7098\n",
      "Epoch 10: val_loss improved from 0.56920 to 0.56877, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5701 - accuracy: 0.7098 - val_loss: 0.5688 - val_accuracy: 0.7223\n",
      "Epoch 11/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.5699 - accuracy: 0.7114\n",
      "Epoch 11: val_loss improved from 0.56877 to 0.56834, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5695 - accuracy: 0.7114 - val_loss: 0.5683 - val_accuracy: 0.7158\n",
      "Epoch 12/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.5686 - accuracy: 0.7102\n",
      "Epoch 12: val_loss improved from 0.56834 to 0.56769, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5689 - accuracy: 0.7104 - val_loss: 0.5677 - val_accuracy: 0.7223\n",
      "Epoch 13/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.5678 - accuracy: 0.7123\n",
      "Epoch 13: val_loss improved from 0.56769 to 0.56707, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5684 - accuracy: 0.7116 - val_loss: 0.5671 - val_accuracy: 0.7193\n",
      "Epoch 14/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.5687 - accuracy: 0.7108\n",
      "Epoch 14: val_loss improved from 0.56707 to 0.56675, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5678 - accuracy: 0.7114 - val_loss: 0.5667 - val_accuracy: 0.7181\n",
      "Epoch 15/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.5691 - accuracy: 0.7088\n",
      "Epoch 15: val_loss improved from 0.56675 to 0.56643, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5674 - accuracy: 0.7108 - val_loss: 0.5664 - val_accuracy: 0.7164\n",
      "Epoch 16/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.5663 - accuracy: 0.7126\n",
      "Epoch 16: val_loss improved from 0.56643 to 0.56609, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5670 - accuracy: 0.7120 - val_loss: 0.5661 - val_accuracy: 0.7228\n",
      "Epoch 17/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.5665 - accuracy: 0.7118\n",
      "Epoch 17: val_loss improved from 0.56609 to 0.56565, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5667 - accuracy: 0.7117 - val_loss: 0.5656 - val_accuracy: 0.7199\n",
      "Epoch 18/60\n",
      "654/682 [===========================>..] - ETA: 0s - loss: 0.5667 - accuracy: 0.7119\n",
      "Epoch 18: val_loss improved from 0.56565 to 0.56542, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5663 - accuracy: 0.7116 - val_loss: 0.5654 - val_accuracy: 0.7193\n",
      "Epoch 19/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.5658 - accuracy: 0.7135\n",
      "Epoch 19: val_loss improved from 0.56542 to 0.56504, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5660 - accuracy: 0.7132 - val_loss: 0.5650 - val_accuracy: 0.7199\n",
      "Epoch 20/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.5652 - accuracy: 0.7154\n",
      "Epoch 20: val_loss did not improve from 0.56504\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5656 - accuracy: 0.7151 - val_loss: 0.5651 - val_accuracy: 0.7223\n",
      "Epoch 21/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.5660 - accuracy: 0.7137\n",
      "Epoch 21: val_loss improved from 0.56504 to 0.56471, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5654 - accuracy: 0.7141 - val_loss: 0.5647 - val_accuracy: 0.7181\n",
      "Epoch 22/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.5628 - accuracy: 0.7171\n",
      "Epoch 22: val_loss improved from 0.56471 to 0.56442, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5651 - accuracy: 0.7145 - val_loss: 0.5644 - val_accuracy: 0.7181\n",
      "Epoch 23/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.5650 - accuracy: 0.7146\n",
      "Epoch 23: val_loss improved from 0.56442 to 0.56422, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5649 - accuracy: 0.7145 - val_loss: 0.5642 - val_accuracy: 0.7176\n",
      "Epoch 24/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.5654 - accuracy: 0.7147\n",
      "Epoch 24: val_loss improved from 0.56422 to 0.56400, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5646 - accuracy: 0.7151 - val_loss: 0.5640 - val_accuracy: 0.7199\n",
      "Epoch 25/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.5643 - accuracy: 0.7150\n",
      "Epoch 25: val_loss improved from 0.56400 to 0.56378, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5644 - accuracy: 0.7150 - val_loss: 0.5638 - val_accuracy: 0.7187\n",
      "Epoch 26/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.5640 - accuracy: 0.7145\n",
      "Epoch 26: val_loss did not improve from 0.56378\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5640 - accuracy: 0.7142 - val_loss: 0.5642 - val_accuracy: 0.7234\n",
      "Epoch 27/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.5637 - accuracy: 0.7151\n",
      "Epoch 27: val_loss improved from 0.56378 to 0.56351, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5641 - accuracy: 0.7148 - val_loss: 0.5635 - val_accuracy: 0.7217\n",
      "Epoch 28/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.5637 - accuracy: 0.7150\n",
      "Epoch 28: val_loss did not improve from 0.56351\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5638 - accuracy: 0.7151 - val_loss: 0.5635 - val_accuracy: 0.7223\n",
      "Epoch 29/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.5636 - accuracy: 0.7171\n",
      "Epoch 29: val_loss improved from 0.56351 to 0.56314, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5637 - accuracy: 0.7172 - val_loss: 0.5631 - val_accuracy: 0.7228\n",
      "Epoch 30/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.5636 - accuracy: 0.7164\n",
      "Epoch 30: val_loss improved from 0.56314 to 0.56302, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5636 - accuracy: 0.7164 - val_loss: 0.5630 - val_accuracy: 0.7240\n",
      "Epoch 31/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.5639 - accuracy: 0.7167\n",
      "Epoch 31: val_loss did not improve from 0.56302\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5635 - accuracy: 0.7163 - val_loss: 0.5632 - val_accuracy: 0.7258\n",
      "Epoch 32/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.5635 - accuracy: 0.7157\n",
      "Epoch 32: val_loss improved from 0.56302 to 0.56286, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5633 - accuracy: 0.7166 - val_loss: 0.5629 - val_accuracy: 0.7234\n",
      "Epoch 33/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.5632 - accuracy: 0.7160\n",
      "Epoch 33: val_loss improved from 0.56286 to 0.56273, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5631 - accuracy: 0.7161 - val_loss: 0.5627 - val_accuracy: 0.7223\n",
      "Epoch 34/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.5635 - accuracy: 0.7169\n",
      "Epoch 34: val_loss improved from 0.56273 to 0.56264, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5631 - accuracy: 0.7163 - val_loss: 0.5626 - val_accuracy: 0.7217\n",
      "Epoch 35/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.5620 - accuracy: 0.7177\n",
      "Epoch 35: val_loss did not improve from 0.56264\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5630 - accuracy: 0.7172 - val_loss: 0.5627 - val_accuracy: 0.7211\n",
      "Epoch 36/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.5627 - accuracy: 0.7184\n",
      "Epoch 36: val_loss improved from 0.56264 to 0.56245, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5629 - accuracy: 0.7182 - val_loss: 0.5625 - val_accuracy: 0.7234\n",
      "Epoch 37/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.5630 - accuracy: 0.7163\n",
      "Epoch 37: val_loss improved from 0.56245 to 0.56238, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5628 - accuracy: 0.7164 - val_loss: 0.5624 - val_accuracy: 0.7223\n",
      "Epoch 38/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.5624 - accuracy: 0.7181\n",
      "Epoch 38: val_loss did not improve from 0.56238\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5627 - accuracy: 0.7180 - val_loss: 0.5624 - val_accuracy: 0.7217\n",
      "Epoch 39/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.5624 - accuracy: 0.7177\n",
      "Epoch 39: val_loss improved from 0.56238 to 0.56223, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5626 - accuracy: 0.7173 - val_loss: 0.5622 - val_accuracy: 0.7223\n",
      "Epoch 40/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.5619 - accuracy: 0.7180\n",
      "Epoch 40: val_loss did not improve from 0.56223\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5625 - accuracy: 0.7170 - val_loss: 0.5622 - val_accuracy: 0.7228\n",
      "Epoch 41/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.5634 - accuracy: 0.7170\n",
      "Epoch 41: val_loss did not improve from 0.56223\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5624 - accuracy: 0.7182 - val_loss: 0.5624 - val_accuracy: 0.7205\n",
      "Epoch 42/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.5617 - accuracy: 0.7198\n",
      "Epoch 42: val_loss did not improve from 0.56223\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5625 - accuracy: 0.7192 - val_loss: 0.5624 - val_accuracy: 0.7211\n",
      "Epoch 43/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.5628 - accuracy: 0.7185\n",
      "Epoch 43: val_loss improved from 0.56223 to 0.56210, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5624 - accuracy: 0.7188 - val_loss: 0.5621 - val_accuracy: 0.7205\n",
      "Epoch 44/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.5621 - accuracy: 0.7186\n",
      "Epoch 44: val_loss improved from 0.56210 to 0.56203, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5622 - accuracy: 0.7185 - val_loss: 0.5620 - val_accuracy: 0.7205\n",
      "Epoch 45/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.5628 - accuracy: 0.7200\n",
      "Epoch 45: val_loss did not improve from 0.56203\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5622 - accuracy: 0.7202 - val_loss: 0.5622 - val_accuracy: 0.7205\n",
      "Epoch 46/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.5620 - accuracy: 0.7185\n",
      "Epoch 46: val_loss improved from 0.56203 to 0.56200, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5621 - accuracy: 0.7185 - val_loss: 0.5620 - val_accuracy: 0.7223\n",
      "Epoch 47/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.5626 - accuracy: 0.7181\n",
      "Epoch 47: val_loss improved from 0.56200 to 0.56194, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5621 - accuracy: 0.7183 - val_loss: 0.5619 - val_accuracy: 0.7217\n",
      "Epoch 48/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.5621 - accuracy: 0.7181\n",
      "Epoch 48: val_loss improved from 0.56194 to 0.56190, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5620 - accuracy: 0.7182 - val_loss: 0.5619 - val_accuracy: 0.7228\n",
      "Epoch 49/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.5635 - accuracy: 0.7191\n",
      "Epoch 49: val_loss did not improve from 0.56190\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5621 - accuracy: 0.7205 - val_loss: 0.5620 - val_accuracy: 0.7211\n",
      "Epoch 50/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.5620 - accuracy: 0.7202\n",
      "Epoch 50: val_loss did not improve from 0.56190\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5619 - accuracy: 0.7201 - val_loss: 0.5620 - val_accuracy: 0.7205\n",
      "Epoch 51/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.5618 - accuracy: 0.7204\n",
      "Epoch 51: val_loss improved from 0.56190 to 0.56181, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5619 - accuracy: 0.7198 - val_loss: 0.5618 - val_accuracy: 0.7223\n",
      "Epoch 52/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.5618 - accuracy: 0.7185\n",
      "Epoch 52: val_loss did not improve from 0.56181\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5616 - accuracy: 0.7189 - val_loss: 0.5618 - val_accuracy: 0.7217\n",
      "Epoch 53/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.5631 - accuracy: 0.7188\n",
      "Epoch 53: val_loss did not improve from 0.56181\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5618 - accuracy: 0.7202 - val_loss: 0.5618 - val_accuracy: 0.7223\n",
      "Epoch 54/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.5615 - accuracy: 0.7189\n",
      "Epoch 54: val_loss did not improve from 0.56181\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7195 - val_loss: 0.5620 - val_accuracy: 0.7217\n",
      "Epoch 55/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.5616 - accuracy: 0.7206\n",
      "Epoch 55: val_loss did not improve from 0.56181\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5618 - accuracy: 0.7207 - val_loss: 0.5620 - val_accuracy: 0.7205\n",
      "Epoch 56/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.5605 - accuracy: 0.7222\n",
      "Epoch 56: val_loss improved from 0.56181 to 0.56173, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5618 - accuracy: 0.7208 - val_loss: 0.5617 - val_accuracy: 0.7199\n",
      "Epoch 57/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.5630 - accuracy: 0.7186\n",
      "Epoch 57: val_loss improved from 0.56173 to 0.56161, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5616 - accuracy: 0.7202 - val_loss: 0.5616 - val_accuracy: 0.7228\n",
      "Epoch 58/60\n",
      "654/682 [===========================>..] - ETA: 0s - loss: 0.5595 - accuracy: 0.7214\n",
      "Epoch 58: val_loss did not improve from 0.56161\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7201 - val_loss: 0.5617 - val_accuracy: 0.7211\n",
      "Epoch 59/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.5624 - accuracy: 0.7205\n",
      "Epoch 59: val_loss did not improve from 0.56161\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7205 - val_loss: 0.5617 - val_accuracy: 0.7211\n",
      "Epoch 60/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.5612 - accuracy: 0.7218\n",
      "Epoch 60: val_loss did not improve from 0.56161\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5614 - accuracy: 0.7210 - val_loss: 0.5619 - val_accuracy: 0.7211\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.5616 - accuracy: 0.7228\n",
      "Drop Feature retired\n",
      "data frame x has shape: (8516, 14)\n",
      "['education', 'campaign', 'pdays', 'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'married', 'single', 'admin.', 'blue-collar', 'student']\n",
      "Epoch 1/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.6641 - accuracy: 0.6229\n",
      "Epoch 1: val_loss improved from inf to 0.61953, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 3ms/step - loss: 0.6641 - accuracy: 0.6228 - val_loss: 0.6195 - val_accuracy: 0.6735\n",
      "Epoch 2/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.6036 - accuracy: 0.6987\n",
      "Epoch 2: val_loss improved from 0.61953 to 0.59141, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6036 - accuracy: 0.6987 - val_loss: 0.5914 - val_accuracy: 0.7134\n",
      "Epoch 3/60\n",
      "654/682 [===========================>..] - ETA: 0s - loss: 0.5866 - accuracy: 0.7038\n",
      "Epoch 3: val_loss improved from 0.59141 to 0.57951, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5859 - accuracy: 0.7050 - val_loss: 0.5795 - val_accuracy: 0.7240\n",
      "Epoch 4/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.5789 - accuracy: 0.7085\n",
      "Epoch 4: val_loss improved from 0.57951 to 0.57366, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5785 - accuracy: 0.7094 - val_loss: 0.5737 - val_accuracy: 0.7187\n",
      "Epoch 5/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.5741 - accuracy: 0.7110\n",
      "Epoch 5: val_loss improved from 0.57366 to 0.57073, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5746 - accuracy: 0.7108 - val_loss: 0.5707 - val_accuracy: 0.7164\n",
      "Epoch 6/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.5733 - accuracy: 0.7093\n",
      "Epoch 6: val_loss improved from 0.57073 to 0.56923, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5725 - accuracy: 0.7106 - val_loss: 0.5692 - val_accuracy: 0.7223\n",
      "Epoch 7/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.5708 - accuracy: 0.7110\n",
      "Epoch 7: val_loss improved from 0.56923 to 0.56790, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5714 - accuracy: 0.7103 - val_loss: 0.5679 - val_accuracy: 0.7164\n",
      "Epoch 8/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.5711 - accuracy: 0.7100\n",
      "Epoch 8: val_loss improved from 0.56790 to 0.56744, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5705 - accuracy: 0.7107 - val_loss: 0.5674 - val_accuracy: 0.7199\n",
      "Epoch 9/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.5699 - accuracy: 0.7097\n",
      "Epoch 9: val_loss improved from 0.56744 to 0.56671, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5699 - accuracy: 0.7100 - val_loss: 0.5667 - val_accuracy: 0.7158\n",
      "Epoch 10/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.5691 - accuracy: 0.7101\n",
      "Epoch 10: val_loss improved from 0.56671 to 0.56622, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5694 - accuracy: 0.7100 - val_loss: 0.5662 - val_accuracy: 0.7152\n",
      "Epoch 11/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.5694 - accuracy: 0.7110\n",
      "Epoch 11: val_loss improved from 0.56622 to 0.56584, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5688 - accuracy: 0.7113 - val_loss: 0.5658 - val_accuracy: 0.7170\n",
      "Epoch 12/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.5681 - accuracy: 0.7118\n",
      "Epoch 12: val_loss improved from 0.56584 to 0.56536, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5684 - accuracy: 0.7110 - val_loss: 0.5654 - val_accuracy: 0.7152\n",
      "Epoch 13/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.5697 - accuracy: 0.7077\n",
      "Epoch 13: val_loss improved from 0.56536 to 0.56503, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5680 - accuracy: 0.7100 - val_loss: 0.5650 - val_accuracy: 0.7158\n",
      "Epoch 14/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.5686 - accuracy: 0.7105\n",
      "Epoch 14: val_loss improved from 0.56503 to 0.56469, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5678 - accuracy: 0.7110 - val_loss: 0.5647 - val_accuracy: 0.7152\n",
      "Epoch 15/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.5678 - accuracy: 0.7111\n",
      "Epoch 15: val_loss improved from 0.56469 to 0.56463, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5673 - accuracy: 0.7114 - val_loss: 0.5646 - val_accuracy: 0.7187\n",
      "Epoch 16/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.5671 - accuracy: 0.7101\n",
      "Epoch 16: val_loss improved from 0.56463 to 0.56436, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5670 - accuracy: 0.7110 - val_loss: 0.5644 - val_accuracy: 0.7170\n",
      "Epoch 17/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.5661 - accuracy: 0.7133\n",
      "Epoch 17: val_loss did not improve from 0.56436\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5667 - accuracy: 0.7128 - val_loss: 0.5646 - val_accuracy: 0.7187\n",
      "Epoch 18/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.5660 - accuracy: 0.7129\n",
      "Epoch 18: val_loss improved from 0.56436 to 0.56413, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5665 - accuracy: 0.7125 - val_loss: 0.5641 - val_accuracy: 0.7193\n",
      "Epoch 19/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.5667 - accuracy: 0.7127\n",
      "Epoch 19: val_loss improved from 0.56413 to 0.56369, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5663 - accuracy: 0.7132 - val_loss: 0.5637 - val_accuracy: 0.7164\n",
      "Epoch 20/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.5669 - accuracy: 0.7101\n",
      "Epoch 20: val_loss improved from 0.56369 to 0.56341, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5660 - accuracy: 0.7119 - val_loss: 0.5634 - val_accuracy: 0.7170\n",
      "Epoch 21/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.5646 - accuracy: 0.7145\n",
      "Epoch 21: val_loss did not improve from 0.56341\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5659 - accuracy: 0.7139 - val_loss: 0.5635 - val_accuracy: 0.7170\n",
      "Epoch 22/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.5657 - accuracy: 0.7125\n",
      "Epoch 22: val_loss improved from 0.56341 to 0.56333, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5656 - accuracy: 0.7126 - val_loss: 0.5633 - val_accuracy: 0.7199\n",
      "Epoch 23/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.5656 - accuracy: 0.7131\n",
      "Epoch 23: val_loss improved from 0.56333 to 0.56306, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5655 - accuracy: 0.7129 - val_loss: 0.5631 - val_accuracy: 0.7187\n",
      "Epoch 24/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.5653 - accuracy: 0.7135\n",
      "Epoch 24: val_loss improved from 0.56306 to 0.56278, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5653 - accuracy: 0.7135 - val_loss: 0.5628 - val_accuracy: 0.7164\n",
      "Epoch 25/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.5640 - accuracy: 0.7157\n",
      "Epoch 25: val_loss improved from 0.56278 to 0.56266, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5652 - accuracy: 0.7145 - val_loss: 0.5627 - val_accuracy: 0.7158\n",
      "Epoch 26/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.5643 - accuracy: 0.7149\n",
      "Epoch 26: val_loss improved from 0.56266 to 0.56259, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5650 - accuracy: 0.7141 - val_loss: 0.5626 - val_accuracy: 0.7164\n",
      "Epoch 27/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.5646 - accuracy: 0.7145\n",
      "Epoch 27: val_loss improved from 0.56259 to 0.56248, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5649 - accuracy: 0.7142 - val_loss: 0.5625 - val_accuracy: 0.7170\n",
      "Epoch 28/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.5659 - accuracy: 0.7128\n",
      "Epoch 28: val_loss did not improve from 0.56248\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5648 - accuracy: 0.7138 - val_loss: 0.5626 - val_accuracy: 0.7199\n",
      "Epoch 29/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.5648 - accuracy: 0.7138\n",
      "Epoch 29: val_loss improved from 0.56248 to 0.56236, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5647 - accuracy: 0.7139 - val_loss: 0.5624 - val_accuracy: 0.7170\n",
      "Epoch 30/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.5648 - accuracy: 0.7140\n",
      "Epoch 30: val_loss did not improve from 0.56236\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5645 - accuracy: 0.7142 - val_loss: 0.5625 - val_accuracy: 0.7193\n",
      "Epoch 31/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.5649 - accuracy: 0.7151\n",
      "Epoch 31: val_loss improved from 0.56236 to 0.56213, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5645 - accuracy: 0.7151 - val_loss: 0.5621 - val_accuracy: 0.7193\n",
      "Epoch 32/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.5653 - accuracy: 0.7140\n",
      "Epoch 32: val_loss did not improve from 0.56213\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5643 - accuracy: 0.7151 - val_loss: 0.5623 - val_accuracy: 0.7158\n",
      "Epoch 33/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.5667 - accuracy: 0.7122\n",
      "Epoch 33: val_loss did not improve from 0.56213\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5643 - accuracy: 0.7145 - val_loss: 0.5621 - val_accuracy: 0.7193\n",
      "Epoch 34/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.5645 - accuracy: 0.7151\n",
      "Epoch 34: val_loss improved from 0.56213 to 0.56204, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5642 - accuracy: 0.7150 - val_loss: 0.5620 - val_accuracy: 0.7176\n",
      "Epoch 35/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.5644 - accuracy: 0.7129\n",
      "Epoch 35: val_loss did not improve from 0.56204\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5641 - accuracy: 0.7135 - val_loss: 0.5622 - val_accuracy: 0.7193\n",
      "Epoch 36/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.5636 - accuracy: 0.7161\n",
      "Epoch 36: val_loss improved from 0.56204 to 0.56182, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5640 - accuracy: 0.7158 - val_loss: 0.5618 - val_accuracy: 0.7176\n",
      "Epoch 37/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.5648 - accuracy: 0.7138\n",
      "Epoch 37: val_loss did not improve from 0.56182\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5641 - accuracy: 0.7144 - val_loss: 0.5618 - val_accuracy: 0.7176\n",
      "Epoch 38/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.5636 - accuracy: 0.7154\n",
      "Epoch 38: val_loss did not improve from 0.56182\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5640 - accuracy: 0.7150 - val_loss: 0.5619 - val_accuracy: 0.7181\n",
      "Epoch 39/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.5628 - accuracy: 0.7160\n",
      "Epoch 39: val_loss improved from 0.56182 to 0.56164, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5639 - accuracy: 0.7147 - val_loss: 0.5616 - val_accuracy: 0.7176\n",
      "Epoch 40/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.5639 - accuracy: 0.7137\n",
      "Epoch 40: val_loss did not improve from 0.56164\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5639 - accuracy: 0.7136 - val_loss: 0.5617 - val_accuracy: 0.7164\n",
      "Epoch 41/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.5620 - accuracy: 0.7166\n",
      "Epoch 41: val_loss improved from 0.56164 to 0.56146, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5638 - accuracy: 0.7151 - val_loss: 0.5615 - val_accuracy: 0.7181\n",
      "Epoch 42/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.5627 - accuracy: 0.7160\n",
      "Epoch 42: val_loss did not improve from 0.56146\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5635 - accuracy: 0.7154 - val_loss: 0.5620 - val_accuracy: 0.7158\n",
      "Epoch 43/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.5656 - accuracy: 0.7124\n",
      "Epoch 43: val_loss improved from 0.56146 to 0.56141, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5638 - accuracy: 0.7142 - val_loss: 0.5614 - val_accuracy: 0.7199\n",
      "Epoch 44/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.5631 - accuracy: 0.7169\n",
      "Epoch 44: val_loss did not improve from 0.56141\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5637 - accuracy: 0.7163 - val_loss: 0.5615 - val_accuracy: 0.7187\n",
      "Epoch 45/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.5613 - accuracy: 0.7179\n",
      "Epoch 45: val_loss did not improve from 0.56141\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5636 - accuracy: 0.7160 - val_loss: 0.5615 - val_accuracy: 0.7181\n",
      "Epoch 46/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.5627 - accuracy: 0.7176\n",
      "Epoch 46: val_loss did not improve from 0.56141\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5636 - accuracy: 0.7164 - val_loss: 0.5614 - val_accuracy: 0.7181\n",
      "Epoch 47/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.5637 - accuracy: 0.7172\n",
      "Epoch 47: val_loss did not improve from 0.56141\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5636 - accuracy: 0.7176 - val_loss: 0.5616 - val_accuracy: 0.7176\n",
      "Epoch 48/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.5642 - accuracy: 0.7151\n",
      "Epoch 48: val_loss did not improve from 0.56141\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5635 - accuracy: 0.7157 - val_loss: 0.5615 - val_accuracy: 0.7176\n",
      "Epoch 49/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.5634 - accuracy: 0.7169\n",
      "Epoch 49: val_loss did not improve from 0.56141\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5634 - accuracy: 0.7169 - val_loss: 0.5614 - val_accuracy: 0.7187\n",
      "Epoch 50/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.5641 - accuracy: 0.7157\n",
      "Epoch 50: val_loss improved from 0.56141 to 0.56128, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5635 - accuracy: 0.7163 - val_loss: 0.5613 - val_accuracy: 0.7187\n",
      "Epoch 51/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.5637 - accuracy: 0.7166\n",
      "Epoch 51: val_loss did not improve from 0.56128\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5634 - accuracy: 0.7169 - val_loss: 0.5613 - val_accuracy: 0.7187\n",
      "Epoch 52/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.5635 - accuracy: 0.7167\n",
      "Epoch 52: val_loss improved from 0.56128 to 0.56127, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5634 - accuracy: 0.7167 - val_loss: 0.5613 - val_accuracy: 0.7176\n",
      "Epoch 53/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.5629 - accuracy: 0.7158\n",
      "Epoch 53: val_loss improved from 0.56127 to 0.56121, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5635 - accuracy: 0.7155 - val_loss: 0.5612 - val_accuracy: 0.7181\n",
      "Epoch 54/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.5623 - accuracy: 0.7191\n",
      "Epoch 54: val_loss did not improve from 0.56121\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5632 - accuracy: 0.7180 - val_loss: 0.5613 - val_accuracy: 0.7170\n",
      "Epoch 55/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.5639 - accuracy: 0.7163\n",
      "Epoch 55: val_loss did not improve from 0.56121\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5633 - accuracy: 0.7169 - val_loss: 0.5614 - val_accuracy: 0.7170\n",
      "Epoch 56/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.5618 - accuracy: 0.7177\n",
      "Epoch 56: val_loss did not improve from 0.56121\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5634 - accuracy: 0.7161 - val_loss: 0.5614 - val_accuracy: 0.7181\n",
      "Epoch 57/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.5634 - accuracy: 0.7178\n",
      "Epoch 57: val_loss improved from 0.56121 to 0.56117, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5634 - accuracy: 0.7177 - val_loss: 0.5612 - val_accuracy: 0.7158\n",
      "Epoch 58/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.5627 - accuracy: 0.7166\n",
      "Epoch 58: val_loss did not improve from 0.56117\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5633 - accuracy: 0.7164 - val_loss: 0.5613 - val_accuracy: 0.7193\n",
      "Epoch 59/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.5645 - accuracy: 0.7170\n",
      "Epoch 59: val_loss did not improve from 0.56117\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5633 - accuracy: 0.7176 - val_loss: 0.5613 - val_accuracy: 0.7181\n",
      "Epoch 60/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.5644 - accuracy: 0.7184\n",
      "Epoch 60: val_loss did not improve from 0.56117\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5632 - accuracy: 0.7186 - val_loss: 0.5613 - val_accuracy: 0.7170\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.5612 - accuracy: 0.7158\n",
      "Drop Feature student\n",
      "data frame x has shape: (8516, 13)\n",
      "['education', 'campaign', 'pdays', 'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'married', 'single', 'admin.', 'blue-collar']\n",
      "Epoch 1/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.6691 - accuracy: 0.6054\n",
      "Epoch 1: val_loss improved from inf to 0.62385, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 3ms/step - loss: 0.6674 - accuracy: 0.6080 - val_loss: 0.6239 - val_accuracy: 0.6782\n",
      "Epoch 2/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.6153 - accuracy: 0.6902\n",
      "Epoch 2: val_loss improved from 0.62385 to 0.59742, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6150 - accuracy: 0.6903 - val_loss: 0.5974 - val_accuracy: 0.7123\n",
      "Epoch 3/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.5956 - accuracy: 0.7071\n",
      "Epoch 3: val_loss improved from 0.59742 to 0.58435, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5954 - accuracy: 0.7073 - val_loss: 0.5844 - val_accuracy: 0.7264\n",
      "Epoch 4/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.5854 - accuracy: 0.7142\n",
      "Epoch 4: val_loss improved from 0.58435 to 0.57770, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5850 - accuracy: 0.7145 - val_loss: 0.5777 - val_accuracy: 0.7287\n",
      "Epoch 5/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.5804 - accuracy: 0.7141\n",
      "Epoch 5: val_loss improved from 0.57770 to 0.57385, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5799 - accuracy: 0.7142 - val_loss: 0.5738 - val_accuracy: 0.7311\n",
      "Epoch 6/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.5766 - accuracy: 0.7110\n",
      "Epoch 6: val_loss improved from 0.57385 to 0.57173, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5768 - accuracy: 0.7107 - val_loss: 0.5717 - val_accuracy: 0.7264\n",
      "Epoch 7/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.5740 - accuracy: 0.7119\n",
      "Epoch 7: val_loss improved from 0.57173 to 0.57048, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5748 - accuracy: 0.7113 - val_loss: 0.5705 - val_accuracy: 0.7287\n",
      "Epoch 8/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.5734 - accuracy: 0.7105\n",
      "Epoch 8: val_loss improved from 0.57048 to 0.56945, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5734 - accuracy: 0.7103 - val_loss: 0.5694 - val_accuracy: 0.7181\n",
      "Epoch 9/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.5727 - accuracy: 0.7089\n",
      "Epoch 9: val_loss improved from 0.56945 to 0.56878, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5723 - accuracy: 0.7095 - val_loss: 0.5688 - val_accuracy: 0.7170\n",
      "Epoch 10/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.5709 - accuracy: 0.7111\n",
      "Epoch 10: val_loss improved from 0.56878 to 0.56813, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5715 - accuracy: 0.7108 - val_loss: 0.5681 - val_accuracy: 0.7158\n",
      "Epoch 11/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.5706 - accuracy: 0.7109\n",
      "Epoch 11: val_loss improved from 0.56813 to 0.56777, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5709 - accuracy: 0.7107 - val_loss: 0.5678 - val_accuracy: 0.7211\n",
      "Epoch 12/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.5706 - accuracy: 0.7104\n",
      "Epoch 12: val_loss improved from 0.56777 to 0.56735, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5702 - accuracy: 0.7110 - val_loss: 0.5674 - val_accuracy: 0.7205\n",
      "Epoch 13/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.5696 - accuracy: 0.7108\n",
      "Epoch 13: val_loss improved from 0.56735 to 0.56664, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5698 - accuracy: 0.7108 - val_loss: 0.5666 - val_accuracy: 0.7176\n",
      "Epoch 14/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.5692 - accuracy: 0.7105\n",
      "Epoch 14: val_loss improved from 0.56664 to 0.56627, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5694 - accuracy: 0.7100 - val_loss: 0.5663 - val_accuracy: 0.7170\n",
      "Epoch 15/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.5686 - accuracy: 0.7108\n",
      "Epoch 15: val_loss improved from 0.56627 to 0.56600, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5689 - accuracy: 0.7103 - val_loss: 0.5660 - val_accuracy: 0.7176\n",
      "Epoch 16/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.5689 - accuracy: 0.7096\n",
      "Epoch 16: val_loss did not improve from 0.56600\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5685 - accuracy: 0.7100 - val_loss: 0.5660 - val_accuracy: 0.7193\n",
      "Epoch 17/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.5684 - accuracy: 0.7111\n",
      "Epoch 17: val_loss improved from 0.56600 to 0.56544, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5683 - accuracy: 0.7104 - val_loss: 0.5654 - val_accuracy: 0.7176\n",
      "Epoch 18/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.5691 - accuracy: 0.7094\n",
      "Epoch 18: val_loss did not improve from 0.56544\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5675 - accuracy: 0.7104 - val_loss: 0.5663 - val_accuracy: 0.7217\n",
      "Epoch 19/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.5669 - accuracy: 0.7141\n",
      "Epoch 19: val_loss did not improve from 0.56544\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5676 - accuracy: 0.7133 - val_loss: 0.5657 - val_accuracy: 0.7164\n",
      "Epoch 20/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.5682 - accuracy: 0.7117\n",
      "Epoch 20: val_loss improved from 0.56544 to 0.56491, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5673 - accuracy: 0.7126 - val_loss: 0.5649 - val_accuracy: 0.7176\n",
      "Epoch 21/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.5691 - accuracy: 0.7111\n",
      "Epoch 21: val_loss improved from 0.56491 to 0.56473, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5669 - accuracy: 0.7136 - val_loss: 0.5647 - val_accuracy: 0.7181\n",
      "Epoch 22/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.5682 - accuracy: 0.7127\n",
      "Epoch 22: val_loss improved from 0.56473 to 0.56455, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5667 - accuracy: 0.7136 - val_loss: 0.5646 - val_accuracy: 0.7170\n",
      "Epoch 23/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.5671 - accuracy: 0.7114\n",
      "Epoch 23: val_loss improved from 0.56455 to 0.56410, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5664 - accuracy: 0.7122 - val_loss: 0.5641 - val_accuracy: 0.7176\n",
      "Epoch 24/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.5662 - accuracy: 0.7130\n",
      "Epoch 24: val_loss did not improve from 0.56410\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5662 - accuracy: 0.7136 - val_loss: 0.5644 - val_accuracy: 0.7187\n",
      "Epoch 25/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.5662 - accuracy: 0.7141\n",
      "Epoch 25: val_loss improved from 0.56410 to 0.56383, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5663 - accuracy: 0.7141 - val_loss: 0.5638 - val_accuracy: 0.7152\n",
      "Epoch 26/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.5653 - accuracy: 0.7159\n",
      "Epoch 26: val_loss improved from 0.56383 to 0.56369, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5661 - accuracy: 0.7153 - val_loss: 0.5637 - val_accuracy: 0.7152\n",
      "Epoch 27/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.5659 - accuracy: 0.7143\n",
      "Epoch 27: val_loss improved from 0.56369 to 0.56364, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5658 - accuracy: 0.7144 - val_loss: 0.5636 - val_accuracy: 0.7152\n",
      "Epoch 28/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.5657 - accuracy: 0.7154\n",
      "Epoch 28: val_loss improved from 0.56364 to 0.56357, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5658 - accuracy: 0.7154 - val_loss: 0.5636 - val_accuracy: 0.7176\n",
      "Epoch 29/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.5642 - accuracy: 0.7156\n",
      "Epoch 29: val_loss did not improve from 0.56357\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5654 - accuracy: 0.7151 - val_loss: 0.5642 - val_accuracy: 0.7164\n",
      "Epoch 30/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.5665 - accuracy: 0.7137\n",
      "Epoch 30: val_loss improved from 0.56357 to 0.56343, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5655 - accuracy: 0.7148 - val_loss: 0.5634 - val_accuracy: 0.7193\n",
      "Epoch 31/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.5653 - accuracy: 0.7149\n",
      "Epoch 31: val_loss did not improve from 0.56343\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5653 - accuracy: 0.7145 - val_loss: 0.5635 - val_accuracy: 0.7176\n",
      "Epoch 32/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.5651 - accuracy: 0.7150\n",
      "Epoch 32: val_loss did not improve from 0.56343\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5651 - accuracy: 0.7151 - val_loss: 0.5637 - val_accuracy: 0.7164\n",
      "Epoch 33/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.5650 - accuracy: 0.7143\n",
      "Epoch 33: val_loss improved from 0.56343 to 0.56308, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5651 - accuracy: 0.7145 - val_loss: 0.5631 - val_accuracy: 0.7164\n",
      "Epoch 34/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.5654 - accuracy: 0.7142\n",
      "Epoch 34: val_loss did not improve from 0.56308\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5651 - accuracy: 0.7145 - val_loss: 0.5634 - val_accuracy: 0.7164\n",
      "Epoch 35/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.5650 - accuracy: 0.7160\n",
      "Epoch 35: val_loss improved from 0.56308 to 0.56303, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5651 - accuracy: 0.7158 - val_loss: 0.5630 - val_accuracy: 0.7158\n",
      "Epoch 36/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.5653 - accuracy: 0.7132\n",
      "Epoch 36: val_loss improved from 0.56303 to 0.56284, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5649 - accuracy: 0.7145 - val_loss: 0.5628 - val_accuracy: 0.7164\n",
      "Epoch 37/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.5639 - accuracy: 0.7152\n",
      "Epoch 37: val_loss did not improve from 0.56284\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5649 - accuracy: 0.7150 - val_loss: 0.5629 - val_accuracy: 0.7176\n",
      "Epoch 38/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.5646 - accuracy: 0.7151\n",
      "Epoch 38: val_loss did not improve from 0.56284\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5645 - accuracy: 0.7154 - val_loss: 0.5630 - val_accuracy: 0.7158\n",
      "Epoch 39/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.5653 - accuracy: 0.7150\n",
      "Epoch 39: val_loss did not improve from 0.56284\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5647 - accuracy: 0.7154 - val_loss: 0.5630 - val_accuracy: 0.7158\n",
      "Epoch 40/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.5650 - accuracy: 0.7163\n",
      "Epoch 40: val_loss improved from 0.56284 to 0.56274, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5647 - accuracy: 0.7157 - val_loss: 0.5627 - val_accuracy: 0.7170\n",
      "Epoch 41/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.5651 - accuracy: 0.7141\n",
      "Epoch 41: val_loss did not improve from 0.56274\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5645 - accuracy: 0.7142 - val_loss: 0.5628 - val_accuracy: 0.7170\n",
      "Epoch 42/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.5643 - accuracy: 0.7149\n",
      "Epoch 42: val_loss did not improve from 0.56274\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5644 - accuracy: 0.7151 - val_loss: 0.5629 - val_accuracy: 0.7158\n",
      "Epoch 43/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.5665 - accuracy: 0.7135\n",
      "Epoch 43: val_loss did not improve from 0.56274\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5643 - accuracy: 0.7160 - val_loss: 0.5628 - val_accuracy: 0.7164\n",
      "Epoch 44/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.5640 - accuracy: 0.7166\n",
      "Epoch 44: val_loss improved from 0.56274 to 0.56250, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5645 - accuracy: 0.7155 - val_loss: 0.5625 - val_accuracy: 0.7170\n",
      "Epoch 45/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.5642 - accuracy: 0.7157\n",
      "Epoch 45: val_loss improved from 0.56250 to 0.56244, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5644 - accuracy: 0.7155 - val_loss: 0.5624 - val_accuracy: 0.7176\n",
      "Epoch 46/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.5635 - accuracy: 0.7168\n",
      "Epoch 46: val_loss did not improve from 0.56244\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5643 - accuracy: 0.7157 - val_loss: 0.5625 - val_accuracy: 0.7176\n",
      "Epoch 47/60\n",
      "654/682 [===========================>..] - ETA: 0s - loss: 0.5652 - accuracy: 0.7150\n",
      "Epoch 47: val_loss improved from 0.56244 to 0.56244, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5642 - accuracy: 0.7157 - val_loss: 0.5624 - val_accuracy: 0.7158\n",
      "Epoch 48/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.5647 - accuracy: 0.7158\n",
      "Epoch 48: val_loss improved from 0.56244 to 0.56232, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5643 - accuracy: 0.7164 - val_loss: 0.5623 - val_accuracy: 0.7158\n",
      "Epoch 49/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.5646 - accuracy: 0.7156\n",
      "Epoch 49: val_loss did not improve from 0.56232\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5643 - accuracy: 0.7157 - val_loss: 0.5623 - val_accuracy: 0.7176\n",
      "Epoch 50/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.5642 - accuracy: 0.7164\n",
      "Epoch 50: val_loss did not improve from 0.56232\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5642 - accuracy: 0.7164 - val_loss: 0.5624 - val_accuracy: 0.7164\n",
      "Epoch 51/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.5617 - accuracy: 0.7194\n",
      "Epoch 51: val_loss did not improve from 0.56232\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5641 - accuracy: 0.7166 - val_loss: 0.5625 - val_accuracy: 0.7164\n",
      "Epoch 52/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.5643 - accuracy: 0.7161\n",
      "Epoch 52: val_loss improved from 0.56232 to 0.56225, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5642 - accuracy: 0.7164 - val_loss: 0.5623 - val_accuracy: 0.7164\n",
      "Epoch 53/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.5642 - accuracy: 0.7158\n",
      "Epoch 53: val_loss improved from 0.56225 to 0.56224, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5642 - accuracy: 0.7158 - val_loss: 0.5622 - val_accuracy: 0.7158\n",
      "Epoch 54/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.5640 - accuracy: 0.7168\n",
      "Epoch 54: val_loss did not improve from 0.56224\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5641 - accuracy: 0.7167 - val_loss: 0.5623 - val_accuracy: 0.7170\n",
      "Epoch 55/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.5642 - accuracy: 0.7167\n",
      "Epoch 55: val_loss improved from 0.56224 to 0.56223, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5641 - accuracy: 0.7166 - val_loss: 0.5622 - val_accuracy: 0.7158\n",
      "Epoch 56/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.5652 - accuracy: 0.7151\n",
      "Epoch 56: val_loss improved from 0.56223 to 0.56220, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5640 - accuracy: 0.7161 - val_loss: 0.5622 - val_accuracy: 0.7164\n",
      "Epoch 57/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.5642 - accuracy: 0.7174\n",
      "Epoch 57: val_loss did not improve from 0.56220\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5639 - accuracy: 0.7176 - val_loss: 0.5622 - val_accuracy: 0.7170\n",
      "Epoch 58/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.5642 - accuracy: 0.7168\n",
      "Epoch 58: val_loss improved from 0.56220 to 0.56214, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5641 - accuracy: 0.7166 - val_loss: 0.5621 - val_accuracy: 0.7164\n",
      "Epoch 59/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.5627 - accuracy: 0.7179\n",
      "Epoch 59: val_loss improved from 0.56214 to 0.56210, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5640 - accuracy: 0.7163 - val_loss: 0.5621 - val_accuracy: 0.7176\n",
      "Epoch 60/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.5648 - accuracy: 0.7159\n",
      "Epoch 60: val_loss did not improve from 0.56210\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5639 - accuracy: 0.7173 - val_loss: 0.5623 - val_accuracy: 0.7170\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.5621 - accuracy: 0.7176\n",
      "Drop Feature campaign\n",
      "data frame x has shape: (8516, 12)\n",
      "['education', 'pdays', 'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'married', 'single', 'admin.', 'blue-collar']\n",
      "Epoch 1/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.6710 - accuracy: 0.6000\n",
      "Epoch 1: val_loss improved from inf to 0.62838, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 3ms/step - loss: 0.6709 - accuracy: 0.6000 - val_loss: 0.6284 - val_accuracy: 0.6788\n",
      "Epoch 2/60\n",
      "650/682 [===========================>..] - ETA: 0s - loss: 0.6161 - accuracy: 0.6932\n",
      "Epoch 2: val_loss improved from 0.62838 to 0.59784, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6152 - accuracy: 0.6934 - val_loss: 0.5978 - val_accuracy: 0.7041\n",
      "Epoch 3/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.5948 - accuracy: 0.7064\n",
      "Epoch 3: val_loss improved from 0.59784 to 0.58425, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5951 - accuracy: 0.7060 - val_loss: 0.5842 - val_accuracy: 0.7076\n",
      "Epoch 4/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.5855 - accuracy: 0.7084\n",
      "Epoch 4: val_loss improved from 0.58425 to 0.57776, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5857 - accuracy: 0.7081 - val_loss: 0.5778 - val_accuracy: 0.7105\n",
      "Epoch 5/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.5813 - accuracy: 0.7084\n",
      "Epoch 5: val_loss improved from 0.57776 to 0.57463, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5811 - accuracy: 0.7085 - val_loss: 0.5746 - val_accuracy: 0.7087\n",
      "Epoch 6/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.5783 - accuracy: 0.7100\n",
      "Epoch 6: val_loss improved from 0.57463 to 0.57247, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5783 - accuracy: 0.7095 - val_loss: 0.5725 - val_accuracy: 0.7111\n",
      "Epoch 7/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.5766 - accuracy: 0.7107\n",
      "Epoch 7: val_loss improved from 0.57247 to 0.57119, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5766 - accuracy: 0.7107 - val_loss: 0.5712 - val_accuracy: 0.7123\n",
      "Epoch 8/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.5756 - accuracy: 0.7112\n",
      "Epoch 8: val_loss improved from 0.57119 to 0.57017, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5752 - accuracy: 0.7116 - val_loss: 0.5702 - val_accuracy: 0.7164\n",
      "Epoch 9/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.5741 - accuracy: 0.7125\n",
      "Epoch 9: val_loss improved from 0.57017 to 0.56943, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5741 - accuracy: 0.7125 - val_loss: 0.5694 - val_accuracy: 0.7140\n",
      "Epoch 10/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.5730 - accuracy: 0.7117\n",
      "Epoch 10: val_loss improved from 0.56943 to 0.56879, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5734 - accuracy: 0.7111 - val_loss: 0.5688 - val_accuracy: 0.7176\n",
      "Epoch 11/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.5726 - accuracy: 0.7134\n",
      "Epoch 11: val_loss improved from 0.56879 to 0.56834, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5725 - accuracy: 0.7133 - val_loss: 0.5683 - val_accuracy: 0.7134\n",
      "Epoch 12/60\n",
      "653/682 [===========================>..] - ETA: 0s - loss: 0.5726 - accuracy: 0.7095\n",
      "Epoch 12: val_loss improved from 0.56834 to 0.56793, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5719 - accuracy: 0.7108 - val_loss: 0.5679 - val_accuracy: 0.7134\n",
      "Epoch 13/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.5709 - accuracy: 0.7119\n",
      "Epoch 13: val_loss improved from 0.56793 to 0.56746, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5712 - accuracy: 0.7111 - val_loss: 0.5675 - val_accuracy: 0.7181\n",
      "Epoch 14/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.5703 - accuracy: 0.7134\n",
      "Epoch 14: val_loss improved from 0.56746 to 0.56736, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5707 - accuracy: 0.7129 - val_loss: 0.5674 - val_accuracy: 0.7176\n",
      "Epoch 15/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.5703 - accuracy: 0.7130\n",
      "Epoch 15: val_loss improved from 0.56736 to 0.56701, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5701 - accuracy: 0.7133 - val_loss: 0.5670 - val_accuracy: 0.7129\n",
      "Epoch 16/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.5693 - accuracy: 0.7128\n",
      "Epoch 16: val_loss improved from 0.56701 to 0.56638, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5698 - accuracy: 0.7116 - val_loss: 0.5664 - val_accuracy: 0.7176\n",
      "Epoch 17/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.5691 - accuracy: 0.7125\n",
      "Epoch 17: val_loss improved from 0.56638 to 0.56634, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5690 - accuracy: 0.7126 - val_loss: 0.5663 - val_accuracy: 0.7134\n",
      "Epoch 18/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.5692 - accuracy: 0.7127\n",
      "Epoch 18: val_loss improved from 0.56634 to 0.56586, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5690 - accuracy: 0.7123 - val_loss: 0.5659 - val_accuracy: 0.7134\n",
      "Epoch 19/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.5692 - accuracy: 0.7128\n",
      "Epoch 19: val_loss improved from 0.56586 to 0.56548, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5687 - accuracy: 0.7128 - val_loss: 0.5655 - val_accuracy: 0.7140\n",
      "Epoch 20/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.5686 - accuracy: 0.7129\n",
      "Epoch 20: val_loss improved from 0.56548 to 0.56535, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5684 - accuracy: 0.7129 - val_loss: 0.5654 - val_accuracy: 0.7187\n",
      "Epoch 21/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.5687 - accuracy: 0.7115\n",
      "Epoch 21: val_loss improved from 0.56535 to 0.56525, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5680 - accuracy: 0.7123 - val_loss: 0.5652 - val_accuracy: 0.7176\n",
      "Epoch 22/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.5675 - accuracy: 0.7122\n",
      "Epoch 22: val_loss improved from 0.56525 to 0.56501, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5678 - accuracy: 0.7122 - val_loss: 0.5650 - val_accuracy: 0.7181\n",
      "Epoch 23/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.5674 - accuracy: 0.7129\n",
      "Epoch 23: val_loss improved from 0.56501 to 0.56489, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5674 - accuracy: 0.7129 - val_loss: 0.5649 - val_accuracy: 0.7134\n",
      "Epoch 24/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.5663 - accuracy: 0.7139\n",
      "Epoch 24: val_loss improved from 0.56489 to 0.56460, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5672 - accuracy: 0.7138 - val_loss: 0.5646 - val_accuracy: 0.7146\n",
      "Epoch 25/60\n",
      "654/682 [===========================>..] - ETA: 0s - loss: 0.5679 - accuracy: 0.7124\n",
      "Epoch 25: val_loss did not improve from 0.56460\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5671 - accuracy: 0.7130 - val_loss: 0.5646 - val_accuracy: 0.7181\n",
      "Epoch 26/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.5670 - accuracy: 0.7133\n",
      "Epoch 26: val_loss improved from 0.56460 to 0.56432, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5669 - accuracy: 0.7128 - val_loss: 0.5643 - val_accuracy: 0.7140\n",
      "Epoch 27/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.5663 - accuracy: 0.7135\n",
      "Epoch 27: val_loss did not improve from 0.56432\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5667 - accuracy: 0.7129 - val_loss: 0.5644 - val_accuracy: 0.7181\n",
      "Epoch 28/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.5665 - accuracy: 0.7135\n",
      "Epoch 28: val_loss improved from 0.56432 to 0.56410, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5665 - accuracy: 0.7135 - val_loss: 0.5641 - val_accuracy: 0.7146\n",
      "Epoch 29/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.5661 - accuracy: 0.7144\n",
      "Epoch 29: val_loss improved from 0.56410 to 0.56406, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5664 - accuracy: 0.7139 - val_loss: 0.5641 - val_accuracy: 0.7146\n",
      "Epoch 30/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.5656 - accuracy: 0.7133\n",
      "Epoch 30: val_loss did not improve from 0.56406\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5662 - accuracy: 0.7130 - val_loss: 0.5641 - val_accuracy: 0.7140\n",
      "Epoch 31/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.5663 - accuracy: 0.7146\n",
      "Epoch 31: val_loss did not improve from 0.56406\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5660 - accuracy: 0.7147 - val_loss: 0.5641 - val_accuracy: 0.7140\n",
      "Epoch 32/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.5661 - accuracy: 0.7137\n",
      "Epoch 32: val_loss improved from 0.56406 to 0.56391, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5660 - accuracy: 0.7139 - val_loss: 0.5639 - val_accuracy: 0.7140\n",
      "Epoch 33/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.5660 - accuracy: 0.7138\n",
      "Epoch 33: val_loss improved from 0.56391 to 0.56373, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5658 - accuracy: 0.7136 - val_loss: 0.5637 - val_accuracy: 0.7146\n",
      "Epoch 34/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.5644 - accuracy: 0.7153\n",
      "Epoch 34: val_loss improved from 0.56373 to 0.56370, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5657 - accuracy: 0.7141 - val_loss: 0.5637 - val_accuracy: 0.7140\n",
      "Epoch 35/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.5655 - accuracy: 0.7147\n",
      "Epoch 35: val_loss improved from 0.56370 to 0.56359, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5657 - accuracy: 0.7142 - val_loss: 0.5636 - val_accuracy: 0.7152\n",
      "Epoch 36/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.5656 - accuracy: 0.7139\n",
      "Epoch 36: val_loss improved from 0.56359 to 0.56347, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5656 - accuracy: 0.7139 - val_loss: 0.5635 - val_accuracy: 0.7140\n",
      "Epoch 37/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.5657 - accuracy: 0.7139\n",
      "Epoch 37: val_loss improved from 0.56347 to 0.56334, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5655 - accuracy: 0.7135 - val_loss: 0.5633 - val_accuracy: 0.7152\n",
      "Epoch 38/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.5656 - accuracy: 0.7135\n",
      "Epoch 38: val_loss improved from 0.56334 to 0.56325, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5654 - accuracy: 0.7138 - val_loss: 0.5633 - val_accuracy: 0.7158\n",
      "Epoch 39/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.5644 - accuracy: 0.7150\n",
      "Epoch 39: val_loss did not improve from 0.56325\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5653 - accuracy: 0.7139 - val_loss: 0.5633 - val_accuracy: 0.7146\n",
      "Epoch 40/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.5663 - accuracy: 0.7127\n",
      "Epoch 40: val_loss did not improve from 0.56325\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5652 - accuracy: 0.7139 - val_loss: 0.5635 - val_accuracy: 0.7152\n",
      "Epoch 41/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.5658 - accuracy: 0.7141\n",
      "Epoch 41: val_loss did not improve from 0.56325\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5651 - accuracy: 0.7147 - val_loss: 0.5634 - val_accuracy: 0.7140\n",
      "Epoch 42/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.5651 - accuracy: 0.7154\n",
      "Epoch 42: val_loss did not improve from 0.56325\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5651 - accuracy: 0.7150 - val_loss: 0.5634 - val_accuracy: 0.7158\n",
      "Epoch 43/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.5643 - accuracy: 0.7146\n",
      "Epoch 43: val_loss improved from 0.56325 to 0.56325, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5651 - accuracy: 0.7139 - val_loss: 0.5632 - val_accuracy: 0.7181\n",
      "Epoch 44/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.5654 - accuracy: 0.7143\n",
      "Epoch 44: val_loss did not improve from 0.56325\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5648 - accuracy: 0.7150 - val_loss: 0.5634 - val_accuracy: 0.7164\n",
      "Epoch 45/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.5648 - accuracy: 0.7147\n",
      "Epoch 45: val_loss did not improve from 0.56325\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5648 - accuracy: 0.7147 - val_loss: 0.5639 - val_accuracy: 0.7158\n",
      "Epoch 46/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.5655 - accuracy: 0.7144\n",
      "Epoch 46: val_loss improved from 0.56325 to 0.56318, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5650 - accuracy: 0.7142 - val_loss: 0.5632 - val_accuracy: 0.7158\n",
      "Epoch 47/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.5643 - accuracy: 0.7153\n",
      "Epoch 47: val_loss did not improve from 0.56318\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5647 - accuracy: 0.7153 - val_loss: 0.5634 - val_accuracy: 0.7158\n",
      "Epoch 48/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.5651 - accuracy: 0.7151\n",
      "Epoch 48: val_loss improved from 0.56318 to 0.56309, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5649 - accuracy: 0.7151 - val_loss: 0.5631 - val_accuracy: 0.7140\n",
      "Epoch 49/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.5648 - accuracy: 0.7148\n",
      "Epoch 49: val_loss did not improve from 0.56309\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5648 - accuracy: 0.7148 - val_loss: 0.5632 - val_accuracy: 0.7152\n",
      "Epoch 50/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.5653 - accuracy: 0.7142\n",
      "Epoch 50: val_loss did not improve from 0.56309\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5647 - accuracy: 0.7150 - val_loss: 0.5631 - val_accuracy: 0.7146\n",
      "Epoch 51/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.5647 - accuracy: 0.7149\n",
      "Epoch 51: val_loss did not improve from 0.56309\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5647 - accuracy: 0.7150 - val_loss: 0.5631 - val_accuracy: 0.7134\n",
      "Epoch 52/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.5640 - accuracy: 0.7162\n",
      "Epoch 52: val_loss did not improve from 0.56309\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5643 - accuracy: 0.7158 - val_loss: 0.5633 - val_accuracy: 0.7146\n",
      "Epoch 53/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.5657 - accuracy: 0.7142\n",
      "Epoch 53: val_loss did not improve from 0.56309\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5646 - accuracy: 0.7148 - val_loss: 0.5631 - val_accuracy: 0.7140\n",
      "Epoch 54/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.5641 - accuracy: 0.7161\n",
      "Epoch 54: val_loss did not improve from 0.56309\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5646 - accuracy: 0.7160 - val_loss: 0.5631 - val_accuracy: 0.7129\n",
      "Epoch 55/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.5649 - accuracy: 0.7162\n",
      "Epoch 55: val_loss did not improve from 0.56309\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5646 - accuracy: 0.7160 - val_loss: 0.5631 - val_accuracy: 0.7140\n",
      "Epoch 56/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.5645 - accuracy: 0.7135\n",
      "Epoch 56: val_loss did not improve from 0.56309\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5646 - accuracy: 0.7138 - val_loss: 0.5631 - val_accuracy: 0.7123\n",
      "Epoch 57/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.5637 - accuracy: 0.7159\n",
      "Epoch 57: val_loss did not improve from 0.56309\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5646 - accuracy: 0.7153 - val_loss: 0.5631 - val_accuracy: 0.7123\n",
      "Epoch 58/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.5659 - accuracy: 0.7140\n",
      "Epoch 58: val_loss did not improve from 0.56309\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5643 - accuracy: 0.7151 - val_loss: 0.5635 - val_accuracy: 0.7170\n",
      "Epoch 59/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.5649 - accuracy: 0.7153\n",
      "Epoch 59: val_loss did not improve from 0.56309\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5643 - accuracy: 0.7154 - val_loss: 0.5634 - val_accuracy: 0.7176\n",
      "Epoch 60/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.5640 - accuracy: 0.7165\n",
      "Epoch 60: val_loss improved from 0.56309 to 0.56306, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5646 - accuracy: 0.7153 - val_loss: 0.5631 - val_accuracy: 0.7146\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.5631 - accuracy: 0.7146\n",
      "Drop Feature married\n",
      "data frame x has shape: (8516, 11)\n",
      "['education', 'pdays', 'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'single', 'admin.', 'blue-collar']\n",
      "Epoch 1/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.6562 - accuracy: 0.6310\n",
      "Epoch 1: val_loss improved from inf to 0.62407, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 3ms/step - loss: 0.6563 - accuracy: 0.6307 - val_loss: 0.6241 - val_accuracy: 0.6894\n",
      "Epoch 2/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.6048 - accuracy: 0.7040\n",
      "Epoch 2: val_loss improved from 0.62407 to 0.59213, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6050 - accuracy: 0.7032 - val_loss: 0.5921 - val_accuracy: 0.7123\n",
      "Epoch 3/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.5860 - accuracy: 0.7075\n",
      "Epoch 3: val_loss improved from 0.59213 to 0.57890, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5863 - accuracy: 0.7066 - val_loss: 0.5789 - val_accuracy: 0.7158\n",
      "Epoch 4/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.5776 - accuracy: 0.7130\n",
      "Epoch 4: val_loss improved from 0.57890 to 0.57390, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5784 - accuracy: 0.7116 - val_loss: 0.5739 - val_accuracy: 0.7193\n",
      "Epoch 5/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.5736 - accuracy: 0.7133\n",
      "Epoch 5: val_loss improved from 0.57390 to 0.57079, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5750 - accuracy: 0.7116 - val_loss: 0.5708 - val_accuracy: 0.7170\n",
      "Epoch 6/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.5741 - accuracy: 0.7101\n",
      "Epoch 6: val_loss improved from 0.57079 to 0.56890, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5729 - accuracy: 0.7107 - val_loss: 0.5689 - val_accuracy: 0.7181\n",
      "Epoch 7/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.5702 - accuracy: 0.7123\n",
      "Epoch 7: val_loss improved from 0.56890 to 0.56783, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5717 - accuracy: 0.7107 - val_loss: 0.5678 - val_accuracy: 0.7134\n",
      "Epoch 8/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.5703 - accuracy: 0.7092\n",
      "Epoch 8: val_loss improved from 0.56783 to 0.56728, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5709 - accuracy: 0.7089 - val_loss: 0.5673 - val_accuracy: 0.7140\n",
      "Epoch 9/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.5697 - accuracy: 0.7104\n",
      "Epoch 9: val_loss improved from 0.56728 to 0.56678, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5702 - accuracy: 0.7098 - val_loss: 0.5668 - val_accuracy: 0.7146\n",
      "Epoch 10/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.5704 - accuracy: 0.7084\n",
      "Epoch 10: val_loss improved from 0.56678 to 0.56674, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5697 - accuracy: 0.7098 - val_loss: 0.5667 - val_accuracy: 0.7134\n",
      "Epoch 11/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.5687 - accuracy: 0.7106\n",
      "Epoch 11: val_loss improved from 0.56674 to 0.56631, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5693 - accuracy: 0.7092 - val_loss: 0.5663 - val_accuracy: 0.7140\n",
      "Epoch 12/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.5699 - accuracy: 0.7103\n",
      "Epoch 12: val_loss improved from 0.56631 to 0.56594, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5688 - accuracy: 0.7110 - val_loss: 0.5659 - val_accuracy: 0.7129\n",
      "Epoch 13/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.5707 - accuracy: 0.7092\n",
      "Epoch 13: val_loss improved from 0.56594 to 0.56573, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5686 - accuracy: 0.7117 - val_loss: 0.5657 - val_accuracy: 0.7123\n",
      "Epoch 14/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.5673 - accuracy: 0.7115\n",
      "Epoch 14: val_loss improved from 0.56573 to 0.56555, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5682 - accuracy: 0.7108 - val_loss: 0.5655 - val_accuracy: 0.7152\n",
      "Epoch 15/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.5684 - accuracy: 0.7112\n",
      "Epoch 15: val_loss improved from 0.56555 to 0.56524, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5680 - accuracy: 0.7119 - val_loss: 0.5652 - val_accuracy: 0.7129\n",
      "Epoch 16/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.5673 - accuracy: 0.7109\n",
      "Epoch 16: val_loss did not improve from 0.56524\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5676 - accuracy: 0.7113 - val_loss: 0.5653 - val_accuracy: 0.7140\n",
      "Epoch 17/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.5670 - accuracy: 0.7120\n",
      "Epoch 17: val_loss improved from 0.56524 to 0.56490, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5674 - accuracy: 0.7110 - val_loss: 0.5649 - val_accuracy: 0.7158\n",
      "Epoch 18/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.5666 - accuracy: 0.7126\n",
      "Epoch 18: val_loss improved from 0.56490 to 0.56469, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5674 - accuracy: 0.7120 - val_loss: 0.5647 - val_accuracy: 0.7164\n",
      "Epoch 19/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.5669 - accuracy: 0.7116\n",
      "Epoch 19: val_loss improved from 0.56469 to 0.56466, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5671 - accuracy: 0.7119 - val_loss: 0.5647 - val_accuracy: 0.7164\n",
      "Epoch 20/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.5676 - accuracy: 0.7105\n",
      "Epoch 20: val_loss improved from 0.56466 to 0.56454, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5668 - accuracy: 0.7113 - val_loss: 0.5645 - val_accuracy: 0.7158\n",
      "Epoch 21/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.5668 - accuracy: 0.7130\n",
      "Epoch 21: val_loss improved from 0.56454 to 0.56428, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5667 - accuracy: 0.7132 - val_loss: 0.5643 - val_accuracy: 0.7170\n",
      "Epoch 22/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.5676 - accuracy: 0.7121\n",
      "Epoch 22: val_loss did not improve from 0.56428\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5664 - accuracy: 0.7130 - val_loss: 0.5647 - val_accuracy: 0.7134\n",
      "Epoch 23/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.5663 - accuracy: 0.7114\n",
      "Epoch 23: val_loss improved from 0.56428 to 0.56425, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5664 - accuracy: 0.7114 - val_loss: 0.5642 - val_accuracy: 0.7170\n",
      "Epoch 24/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.5662 - accuracy: 0.7129\n",
      "Epoch 24: val_loss improved from 0.56425 to 0.56405, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5662 - accuracy: 0.7129 - val_loss: 0.5641 - val_accuracy: 0.7158\n",
      "Epoch 25/60\n",
      "653/682 [===========================>..] - ETA: 0s - loss: 0.5650 - accuracy: 0.7142\n",
      "Epoch 25: val_loss improved from 0.56405 to 0.56401, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5662 - accuracy: 0.7129 - val_loss: 0.5640 - val_accuracy: 0.7152\n",
      "Epoch 26/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.5663 - accuracy: 0.7126\n",
      "Epoch 26: val_loss improved from 0.56401 to 0.56394, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5661 - accuracy: 0.7119 - val_loss: 0.5639 - val_accuracy: 0.7140\n",
      "Epoch 27/60\n",
      "654/682 [===========================>..] - ETA: 0s - loss: 0.5665 - accuracy: 0.7118\n",
      "Epoch 27: val_loss improved from 0.56394 to 0.56381, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5659 - accuracy: 0.7122 - val_loss: 0.5638 - val_accuracy: 0.7146\n",
      "Epoch 28/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.5650 - accuracy: 0.7149\n",
      "Epoch 28: val_loss did not improve from 0.56381\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5657 - accuracy: 0.7130 - val_loss: 0.5643 - val_accuracy: 0.7146\n",
      "Epoch 29/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.5655 - accuracy: 0.7127\n",
      "Epoch 29: val_loss did not improve from 0.56381\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5657 - accuracy: 0.7123 - val_loss: 0.5639 - val_accuracy: 0.7158\n",
      "Epoch 30/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.5655 - accuracy: 0.7118\n",
      "Epoch 30: val_loss improved from 0.56381 to 0.56379, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5654 - accuracy: 0.7125 - val_loss: 0.5638 - val_accuracy: 0.7117\n",
      "Epoch 31/60\n",
      "653/682 [===========================>..] - ETA: 0s - loss: 0.5666 - accuracy: 0.7133\n",
      "Epoch 31: val_loss improved from 0.56379 to 0.56356, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5656 - accuracy: 0.7144 - val_loss: 0.5636 - val_accuracy: 0.7129\n",
      "Epoch 32/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.5656 - accuracy: 0.7130\n",
      "Epoch 32: val_loss improved from 0.56356 to 0.56347, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5655 - accuracy: 0.7130 - val_loss: 0.5635 - val_accuracy: 0.7146\n",
      "Epoch 33/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.5650 - accuracy: 0.7131\n",
      "Epoch 33: val_loss did not improve from 0.56347\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5652 - accuracy: 0.7132 - val_loss: 0.5641 - val_accuracy: 0.7152\n",
      "Epoch 34/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.5662 - accuracy: 0.7114\n",
      "Epoch 34: val_loss improved from 0.56347 to 0.56335, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5654 - accuracy: 0.7119 - val_loss: 0.5633 - val_accuracy: 0.7146\n",
      "Epoch 35/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.5632 - accuracy: 0.7150\n",
      "Epoch 35: val_loss did not improve from 0.56335\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5652 - accuracy: 0.7128 - val_loss: 0.5634 - val_accuracy: 0.7158\n",
      "Epoch 36/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.5653 - accuracy: 0.7130\n",
      "Epoch 36: val_loss improved from 0.56335 to 0.56326, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5653 - accuracy: 0.7130 - val_loss: 0.5633 - val_accuracy: 0.7164\n",
      "Epoch 37/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.5666 - accuracy: 0.7126\n",
      "Epoch 37: val_loss did not improve from 0.56326\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5650 - accuracy: 0.7138 - val_loss: 0.5638 - val_accuracy: 0.7164\n",
      "Epoch 38/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.5641 - accuracy: 0.7148\n",
      "Epoch 38: val_loss improved from 0.56326 to 0.56318, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5653 - accuracy: 0.7136 - val_loss: 0.5632 - val_accuracy: 0.7158\n",
      "Epoch 39/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.5648 - accuracy: 0.7130\n",
      "Epoch 39: val_loss did not improve from 0.56318\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5650 - accuracy: 0.7128 - val_loss: 0.5633 - val_accuracy: 0.7158\n",
      "Epoch 40/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.5662 - accuracy: 0.7131\n",
      "Epoch 40: val_loss improved from 0.56318 to 0.56314, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5650 - accuracy: 0.7139 - val_loss: 0.5631 - val_accuracy: 0.7158\n",
      "Epoch 41/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.5660 - accuracy: 0.7134\n",
      "Epoch 41: val_loss did not improve from 0.56314\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5650 - accuracy: 0.7138 - val_loss: 0.5635 - val_accuracy: 0.7164\n",
      "Epoch 42/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.5648 - accuracy: 0.7135\n",
      "Epoch 42: val_loss did not improve from 0.56314\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5651 - accuracy: 0.7130 - val_loss: 0.5631 - val_accuracy: 0.7152\n",
      "Epoch 43/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.5638 - accuracy: 0.7139\n",
      "Epoch 43: val_loss did not improve from 0.56314\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5648 - accuracy: 0.7129 - val_loss: 0.5631 - val_accuracy: 0.7117\n",
      "Epoch 44/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.5650 - accuracy: 0.7146\n",
      "Epoch 44: val_loss did not improve from 0.56314\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5649 - accuracy: 0.7147 - val_loss: 0.5632 - val_accuracy: 0.7111\n",
      "Epoch 45/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.5662 - accuracy: 0.7127\n",
      "Epoch 45: val_loss did not improve from 0.56314\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5646 - accuracy: 0.7138 - val_loss: 0.5633 - val_accuracy: 0.7123\n",
      "Epoch 46/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.5653 - accuracy: 0.7138\n",
      "Epoch 46: val_loss did not improve from 0.56314\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5648 - accuracy: 0.7142 - val_loss: 0.5633 - val_accuracy: 0.7146\n",
      "Epoch 47/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.5644 - accuracy: 0.7142\n",
      "Epoch 47: val_loss did not improve from 0.56314\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5648 - accuracy: 0.7141 - val_loss: 0.5633 - val_accuracy: 0.7158\n",
      "Epoch 48/60\n",
      "652/682 [===========================>..] - ETA: 0s - loss: 0.5646 - accuracy: 0.7143\n",
      "Epoch 48: val_loss did not improve from 0.56314\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5647 - accuracy: 0.7142 - val_loss: 0.5632 - val_accuracy: 0.7146\n",
      "Epoch 49/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.5645 - accuracy: 0.7149\n",
      "Epoch 49: val_loss improved from 0.56314 to 0.56305, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5647 - accuracy: 0.7147 - val_loss: 0.5630 - val_accuracy: 0.7105\n",
      "Epoch 50/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.5647 - accuracy: 0.7143\n",
      "Epoch 50: val_loss did not improve from 0.56305\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5646 - accuracy: 0.7144 - val_loss: 0.5631 - val_accuracy: 0.7140\n",
      "Epoch 51/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.5654 - accuracy: 0.7146\n",
      "Epoch 51: val_loss did not improve from 0.56305\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5647 - accuracy: 0.7150 - val_loss: 0.5631 - val_accuracy: 0.7164\n",
      "Epoch 52/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.5634 - accuracy: 0.7157\n",
      "Epoch 52: val_loss improved from 0.56305 to 0.56295, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5646 - accuracy: 0.7153 - val_loss: 0.5630 - val_accuracy: 0.7158\n",
      "Epoch 53/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.5645 - accuracy: 0.7139\n",
      "Epoch 53: val_loss did not improve from 0.56295\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5645 - accuracy: 0.7141 - val_loss: 0.5636 - val_accuracy: 0.7170\n",
      "Epoch 54/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.5627 - accuracy: 0.7154\n",
      "Epoch 54: val_loss did not improve from 0.56295\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5644 - accuracy: 0.7147 - val_loss: 0.5645 - val_accuracy: 0.7164\n",
      "Epoch 55/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.5651 - accuracy: 0.7139\n",
      "Epoch 55: val_loss did not improve from 0.56295\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5648 - accuracy: 0.7141 - val_loss: 0.5633 - val_accuracy: 0.7164\n",
      "Epoch 56/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.5629 - accuracy: 0.7147\n",
      "Epoch 56: val_loss did not improve from 0.56295\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5646 - accuracy: 0.7136 - val_loss: 0.5630 - val_accuracy: 0.7129\n",
      "Epoch 57/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.5639 - accuracy: 0.7147\n",
      "Epoch 57: val_loss did not improve from 0.56295\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5645 - accuracy: 0.7139 - val_loss: 0.5630 - val_accuracy: 0.7140\n",
      "Epoch 58/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.5641 - accuracy: 0.7158\n",
      "Epoch 58: val_loss did not improve from 0.56295\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5647 - accuracy: 0.7148 - val_loss: 0.5630 - val_accuracy: 0.7164\n",
      "Epoch 59/60\n",
      "654/682 [===========================>..] - ETA: 0s - loss: 0.5641 - accuracy: 0.7150\n",
      "Epoch 59: val_loss did not improve from 0.56295\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5646 - accuracy: 0.7153 - val_loss: 0.5632 - val_accuracy: 0.7164\n",
      "Epoch 60/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.5644 - accuracy: 0.7148\n",
      "Epoch 60: val_loss did not improve from 0.56295\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5645 - accuracy: 0.7147 - val_loss: 0.5633 - val_accuracy: 0.7164\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.5630 - accuracy: 0.7158\n",
      "Drop Feature admin.\n",
      "data frame x has shape: (8516, 10)\n",
      "['education', 'pdays', 'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'single', 'blue-collar']\n",
      "Epoch 1/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.7413 - accuracy: 0.4741\n",
      "Epoch 1: val_loss improved from inf to 0.68365, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 3s 4ms/step - loss: 0.7395 - accuracy: 0.4759 - val_loss: 0.6837 - val_accuracy: 0.5608\n",
      "Epoch 2/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.6589 - accuracy: 0.6201\n",
      "Epoch 2: val_loss improved from 0.68365 to 0.62863, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6588 - accuracy: 0.6203 - val_loss: 0.6286 - val_accuracy: 0.6741\n",
      "Epoch 3/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.6197 - accuracy: 0.6782\n",
      "Epoch 3: val_loss improved from 0.62863 to 0.60156, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6197 - accuracy: 0.6784 - val_loss: 0.6016 - val_accuracy: 0.6964\n",
      "Epoch 4/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.6007 - accuracy: 0.6957\n",
      "Epoch 4: val_loss improved from 0.60156 to 0.58834, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6006 - accuracy: 0.6959 - val_loss: 0.5883 - val_accuracy: 0.7087\n",
      "Epoch 5/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.5904 - accuracy: 0.7045\n",
      "Epoch 5: val_loss improved from 0.58834 to 0.58092, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5906 - accuracy: 0.7047 - val_loss: 0.5809 - val_accuracy: 0.7111\n",
      "Epoch 6/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.5851 - accuracy: 0.7094\n",
      "Epoch 6: val_loss improved from 0.58092 to 0.57713, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5849 - accuracy: 0.7089 - val_loss: 0.5771 - val_accuracy: 0.7140\n",
      "Epoch 7/60\n",
      "651/682 [===========================>..] - ETA: 0s - loss: 0.5827 - accuracy: 0.7098\n",
      "Epoch 7: val_loss improved from 0.57713 to 0.57476, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5816 - accuracy: 0.7113 - val_loss: 0.5748 - val_accuracy: 0.7158\n",
      "Epoch 8/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.5791 - accuracy: 0.7127\n",
      "Epoch 8: val_loss improved from 0.57476 to 0.57330, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5793 - accuracy: 0.7123 - val_loss: 0.5733 - val_accuracy: 0.7158\n",
      "Epoch 9/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.5778 - accuracy: 0.7120\n",
      "Epoch 9: val_loss improved from 0.57330 to 0.57229, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5778 - accuracy: 0.7120 - val_loss: 0.5723 - val_accuracy: 0.7152\n",
      "Epoch 10/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.5761 - accuracy: 0.7134\n",
      "Epoch 10: val_loss improved from 0.57229 to 0.57142, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5765 - accuracy: 0.7126 - val_loss: 0.5714 - val_accuracy: 0.7146\n",
      "Epoch 11/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.5743 - accuracy: 0.7124\n",
      "Epoch 11: val_loss improved from 0.57142 to 0.57081, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5755 - accuracy: 0.7125 - val_loss: 0.5708 - val_accuracy: 0.7146\n",
      "Epoch 12/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.5744 - accuracy: 0.7131\n",
      "Epoch 12: val_loss improved from 0.57081 to 0.57030, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5747 - accuracy: 0.7128 - val_loss: 0.5703 - val_accuracy: 0.7146\n",
      "Epoch 13/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.5730 - accuracy: 0.7142\n",
      "Epoch 13: val_loss improved from 0.57030 to 0.56980, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5740 - accuracy: 0.7128 - val_loss: 0.5698 - val_accuracy: 0.7146\n",
      "Epoch 14/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.5739 - accuracy: 0.7116\n",
      "Epoch 14: val_loss improved from 0.56980 to 0.56971, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5733 - accuracy: 0.7126 - val_loss: 0.5697 - val_accuracy: 0.7152\n",
      "Epoch 15/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.5719 - accuracy: 0.7137\n",
      "Epoch 15: val_loss improved from 0.56971 to 0.56898, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5727 - accuracy: 0.7128 - val_loss: 0.5690 - val_accuracy: 0.7146\n",
      "Epoch 16/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.5712 - accuracy: 0.7133\n",
      "Epoch 16: val_loss improved from 0.56898 to 0.56877, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5721 - accuracy: 0.7126 - val_loss: 0.5688 - val_accuracy: 0.7146\n",
      "Epoch 17/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.5714 - accuracy: 0.7130\n",
      "Epoch 17: val_loss improved from 0.56877 to 0.56829, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5718 - accuracy: 0.7126 - val_loss: 0.5683 - val_accuracy: 0.7146\n",
      "Epoch 18/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.5692 - accuracy: 0.7136\n",
      "Epoch 18: val_loss improved from 0.56829 to 0.56789, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5713 - accuracy: 0.7126 - val_loss: 0.5679 - val_accuracy: 0.7146\n",
      "Epoch 19/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.5718 - accuracy: 0.7115\n",
      "Epoch 19: val_loss improved from 0.56789 to 0.56761, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5709 - accuracy: 0.7126 - val_loss: 0.5676 - val_accuracy: 0.7146\n",
      "Epoch 20/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.5714 - accuracy: 0.7126\n",
      "Epoch 20: val_loss improved from 0.56761 to 0.56729, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5705 - accuracy: 0.7128 - val_loss: 0.5673 - val_accuracy: 0.7146\n",
      "Epoch 21/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.5704 - accuracy: 0.7124\n",
      "Epoch 21: val_loss improved from 0.56729 to 0.56702, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5701 - accuracy: 0.7126 - val_loss: 0.5670 - val_accuracy: 0.7146\n",
      "Epoch 22/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.5697 - accuracy: 0.7126\n",
      "Epoch 22: val_loss improved from 0.56702 to 0.56674, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5697 - accuracy: 0.7126 - val_loss: 0.5667 - val_accuracy: 0.7146\n",
      "Epoch 23/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.5693 - accuracy: 0.7135\n",
      "Epoch 23: val_loss improved from 0.56674 to 0.56655, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5694 - accuracy: 0.7128 - val_loss: 0.5666 - val_accuracy: 0.7146\n",
      "Epoch 24/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.5692 - accuracy: 0.7126\n",
      "Epoch 24: val_loss improved from 0.56655 to 0.56641, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5691 - accuracy: 0.7126 - val_loss: 0.5664 - val_accuracy: 0.7146\n",
      "Epoch 25/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.5703 - accuracy: 0.7115\n",
      "Epoch 25: val_loss improved from 0.56641 to 0.56608, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5688 - accuracy: 0.7129 - val_loss: 0.5661 - val_accuracy: 0.7146\n",
      "Epoch 26/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.5680 - accuracy: 0.7144\n",
      "Epoch 26: val_loss improved from 0.56608 to 0.56590, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5685 - accuracy: 0.7133 - val_loss: 0.5659 - val_accuracy: 0.7146\n",
      "Epoch 27/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.5688 - accuracy: 0.7126\n",
      "Epoch 27: val_loss improved from 0.56590 to 0.56572, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5684 - accuracy: 0.7129 - val_loss: 0.5657 - val_accuracy: 0.7152\n",
      "Epoch 28/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.5685 - accuracy: 0.7123\n",
      "Epoch 28: val_loss did not improve from 0.56572\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5681 - accuracy: 0.7125 - val_loss: 0.5658 - val_accuracy: 0.7146\n",
      "Epoch 29/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.5681 - accuracy: 0.7144\n",
      "Epoch 29: val_loss improved from 0.56572 to 0.56558, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5679 - accuracy: 0.7147 - val_loss: 0.5656 - val_accuracy: 0.7140\n",
      "Epoch 30/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.5682 - accuracy: 0.7135\n",
      "Epoch 30: val_loss improved from 0.56558 to 0.56522, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5677 - accuracy: 0.7147 - val_loss: 0.5652 - val_accuracy: 0.7146\n",
      "Epoch 31/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.5685 - accuracy: 0.7128\n",
      "Epoch 31: val_loss did not improve from 0.56522\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5675 - accuracy: 0.7141 - val_loss: 0.5653 - val_accuracy: 0.7140\n",
      "Epoch 32/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.5670 - accuracy: 0.7143\n",
      "Epoch 32: val_loss improved from 0.56522 to 0.56500, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5673 - accuracy: 0.7138 - val_loss: 0.5650 - val_accuracy: 0.7140\n",
      "Epoch 33/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.5674 - accuracy: 0.7138\n",
      "Epoch 33: val_loss did not improve from 0.56500\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5671 - accuracy: 0.7142 - val_loss: 0.5652 - val_accuracy: 0.7123\n",
      "Epoch 34/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.5669 - accuracy: 0.7152\n",
      "Epoch 34: val_loss improved from 0.56500 to 0.56497, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5671 - accuracy: 0.7142 - val_loss: 0.5650 - val_accuracy: 0.7123\n",
      "Epoch 35/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.5664 - accuracy: 0.7148\n",
      "Epoch 35: val_loss did not improve from 0.56497\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5668 - accuracy: 0.7141 - val_loss: 0.5651 - val_accuracy: 0.7152\n",
      "Epoch 36/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.5679 - accuracy: 0.7127\n",
      "Epoch 36: val_loss improved from 0.56497 to 0.56453, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5668 - accuracy: 0.7136 - val_loss: 0.5645 - val_accuracy: 0.7134\n",
      "Epoch 37/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.5667 - accuracy: 0.7146\n",
      "Epoch 37: val_loss improved from 0.56453 to 0.56450, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5666 - accuracy: 0.7148 - val_loss: 0.5645 - val_accuracy: 0.7123\n",
      "Epoch 38/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.5663 - accuracy: 0.7137\n",
      "Epoch 38: val_loss did not improve from 0.56450\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5663 - accuracy: 0.7141 - val_loss: 0.5646 - val_accuracy: 0.7140\n",
      "Epoch 39/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.5659 - accuracy: 0.7151\n",
      "Epoch 39: val_loss improved from 0.56450 to 0.56424, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5665 - accuracy: 0.7150 - val_loss: 0.5642 - val_accuracy: 0.7134\n",
      "Epoch 40/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.5657 - accuracy: 0.7148\n",
      "Epoch 40: val_loss did not improve from 0.56424\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5662 - accuracy: 0.7138 - val_loss: 0.5644 - val_accuracy: 0.7152\n",
      "Epoch 41/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.5659 - accuracy: 0.7147\n",
      "Epoch 41: val_loss did not improve from 0.56424\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5662 - accuracy: 0.7147 - val_loss: 0.5643 - val_accuracy: 0.7140\n",
      "Epoch 42/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.5655 - accuracy: 0.7147\n",
      "Epoch 42: val_loss improved from 0.56424 to 0.56397, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5661 - accuracy: 0.7138 - val_loss: 0.5640 - val_accuracy: 0.7134\n",
      "Epoch 43/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.5657 - accuracy: 0.7157\n",
      "Epoch 43: val_loss improved from 0.56397 to 0.56391, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5661 - accuracy: 0.7150 - val_loss: 0.5639 - val_accuracy: 0.7134\n",
      "Epoch 44/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.5659 - accuracy: 0.7144\n",
      "Epoch 44: val_loss did not improve from 0.56391\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5659 - accuracy: 0.7144 - val_loss: 0.5640 - val_accuracy: 0.7146\n",
      "Epoch 45/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.5664 - accuracy: 0.7143\n",
      "Epoch 45: val_loss improved from 0.56391 to 0.56390, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5658 - accuracy: 0.7147 - val_loss: 0.5639 - val_accuracy: 0.7129\n",
      "Epoch 46/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.5653 - accuracy: 0.7142\n",
      "Epoch 46: val_loss improved from 0.56390 to 0.56376, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5658 - accuracy: 0.7142 - val_loss: 0.5638 - val_accuracy: 0.7134\n",
      "Epoch 47/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.5652 - accuracy: 0.7149\n",
      "Epoch 47: val_loss did not improve from 0.56376\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5657 - accuracy: 0.7147 - val_loss: 0.5638 - val_accuracy: 0.7140\n",
      "Epoch 48/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.5648 - accuracy: 0.7152\n",
      "Epoch 48: val_loss improved from 0.56376 to 0.56371, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5657 - accuracy: 0.7147 - val_loss: 0.5637 - val_accuracy: 0.7134\n",
      "Epoch 49/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.5657 - accuracy: 0.7143\n",
      "Epoch 49: val_loss did not improve from 0.56371\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5656 - accuracy: 0.7142 - val_loss: 0.5637 - val_accuracy: 0.7140\n",
      "Epoch 50/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.5655 - accuracy: 0.7142\n",
      "Epoch 50: val_loss improved from 0.56371 to 0.56371, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5655 - accuracy: 0.7142 - val_loss: 0.5637 - val_accuracy: 0.7134\n",
      "Epoch 51/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.5659 - accuracy: 0.7146\n",
      "Epoch 51: val_loss improved from 0.56371 to 0.56359, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5654 - accuracy: 0.7150 - val_loss: 0.5636 - val_accuracy: 0.7134\n",
      "Epoch 52/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.5654 - accuracy: 0.7151\n",
      "Epoch 52: val_loss improved from 0.56359 to 0.56354, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5654 - accuracy: 0.7157 - val_loss: 0.5635 - val_accuracy: 0.7140\n",
      "Epoch 53/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.5646 - accuracy: 0.7164\n",
      "Epoch 53: val_loss did not improve from 0.56354\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5652 - accuracy: 0.7166 - val_loss: 0.5636 - val_accuracy: 0.7134\n",
      "Epoch 54/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.5658 - accuracy: 0.7147\n",
      "Epoch 54: val_loss improved from 0.56354 to 0.56353, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5653 - accuracy: 0.7154 - val_loss: 0.5635 - val_accuracy: 0.7140\n",
      "Epoch 55/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.5654 - accuracy: 0.7153\n",
      "Epoch 55: val_loss improved from 0.56353 to 0.56344, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5653 - accuracy: 0.7153 - val_loss: 0.5634 - val_accuracy: 0.7146\n",
      "Epoch 56/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.5660 - accuracy: 0.7151\n",
      "Epoch 56: val_loss improved from 0.56344 to 0.56339, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5652 - accuracy: 0.7157 - val_loss: 0.5634 - val_accuracy: 0.7146\n",
      "Epoch 57/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.5652 - accuracy: 0.7153\n",
      "Epoch 57: val_loss improved from 0.56339 to 0.56337, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5652 - accuracy: 0.7154 - val_loss: 0.5634 - val_accuracy: 0.7146\n",
      "Epoch 58/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.5648 - accuracy: 0.7154\n",
      "Epoch 58: val_loss improved from 0.56337 to 0.56333, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5650 - accuracy: 0.7157 - val_loss: 0.5633 - val_accuracy: 0.7146\n",
      "Epoch 59/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.5642 - accuracy: 0.7184\n",
      "Epoch 59: val_loss did not improve from 0.56333\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5651 - accuracy: 0.7166 - val_loss: 0.5633 - val_accuracy: 0.7146\n",
      "Epoch 60/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.5638 - accuracy: 0.7163\n",
      "Epoch 60: val_loss did not improve from 0.56333\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5650 - accuracy: 0.7154 - val_loss: 0.5634 - val_accuracy: 0.7140\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.5633 - accuracy: 0.7146\n",
      "Drop Feature single\n",
      "data frame x has shape: (8516, 9)\n",
      "['education', 'pdays', 'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'blue-collar']\n",
      "Epoch 1/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.7299 - accuracy: 0.4470\n",
      "Epoch 1: val_loss improved from inf to 0.67669, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 3ms/step - loss: 0.7290 - accuracy: 0.4497 - val_loss: 0.6767 - val_accuracy: 0.5966\n",
      "Epoch 2/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.6535 - accuracy: 0.6400\n",
      "Epoch 2: val_loss improved from 0.67669 to 0.62523, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6533 - accuracy: 0.6402 - val_loss: 0.6252 - val_accuracy: 0.6882\n",
      "Epoch 3/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.6170 - accuracy: 0.6891\n",
      "Epoch 3: val_loss improved from 0.62523 to 0.59960, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6168 - accuracy: 0.6894 - val_loss: 0.5996 - val_accuracy: 0.7035\n",
      "Epoch 4/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.5985 - accuracy: 0.7006\n",
      "Epoch 4: val_loss improved from 0.59960 to 0.58719, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5986 - accuracy: 0.7003 - val_loss: 0.5872 - val_accuracy: 0.7099\n",
      "Epoch 5/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.5895 - accuracy: 0.7038\n",
      "Epoch 5: val_loss improved from 0.58719 to 0.58071, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5895 - accuracy: 0.7038 - val_loss: 0.5807 - val_accuracy: 0.7123\n",
      "Epoch 6/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.5841 - accuracy: 0.7067\n",
      "Epoch 6: val_loss improved from 0.58071 to 0.57671, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5843 - accuracy: 0.7069 - val_loss: 0.5767 - val_accuracy: 0.7140\n",
      "Epoch 7/60\n",
      "654/682 [===========================>..] - ETA: 0s - loss: 0.5807 - accuracy: 0.7093\n",
      "Epoch 7: val_loss improved from 0.57671 to 0.57449, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5811 - accuracy: 0.7089 - val_loss: 0.5745 - val_accuracy: 0.7152\n",
      "Epoch 8/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.5790 - accuracy: 0.7095\n",
      "Epoch 8: val_loss improved from 0.57449 to 0.57304, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5790 - accuracy: 0.7095 - val_loss: 0.5730 - val_accuracy: 0.7152\n",
      "Epoch 9/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.5765 - accuracy: 0.7119\n",
      "Epoch 9: val_loss improved from 0.57304 to 0.57228, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5774 - accuracy: 0.7106 - val_loss: 0.5723 - val_accuracy: 0.7129\n",
      "Epoch 10/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.5765 - accuracy: 0.7110\n",
      "Epoch 10: val_loss improved from 0.57228 to 0.57109, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5764 - accuracy: 0.7113 - val_loss: 0.5711 - val_accuracy: 0.7170\n",
      "Epoch 11/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.5752 - accuracy: 0.7110\n",
      "Epoch 11: val_loss improved from 0.57109 to 0.57072, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5752 - accuracy: 0.7110 - val_loss: 0.5707 - val_accuracy: 0.7129\n",
      "Epoch 12/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.5750 - accuracy: 0.7100\n",
      "Epoch 12: val_loss improved from 0.57072 to 0.56984, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5745 - accuracy: 0.7104 - val_loss: 0.5698 - val_accuracy: 0.7129\n",
      "Epoch 13/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.5739 - accuracy: 0.7104\n",
      "Epoch 13: val_loss improved from 0.56984 to 0.56932, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5736 - accuracy: 0.7104 - val_loss: 0.5693 - val_accuracy: 0.7129\n",
      "Epoch 14/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.5732 - accuracy: 0.7116\n",
      "Epoch 14: val_loss improved from 0.56932 to 0.56887, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5730 - accuracy: 0.7114 - val_loss: 0.5689 - val_accuracy: 0.7129\n",
      "Epoch 15/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.5727 - accuracy: 0.7123\n",
      "Epoch 15: val_loss improved from 0.56887 to 0.56848, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5724 - accuracy: 0.7128 - val_loss: 0.5685 - val_accuracy: 0.7129\n",
      "Epoch 16/60\n",
      "653/682 [===========================>..] - ETA: 0s - loss: 0.5714 - accuracy: 0.7129\n",
      "Epoch 16: val_loss improved from 0.56848 to 0.56798, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5718 - accuracy: 0.7119 - val_loss: 0.5680 - val_accuracy: 0.7129\n",
      "Epoch 17/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.5719 - accuracy: 0.7121\n",
      "Epoch 17: val_loss improved from 0.56798 to 0.56760, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5714 - accuracy: 0.7122 - val_loss: 0.5676 - val_accuracy: 0.7129\n",
      "Epoch 18/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.5707 - accuracy: 0.7118\n",
      "Epoch 18: val_loss improved from 0.56760 to 0.56753, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5707 - accuracy: 0.7116 - val_loss: 0.5675 - val_accuracy: 0.7181\n",
      "Epoch 19/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.5717 - accuracy: 0.7111\n",
      "Epoch 19: val_loss improved from 0.56753 to 0.56706, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5705 - accuracy: 0.7116 - val_loss: 0.5671 - val_accuracy: 0.7123\n",
      "Epoch 20/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.5699 - accuracy: 0.7110\n",
      "Epoch 20: val_loss improved from 0.56706 to 0.56678, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5698 - accuracy: 0.7111 - val_loss: 0.5668 - val_accuracy: 0.7134\n",
      "Epoch 21/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.5689 - accuracy: 0.7133\n",
      "Epoch 21: val_loss improved from 0.56678 to 0.56653, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5696 - accuracy: 0.7126 - val_loss: 0.5665 - val_accuracy: 0.7129\n",
      "Epoch 22/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.5684 - accuracy: 0.7124\n",
      "Epoch 22: val_loss improved from 0.56653 to 0.56631, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5692 - accuracy: 0.7116 - val_loss: 0.5663 - val_accuracy: 0.7129\n",
      "Epoch 23/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.5688 - accuracy: 0.7130\n",
      "Epoch 23: val_loss improved from 0.56631 to 0.56610, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5689 - accuracy: 0.7130 - val_loss: 0.5661 - val_accuracy: 0.7134\n",
      "Epoch 24/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.5695 - accuracy: 0.7118\n",
      "Epoch 24: val_loss improved from 0.56610 to 0.56591, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5687 - accuracy: 0.7125 - val_loss: 0.5659 - val_accuracy: 0.7134\n",
      "Epoch 25/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.5678 - accuracy: 0.7143\n",
      "Epoch 25: val_loss improved from 0.56591 to 0.56569, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5684 - accuracy: 0.7132 - val_loss: 0.5657 - val_accuracy: 0.7134\n",
      "Epoch 26/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.5688 - accuracy: 0.7120\n",
      "Epoch 26: val_loss improved from 0.56569 to 0.56552, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5681 - accuracy: 0.7126 - val_loss: 0.5655 - val_accuracy: 0.7134\n",
      "Epoch 27/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.5682 - accuracy: 0.7129\n",
      "Epoch 27: val_loss improved from 0.56552 to 0.56534, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5679 - accuracy: 0.7129 - val_loss: 0.5653 - val_accuracy: 0.7117\n",
      "Epoch 28/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.5683 - accuracy: 0.7120\n",
      "Epoch 28: val_loss did not improve from 0.56534\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5676 - accuracy: 0.7129 - val_loss: 0.5656 - val_accuracy: 0.7140\n",
      "Epoch 29/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.5666 - accuracy: 0.7147\n",
      "Epoch 29: val_loss improved from 0.56534 to 0.56513, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5675 - accuracy: 0.7136 - val_loss: 0.5651 - val_accuracy: 0.7134\n",
      "Epoch 30/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.5663 - accuracy: 0.7147\n",
      "Epoch 30: val_loss improved from 0.56513 to 0.56489, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5672 - accuracy: 0.7132 - val_loss: 0.5649 - val_accuracy: 0.7123\n",
      "Epoch 31/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.5670 - accuracy: 0.7128\n",
      "Epoch 31: val_loss improved from 0.56489 to 0.56484, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5672 - accuracy: 0.7130 - val_loss: 0.5648 - val_accuracy: 0.7129\n",
      "Epoch 32/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.5677 - accuracy: 0.7134\n",
      "Epoch 32: val_loss improved from 0.56484 to 0.56468, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5669 - accuracy: 0.7136 - val_loss: 0.5647 - val_accuracy: 0.7123\n",
      "Epoch 33/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.5672 - accuracy: 0.7136\n",
      "Epoch 33: val_loss did not improve from 0.56468\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5668 - accuracy: 0.7135 - val_loss: 0.5647 - val_accuracy: 0.7123\n",
      "Epoch 34/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.5666 - accuracy: 0.7133\n",
      "Epoch 34: val_loss improved from 0.56468 to 0.56465, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5666 - accuracy: 0.7133 - val_loss: 0.5647 - val_accuracy: 0.7123\n",
      "Epoch 35/60\n",
      "654/682 [===========================>..] - ETA: 0s - loss: 0.5686 - accuracy: 0.7116\n",
      "Epoch 35: val_loss improved from 0.56465 to 0.56441, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5666 - accuracy: 0.7138 - val_loss: 0.5644 - val_accuracy: 0.7123\n",
      "Epoch 36/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.5671 - accuracy: 0.7124\n",
      "Epoch 36: val_loss improved from 0.56441 to 0.56434, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5663 - accuracy: 0.7132 - val_loss: 0.5643 - val_accuracy: 0.7123\n",
      "Epoch 37/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.5661 - accuracy: 0.7126\n",
      "Epoch 37: val_loss improved from 0.56434 to 0.56428, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5664 - accuracy: 0.7123 - val_loss: 0.5643 - val_accuracy: 0.7123\n",
      "Epoch 38/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.5662 - accuracy: 0.7132\n",
      "Epoch 38: val_loss improved from 0.56428 to 0.56419, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5662 - accuracy: 0.7128 - val_loss: 0.5642 - val_accuracy: 0.7123\n",
      "Epoch 39/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.5668 - accuracy: 0.7125\n",
      "Epoch 39: val_loss improved from 0.56419 to 0.56411, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5661 - accuracy: 0.7129 - val_loss: 0.5641 - val_accuracy: 0.7123\n",
      "Epoch 40/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.5660 - accuracy: 0.7131\n",
      "Epoch 40: val_loss did not improve from 0.56411\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5660 - accuracy: 0.7128 - val_loss: 0.5642 - val_accuracy: 0.7134\n",
      "Epoch 41/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.5649 - accuracy: 0.7144\n",
      "Epoch 41: val_loss did not improve from 0.56411\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5659 - accuracy: 0.7133 - val_loss: 0.5641 - val_accuracy: 0.7123\n",
      "Epoch 42/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.5659 - accuracy: 0.7124\n",
      "Epoch 42: val_loss did not improve from 0.56411\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5658 - accuracy: 0.7126 - val_loss: 0.5642 - val_accuracy: 0.7140\n",
      "Epoch 43/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.5649 - accuracy: 0.7141\n",
      "Epoch 43: val_loss improved from 0.56411 to 0.56409, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5657 - accuracy: 0.7132 - val_loss: 0.5641 - val_accuracy: 0.7129\n",
      "Epoch 44/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.5665 - accuracy: 0.7127\n",
      "Epoch 44: val_loss improved from 0.56409 to 0.56391, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5657 - accuracy: 0.7135 - val_loss: 0.5639 - val_accuracy: 0.7123\n",
      "Epoch 45/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.5656 - accuracy: 0.7132\n",
      "Epoch 45: val_loss improved from 0.56391 to 0.56382, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5657 - accuracy: 0.7132 - val_loss: 0.5638 - val_accuracy: 0.7123\n",
      "Epoch 46/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.5654 - accuracy: 0.7135\n",
      "Epoch 46: val_loss did not improve from 0.56382\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5655 - accuracy: 0.7135 - val_loss: 0.5642 - val_accuracy: 0.7134\n",
      "Epoch 47/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.5664 - accuracy: 0.7142\n",
      "Epoch 47: val_loss improved from 0.56382 to 0.56382, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5655 - accuracy: 0.7147 - val_loss: 0.5638 - val_accuracy: 0.7129\n",
      "Epoch 48/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.5664 - accuracy: 0.7123\n",
      "Epoch 48: val_loss improved from 0.56382 to 0.56379, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5654 - accuracy: 0.7136 - val_loss: 0.5638 - val_accuracy: 0.7123\n",
      "Epoch 49/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.5669 - accuracy: 0.7133\n",
      "Epoch 49: val_loss improved from 0.56379 to 0.56373, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5654 - accuracy: 0.7141 - val_loss: 0.5637 - val_accuracy: 0.7140\n",
      "Epoch 50/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.5645 - accuracy: 0.7156\n",
      "Epoch 50: val_loss did not improve from 0.56373\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5653 - accuracy: 0.7148 - val_loss: 0.5638 - val_accuracy: 0.7123\n",
      "Epoch 51/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.5653 - accuracy: 0.7136\n",
      "Epoch 51: val_loss did not improve from 0.56373\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5653 - accuracy: 0.7136 - val_loss: 0.5638 - val_accuracy: 0.7123\n",
      "Epoch 52/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.5657 - accuracy: 0.7148\n",
      "Epoch 52: val_loss improved from 0.56373 to 0.56370, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5654 - accuracy: 0.7148 - val_loss: 0.5637 - val_accuracy: 0.7140\n",
      "Epoch 53/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.5661 - accuracy: 0.7141\n",
      "Epoch 53: val_loss improved from 0.56370 to 0.56369, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5652 - accuracy: 0.7150 - val_loss: 0.5637 - val_accuracy: 0.7140\n",
      "Epoch 54/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.5653 - accuracy: 0.7160\n",
      "Epoch 54: val_loss did not improve from 0.56369\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5651 - accuracy: 0.7155 - val_loss: 0.5637 - val_accuracy: 0.7123\n",
      "Epoch 55/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.5632 - accuracy: 0.7162\n",
      "Epoch 55: val_loss improved from 0.56369 to 0.56364, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5651 - accuracy: 0.7148 - val_loss: 0.5636 - val_accuracy: 0.7123\n",
      "Epoch 56/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.5641 - accuracy: 0.7158\n",
      "Epoch 56: val_loss improved from 0.56364 to 0.56360, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5650 - accuracy: 0.7147 - val_loss: 0.5636 - val_accuracy: 0.7129\n",
      "Epoch 57/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.5651 - accuracy: 0.7144\n",
      "Epoch 57: val_loss did not improve from 0.56360\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5651 - accuracy: 0.7144 - val_loss: 0.5636 - val_accuracy: 0.7134\n",
      "Epoch 58/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.5652 - accuracy: 0.7139\n",
      "Epoch 58: val_loss did not improve from 0.56360\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5650 - accuracy: 0.7142 - val_loss: 0.5637 - val_accuracy: 0.7134\n",
      "Epoch 59/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.5655 - accuracy: 0.7142\n",
      "Epoch 59: val_loss did not improve from 0.56360\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5649 - accuracy: 0.7147 - val_loss: 0.5639 - val_accuracy: 0.7134\n",
      "Epoch 60/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.5640 - accuracy: 0.7171\n",
      "Epoch 60: val_loss improved from 0.56360 to 0.56360, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5650 - accuracy: 0.7150 - val_loss: 0.5636 - val_accuracy: 0.7117\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.5636 - accuracy: 0.7117\n",
      "Drop Feature cons.conf.idx\n",
      "data frame x has shape: (8516, 8)\n",
      "['education', 'pdays', 'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx', 'euribor3m', 'blue-collar']\n",
      "Epoch 1/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.6345 - accuracy: 0.6295\n",
      "Epoch 1: val_loss improved from inf to 0.61603, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 3ms/step - loss: 0.6345 - accuracy: 0.6295 - val_loss: 0.6160 - val_accuracy: 0.6671\n",
      "Epoch 2/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.6039 - accuracy: 0.6919\n",
      "Epoch 2: val_loss improved from 0.61603 to 0.59520, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.6037 - accuracy: 0.6922 - val_loss: 0.5952 - val_accuracy: 0.7029\n",
      "Epoch 3/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.5908 - accuracy: 0.7030\n",
      "Epoch 3: val_loss improved from 0.59520 to 0.58479, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5901 - accuracy: 0.7038 - val_loss: 0.5848 - val_accuracy: 0.7093\n",
      "Epoch 4/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.5831 - accuracy: 0.7097\n",
      "Epoch 4: val_loss improved from 0.58479 to 0.57957, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5830 - accuracy: 0.7097 - val_loss: 0.5796 - val_accuracy: 0.7082\n",
      "Epoch 5/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.5798 - accuracy: 0.7113\n",
      "Epoch 5: val_loss improved from 0.57957 to 0.57610, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5798 - accuracy: 0.7111 - val_loss: 0.5761 - val_accuracy: 0.7140\n",
      "Epoch 6/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.5778 - accuracy: 0.7124\n",
      "Epoch 6: val_loss improved from 0.57610 to 0.57432, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5777 - accuracy: 0.7126 - val_loss: 0.5743 - val_accuracy: 0.7129\n",
      "Epoch 7/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.5760 - accuracy: 0.7145\n",
      "Epoch 7: val_loss improved from 0.57432 to 0.57319, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5764 - accuracy: 0.7133 - val_loss: 0.5732 - val_accuracy: 0.7123\n",
      "Epoch 8/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.5742 - accuracy: 0.7128\n",
      "Epoch 8: val_loss improved from 0.57319 to 0.57226, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5757 - accuracy: 0.7113 - val_loss: 0.5723 - val_accuracy: 0.7134\n",
      "Epoch 9/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.5748 - accuracy: 0.7119\n",
      "Epoch 9: val_loss improved from 0.57226 to 0.57210, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5748 - accuracy: 0.7119 - val_loss: 0.5721 - val_accuracy: 0.7176\n",
      "Epoch 10/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.5735 - accuracy: 0.7130\n",
      "Epoch 10: val_loss improved from 0.57210 to 0.57118, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5747 - accuracy: 0.7117 - val_loss: 0.5712 - val_accuracy: 0.7129\n",
      "Epoch 11/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.5739 - accuracy: 0.7124\n",
      "Epoch 11: val_loss improved from 0.57118 to 0.57079, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5741 - accuracy: 0.7122 - val_loss: 0.5708 - val_accuracy: 0.7146\n",
      "Epoch 12/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.5729 - accuracy: 0.7140\n",
      "Epoch 12: val_loss improved from 0.57079 to 0.57051, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5737 - accuracy: 0.7126 - val_loss: 0.5705 - val_accuracy: 0.7146\n",
      "Epoch 13/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.5735 - accuracy: 0.7119\n",
      "Epoch 13: val_loss improved from 0.57051 to 0.57028, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5735 - accuracy: 0.7120 - val_loss: 0.5703 - val_accuracy: 0.7146\n",
      "Epoch 14/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.5737 - accuracy: 0.7120\n",
      "Epoch 14: val_loss improved from 0.57028 to 0.57000, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5731 - accuracy: 0.7126 - val_loss: 0.5700 - val_accuracy: 0.7146\n",
      "Epoch 15/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.5727 - accuracy: 0.7128\n",
      "Epoch 15: val_loss improved from 0.57000 to 0.56983, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5729 - accuracy: 0.7126 - val_loss: 0.5698 - val_accuracy: 0.7146\n",
      "Epoch 16/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.5714 - accuracy: 0.7144\n",
      "Epoch 16: val_loss improved from 0.56983 to 0.56978, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5725 - accuracy: 0.7128 - val_loss: 0.5698 - val_accuracy: 0.7146\n",
      "Epoch 17/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.5726 - accuracy: 0.7128\n",
      "Epoch 17: val_loss improved from 0.56978 to 0.56955, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5723 - accuracy: 0.7130 - val_loss: 0.5695 - val_accuracy: 0.7152\n",
      "Epoch 18/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.5719 - accuracy: 0.7127\n",
      "Epoch 18: val_loss improved from 0.56955 to 0.56946, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5721 - accuracy: 0.7125 - val_loss: 0.5695 - val_accuracy: 0.7152\n",
      "Epoch 19/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.5720 - accuracy: 0.7128\n",
      "Epoch 19: val_loss improved from 0.56946 to 0.56945, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5719 - accuracy: 0.7129 - val_loss: 0.5694 - val_accuracy: 0.7152\n",
      "Epoch 20/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.5718 - accuracy: 0.7131\n",
      "Epoch 20: val_loss improved from 0.56945 to 0.56915, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5718 - accuracy: 0.7130 - val_loss: 0.5691 - val_accuracy: 0.7152\n",
      "Epoch 21/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.5715 - accuracy: 0.7132\n",
      "Epoch 21: val_loss did not improve from 0.56915\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5714 - accuracy: 0.7129 - val_loss: 0.5696 - val_accuracy: 0.7152\n",
      "Epoch 22/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.5724 - accuracy: 0.7118\n",
      "Epoch 22: val_loss improved from 0.56915 to 0.56892, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5715 - accuracy: 0.7132 - val_loss: 0.5689 - val_accuracy: 0.7158\n",
      "Epoch 23/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.5711 - accuracy: 0.7131\n",
      "Epoch 23: val_loss improved from 0.56892 to 0.56882, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5714 - accuracy: 0.7129 - val_loss: 0.5688 - val_accuracy: 0.7158\n",
      "Epoch 24/60\n",
      "670/682 [============================>.] - ETA: 0s - loss: 0.5715 - accuracy: 0.7125\n",
      "Epoch 24: val_loss improved from 0.56882 to 0.56875, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5711 - accuracy: 0.7133 - val_loss: 0.5688 - val_accuracy: 0.7158\n",
      "Epoch 25/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.5710 - accuracy: 0.7134\n",
      "Epoch 25: val_loss improved from 0.56875 to 0.56869, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5711 - accuracy: 0.7133 - val_loss: 0.5687 - val_accuracy: 0.7158\n",
      "Epoch 26/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.5714 - accuracy: 0.7129\n",
      "Epoch 26: val_loss improved from 0.56869 to 0.56859, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5710 - accuracy: 0.7132 - val_loss: 0.5686 - val_accuracy: 0.7158\n",
      "Epoch 27/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.5706 - accuracy: 0.7136\n",
      "Epoch 27: val_loss improved from 0.56859 to 0.56854, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5708 - accuracy: 0.7132 - val_loss: 0.5685 - val_accuracy: 0.7158\n",
      "Epoch 28/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.5710 - accuracy: 0.7132\n",
      "Epoch 28: val_loss improved from 0.56854 to 0.56845, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5710 - accuracy: 0.7133 - val_loss: 0.5685 - val_accuracy: 0.7158\n",
      "Epoch 29/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.5691 - accuracy: 0.7143\n",
      "Epoch 29: val_loss improved from 0.56845 to 0.56842, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5707 - accuracy: 0.7135 - val_loss: 0.5684 - val_accuracy: 0.7158\n",
      "Epoch 30/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.5696 - accuracy: 0.7151\n",
      "Epoch 30: val_loss improved from 0.56842 to 0.56832, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5706 - accuracy: 0.7133 - val_loss: 0.5683 - val_accuracy: 0.7158\n",
      "Epoch 31/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.5710 - accuracy: 0.7125\n",
      "Epoch 31: val_loss improved from 0.56832 to 0.56827, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5704 - accuracy: 0.7135 - val_loss: 0.5683 - val_accuracy: 0.7158\n",
      "Epoch 32/60\n",
      "654/682 [===========================>..] - ETA: 0s - loss: 0.5683 - accuracy: 0.7150\n",
      "Epoch 32: val_loss improved from 0.56827 to 0.56826, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5703 - accuracy: 0.7135 - val_loss: 0.5683 - val_accuracy: 0.7158\n",
      "Epoch 33/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.5699 - accuracy: 0.7150\n",
      "Epoch 33: val_loss improved from 0.56826 to 0.56820, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5704 - accuracy: 0.7136 - val_loss: 0.5682 - val_accuracy: 0.7158\n",
      "Epoch 34/60\n",
      "663/682 [============================>.] - ETA: 0s - loss: 0.5717 - accuracy: 0.7125\n",
      "Epoch 34: val_loss improved from 0.56820 to 0.56814, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5703 - accuracy: 0.7138 - val_loss: 0.5681 - val_accuracy: 0.7158\n",
      "Epoch 35/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.5718 - accuracy: 0.7120\n",
      "Epoch 35: val_loss did not improve from 0.56814\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5702 - accuracy: 0.7135 - val_loss: 0.5683 - val_accuracy: 0.7164\n",
      "Epoch 36/60\n",
      "678/682 [============================>.] - ETA: 0s - loss: 0.5704 - accuracy: 0.7139\n",
      "Epoch 36: val_loss improved from 0.56814 to 0.56811, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5702 - accuracy: 0.7141 - val_loss: 0.5681 - val_accuracy: 0.7158\n",
      "Epoch 37/60\n",
      "653/682 [===========================>..] - ETA: 0s - loss: 0.5704 - accuracy: 0.7139\n",
      "Epoch 37: val_loss improved from 0.56811 to 0.56811, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5700 - accuracy: 0.7138 - val_loss: 0.5681 - val_accuracy: 0.7158\n",
      "Epoch 38/60\n",
      "674/682 [============================>.] - ETA: 0s - loss: 0.5699 - accuracy: 0.7142\n",
      "Epoch 38: val_loss improved from 0.56811 to 0.56806, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5700 - accuracy: 0.7139 - val_loss: 0.5681 - val_accuracy: 0.7158\n",
      "Epoch 39/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.5697 - accuracy: 0.7141\n",
      "Epoch 39: val_loss did not improve from 0.56806\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5699 - accuracy: 0.7139 - val_loss: 0.5681 - val_accuracy: 0.7158\n",
      "Epoch 40/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.5710 - accuracy: 0.7124\n",
      "Epoch 40: val_loss improved from 0.56806 to 0.56803, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5699 - accuracy: 0.7135 - val_loss: 0.5680 - val_accuracy: 0.7170\n",
      "Epoch 41/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.5697 - accuracy: 0.7141\n",
      "Epoch 41: val_loss improved from 0.56803 to 0.56794, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5698 - accuracy: 0.7139 - val_loss: 0.5679 - val_accuracy: 0.7158\n",
      "Epoch 42/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.5702 - accuracy: 0.7138\n",
      "Epoch 42: val_loss did not improve from 0.56794\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5697 - accuracy: 0.7141 - val_loss: 0.5680 - val_accuracy: 0.7158\n",
      "Epoch 43/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.5672 - accuracy: 0.7167\n",
      "Epoch 43: val_loss did not improve from 0.56794\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5697 - accuracy: 0.7141 - val_loss: 0.5682 - val_accuracy: 0.7158\n",
      "Epoch 44/60\n",
      "650/682 [===========================>..] - ETA: 0s - loss: 0.5704 - accuracy: 0.7131\n",
      "Epoch 44: val_loss did not improve from 0.56794\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5696 - accuracy: 0.7144 - val_loss: 0.5680 - val_accuracy: 0.7158\n",
      "Epoch 45/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.5680 - accuracy: 0.7158\n",
      "Epoch 45: val_loss improved from 0.56794 to 0.56791, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5696 - accuracy: 0.7144 - val_loss: 0.5679 - val_accuracy: 0.7164\n",
      "Epoch 46/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.5681 - accuracy: 0.7154\n",
      "Epoch 46: val_loss improved from 0.56791 to 0.56782, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5696 - accuracy: 0.7138 - val_loss: 0.5678 - val_accuracy: 0.7164\n",
      "Epoch 47/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.5695 - accuracy: 0.7136\n",
      "Epoch 47: val_loss improved from 0.56782 to 0.56781, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5695 - accuracy: 0.7136 - val_loss: 0.5678 - val_accuracy: 0.7158\n",
      "Epoch 48/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.5692 - accuracy: 0.7153\n",
      "Epoch 48: val_loss did not improve from 0.56781\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5694 - accuracy: 0.7147 - val_loss: 0.5680 - val_accuracy: 0.7158\n",
      "Epoch 49/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.5690 - accuracy: 0.7142\n",
      "Epoch 49: val_loss did not improve from 0.56781\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5694 - accuracy: 0.7141 - val_loss: 0.5681 - val_accuracy: 0.7158\n",
      "Epoch 50/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.5694 - accuracy: 0.7152\n",
      "Epoch 50: val_loss improved from 0.56781 to 0.56778, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5694 - accuracy: 0.7148 - val_loss: 0.5678 - val_accuracy: 0.7170\n",
      "Epoch 51/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.5678 - accuracy: 0.7150\n",
      "Epoch 51: val_loss did not improve from 0.56778\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5693 - accuracy: 0.7145 - val_loss: 0.5679 - val_accuracy: 0.7164\n",
      "Epoch 52/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.5681 - accuracy: 0.7160\n",
      "Epoch 52: val_loss did not improve from 0.56778\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5694 - accuracy: 0.7144 - val_loss: 0.5679 - val_accuracy: 0.7158\n",
      "Epoch 53/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.5697 - accuracy: 0.7146\n",
      "Epoch 53: val_loss improved from 0.56778 to 0.56772, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5694 - accuracy: 0.7148 - val_loss: 0.5677 - val_accuracy: 0.7158\n",
      "Epoch 54/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.5668 - accuracy: 0.7173\n",
      "Epoch 54: val_loss did not improve from 0.56772\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5694 - accuracy: 0.7148 - val_loss: 0.5679 - val_accuracy: 0.7164\n",
      "Epoch 55/60\n",
      "662/682 [============================>.] - ETA: 0s - loss: 0.5679 - accuracy: 0.7156\n",
      "Epoch 55: val_loss improved from 0.56772 to 0.56769, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5693 - accuracy: 0.7151 - val_loss: 0.5677 - val_accuracy: 0.7158\n",
      "Epoch 56/60\n",
      "653/682 [===========================>..] - ETA: 0s - loss: 0.5686 - accuracy: 0.7159\n",
      "Epoch 56: val_loss did not improve from 0.56769\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5693 - accuracy: 0.7144 - val_loss: 0.5677 - val_accuracy: 0.7158\n",
      "Epoch 57/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.5698 - accuracy: 0.7148\n",
      "Epoch 57: val_loss did not improve from 0.56769\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5690 - accuracy: 0.7150 - val_loss: 0.5680 - val_accuracy: 0.7164\n",
      "Epoch 58/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.5690 - accuracy: 0.7149\n",
      "Epoch 58: val_loss improved from 0.56769 to 0.56767, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5692 - accuracy: 0.7148 - val_loss: 0.5677 - val_accuracy: 0.7158\n",
      "Epoch 59/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.5695 - accuracy: 0.7136\n",
      "Epoch 59: val_loss did not improve from 0.56767\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5692 - accuracy: 0.7142 - val_loss: 0.5679 - val_accuracy: 0.7164\n",
      "Epoch 60/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.5684 - accuracy: 0.7162\n",
      "Epoch 60: val_loss did not improve from 0.56767\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5693 - accuracy: 0.7148 - val_loss: 0.5678 - val_accuracy: 0.7152\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.5677 - accuracy: 0.7158\n",
      "Drop Feature blue-collar\n",
      "data frame x has shape: (8516, 7)\n",
      "['education', 'pdays', 'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx', 'euribor3m']\n",
      "Epoch 1/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.6868 - accuracy: 0.5790\n",
      "Epoch 1: val_loss improved from inf to 0.64977, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6858 - accuracy: 0.5821 - val_loss: 0.6498 - val_accuracy: 0.7111\n",
      "Epoch 2/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.6298 - accuracy: 0.7115\n",
      "Epoch 2: val_loss improved from 0.64977 to 0.61118, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6298 - accuracy: 0.7116 - val_loss: 0.6112 - val_accuracy: 0.7129\n",
      "Epoch 3/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.6020 - accuracy: 0.7131\n",
      "Epoch 3: val_loss improved from 0.61118 to 0.59191, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.6025 - accuracy: 0.7114 - val_loss: 0.5919 - val_accuracy: 0.7123\n",
      "Epoch 4/60\n",
      "654/682 [===========================>..] - ETA: 0s - loss: 0.5894 - accuracy: 0.7119\n",
      "Epoch 4: val_loss improved from 0.59191 to 0.58280, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5890 - accuracy: 0.7122 - val_loss: 0.5828 - val_accuracy: 0.7152\n",
      "Epoch 5/60\n",
      "654/682 [===========================>..] - ETA: 0s - loss: 0.5835 - accuracy: 0.7098\n",
      "Epoch 5: val_loss improved from 0.58280 to 0.57799, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5824 - accuracy: 0.7117 - val_loss: 0.5780 - val_accuracy: 0.7152\n",
      "Epoch 6/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.5789 - accuracy: 0.7122\n",
      "Epoch 6: val_loss improved from 0.57799 to 0.57507, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5788 - accuracy: 0.7125 - val_loss: 0.5751 - val_accuracy: 0.7134\n",
      "Epoch 7/60\n",
      "679/682 [============================>.] - ETA: 0s - loss: 0.5763 - accuracy: 0.7131\n",
      "Epoch 7: val_loss improved from 0.57507 to 0.57359, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5768 - accuracy: 0.7126 - val_loss: 0.5736 - val_accuracy: 0.7152\n",
      "Epoch 8/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.5755 - accuracy: 0.7123\n",
      "Epoch 8: val_loss improved from 0.57359 to 0.57264, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5755 - accuracy: 0.7123 - val_loss: 0.5726 - val_accuracy: 0.7152\n",
      "Epoch 9/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.5746 - accuracy: 0.7131\n",
      "Epoch 9: val_loss improved from 0.57264 to 0.57205, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5747 - accuracy: 0.7128 - val_loss: 0.5720 - val_accuracy: 0.7152\n",
      "Epoch 10/60\n",
      "665/682 [============================>.] - ETA: 0s - loss: 0.5738 - accuracy: 0.7125\n",
      "Epoch 10: val_loss improved from 0.57205 to 0.57172, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5741 - accuracy: 0.7128 - val_loss: 0.5717 - val_accuracy: 0.7152\n",
      "Epoch 11/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.5740 - accuracy: 0.7126\n",
      "Epoch 11: val_loss improved from 0.57172 to 0.57141, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5736 - accuracy: 0.7129 - val_loss: 0.5714 - val_accuracy: 0.7152\n",
      "Epoch 12/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.5738 - accuracy: 0.7126\n",
      "Epoch 12: val_loss improved from 0.57141 to 0.57128, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5733 - accuracy: 0.7129 - val_loss: 0.5713 - val_accuracy: 0.7152\n",
      "Epoch 13/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.5728 - accuracy: 0.7134\n",
      "Epoch 13: val_loss improved from 0.57128 to 0.57098, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5729 - accuracy: 0.7129 - val_loss: 0.5710 - val_accuracy: 0.7152\n",
      "Epoch 14/60\n",
      "671/682 [============================>.] - ETA: 0s - loss: 0.5721 - accuracy: 0.7134\n",
      "Epoch 14: val_loss improved from 0.57098 to 0.57076, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5728 - accuracy: 0.7128 - val_loss: 0.5708 - val_accuracy: 0.7152\n",
      "Epoch 15/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.5720 - accuracy: 0.7138\n",
      "Epoch 15: val_loss improved from 0.57076 to 0.57061, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5725 - accuracy: 0.7129 - val_loss: 0.5706 - val_accuracy: 0.7152\n",
      "Epoch 16/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.5721 - accuracy: 0.7131\n",
      "Epoch 16: val_loss improved from 0.57061 to 0.57048, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5723 - accuracy: 0.7129 - val_loss: 0.5705 - val_accuracy: 0.7152\n",
      "Epoch 17/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.5722 - accuracy: 0.7131\n",
      "Epoch 17: val_loss improved from 0.57048 to 0.57036, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5721 - accuracy: 0.7129 - val_loss: 0.5704 - val_accuracy: 0.7152\n",
      "Epoch 18/60\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.5721 - accuracy: 0.7126\n",
      "Epoch 18: val_loss improved from 0.57036 to 0.57034, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5719 - accuracy: 0.7129 - val_loss: 0.5703 - val_accuracy: 0.7152\n",
      "Epoch 19/60\n",
      "661/682 [============================>.] - ETA: 0s - loss: 0.5727 - accuracy: 0.7120\n",
      "Epoch 19: val_loss improved from 0.57034 to 0.57017, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5716 - accuracy: 0.7129 - val_loss: 0.5702 - val_accuracy: 0.7152\n",
      "Epoch 20/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.5710 - accuracy: 0.7135\n",
      "Epoch 20: val_loss improved from 0.57017 to 0.57007, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5714 - accuracy: 0.7129 - val_loss: 0.5701 - val_accuracy: 0.7152\n",
      "Epoch 21/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.5706 - accuracy: 0.7131\n",
      "Epoch 21: val_loss did not improve from 0.57007\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5713 - accuracy: 0.7129 - val_loss: 0.5701 - val_accuracy: 0.7152\n",
      "Epoch 22/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.5712 - accuracy: 0.7120\n",
      "Epoch 22: val_loss improved from 0.57007 to 0.56996, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5712 - accuracy: 0.7129 - val_loss: 0.5700 - val_accuracy: 0.7152\n",
      "Epoch 23/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.5706 - accuracy: 0.7134\n",
      "Epoch 23: val_loss improved from 0.56996 to 0.56987, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5710 - accuracy: 0.7129 - val_loss: 0.5699 - val_accuracy: 0.7152\n",
      "Epoch 24/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.5707 - accuracy: 0.7128\n",
      "Epoch 24: val_loss did not improve from 0.56987\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5707 - accuracy: 0.7128 - val_loss: 0.5699 - val_accuracy: 0.7152\n",
      "Epoch 25/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.5702 - accuracy: 0.7137\n",
      "Epoch 25: val_loss improved from 0.56987 to 0.56967, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5709 - accuracy: 0.7129 - val_loss: 0.5697 - val_accuracy: 0.7152\n",
      "Epoch 26/60\n",
      "664/682 [============================>.] - ETA: 0s - loss: 0.5699 - accuracy: 0.7136\n",
      "Epoch 26: val_loss improved from 0.56967 to 0.56967, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5707 - accuracy: 0.7129 - val_loss: 0.5697 - val_accuracy: 0.7152\n",
      "Epoch 27/60\n",
      "676/682 [============================>.] - ETA: 0s - loss: 0.5702 - accuracy: 0.7130\n",
      "Epoch 27: val_loss did not improve from 0.56967\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5704 - accuracy: 0.7129 - val_loss: 0.5700 - val_accuracy: 0.7152\n",
      "Epoch 28/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.5705 - accuracy: 0.7129\n",
      "Epoch 28: val_loss improved from 0.56967 to 0.56958, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5705 - accuracy: 0.7129 - val_loss: 0.5696 - val_accuracy: 0.7152\n",
      "Epoch 29/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.5699 - accuracy: 0.7132\n",
      "Epoch 29: val_loss improved from 0.56958 to 0.56944, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5704 - accuracy: 0.7129 - val_loss: 0.5694 - val_accuracy: 0.7152\n",
      "Epoch 30/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.5699 - accuracy: 0.7128\n",
      "Epoch 30: val_loss did not improve from 0.56944\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5702 - accuracy: 0.7130 - val_loss: 0.5695 - val_accuracy: 0.7152\n",
      "Epoch 31/60\n",
      "653/682 [===========================>..] - ETA: 0s - loss: 0.5709 - accuracy: 0.7121\n",
      "Epoch 31: val_loss improved from 0.56944 to 0.56943, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5702 - accuracy: 0.7129 - val_loss: 0.5694 - val_accuracy: 0.7158\n",
      "Epoch 32/60\n",
      "682/682 [==============================] - ETA: 0s - loss: 0.5702 - accuracy: 0.7129\n",
      "Epoch 32: val_loss improved from 0.56943 to 0.56934, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5702 - accuracy: 0.7129 - val_loss: 0.5693 - val_accuracy: 0.7158\n",
      "Epoch 33/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.5709 - accuracy: 0.7120\n",
      "Epoch 33: val_loss did not improve from 0.56934\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5701 - accuracy: 0.7129 - val_loss: 0.5694 - val_accuracy: 0.7158\n",
      "Epoch 34/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.5706 - accuracy: 0.7126\n",
      "Epoch 34: val_loss did not improve from 0.56934\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5700 - accuracy: 0.7132 - val_loss: 0.5694 - val_accuracy: 0.7158\n",
      "Epoch 35/60\n",
      "675/682 [============================>.] - ETA: 0s - loss: 0.5696 - accuracy: 0.7130\n",
      "Epoch 35: val_loss did not improve from 0.56934\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5700 - accuracy: 0.7130 - val_loss: 0.5694 - val_accuracy: 0.7158\n",
      "Epoch 36/60\n",
      "655/682 [===========================>..] - ETA: 0s - loss: 0.5699 - accuracy: 0.7133\n",
      "Epoch 36: val_loss improved from 0.56934 to 0.56915, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5699 - accuracy: 0.7132 - val_loss: 0.5691 - val_accuracy: 0.7158\n",
      "Epoch 37/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.5705 - accuracy: 0.7123\n",
      "Epoch 37: val_loss did not improve from 0.56915\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5696 - accuracy: 0.7130 - val_loss: 0.5698 - val_accuracy: 0.7158\n",
      "Epoch 38/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.5698 - accuracy: 0.7132\n",
      "Epoch 38: val_loss improved from 0.56915 to 0.56908, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5698 - accuracy: 0.7132 - val_loss: 0.5691 - val_accuracy: 0.7158\n",
      "Epoch 39/60\n",
      "666/682 [============================>.] - ETA: 0s - loss: 0.5692 - accuracy: 0.7129\n",
      "Epoch 39: val_loss did not improve from 0.56908\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5697 - accuracy: 0.7130 - val_loss: 0.5692 - val_accuracy: 0.7158\n",
      "Epoch 40/60\n",
      "657/682 [===========================>..] - ETA: 0s - loss: 0.5700 - accuracy: 0.7129\n",
      "Epoch 40: val_loss improved from 0.56908 to 0.56907, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5697 - accuracy: 0.7132 - val_loss: 0.5691 - val_accuracy: 0.7158\n",
      "Epoch 41/60\n",
      "672/682 [============================>.] - ETA: 0s - loss: 0.5706 - accuracy: 0.7131\n",
      "Epoch 41: val_loss improved from 0.56907 to 0.56900, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5697 - accuracy: 0.7133 - val_loss: 0.5690 - val_accuracy: 0.7158\n",
      "Epoch 42/60\n",
      "656/682 [===========================>..] - ETA: 0s - loss: 0.5703 - accuracy: 0.7133\n",
      "Epoch 42: val_loss improved from 0.56900 to 0.56894, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5696 - accuracy: 0.7133 - val_loss: 0.5689 - val_accuracy: 0.7158\n",
      "Epoch 43/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.5695 - accuracy: 0.7134\n",
      "Epoch 43: val_loss improved from 0.56894 to 0.56893, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5696 - accuracy: 0.7133 - val_loss: 0.5689 - val_accuracy: 0.7158\n",
      "Epoch 44/60\n",
      "654/682 [===========================>..] - ETA: 0s - loss: 0.5687 - accuracy: 0.7135\n",
      "Epoch 44: val_loss improved from 0.56893 to 0.56892, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5695 - accuracy: 0.7130 - val_loss: 0.5689 - val_accuracy: 0.7158\n",
      "Epoch 45/60\n",
      "677/682 [============================>.] - ETA: 0s - loss: 0.5694 - accuracy: 0.7129\n",
      "Epoch 45: val_loss did not improve from 0.56892\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5694 - accuracy: 0.7132 - val_loss: 0.5694 - val_accuracy: 0.7158\n",
      "Epoch 46/60\n",
      "673/682 [============================>.] - ETA: 0s - loss: 0.5687 - accuracy: 0.7135\n",
      "Epoch 46: val_loss improved from 0.56892 to 0.56887, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5695 - accuracy: 0.7132 - val_loss: 0.5689 - val_accuracy: 0.7158\n",
      "Epoch 47/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.5683 - accuracy: 0.7140\n",
      "Epoch 47: val_loss did not improve from 0.56887\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5693 - accuracy: 0.7133 - val_loss: 0.5689 - val_accuracy: 0.7158\n",
      "Epoch 48/60\n",
      "681/682 [============================>.] - ETA: 0s - loss: 0.5693 - accuracy: 0.7137\n",
      "Epoch 48: val_loss did not improve from 0.56887\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5694 - accuracy: 0.7135 - val_loss: 0.5689 - val_accuracy: 0.7158\n",
      "Epoch 49/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.5691 - accuracy: 0.7137\n",
      "Epoch 49: val_loss improved from 0.56887 to 0.56880, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5694 - accuracy: 0.7133 - val_loss: 0.5688 - val_accuracy: 0.7158\n",
      "Epoch 50/60\n",
      "660/682 [============================>.] - ETA: 0s - loss: 0.5687 - accuracy: 0.7138\n",
      "Epoch 50: val_loss did not improve from 0.56880\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5692 - accuracy: 0.7133 - val_loss: 0.5689 - val_accuracy: 0.7158\n",
      "Epoch 51/60\n",
      "667/682 [============================>.] - ETA: 0s - loss: 0.5702 - accuracy: 0.7129\n",
      "Epoch 51: val_loss improved from 0.56880 to 0.56879, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5694 - accuracy: 0.7133 - val_loss: 0.5688 - val_accuracy: 0.7158\n",
      "Epoch 52/60\n",
      "668/682 [============================>.] - ETA: 0s - loss: 0.5703 - accuracy: 0.7118\n",
      "Epoch 52: val_loss improved from 0.56879 to 0.56877, saving model to best_drop.hdf5\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5692 - accuracy: 0.7133 - val_loss: 0.5688 - val_accuracy: 0.7158\n",
      "Epoch 53/60\n",
      "659/682 [===========================>..] - ETA: 0s - loss: 0.5686 - accuracy: 0.7141\n",
      "Epoch 53: val_loss did not improve from 0.56877\n",
      "682/682 [==============================] - 1s 2ms/step - loss: 0.5693 - accuracy: 0.7133 - val_loss: 0.5688 - val_accuracy: 0.7158\n",
      "Epoch 54/60\n",
      "658/682 [===========================>..] - ETA: 0s - loss: 0.5694 - accuracy: 0.7135\n",
      "Epoch 54: val_loss did not improve from 0.56877\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5692 - accuracy: 0.7135 - val_loss: 0.5688 - val_accuracy: 0.7158\n",
      "Epoch 55/60\n",
      "669/682 [============================>.] - ETA: 0s - loss: 0.5696 - accuracy: 0.7126\n",
      "Epoch 55: val_loss did not improve from 0.56877\n",
      "682/682 [==============================] - 2s 2ms/step - loss: 0.5690 - accuracy: 0.7133 - val_loss: 0.5691 - val_accuracy: 0.7164\n",
      "Epoch 56/60\n",
      " 31/682 [>.............................] - ETA: 1s - loss: 0.6039 - accuracy: 0.6645"
     ]
    }
   ],
   "source": [
    "drop_X = banking_data.drop(['output'], axis = 'columns')\n",
    "drop_result = pd.DataFrame(columns=['Features_Removed', 'Validation_Accuracy'])\n",
    "\n",
    "\n",
    "for index, row in results.iterrows():\n",
    "\n",
    "    feature_value = row['Feature']\n",
    "    print(f\"Drop Feature {feature_value}\")\n",
    "    drop_X = drop_X.drop([feature_value], axis = 'columns')\n",
    "    print(f\"data frame x has shape: {drop_X.shape}\")\n",
    "    print(drop_X.columns.tolist())\n",
    "    # Split x into validation (20%) and training (80%)\n",
    "    X_train = drop_X.iloc[index_20:, :]\n",
    "    X_test = drop_X.iloc[:index_20, :]\n",
    "    min = X_train.min(axis = 0) \n",
    "    max = X_train.max(axis = 0) \n",
    "    X_train = (X_train - min) / (max - min)\n",
    "    X_test = (X_test - min) / (max - min)\n",
    "\n",
    "    drop_regression_model = Sequential()\n",
    "    drop_regression_model.add(Dense(1, input_dim = len(X_train.columns), activation='sigmoid'))\n",
    "\n",
    "    drop_regression_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    callback_c = ModelCheckpoint(filepath = 'best_drop.hdf5', monitor='val_loss', save_best_only = True, save_weights_only = True, verbose = 1)\n",
    "    callback_d = EarlyStopping(monitor='val_loss', mode='min', patience=20, verbose=1)\n",
    "    history_regression_drop = drop_regression_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=60, batch_size=10, callbacks = [callback_c, callback_d])\n",
    "    drop_regression_model.load_weights('best_drop.hdf5')\n",
    "    drop_regression_model_scores =  drop_regression_model.evaluate(X_test, y_test)\n",
    "    accuracy = (drop_regression_model_scores[1]*100)\n",
    "\n",
    "    drop_result.loc[len(drop_result.index)] = [f\"Dropped {feature_value}\", accuracy] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7501bfba-da07-4d91-83f7-8567ea75dfd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of           Features_Removed  Validation_Accuracy\n",
       "0              Dropped age            72.108042\n",
       "1       Dropped unemployed            72.166765\n",
       "2        Dropped housemaid            72.108042\n",
       "3       Dropped management            72.108042\n",
       "4       Dropped technician            72.108042\n",
       "5             Dropped loan            71.931887\n",
       "6     Dropped entrepreneur            72.049326\n",
       "7    Dropped self-employed            72.108042\n",
       "8         Dropped divorced            71.990603\n",
       "9          Dropped housing            72.049326\n",
       "10        Dropped services            72.284204\n",
       "11         Dropped retired            71.579564\n",
       "12         Dropped student            71.755725\n",
       "13        Dropped campaign            71.462125\n",
       "14         Dropped married            71.579564\n",
       "15          Dropped admin.            71.462125\n",
       "16          Dropped single            71.168524\n",
       "17   Dropped cons.conf.idx            71.579564\n",
       "18     Dropped blue-collar            71.579564\n",
       "19       Dropped education            71.520847\n",
       "20  Dropped cons.price.idx            71.520847\n",
       "21        Dropped poutcome            71.520847\n",
       "22           Dropped pdays            71.520847\n",
       "23        Dropped previous            71.579564\n",
       "24       Dropped euribor3m            71.579564\n",
       "25    Dropped emp.var.rate            50.088078>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_result.head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e61d0cd-d7de-463e-b5bd-f1b7705844c3",
   "metadata": {},
   "source": [
    "#### 4.6 Graph Results of Dropping Certain Features from the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae99a790-0be3-4489-b0b3-d2fbbd5d5b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the accuracy by feature bar graph\n",
    "plt.figure(figsize=(10, 6)),\n",
    "plt.bar(drop_result['Features_Removed'], drop_result['Validation_Accuracy'])\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Validation Accuracy (%)')\n",
    "plt.title('Validation Accuracy After Dropping Specific Features')\n",
    "plt.ylim(0, 100)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd797ca-2d94-4d11-8e3e-32efce442c55",
   "metadata": {},
   "source": [
    "#### Conclusion \n",
    "Looking at the results of dropping specific columns it does not appear that there were any particular columns that were adding severe noise or bias to the model because little changed in the overall accuracy of the model as the columns with low effect on the model's accuracy were removed from the dataset. Accuracy for the most part stayed about the same as the initial model built in Phase 3 that had about a 72% validation accuracy. I thought there would have been a bigger difference once columns like age, the euribor and previous contacts features were removed. This is because the age was not a good predictor for the output variable with only 44% accuracy and the euribor and previous features had higher accuracies with 71% and 59% respectively so I thought accuracy would have dropped more with their removal. In the end there was little change from iteration to iteration of the loop until the emp.var.rate feature was dropped which is when the model had no input features avaliable to train from so the model simply had to guess and ended up with a 50% accuracy rate because of basic probability. I suspect the minimal impact is because as seen earlier in Phase 1 none of these features had a strong correlation with the output variable so it does not appear that any of them have a strong impact on the prediction model. I believe that if there were features with a stronger correlation to the output variable then this model would potentially perform better overall and we would have seen a greater change in this phase. At the same time this was a good exercise because I'd questioned in Phase 1 if I should have removed the poutcome and pdays features from the model to prevent noise and looking at the results here that removing them would have not drastically changed my overall results as initially thought. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
