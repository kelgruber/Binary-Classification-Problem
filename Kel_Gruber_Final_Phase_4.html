<!DOCTYPE html>

<html lang="en">
<head><meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Kel_Gruber_Final_Phase_4</title><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
<style type="text/css">
    pre { line-height: 125%; }
td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
.highlight .hll { background-color: var(--jp-cell-editor-active-background) }
.highlight { background: var(--jp-cell-editor-background); color: var(--jp-mirror-editor-variable-color) }
.highlight .c { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment */
.highlight .err { color: var(--jp-mirror-editor-error-color) } /* Error */
.highlight .k { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword */
.highlight .o { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator */
.highlight .p { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation */
.highlight .ch { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Multiline */
.highlight .cp { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Preproc */
.highlight .cpf { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Single */
.highlight .cs { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Special */
.highlight .kc { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Pseudo */
.highlight .kr { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Type */
.highlight .m { color: var(--jp-mirror-editor-number-color) } /* Literal.Number */
.highlight .s { color: var(--jp-mirror-editor-string-color) } /* Literal.String */
.highlight .ow { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator.Word */
.highlight .pm { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation.Marker */
.highlight .w { color: var(--jp-mirror-editor-variable-color) } /* Text.Whitespace */
.highlight .mb { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Bin */
.highlight .mf { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Float */
.highlight .mh { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Hex */
.highlight .mi { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer */
.highlight .mo { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Oct */
.highlight .sa { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Affix */
.highlight .sb { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Backtick */
.highlight .sc { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Char */
.highlight .dl { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Delimiter */
.highlight .sd { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Doc */
.highlight .s2 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Double */
.highlight .se { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Escape */
.highlight .sh { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Heredoc */
.highlight .si { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Interpol */
.highlight .sx { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Other */
.highlight .sr { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Regex */
.highlight .s1 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Single */
.highlight .ss { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Symbol */
.highlight .il { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer.Long */
  </style>
<style type="text/css">
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*
 * Mozilla scrollbar styling
 */

/* use standard opaque scrollbars for most nodes */
[data-jp-theme-scrollbars='true'] {
  scrollbar-color: rgb(var(--jp-scrollbar-thumb-color))
    var(--jp-scrollbar-background-color);
}

/* for code nodes, use a transparent style of scrollbar. These selectors
 * will match lower in the tree, and so will override the above */
[data-jp-theme-scrollbars='true'] .CodeMirror-hscrollbar,
[data-jp-theme-scrollbars='true'] .CodeMirror-vscrollbar {
  scrollbar-color: rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent;
}

/* tiny scrollbar */

.jp-scrollbar-tiny {
  scrollbar-color: rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent;
  scrollbar-width: thin;
}

/* tiny scrollbar */

.jp-scrollbar-tiny::-webkit-scrollbar,
.jp-scrollbar-tiny::-webkit-scrollbar-corner {
  background-color: transparent;
  height: 4px;
  width: 4px;
}

.jp-scrollbar-tiny::-webkit-scrollbar-thumb {
  background: rgba(var(--jp-scrollbar-thumb-color), 0.5);
}

.jp-scrollbar-tiny::-webkit-scrollbar-track:horizontal {
  border-left: 0 solid transparent;
  border-right: 0 solid transparent;
}

.jp-scrollbar-tiny::-webkit-scrollbar-track:vertical {
  border-top: 0 solid transparent;
  border-bottom: 0 solid transparent;
}

/*
 * Lumino
 */

.lm-ScrollBar[data-orientation='horizontal'] {
  min-height: 16px;
  max-height: 16px;
  min-width: 45px;
  border-top: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='vertical'] {
  min-width: 16px;
  max-width: 16px;
  min-height: 45px;
  border-left: 1px solid #a0a0a0;
}

.lm-ScrollBar-button {
  background-color: #f0f0f0;
  background-position: center center;
  min-height: 15px;
  max-height: 15px;
  min-width: 15px;
  max-width: 15px;
}

.lm-ScrollBar-button:hover {
  background-color: #dadada;
}

.lm-ScrollBar-button.lm-mod-active {
  background-color: #cdcdcd;
}

.lm-ScrollBar-track {
  background: #f0f0f0;
}

.lm-ScrollBar-thumb {
  background: #cdcdcd;
}

.lm-ScrollBar-thumb:hover {
  background: #bababa;
}

.lm-ScrollBar-thumb.lm-mod-active {
  background: #a0a0a0;
}

.lm-ScrollBar[data-orientation='horizontal'] .lm-ScrollBar-thumb {
  height: 100%;
  min-width: 15px;
  border-left: 1px solid #a0a0a0;
  border-right: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='vertical'] .lm-ScrollBar-thumb {
  width: 100%;
  min-height: 15px;
  border-top: 1px solid #a0a0a0;
  border-bottom: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='horizontal']
  .lm-ScrollBar-button[data-action='decrement'] {
  background-image: var(--jp-icon-caret-left);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='horizontal']
  .lm-ScrollBar-button[data-action='increment'] {
  background-image: var(--jp-icon-caret-right);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='vertical']
  .lm-ScrollBar-button[data-action='decrement'] {
  background-image: var(--jp-icon-caret-up);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='vertical']
  .lm-ScrollBar-button[data-action='increment'] {
  background-image: var(--jp-icon-caret-down);
  background-size: 17px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-Widget {
  box-sizing: border-box;
  position: relative;
  overflow: hidden;
}

.lm-Widget.lm-mod-hidden {
  display: none !important;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.lm-AccordionPanel[data-orientation='horizontal'] > .lm-AccordionPanel-title {
  /* Title is rotated for horizontal accordion panel using CSS */
  display: block;
  transform-origin: top left;
  transform: rotate(-90deg) translate(-100%);
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-CommandPalette {
  display: flex;
  flex-direction: column;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-CommandPalette-search {
  flex: 0 0 auto;
}

.lm-CommandPalette-content {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  min-height: 0;
  overflow: auto;
  list-style-type: none;
}

.lm-CommandPalette-header {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.lm-CommandPalette-item {
  display: flex;
  flex-direction: row;
}

.lm-CommandPalette-itemIcon {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemContent {
  flex: 1 1 auto;
  overflow: hidden;
}

.lm-CommandPalette-itemShortcut {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemLabel {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.lm-close-icon {
  border: 1px solid transparent;
  background-color: transparent;
  position: absolute;
  z-index: 1;
  right: 3%;
  top: 0;
  bottom: 0;
  margin: auto;
  padding: 7px 0;
  display: none;
  vertical-align: middle;
  outline: 0;
  cursor: pointer;
}
.lm-close-icon:after {
  content: 'X';
  display: block;
  width: 15px;
  height: 15px;
  text-align: center;
  color: #000;
  font-weight: normal;
  font-size: 12px;
  cursor: pointer;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-DockPanel {
  z-index: 0;
}

.lm-DockPanel-widget {
  z-index: 0;
}

.lm-DockPanel-tabBar {
  z-index: 1;
}

.lm-DockPanel-handle {
  z-index: 2;
}

.lm-DockPanel-handle.lm-mod-hidden {
  display: none !important;
}

.lm-DockPanel-handle:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  content: '';
}

.lm-DockPanel-handle[data-orientation='horizontal'] {
  cursor: ew-resize;
}

.lm-DockPanel-handle[data-orientation='vertical'] {
  cursor: ns-resize;
}

.lm-DockPanel-handle[data-orientation='horizontal']:after {
  left: 50%;
  min-width: 8px;
  transform: translateX(-50%);
}

.lm-DockPanel-handle[data-orientation='vertical']:after {
  top: 50%;
  min-height: 8px;
  transform: translateY(-50%);
}

.lm-DockPanel-overlay {
  z-index: 3;
  box-sizing: border-box;
  pointer-events: none;
}

.lm-DockPanel-overlay.lm-mod-hidden {
  display: none !important;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-Menu {
  z-index: 10000;
  position: absolute;
  white-space: nowrap;
  overflow-x: hidden;
  overflow-y: auto;
  outline: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-Menu-content {
  margin: 0;
  padding: 0;
  display: table;
  list-style-type: none;
}

.lm-Menu-item {
  display: table-row;
}

.lm-Menu-item.lm-mod-hidden,
.lm-Menu-item.lm-mod-collapsed {
  display: none !important;
}

.lm-Menu-itemIcon,
.lm-Menu-itemSubmenuIcon {
  display: table-cell;
  text-align: center;
}

.lm-Menu-itemLabel {
  display: table-cell;
  text-align: left;
}

.lm-Menu-itemShortcut {
  display: table-cell;
  text-align: right;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-MenuBar {
  outline: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-MenuBar-content {
  margin: 0;
  padding: 0;
  display: flex;
  flex-direction: row;
  list-style-type: none;
}

.lm-MenuBar-item {
  box-sizing: border-box;
}

.lm-MenuBar-itemIcon,
.lm-MenuBar-itemLabel {
  display: inline-block;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-ScrollBar {
  display: flex;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-ScrollBar[data-orientation='horizontal'] {
  flex-direction: row;
}

.lm-ScrollBar[data-orientation='vertical'] {
  flex-direction: column;
}

.lm-ScrollBar-button {
  box-sizing: border-box;
  flex: 0 0 auto;
}

.lm-ScrollBar-track {
  box-sizing: border-box;
  position: relative;
  overflow: hidden;
  flex: 1 1 auto;
}

.lm-ScrollBar-thumb {
  box-sizing: border-box;
  position: absolute;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-SplitPanel-child {
  z-index: 0;
}

.lm-SplitPanel-handle {
  z-index: 1;
}

.lm-SplitPanel-handle.lm-mod-hidden {
  display: none !important;
}

.lm-SplitPanel-handle:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  content: '';
}

.lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle {
  cursor: ew-resize;
}

.lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle {
  cursor: ns-resize;
}

.lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle:after {
  left: 50%;
  min-width: 8px;
  transform: translateX(-50%);
}

.lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle:after {
  top: 50%;
  min-height: 8px;
  transform: translateY(-50%);
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-TabBar {
  display: flex;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-TabBar[data-orientation='horizontal'] {
  flex-direction: row;
  align-items: flex-end;
}

.lm-TabBar[data-orientation='vertical'] {
  flex-direction: column;
  align-items: flex-end;
}

.lm-TabBar-content {
  margin: 0;
  padding: 0;
  display: flex;
  flex: 1 1 auto;
  list-style-type: none;
}

.lm-TabBar[data-orientation='horizontal'] > .lm-TabBar-content {
  flex-direction: row;
}

.lm-TabBar[data-orientation='vertical'] > .lm-TabBar-content {
  flex-direction: column;
}

.lm-TabBar-tab {
  display: flex;
  flex-direction: row;
  box-sizing: border-box;
  overflow: hidden;
  touch-action: none; /* Disable native Drag/Drop */
}

.lm-TabBar-tabIcon,
.lm-TabBar-tabCloseIcon {
  flex: 0 0 auto;
}

.lm-TabBar-tabLabel {
  flex: 1 1 auto;
  overflow: hidden;
  white-space: nowrap;
}

.lm-TabBar-tabInput {
  user-select: all;
  width: 100%;
  box-sizing: border-box;
}

.lm-TabBar-tab.lm-mod-hidden {
  display: none !important;
}

.lm-TabBar-addButton.lm-mod-hidden {
  display: none !important;
}

.lm-TabBar.lm-mod-dragging .lm-TabBar-tab {
  position: relative;
}

.lm-TabBar.lm-mod-dragging[data-orientation='horizontal'] .lm-TabBar-tab {
  left: 0;
  transition: left 150ms ease;
}

.lm-TabBar.lm-mod-dragging[data-orientation='vertical'] .lm-TabBar-tab {
  top: 0;
  transition: top 150ms ease;
}

.lm-TabBar.lm-mod-dragging .lm-TabBar-tab.lm-mod-dragging {
  transition: none;
}

.lm-TabBar-tabLabel .lm-TabBar-tabInput {
  user-select: all;
  width: 100%;
  box-sizing: border-box;
  background: inherit;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-TabPanel-tabBar {
  z-index: 1;
}

.lm-TabPanel-stackedPanel {
  z-index: 0;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Collapse {
  display: flex;
  flex-direction: column;
  align-items: stretch;
}

.jp-Collapse-header {
  padding: 1px 12px;
  background-color: var(--jp-layout-color1);
  border-bottom: solid var(--jp-border-width) var(--jp-border-color2);
  color: var(--jp-ui-font-color1);
  cursor: pointer;
  display: flex;
  align-items: center;
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  text-transform: uppercase;
  user-select: none;
}

.jp-Collapser-icon {
  height: 16px;
}

.jp-Collapse-header-collapsed .jp-Collapser-icon {
  transform: rotate(-90deg);
  margin: auto 0;
}

.jp-Collapser-title {
  line-height: 25px;
}

.jp-Collapse-contents {
  padding: 0 12px;
  background-color: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  overflow: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* This file was auto-generated by ensureUiComponents() in @jupyterlab/buildutils */

/**
 * (DEPRECATED) Support for consuming icons as CSS background images
 */

/* Icons urls */

:root {
  --jp-icon-add-above: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgY2xpcC1wYXRoPSJ1cmwoI2NsaXAwXzEzN18xOTQ5MikiPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGQ9Ik00Ljc1IDQuOTMwNjZINi42MjVWNi44MDU2NkM2LjYyNSA3LjAxMTkxIDYuNzkzNzUgNy4xODA2NiA3IDcuMTgwNjZDNy4yMDYyNSA3LjE4MDY2IDcuMzc1IDcuMDExOTEgNy4zNzUgNi44MDU2NlY0LjkzMDY2SDkuMjVDOS40NTYyNSA0LjkzMDY2IDkuNjI1IDQuNzYxOTEgOS42MjUgNC41NTU2NkM5LjYyNSA0LjM0OTQxIDkuNDU2MjUgNC4xODA2NiA5LjI1IDQuMTgwNjZINy4zNzVWMi4zMDU2NkM3LjM3NSAyLjA5OTQxIDcuMjA2MjUgMS45MzA2NiA3IDEuOTMwNjZDNi43OTM3NSAxLjkzMDY2IDYuNjI1IDIuMDk5NDEgNi42MjUgMi4zMDU2NlY0LjE4MDY2SDQuNzVDNC41NDM3NSA0LjE4MDY2IDQuMzc1IDQuMzQ5NDEgNC4zNzUgNC41NTU2NkM0LjM3NSA0Ljc2MTkxIDQuNTQzNzUgNC45MzA2NiA0Ljc1IDQuOTMwNjZaIiBmaWxsPSIjNjE2MTYxIiBzdHJva2U9IiM2MTYxNjEiIHN0cm9rZS13aWR0aD0iMC43Ii8+CjwvZz4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTExLjUgOS41VjExLjVMMi41IDExLjVWOS41TDExLjUgOS41Wk0xMiA4QzEyLjU1MjMgOCAxMyA4LjQ0NzcyIDEzIDlWMTJDMTMgMTIuNTUyMyAxMi41NTIzIDEzIDEyIDEzTDIgMTNDMS40NDc3MiAxMyAxIDEyLjU1MjMgMSAxMlY5QzEgOC40NDc3MiAxLjQ0NzcxIDggMiA4TDEyIDhaIiBmaWxsPSIjNjE2MTYxIi8+CjxkZWZzPgo8Y2xpcFBhdGggaWQ9ImNsaXAwXzEzN18xOTQ5MiI+CjxyZWN0IGNsYXNzPSJqcC1pY29uMyIgd2lkdGg9IjYiIGhlaWdodD0iNiIgZmlsbD0id2hpdGUiIHRyYW5zZm9ybT0ibWF0cml4KC0xIDAgMCAxIDEwIDEuNTU1NjYpIi8+CjwvY2xpcFBhdGg+CjwvZGVmcz4KPC9zdmc+Cg==);
  --jp-icon-add-below: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgY2xpcC1wYXRoPSJ1cmwoI2NsaXAwXzEzN18xOTQ5OCkiPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGQ9Ik05LjI1IDEwLjA2OTNMNy4zNzUgMTAuMDY5M0w3LjM3NSA4LjE5NDM0QzcuMzc1IDcuOTg4MDkgNy4yMDYyNSA3LjgxOTM0IDcgNy44MTkzNEM2Ljc5Mzc1IDcuODE5MzQgNi42MjUgNy45ODgwOSA2LjYyNSA4LjE5NDM0TDYuNjI1IDEwLjA2OTNMNC43NSAxMC4wNjkzQzQuNTQzNzUgMTAuMDY5MyA0LjM3NSAxMC4yMzgxIDQuMzc1IDEwLjQ0NDNDNC4zNzUgMTAuNjUwNiA0LjU0Mzc1IDEwLjgxOTMgNC43NSAxMC44MTkzTDYuNjI1IDEwLjgxOTNMNi42MjUgMTIuNjk0M0M2LjYyNSAxMi45MDA2IDYuNzkzNzUgMTMuMDY5MyA3IDEzLjA2OTNDNy4yMDYyNSAxMy4wNjkzIDcuMzc1IDEyLjkwMDYgNy4zNzUgMTIuNjk0M0w3LjM3NSAxMC44MTkzTDkuMjUgMTAuODE5M0M5LjQ1NjI1IDEwLjgxOTMgOS42MjUgMTAuNjUwNiA5LjYyNSAxMC40NDQzQzkuNjI1IDEwLjIzODEgOS40NTYyNSAxMC4wNjkzIDkuMjUgMTAuMDY5M1oiIGZpbGw9IiM2MTYxNjEiIHN0cm9rZT0iIzYxNjE2MSIgc3Ryb2tlLXdpZHRoPSIwLjciLz4KPC9nPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMi41IDUuNUwyLjUgMy41TDExLjUgMy41TDExLjUgNS41TDIuNSA1LjVaTTIgN0MxLjQ0NzcyIDcgMSA2LjU1MjI4IDEgNkwxIDNDMSAyLjQ0NzcyIDEuNDQ3NzIgMiAyIDJMMTIgMkMxMi41NTIzIDIgMTMgMi40NDc3MiAxMyAzTDEzIDZDMTMgNi41NTIyOSAxMi41NTIzIDcgMTIgN0wyIDdaIiBmaWxsPSIjNjE2MTYxIi8+CjxkZWZzPgo8Y2xpcFBhdGggaWQ9ImNsaXAwXzEzN18xOTQ5OCI+CjxyZWN0IGNsYXNzPSJqcC1pY29uMyIgd2lkdGg9IjYiIGhlaWdodD0iNiIgZmlsbD0id2hpdGUiIHRyYW5zZm9ybT0ibWF0cml4KDEgMS43NDg0NmUtMDcgMS43NDg0NmUtMDcgLTEgNCAxMy40NDQzKSIvPgo8L2NsaXBQYXRoPgo8L2RlZnM+Cjwvc3ZnPgo=);
  --jp-icon-add: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDEzaC02djZoLTJ2LTZINXYtMmg2VjVoMnY2aDZ2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-bell: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE2IDE2IiB2ZXJzaW9uPSIxLjEiPgogICA8cGF0aCBjbGFzcz0ianAtaWNvbjIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMzMzMzMzIgogICAgICBkPSJtOCAwLjI5Yy0xLjQgMC0yLjcgMC43My0zLjYgMS44LTEuMiAxLjUtMS40IDMuNC0xLjUgNS4yLTAuMTggMi4yLTAuNDQgNC0yLjMgNS4zbDAuMjggMS4zaDVjMC4wMjYgMC42NiAwLjMyIDEuMSAwLjcxIDEuNSAwLjg0IDAuNjEgMiAwLjYxIDIuOCAwIDAuNTItMC40IDAuNi0xIDAuNzEtMS41aDVsMC4yOC0xLjNjLTEuOS0wLjk3LTIuMi0zLjMtMi4zLTUuMy0wLjEzLTEuOC0wLjI2LTMuNy0xLjUtNS4yLTAuODUtMS0yLjItMS44LTMuNi0xLjh6bTAgMS40YzAuODggMCAxLjkgMC41NSAyLjUgMS4zIDAuODggMS4xIDEuMSAyLjcgMS4yIDQuNCAwLjEzIDEuNyAwLjIzIDMuNiAxLjMgNS4yaC0xMGMxLjEtMS42IDEuMi0zLjQgMS4zLTUuMiAwLjEzLTEuNyAwLjMtMy4zIDEuMi00LjQgMC41OS0wLjcyIDEuNi0xLjMgMi41LTEuM3ptLTAuNzQgMTJoMS41Yy0wLjAwMTUgMC4yOCAwLjAxNSAwLjc5LTAuNzQgMC43OS0wLjczIDAuMDAxNi0wLjcyLTAuNTMtMC43NC0wLjc5eiIgLz4KPC9zdmc+Cg==);
  --jp-icon-bug-dot: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiPgogICAgICAgIDxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMTcuMTkgOEgyMFYxMEgxNy45MUMxNy45NiAxMC4zMyAxOCAxMC42NiAxOCAxMVYxMkgyMFYxNEgxOC41SDE4VjE0LjAyNzVDMTUuNzUgMTQuMjc2MiAxNCAxNi4xODM3IDE0IDE4LjVDMTQgMTkuMjA4IDE0LjE2MzUgMTkuODc3OSAxNC40NTQ5IDIwLjQ3MzlDMTMuNzA2MyAyMC44MTE3IDEyLjg3NTcgMjEgMTIgMjFDOS43OCAyMSA3Ljg1IDE5Ljc5IDYuODEgMThINFYxNkg2LjA5QzYuMDQgMTUuNjcgNiAxNS4zNCA2IDE1VjE0SDRWMTJINlYxMUM2IDEwLjY2IDYuMDQgMTAuMzMgNi4wOSAxMEg0VjhINi44MUM3LjI2IDcuMjIgNy44OCA2LjU1IDguNjIgNi4wNEw3IDQuNDFMOC40MSAzTDEwLjU5IDUuMTdDMTEuMDQgNS4wNiAxMS41MSA1IDEyIDVDMTIuNDkgNSAxMi45NiA1LjA2IDEzLjQyIDUuMTdMMTUuNTkgM0wxNyA0LjQxTDE1LjM3IDYuMDRDMTYuMTIgNi41NSAxNi43NCA3LjIyIDE3LjE5IDhaTTEwIDE2SDE0VjE0SDEwVjE2Wk0xMCAxMkgxNFYxMEgxMFYxMloiIGZpbGw9IiM2MTYxNjEiLz4KICAgICAgICA8cGF0aCBkPSJNMjIgMTguNUMyMiAyMC40MzMgMjAuNDMzIDIyIDE4LjUgMjJDMTYuNTY3IDIyIDE1IDIwLjQzMyAxNSAxOC41QzE1IDE2LjU2NyAxNi41NjcgMTUgMTguNSAxNUMyMC40MzMgMTUgMjIgMTYuNTY3IDIyIDE4LjVaIiBmaWxsPSIjNjE2MTYxIi8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-bug: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0yMCA4aC0yLjgxYy0uNDUtLjc4LTEuMDctMS40NS0xLjgyLTEuOTZMMTcgNC40MSAxNS41OSAzbC0yLjE3IDIuMTdDMTIuOTYgNS4wNiAxMi40OSA1IDEyIDVjLS40OSAwLS45Ni4wNi0xLjQxLjE3TDguNDEgMyA3IDQuNDFsMS42MiAxLjYzQzcuODggNi41NSA3LjI2IDcuMjIgNi44MSA4SDR2MmgyLjA5Yy0uMDUuMzMtLjA5LjY2LS4wOSAxdjFINHYyaDJ2MWMwIC4zNC4wNC42Ny4wOSAxSDR2MmgyLjgxYzEuMDQgMS43OSAyLjk3IDMgNS4xOSAzczQuMTUtMS4yMSA1LjE5LTNIMjB2LTJoLTIuMDljLjA1LS4zMy4wOS0uNjYuMDktMXYtMWgydi0yaC0ydi0xYzAtLjM0LS4wNC0uNjctLjA5LTFIMjBWOHptLTYgOGgtNHYtMmg0djJ6bTAtNGgtNHYtMmg0djJ6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-build: url(data:image/svg+xml;base64,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);
  --jp-icon-caret-down-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iOS45LDEzLjYgMy42LDcuNCA0LjQsNi42IDkuOSwxMi4yIDE1LjQsNi43IDE2LjEsNy40ICIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNS45TDksOS43bDMuOC0zLjhsMS4yLDEuMmwtNC45LDVsLTQuOS01TDUuMiw1Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNy41TDksMTEuMmwzLjgtMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-caret-left: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik0xMC44LDEyLjhMNy4xLDlsMy44LTMuOGwwLDcuNkgxMC44eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-caret-right: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik03LjIsNS4yTDEwLjksOWwtMy44LDMuOFY1LjJINy4yeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-caret-up-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iMTUuNCwxMy4zIDkuOSw3LjcgNC40LDEzLjIgMy42LDEyLjUgOS45LDYuMyAxNi4xLDEyLjYgIi8+Cgk8L2c+Cjwvc3ZnPgo=);
  --jp-icon-caret-up: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik01LjIsMTAuNUw5LDYuOGwzLjgsMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-case-sensitive: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWFjY2VudDIiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTcuNiw4aDAuOWwzLjUsOGgtMS4xTDEwLDE0SDZsLTAuOSwySDRMNy42LDh6IE04LDkuMUw2LjQsMTNoMy4yTDgsOS4xeiIvPgogICAgPHBhdGggZD0iTTE2LjYsOS44Yy0wLjIsMC4xLTAuNCwwLjEtMC43LDAuMWMtMC4yLDAtMC40LTAuMS0wLjYtMC4yYy0wLjEtMC4xLTAuMi0wLjQtMC4yLTAuNyBjLTAuMywwLjMtMC42LDAuNS0wLjksMC43Yy0wLjMsMC4xLTAuNywwLjItMS4xLDAuMmMtMC4zLDAtMC41LDAtMC43LTAuMWMtMC4yLTAuMS0wLjQtMC4yLTAuNi0wLjNjLTAuMi0wLjEtMC4zLTAuMy0wLjQtMC41IGMtMC4xLTAuMi0wLjEtMC40LTAuMS0wLjdjMC0wLjMsMC4xLTAuNiwwLjItMC44YzAuMS0wLjIsMC4zLTAuNCwwLjQtMC41QzEyLDcsMTIuMiw2LjksMTIuNSw2LjhjMC4yLTAuMSwwLjUtMC4xLDAuNy0wLjIgYzAuMy0wLjEsMC41LTAuMSwwLjctMC4xYzAuMiwwLDAuNC0wLjEsMC42LTAuMWMwLjIsMCwwLjMtMC4xLDAuNC0wLjJjMC4xLTAuMSwwLjItMC4yLDAuMi0wLjRjMC0xLTEuMS0xLTEuMy0xIGMtMC40LDAtMS40LDAtMS40LDEuMmgtMC45YzAtMC40LDAuMS0wLjcsMC4yLTFjMC4xLTAuMiwwLjMtMC40LDAuNS0wLjZjMC4yLTAuMiwwLjUtMC4zLDAuOC0wLjNDMTMuMyw0LDEzLjYsNCwxMy45LDQgYzAuMywwLDAuNSwwLDAuOCwwLjFjMC4zLDAsMC41LDAuMSwwLjcsMC4yYzAuMiwwLjEsMC40LDAuMywwLjUsMC41QzE2LDUsMTYsNS4yLDE2LDUuNnYyLjljMCwwLjIsMCwwLjQsMCwwLjUgYzAsMC4xLDAuMSwwLjIsMC4zLDAuMmMwLjEsMCwwLjIsMCwwLjMsMFY5Ljh6IE0xNS4yLDYuOWMtMS4yLDAuNi0zLjEsMC4yLTMuMSwxLjRjMCwxLjQsMy4xLDEsMy4xLTAuNVY2Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik05IDE2LjE3TDQuODMgMTJsLTEuNDIgMS40MUw5IDE5IDIxIDdsLTEuNDEtMS40MXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-circle-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDJDNi40NyAyIDIgNi40NyAyIDEyczQuNDcgMTAgMTAgMTAgMTAtNC40NyAxMC0xMFMxNy41MyAyIDEyIDJ6bTAgMThjLTQuNDEgMC04LTMuNTktOC04czMuNTktOCA4LTggOCAzLjU5IDggOC0zLjU5IDgtOCA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-circle: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iOSIgY3k9IjkiIHI9IjgiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-clear: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8bWFzayBpZD0iZG9udXRIb2xlIj4KICAgIDxyZWN0IHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgZmlsbD0id2hpdGUiIC8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSI4IiBmaWxsPSJibGFjayIvPgogIDwvbWFzaz4KCiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxyZWN0IGhlaWdodD0iMTgiIHdpZHRoPSIyIiB4PSIxMSIgeT0iMyIgdHJhbnNmb3JtPSJyb3RhdGUoMzE1LCAxMiwgMTIpIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIxMCIgbWFzaz0idXJsKCNkb251dEhvbGUpIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-close: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1ub25lIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIGpwLWljb24zLWhvdmVyIiBmaWxsPSJub25lIj4KICAgIDxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjExIi8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIGpwLWljb24tYWNjZW50Mi1ob3ZlciIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMTkgNi40MUwxNy41OSA1IDEyIDEwLjU5IDYuNDEgNSA1IDYuNDEgMTAuNTkgMTIgNSAxNy41OSA2LjQxIDE5IDEyIDEzLjQxIDE3LjU5IDE5IDE5IDE3LjU5IDEzLjQxIDEyeiIvPgogIDwvZz4KCiAgPGcgY2xhc3M9ImpwLWljb24tbm9uZSBqcC1pY29uLWJ1c3kiIGZpbGw9Im5vbmUiPgogICAgPGNpcmNsZSBjeD0iMTIiIGN5PSIxMiIgcj0iNyIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-code-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBzaGFwZS1yZW5kZXJpbmc9Imdlb21ldHJpY1ByZWNpc2lvbiI+CiAgICA8cGF0aCBkPSJNNi41OSwzLjQxTDIsOEw2LjU5LDEyLjZMOCwxMS4xOEw0LjgyLDhMOCw0LjgyTDYuNTksMy40MU0xMi40MSwzLjQxTDExLDQuODJMMTQuMTgsOEwxMSwxMS4xOEwxMi40MSwxMi42TDE3LDhMMTIuNDEsMy40MU0yMS41OSwxMS41OUwxMy41LDE5LjY4TDkuODMsMTZMOC40MiwxNy40MUwxMy41LDIyLjVMMjMsMTNMMjEuNTksMTEuNTlaIiAvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-code: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIiIGhlaWdodD0iMjIiIHZpZXdCb3g9IjAgMCAyOCAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTExLjQgMTguNkw2LjggMTRMMTEuNCA5LjRMMTAgOEw0IDE0TDEwIDIwTDExLjQgMTguNlpNMTYuNiAxOC42TDIxLjIgMTRMMTYuNiA5LjRMMTggOEwyNCAxNEwxOCAyMEwxNi42IDE4LjZWMTguNloiLz4KCTwvZz4KPC9zdmc+Cg==);
  --jp-icon-collapse-all: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTggMmMxIDAgMTEgMCAxMiAwczIgMSAyIDJjMCAxIDAgMTEgMCAxMnMwIDItMiAyQzIwIDE0IDIwIDQgMjAgNFMxMCA0IDYgNGMwLTIgMS0yIDItMnoiIC8+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTE4IDhjMC0xLTEtMi0yLTJTNSA2IDQgNnMtMiAxLTIgMmMwIDEgMCAxMSAwIDEyczEgMiAyIDJjMSAwIDExIDAgMTIgMHMyLTEgMi0yYzAtMSAwLTExIDAtMTJ6bS0yIDB2MTJINFY4eiIgLz4KICAgICAgICA8cGF0aCBkPSJNNiAxM3YyaDh2LTJ6IiAvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-console: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwMCAyMDAiPgogIDxnIGNsYXNzPSJqcC1jb25zb2xlLWljb24tYmFja2dyb3VuZC1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMjg4RDEiPgogICAgPHBhdGggZD0iTTIwIDE5LjhoMTYwdjE1OS45SDIweiIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtY29uc29sZS1pY29uLWNvbG9yIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIj4KICAgIDxwYXRoIGQ9Ik0xMDUgMTI3LjNoNDB2MTIuOGgtNDB6TTUxLjEgNzdMNzQgOTkuOWwtMjMuMyAyMy4zIDEwLjUgMTAuNSAyMy4zLTIzLjNMOTUgOTkuOSA4NC41IDg5LjQgNjEuNiA2Ni41eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-copy: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTExLjksMUgzLjJDMi40LDEsMS43LDEuNywxLjcsMi41djEwLjJoMS41VjIuNWg4LjdWMXogTTE0LjEsMy45aC04Yy0wLjgsMC0xLjUsMC43LTEuNSwxLjV2MTAuMmMwLDAuOCwwLjcsMS41LDEuNSwxLjVoOCBjMC44LDAsMS41LTAuNywxLjUtMS41VjUuNEMxNS41LDQuNiwxNC45LDMuOSwxNC4xLDMuOXogTTE0LjEsMTUuNWgtOFY1LjRoOFYxNS41eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-copyright: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGVuYWJsZS1iYWNrZ3JvdW5kPSJuZXcgMCAwIDI0IDI0IiBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCI+CiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0xMS44OCw5LjE0YzEuMjgsMC4wNiwxLjYxLDEuMTUsMS42MywxLjY2aDEuNzljLTAuMDgtMS45OC0xLjQ5LTMuMTktMy40NS0zLjE5QzkuNjQsNy42MSw4LDksOCwxMi4xNCBjMCwxLjk0LDAuOTMsNC4yNCwzLjg0LDQuMjRjMi4yMiwwLDMuNDEtMS42NSwzLjQ0LTIuOTVoLTEuNzljLTAuMDMsMC41OS0wLjQ1LDEuMzgtMS42MywxLjQ0QzEwLjU1LDE0LjgzLDEwLDEzLjgxLDEwLDEyLjE0IEMxMCw5LjI1LDExLjI4LDkuMTYsMTEuODgsOS4xNHogTTEyLDJDNi40OCwyLDIsNi40OCwyLDEyczQuNDgsMTAsMTAsMTBzMTAtNC40OCwxMC0xMFMxNy41MiwyLDEyLDJ6IE0xMiwyMGMtNC40MSwwLTgtMy41OS04LTggczMuNTktOCw4LThzOCwzLjU5LDgsOFMxNi40MSwyMCwxMiwyMHoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-cut: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkuNjQgNy42NGMuMjMtLjUuMzYtMS4wNS4zNi0xLjY0IDAtMi4yMS0xLjc5LTQtNC00UzIgMy43OSAyIDZzMS43OSA0IDQgNGMuNTkgMCAxLjE0LS4xMyAxLjY0LS4zNkwxMCAxMmwtMi4zNiAyLjM2QzcuMTQgMTQuMTMgNi41OSAxNCA2IDE0Yy0yLjIxIDAtNCAxLjc5LTQgNHMxLjc5IDQgNCA0IDQtMS43OSA0LTRjMC0uNTktLjEzLTEuMTQtLjM2LTEuNjRMMTIgMTRsNyA3aDN2LTFMOS42NCA3LjY0ek02IDhjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTAgMTJjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTYtNy41Yy0uMjggMC0uNS0uMjItLjUtLjVzLjIyLS41LjUtLjUuNS4yMi41LjUtLjIyLjUtLjUuNXpNMTkgM2wtNiA2IDIgMiA3LTdWM3oiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-delete: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2cHgiIGhlaWdodD0iMTZweCI+CiAgICA8cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIiAvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjI2MjYyIiBkPSJNNiAxOWMwIDEuMS45IDIgMiAyaDhjMS4xIDAgMi0uOSAyLTJWN0g2djEyek0xOSA0aC0zLjVsLTEtMWgtNWwtMSAxSDV2MmgxNFY0eiIgLz4KPC9zdmc+Cg==);
  --jp-icon-download: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDloLTRWM0g5djZINWw3IDcgNy03ek01IDE4djJoMTR2LTJINXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-duplicate: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTIuNzk5OTggMC44NzVIOC44OTU4MkM5LjIwMDYxIDAuODc1IDkuNDQ5OTggMS4xMzkxNCA5LjQ0OTk4IDEuNDYxOThDOS40NDk5OCAxLjc4NDgyIDkuMjAwNjEgMi4wNDg5NiA4Ljg5NTgyIDIuMDQ4OTZIMy4zNTQxNUMzLjA0OTM2IDIuMDQ4OTYgMi43OTk5OCAyLjMxMzEgMi43OTk5OCAyLjYzNTk0VjkuNjc5NjlDMi43OTk5OCAxMC4wMDI1IDIuNTUwNjEgMTAuMjY2NyAyLjI0NTgyIDEwLjI2NjdDMS45NDEwMyAxMC4yNjY3IDEuNjkxNjUgMTAuMDAyNSAxLjY5MTY1IDkuNjc5NjlWMi4wNDg5NkMxLjY5MTY1IDEuNDAzMjggMi4xOTA0IDAuODc1IDIuNzk5OTggMC44NzVaTTUuMzY2NjUgMTEuOVY0LjU1SDExLjA4MzNWMTEuOUg1LjM2NjY1Wk00LjE0MTY1IDQuMTQxNjdDNC4xNDE2NSAzLjY5MDYzIDQuNTA3MjggMy4zMjUgNC45NTgzMiAzLjMyNUgxMS40OTE3QzExLjk0MjcgMy4zMjUgMTIuMzA4MyAzLjY5MDYzIDEyLjMwODMgNC4xNDE2N1YxMi4zMDgzQzEyLjMwODMgMTIuNzU5NCAxMS45NDI3IDEzLjEyNSAxMS40OTE3IDEzLjEyNUg0Ljk1ODMyQzQuNTA3MjggMTMuMTI1IDQuMTQxNjUgMTIuNzU5NCA0LjE0MTY1IDEyLjMwODNWNC4xNDE2N1oiIGZpbGw9IiM2MTYxNjEiLz4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNOS40MzU3NCA4LjI2NTA3SDguMzY0MzFWOS4zMzY1QzguMzY0MzEgOS40NTQzNSA4LjI2Nzg4IDkuNTUwNzggOC4xNTAwMiA5LjU1MDc4QzguMDMyMTcgOS41NTA3OCA3LjkzNTc0IDkuNDU0MzUgNy45MzU3NCA5LjMzNjVWOC4yNjUwN0g2Ljg2NDMxQzYuNzQ2NDUgOC4yNjUwNyA2LjY1MDAyIDguMTY4NjQgNi42NTAwMiA4LjA1MDc4QzYuNjUwMDIgNy45MzI5MiA2Ljc0NjQ1IDcuODM2NSA2Ljg2NDMxIDcuODM2NUg3LjkzNTc0VjYuNzY1MDdDNy45MzU3NCA2LjY0NzIxIDguMDMyMTcgNi41NTA3OCA4LjE1MDAyIDYuNTUwNzhDOC4yNjc4OCA2LjU1MDc4IDguMzY0MzEgNi42NDcyMSA4LjM2NDMxIDYuNzY1MDdWNy44MzY1SDkuNDM1NzRDOS41NTM2IDcuODM2NSA5LjY1MDAyIDcuOTMyOTIgOS42NTAwMiA4LjA1MDc4QzkuNjUwMDIgOC4xNjg2NCA5LjU1MzYgOC4yNjUwNyA5LjQzNTc0IDguMjY1MDdaIiBmaWxsPSIjNjE2MTYxIiBzdHJva2U9IiM2MTYxNjEiIHN0cm9rZS13aWR0aD0iMC41Ii8+Cjwvc3ZnPgo=);
  --jp-icon-edit: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMgMTcuMjVWMjFoMy43NUwxNy44MSA5Ljk0bC0zLjc1LTMuNzVMMyAxNy4yNXpNMjAuNzEgNy4wNGMuMzktLjM5LjM5LTEuMDIgMC0xLjQxbC0yLjM0LTIuMzRjLS4zOS0uMzktMS4wMi0uMzktMS40MSAwbC0xLjgzIDEuODMgMy43NSAzLjc1IDEuODMtMS44M3oiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-ellipses: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iNSIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxOSIgY3k9IjEyIiByPSIyIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-error: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj48Y2lyY2xlIGN4PSIxMiIgY3k9IjE5IiByPSIyIi8+PHBhdGggZD0iTTEwIDNoNHYxMmgtNHoiLz48L2c+CjxwYXRoIGZpbGw9Im5vbmUiIGQ9Ik0wIDBoMjR2MjRIMHoiLz4KPC9zdmc+Cg==);
  --jp-icon-expand-all: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTggMmMxIDAgMTEgMCAxMiAwczIgMSAyIDJjMCAxIDAgMTEgMCAxMnMwIDItMiAyQzIwIDE0IDIwIDQgMjAgNFMxMCA0IDYgNGMwLTIgMS0yIDItMnoiIC8+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTE4IDhjMC0xLTEtMi0yLTJTNSA2IDQgNnMtMiAxLTIgMmMwIDEgMCAxMSAwIDEyczEgMiAyIDJjMSAwIDExIDAgMTIgMHMyLTEgMi0yYzAtMSAwLTExIDAtMTJ6bS0yIDB2MTJINFY4eiIgLz4KICAgICAgICA8cGF0aCBkPSJNMTEgMTBIOXYzSDZ2MmgzdjNoMnYtM2gzdi0yaC0zeiIgLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-extension: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwLjUgMTFIMTlWN2MwLTEuMS0uOS0yLTItMmgtNFYzLjVDMTMgMi4xMiAxMS44OCAxIDEwLjUgMVM4IDIuMTIgOCAzLjVWNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAydjMuOEgzLjVjMS40OSAwIDIuNyAxLjIxIDIuNyAyLjdzLTEuMjEgMi43LTIuNyAyLjdIMlYyMGMwIDEuMS45IDIgMiAyaDMuOHYtMS41YzAtMS40OSAxLjIxLTIuNyAyLjctMi43IDEuNDkgMCAyLjcgMS4yMSAyLjcgMi43VjIySDE3YzEuMSAwIDItLjkgMi0ydi00aDEuNWMxLjM4IDAgMi41LTEuMTIgMi41LTIuNVMyMS44OCAxMSAyMC41IDExeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-fast-forward: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTQgMThsOC41LTZMNCA2djEyem05LTEydjEybDguNS02TDEzIDZ6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-file-upload: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkgMTZoNnYtNmg0bC03LTctNyA3aDR6bS00IDJoMTR2Mkg1eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-file: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuMyA4LjJsLTUuNS01LjVjLS4zLS4zLS43LS41LTEuMi0uNUgzLjljLS44LjEtMS42LjktMS42IDEuOHYxNC4xYzAgLjkuNyAxLjYgMS42IDEuNmgxNC4yYy45IDAgMS42LS43IDEuNi0xLjZWOS40Yy4xLS41LS4xLS45LS40LTEuMnptLTUuOC0zLjNsMy40IDMuNmgtMy40VjQuOXptMy45IDEyLjdINC43Yy0uMSAwLS4yIDAtLjItLjJWNC43YzAtLjIuMS0uMy4yLS4zaDcuMnY0LjRzMCAuOC4zIDEuMWMuMy4zIDEuMS4zIDEuMS4zaDQuM3Y3LjJzLS4xLjItLjIuMnoiLz4KPC9zdmc+Cg==);
  --jp-icon-filter-dot: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTE0LDEyVjE5Ljg4QzE0LjA0LDIwLjE4IDEzLjk0LDIwLjUgMTMuNzEsMjAuNzFDMTMuMzIsMjEuMSAxMi42OSwyMS4xIDEyLjMsMjAuNzFMMTAuMjksMTguN0MxMC4wNiwxOC40NyA5Ljk2LDE4LjE2IDEwLDE3Ljg3VjEySDkuOTdMNC4yMSw0LjYyQzMuODcsNC4xOSAzLjk1LDMuNTYgNC4zOCwzLjIyQzQuNTcsMy4wOCA0Ljc4LDMgNSwzVjNIMTlWM0MxOS4yMiwzIDE5LjQzLDMuMDggMTkuNjIsMy4yMkMyMC4wNSwzLjU2IDIwLjEzLDQuMTkgMTkuNzksNC42MkwxNC4wMywxMkgxNFoiIC8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWRvdCIgZmlsbD0iI0ZGRiI+CiAgICA8Y2lyY2xlIGN4PSIxOCIgY3k9IjE3IiByPSIzIj48L2NpcmNsZT4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-filter-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEwIDE4aDR2LTJoLTR2MnpNMyA2djJoMThWNkgzem0zIDdoMTJ2LTJINnYyeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-filter: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTE0LDEyVjE5Ljg4QzE0LjA0LDIwLjE4IDEzLjk0LDIwLjUgMTMuNzEsMjAuNzFDMTMuMzIsMjEuMSAxMi42OSwyMS4xIDEyLjMsMjAuNzFMMTAuMjksMTguN0MxMC4wNiwxOC40NyA5Ljk2LDE4LjE2IDEwLDE3Ljg3VjEySDkuOTdMNC4yMSw0LjYyQzMuODcsNC4xOSAzLjk1LDMuNTYgNC4zOCwzLjIyQzQuNTcsMy4wOCA0Ljc4LDMgNSwzVjNIMTlWM0MxOS4yMiwzIDE5LjQzLDMuMDggMTkuNjIsMy4yMkMyMC4wNSwzLjU2IDIwLjEzLDQuMTkgMTkuNzksNC42MkwxNC4wMywxMkgxNFoiIC8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-folder-favorite: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjRweCIgdmlld0JveD0iMCAwIDI0IDI0IiB3aWR0aD0iMjRweCIgZmlsbD0iIzAwMDAwMCI+CiAgPHBhdGggZD0iTTAgMGgyNHYyNEgwVjB6IiBmaWxsPSJub25lIi8+PHBhdGggY2xhc3M9ImpwLWljb24zIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzYxNjE2MSIgZD0iTTIwIDZoLThsLTItMkg0Yy0xLjEgMC0yIC45LTIgMnYxMmMwIDEuMS45IDIgMiAyaDE2YzEuMSAwIDItLjkgMi0yVjhjMC0xLjEtLjktMi0yLTJ6bS0yLjA2IDExTDE1IDE1LjI4IDEyLjA2IDE3bC43OC0zLjMzLTIuNTktMi4yNCAzLjQxLS4yOUwxNSA4bDEuMzQgMy4xNCAzLjQxLjI5LTIuNTkgMi4yNC43OCAzLjMzeiIvPgo8L3N2Zz4K);
  --jp-icon-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY4YzAtMS4xLS45LTItMi0yaC04bC0yLTJ6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-home: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjRweCIgdmlld0JveD0iMCAwIDI0IDI0IiB3aWR0aD0iMjRweCIgZmlsbD0iIzAwMDAwMCI+CiAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPjxwYXRoIGNsYXNzPSJqcC1pY29uMyBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xMCAyMHYtNmg0djZoNXYtOGgzTDEyIDMgMiAxMmgzdjh6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-html5: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uMCBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMDAiIGQ9Ik0xMDguNCAwaDIzdjIyLjhoMjEuMlYwaDIzdjY5aC0yM1Y0NmgtMjF2MjNoLTIzLjJNMjA2IDIzaC0yMC4zVjBoNjMuN3YyM0gyMjl2NDZoLTIzbTUzLjUtNjloMjQuMWwxNC44IDI0LjNMMzEzLjIgMGgyNC4xdjY5aC0yM1YzNC44bC0xNi4xIDI0LjgtMTYuMS0yNC44VjY5aC0yMi42bTg5LjItNjloMjN2NDYuMmgzMi42VjY5aC01NS42Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI2U0NGQyNiIgZD0iTTEwNy42IDQ3MWwtMzMtMzcwLjRoMzYyLjhsLTMzIDM3MC4yTDI1NS43IDUxMiIvPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNmMTY1MjkiIGQ9Ik0yNTYgNDgwLjVWMTMxaDE0OC4zTDM3NiA0NDciLz4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNlYmViZWIiIGQ9Ik0xNDIgMTc2LjNoMTE0djQ1LjRoLTY0LjJsNC4yIDQ2LjVoNjB2NDUuM0gxNTQuNG0yIDIyLjhIMjAybDMuMiAzNi4zIDUwLjggMTMuNnY0Ny40bC05My4yLTI2Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIiBkPSJNMzY5LjYgMTc2LjNIMjU1Ljh2NDUuNGgxMDkuNm0tNC4xIDQ2LjVIMjU1Ljh2NDUuNGg1NmwtNS4zIDU5LTUwLjcgMTMuNnY0Ny4ybDkzLTI1LjgiLz4KPC9zdmc+Cg==);
  --jp-icon-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1icmFuZDQganAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNGRkYiIGQ9Ik0yLjIgMi4yaDE3LjV2MTcuNUgyLjJ6Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzNGNTFCNSIgZD0iTTIuMiAyLjJ2MTcuNWgxNy41bC4xLTE3LjVIMi4yem0xMi4xIDIuMmMxLjIgMCAyLjIgMSAyLjIgMi4ycy0xIDIuMi0yLjIgMi4yLTIuMi0xLTIuMi0yLjIgMS0yLjIgMi4yLTIuMnpNNC40IDE3LjZsMy4zLTguOCAzLjMgNi42IDIuMi0zLjIgNC40IDUuNEg0LjR6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-info: url(data:image/svg+xml;base64,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);
  --jp-icon-inspector: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaW5zcGVjdG9yLWljb24tY29sb3IganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY2YzAtMS4xLS45LTItMi0yem0tNSAxNEg0di00aDExdjR6bTAtNUg0VjloMTF2NHptNSA1aC00VjloNHY5eiIvPgo8L3N2Zz4K);
  --jp-icon-json: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtanNvbi1pY29uLWNvbG9yIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI0Y5QTgyNSI+CiAgICA8cGF0aCBkPSJNMjAuMiAxMS44Yy0xLjYgMC0xLjcuNS0xLjcgMSAwIC40LjEuOS4xIDEuMy4xLjUuMS45LjEgMS4zIDAgMS43LTEuNCAyLjMtMy41IDIuM2gtLjl2LTEuOWguNWMxLjEgMCAxLjQgMCAxLjQtLjggMC0uMyAwLS42LS4xLTEgMC0uNC0uMS0uOC0uMS0xLjIgMC0xLjMgMC0xLjggMS4zLTItMS4zLS4yLTEuMy0uNy0xLjMtMiAwLS40LjEtLjguMS0xLjIuMS0uNC4xLS43LjEtMSAwLS44LS40LS43LTEuNC0uOGgtLjVWNC4xaC45YzIuMiAwIDMuNS43IDMuNSAyLjMgMCAuNC0uMS45LS4xIDEuMy0uMS41LS4xLjktLjEgMS4zIDAgLjUuMiAxIDEuNyAxdjEuOHpNMS44IDEwLjFjMS42IDAgMS43LS41IDEuNy0xIDAtLjQtLjEtLjktLjEtMS4zLS4xLS41LS4xLS45LS4xLTEuMyAwLTEuNiAxLjQtMi4zIDMuNS0yLjNoLjl2MS45aC0uNWMtMSAwLTEuNCAwLTEuNC44IDAgLjMgMCAuNi4xIDEgMCAuMi4xLjYuMSAxIDAgMS4zIDAgMS44LTEuMyAyQzYgMTEuMiA2IDExLjcgNiAxM2MwIC40LS4xLjgtLjEgMS4yLS4xLjMtLjEuNy0uMSAxIDAgLjguMy44IDEuNC44aC41djEuOWgtLjljLTIuMSAwLTMuNS0uNi0zLjUtMi4zIDAtLjQuMS0uOS4xLTEuMy4xLS41LjEtLjkuMS0xLjMgMC0uNS0uMi0xLTEuNy0xdi0xLjl6Ii8+CiAgICA8Y2lyY2xlIGN4PSIxMSIgY3k9IjEzLjgiIHI9IjIuMSIvPgogICAgPGNpcmNsZSBjeD0iMTEiIGN5PSI4LjIiIHI9IjIuMSIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-julia: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDMyNSAzMDAiPgogIDxnIGNsYXNzPSJqcC1icmFuZDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjY2IzYzMzIj4KICAgIDxwYXRoIGQ9Ik0gMTUwLjg5ODQzOCAyMjUgQyAxNTAuODk4NDM4IDI2Ni40MjE4NzUgMTE3LjMyMDMxMiAzMDAgNzUuODk4NDM4IDMwMCBDIDM0LjQ3NjU2MiAzMDAgMC44OTg0MzggMjY2LjQyMTg3NSAwLjg5ODQzOCAyMjUgQyAwLjg5ODQzOCAxODMuNTc4MTI1IDM0LjQ3NjU2MiAxNTAgNzUuODk4NDM4IDE1MCBDIDExNy4zMjAzMTIgMTUwIDE1MC44OTg0MzggMTgzLjU3ODEyNSAxNTAuODk4NDM4IDIyNSIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzM4OTgyNiI+CiAgICA8cGF0aCBkPSJNIDIzNy41IDc1IEMgMjM3LjUgMTE2LjQyMTg3NSAyMDMuOTIxODc1IDE1MCAxNjIuNSAxNTAgQyAxMjEuMDc4MTI1IDE1MCA4Ny41IDExNi40MjE4NzUgODcuNSA3NSBDIDg3LjUgMzMuNTc4MTI1IDEyMS4wNzgxMjUgMCAxNjIuNSAwIEMgMjAzLjkyMTg3NSAwIDIzNy41IDMzLjU3ODEyNSAyMzcuNSA3NSIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzk1NThiMiI+CiAgICA8cGF0aCBkPSJNIDMyNC4xMDE1NjIgMjI1IEMgMzI0LjEwMTU2MiAyNjYuNDIxODc1IDI5MC41MjM0MzggMzAwIDI0OS4xMDE1NjIgMzAwIEMgMjA3LjY3OTY4OCAzMDAgMTc0LjEwMTU2MiAyNjYuNDIxODc1IDE3NC4xMDE1NjIgMjI1IEMgMTc0LjEwMTU2MiAxODMuNTc4MTI1IDIwNy42Nzk2ODggMTUwIDI0OS4xMDE1NjIgMTUwIEMgMjkwLjUyMzQzOCAxNTAgMzI0LjEwMTU2MiAxODMuNTc4MTI1IDMyNC4xMDE1NjIgMjI1Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-jupyter-favicon: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTUyIiBoZWlnaHQ9IjE2NSIgdmlld0JveD0iMCAwIDE1MiAxNjUiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgPGcgY2xhc3M9ImpwLWp1cHl0ZXItaWNvbi1jb2xvciIgZmlsbD0iI0YzNzcyNiI+CiAgICA8cGF0aCB0cmFuc2Zvcm09InRyYW5zbGF0ZSgwLjA3ODk0NywgMTEwLjU4MjkyNykiIGQ9Ik03NS45NDIyODQyLDI5LjU4MDQ1NjEgQzQzLjMwMjM5NDcsMjkuNTgwNDU2MSAxNC43OTY3ODMyLDE3LjY1MzQ2MzQgMCwwIEM1LjUxMDgzMjExLDE1Ljg0MDY4MjkgMTUuNzgxNTM4OSwyOS41NjY3NzMyIDI5LjM5MDQ5NDcsMzkuMjc4NDE3MSBDNDIuOTk5Nyw0OC45ODk4NTM3IDU5LjI3MzcsNTQuMjA2NzgwNSA3NS45NjA1Nzg5LDU0LjIwNjc4MDUgQzkyLjY0NzQ1NzksNTQuMjA2NzgwNSAxMDguOTIxNDU4LDQ4Ljk4OTg1MzcgMTIyLjUzMDY2MywzOS4yNzg0MTcxIEMxMzYuMTM5NDUzLDI5LjU2Njc3MzIgMTQ2LjQxMDI4NCwxNS44NDA2ODI5IDE1MS45MjExNTgsMCBDMTM3LjA4Nzg2OCwxNy42NTM0NjM0IDEwOC41ODI1ODksMjkuNTgwNDU2MSA3NS45NDIyODQyLDI5LjU4MDQ1NjEgTDc1Ljk0MjI4NDIsMjkuNTgwNDU2MSBaIiAvPgogICAgPHBhdGggdHJhbnNmb3JtPSJ0cmFuc2xhdGUoMC4wMzczNjgsIDAuNzA0ODc4KSIgZD0iTTc1Ljk3ODQ1NzksMjQuNjI2NDA3MyBDMTA4LjYxODc2MywyNC42MjY0MDczIDEzNy4xMjQ0NTgsMzYuNTUzNDQxNSAxNTEuOTIxMTU4LDU0LjIwNjc4MDUgQzE0Ni40MTAyODQsMzguMzY2MjIyIDEzNi4xMzk0NTMsMjQuNjQwMTMxNyAxMjIuNTMwNjYzLDE0LjkyODQ4NzggQzEwOC45MjE0NTgsNS4yMTY4NDM5IDkyLjY0NzQ1NzksMCA3NS45NjA1Nzg5LDAgQzU5LjI3MzcsMCA0Mi45OTk3LDUuMjE2ODQzOSAyOS4zOTA0OTQ3LDE0LjkyODQ4NzggQzE1Ljc4MTUzODksMjQuNjQwMTMxNyA1LjUxMDgzMjExLDM4LjM2NjIyMiAwLDU0LjIwNjc4MDUgQzE0LjgzMzA4MTYsMzYuNTg5OTI5MyA0My4zMzg1Njg0LDI0LjYyNjQwNzMgNzUuOTc4NDU3OSwyNC42MjY0MDczIEw3NS45Nzg0NTc5LDI0LjYyNjQwNzMgWiIgLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-jupyter: url(data:image/svg+xml;base64,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);
  --jp-icon-jupyterlab-wordmark: url(data:image/svg+xml;base64,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);
  --jp-icon-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgZmlsbD0iIzYxNjE2MSIgZD0iTTE1IDlIOXY2aDZWOXptLTIgNGgtMnYtMmgydjJ6bTgtMlY5aC0yVjdjMC0xLjEtLjktMi0yLTJoLTJWM2gtMnYyaC0yVjNIOXYySDdjLTEuMSAwLTIgLjktMiAydjJIM3YyaDJ2MkgzdjJoMnYyYzAgMS4xLjkgMiAyIDJoMnYyaDJ2LTJoMnYyaDJ2LTJoMmMxLjEgMCAyLS45IDItMnYtMmgydi0yaC0ydi0yaDJ6bS00IDZIN1Y3aDEwdjEweiIvPgo8L3N2Zz4K);
  --jp-icon-keyboard: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMTdjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY3YzAtMS4xLS45LTItMi0yem0tOSAzaDJ2MmgtMlY4em0wIDNoMnYyaC0ydi0yek04IDhoMnYySDhWOHptMCAzaDJ2Mkg4di0yem0tMSAySDV2LTJoMnYyem0wLTNINVY4aDJ2MnptOSA3SDh2LTJoOHYyem0wLTRoLTJ2LTJoMnYyem0wLTNoLTJWOGgydjJ6bTMgM2gtMnYtMmgydjJ6bTAtM2gtMlY4aDJ2MnoiLz4KPC9zdmc+Cg==);
  --jp-icon-launch: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMzIgMzIiIHdpZHRoPSIzMiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0yNiwyOEg2YTIuMDAyNywyLjAwMjcsMCwwLDEtMi0yVjZBMi4wMDI3LDIuMDAyNywwLDAsMSw2LDRIMTZWNkg2VjI2SDI2VjE2aDJWMjZBMi4wMDI3LDIuMDAyNywwLDAsMSwyNiwyOFoiLz4KICAgIDxwb2x5Z29uIHBvaW50cz0iMjAgMiAyMCA0IDI2LjU4NiA0IDE4IDEyLjU4NiAxOS40MTQgMTQgMjggNS40MTQgMjggMTIgMzAgMTIgMzAgMiAyMCAyIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-launcher: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkgMTlINVY1aDdWM0g1YTIgMiAwIDAwLTIgMnYxNGEyIDIgMCAwMDIgMmgxNGMxLjEgMCAyLS45IDItMnYtN2gtMnY3ek0xNCAzdjJoMy41OWwtOS44MyA5LjgzIDEuNDEgMS40MUwxOSA2LjQxVjEwaDJWM2gtN3oiLz4KPC9zdmc+Cg==);
  --jp-icon-line-form: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGZpbGw9IndoaXRlIiBkPSJNNS44OCA0LjEyTDEzLjc2IDEybC03Ljg4IDcuODhMOCAyMmwxMC0xMEw4IDJ6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-link: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMuOSAxMmMwLTEuNzEgMS4zOS0zLjEgMy4xLTMuMWg0VjdIN2MtMi43NiAwLTUgMi4yNC01IDVzMi4yNCA1IDUgNWg0di0xLjlIN2MtMS43MSAwLTMuMS0xLjM5LTMuMS0zLjF6TTggMTNoOHYtMkg4djJ6bTktNmgtNHYxLjloNGMxLjcxIDAgMy4xIDEuMzkgMy4xIDMuMXMtMS4zOSAzLjEtMy4xIDMuMWgtNFYxN2g0YzIuNzYgMCA1LTIuMjQgNS01cy0yLjI0LTUtNS01eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xOSA1djE0SDVWNWgxNG0xLjEtMkgzLjljLS41IDAtLjkuNC0uOS45djE2LjJjMCAuNC40LjkuOS45aDE2LjJjLjQgMCAuOS0uNS45LS45VjMuOWMwLS41LS41LS45LS45LS45ek0xMSA3aDZ2MmgtNlY3em0wIDRoNnYyaC02di0yem0wIDRoNnYyaC02ek03IDdoMnYySDd6bTAgNGgydjJIN3ptMCA0aDJ2Mkg3eiIvPgo8L3N2Zz4K);
  --jp-icon-markdown: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjN0IxRkEyIiBkPSJNNSAxNC45aDEybC02LjEgNnptOS40LTYuOGMwLTEuMy0uMS0yLjktLjEtNC41LS40IDEuNC0uOSAyLjktMS4zIDQuM2wtMS4zIDQuM2gtMkw4LjUgNy45Yy0uNC0xLjMtLjctMi45LTEtNC4zLS4xIDEuNi0uMSAzLjItLjIgNC42TDcgMTIuNEg0LjhsLjctMTFoMy4zTDEwIDVjLjQgMS4yLjcgMi43IDEgMy45LjMtMS4yLjctMi42IDEtMy45bDEuMi0zLjdoMy4zbC42IDExaC0yLjRsLS4zLTQuMnoiLz4KPC9zdmc+Cg==);
  --jp-icon-move-down: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNMTIuNDcxIDcuNTI4OTlDMTIuNzYzMiA3LjIzNjg0IDEyLjc2MzIgNi43NjMxNiAxMi40NzEgNi40NzEwMVY2LjQ3MTAxQzEyLjE3OSA2LjE3OTA1IDExLjcwNTcgNi4xNzg4NCAxMS40MTM1IDYuNDcwNTRMNy43NSAxMC4xMjc1VjEuNzVDNy43NSAxLjMzNTc5IDcuNDE0MjEgMSA3IDFWMUM2LjU4NTc5IDEgNi4yNSAxLjMzNTc5IDYuMjUgMS43NVYxMC4xMjc1TDIuNTk3MjYgNi40NjgyMkMyLjMwMzM4IDYuMTczODEgMS44MjY0MSA2LjE3MzU5IDEuNTMyMjYgNi40Njc3NFY2LjQ2Nzc0QzEuMjM4MyA2Ljc2MTcgMS4yMzgzIDcuMjM4MyAxLjUzMjI2IDcuNTMyMjZMNi4yOTI4OSAxMi4yOTI5QzYuNjgzNDIgMTIuNjgzNCA3LjMxNjU4IDEyLjY4MzQgNy43MDcxMSAxMi4yOTI5TDEyLjQ3MSA3LjUyODk5WiIgZmlsbD0iIzYxNjE2MSIvPgo8L3N2Zz4K);
  --jp-icon-move-up: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNMS41Mjg5OSA2LjQ3MTAxQzEuMjM2ODQgNi43NjMxNiAxLjIzNjg0IDcuMjM2ODQgMS41Mjg5OSA3LjUyODk5VjcuNTI4OTlDMS44MjA5NSA3LjgyMDk1IDIuMjk0MjYgNy44MjExNiAyLjU4NjQ5IDcuNTI5NDZMNi4yNSAzLjg3MjVWMTIuMjVDNi4yNSAxMi42NjQyIDYuNTg1NzkgMTMgNyAxM1YxM0M3LjQxNDIxIDEzIDcuNzUgMTIuNjY0MiA3Ljc1IDEyLjI1VjMuODcyNUwxMS40MDI3IDcuNTMxNzhDMTEuNjk2NiA3LjgyNjE5IDEyLjE3MzYgNy44MjY0MSAxMi40Njc3IDcuNTMyMjZWNy41MzIyNkMxMi43NjE3IDcuMjM4MyAxMi43NjE3IDYuNzYxNyAxMi40Njc3IDYuNDY3NzRMNy43MDcxMSAxLjcwNzExQzcuMzE2NTggMS4zMTY1OCA2LjY4MzQyIDEuMzE2NTggNi4yOTI4OSAxLjcwNzExTDEuNTI4OTkgNi40NzEwMVoiIGZpbGw9IiM2MTYxNjEiLz4KPC9zdmc+Cg==);
  --jp-icon-new-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwIDZoLThsLTItMkg0Yy0xLjExIDAtMS45OS44OS0xLjk5IDJMMiAxOGMwIDEuMTEuODkgMiAyIDJoMTZjMS4xMSAwIDItLjg5IDItMlY4YzAtMS4xMS0uODktMi0yLTJ6bS0xIDhoLTN2M2gtMnYtM2gtM3YtMmgzVjloMnYzaDN2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-not-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI1IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDMgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMTkgMTcuMTg0NCAyLjk2OTY4IDE0LjMwMzIgMS44NjA5NCAxMS40NDA5WiIvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24yIiBzdHJva2U9IiMzMzMzMzMiIHN0cm9rZS13aWR0aD0iMiIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOS4zMTU5MiA5LjMyMDMxKSIgZD0iTTcuMzY4NDIgMEwwIDcuMzY0NzkiLz4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDkuMzE1OTIgMTYuNjgzNikgc2NhbGUoMSAtMSkiIGQ9Ik03LjM2ODQyIDBMMCA3LjM2NDc5Ii8+Cjwvc3ZnPgo=);
  --jp-icon-notebook: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtbm90ZWJvb2staWNvbi1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNFRjZDMDAiPgogICAgPHBhdGggZD0iTTE4LjcgMy4zdjE1LjRIMy4zVjMuM2gxNS40bTEuNS0xLjVIMS44djE4LjNoMTguM2wuMS0xOC4zeiIvPgogICAgPHBhdGggZD0iTTE2LjUgMTYuNWwtNS40LTQuMy01LjYgNC4zdi0xMWgxMXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-numbering: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIiIGhlaWdodD0iMjIiIHZpZXdCb3g9IjAgMCAyOCAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTQgMTlINlYxOS41SDVWMjAuNUg2VjIxSDRWMjJIN1YxOEg0VjE5Wk01IDEwSDZWNkg0VjdINVYxMFpNNCAxM0g1LjhMNCAxNS4xVjE2SDdWMTVINS4yTDcgMTIuOVYxMkg0VjEzWk05IDdWOUgyM1Y3SDlaTTkgMjFIMjNWMTlIOVYyMVpNOSAxNUgyM1YxM0g5VjE1WiIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-offline-bolt: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDIuMDJjLTUuNTEgMC05Ljk4IDQuNDctOS45OCA5Ljk4czQuNDcgOS45OCA5Ljk4IDkuOTggOS45OC00LjQ3IDkuOTgtOS45OFMxNy41MSAyLjAyIDEyIDIuMDJ6TTExLjQ4IDIwdi02LjI2SDhMMTMgNHY2LjI2aDMuMzVMMTEuNDggMjB6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-palette: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE4IDEzVjIwSDRWNkg5LjAyQzkuMDcgNS4yOSA5LjI0IDQuNjIgOS41IDRINEMyLjkgNCAyIDQuOSAyIDZWMjBDMiAyMS4xIDIuOSAyMiA0IDIySDE4QzE5LjEgMjIgMjAgMjEuMSAyMCAyMFYxNUwxOCAxM1pNMTkuMyA4Ljg5QzE5Ljc0IDguMTkgMjAgNy4zOCAyMCA2LjVDMjAgNC4wMSAxNy45OSAyIDE1LjUgMkMxMy4wMSAyIDExIDQuMDEgMTEgNi41QzExIDguOTkgMTMuMDEgMTEgMTUuNDkgMTFDMTYuMzcgMTEgMTcuMTkgMTAuNzQgMTcuODggMTAuM0wyMSAxMy40MkwyMi40MiAxMkwxOS4zIDguODlaTTE1LjUgOUMxNC4xMiA5IDEzIDcuODggMTMgNi41QzEzIDUuMTIgMTQuMTIgNCAxNS41IDRDMTYuODggNCAxOCA1LjEyIDE4IDYuNUMxOCA3Ljg4IDE2Ljg4IDkgMTUuNSA5WiIvPgogICAgPHBhdGggZmlsbC1ydWxlPSJldmVub2RkIiBjbGlwLXJ1bGU9ImV2ZW5vZGQiIGQ9Ik00IDZIOS4wMTg5NEM5LjAwNjM5IDYuMTY1MDIgOSA2LjMzMTc2IDkgNi41QzkgOC44MTU3NyAxMC4yMTEgMTAuODQ4NyAxMi4wMzQzIDEySDlWMTRIMTZWMTIuOTgxMUMxNi41NzAzIDEyLjkzNzcgMTcuMTIgMTIuODIwNyAxNy42Mzk2IDEyLjYzOTZMMTggMTNWMjBINFY2Wk04IDhINlYxMEg4VjhaTTYgMTJIOFYxNEg2VjEyWk04IDE2SDZWMThIOFYxNlpNOSAxNkgxNlYxOEg5VjE2WiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-paste: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE5IDJoLTQuMThDMTQuNC44NCAxMy4zIDAgMTIgMGMtMS4zIDAtMi40Ljg0LTIuODIgMkg1Yy0xLjEgMC0yIC45LTIgMnYxNmMwIDEuMS45IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjRjMC0xLjEtLjktMi0yLTJ6bS03IDBjLjU1IDAgMSAuNDUgMSAxcy0uNDUgMS0xIDEtMS0uNDUtMS0xIC40NS0xIDEtMXptNyAxOEg1VjRoMnYzaDEwVjRoMnYxNnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-pdf: url(data:image/svg+xml;base64,PHN2ZwogICB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyMiAyMiIgd2lkdGg9IjE2Ij4KICAgIDxwYXRoIHRyYW5zZm9ybT0icm90YXRlKDQ1KSIgY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI0ZGMkEyQSIKICAgICAgIGQ9Im0gMjIuMzQ0MzY5LC0zLjAxNjM2NDIgaCA1LjYzODYwNCB2IDEuNTc5MjQzMyBoIC0zLjU0OTIyNyB2IDEuNTA4NjkyOTkgaCAzLjMzNzU3NiBWIDEuNjUwODE1NCBoIC0zLjMzNzU3NiB2IDMuNDM1MjYxMyBoIC0yLjA4OTM3NyB6IG0gLTcuMTM2NDQ0LDEuNTc5MjQzMyB2IDQuOTQzOTU0MyBoIDAuNzQ4OTIgcSAxLjI4MDc2MSwwIDEuOTUzNzAzLC0wLjYzNDk1MzUgMC42NzgzNjksLTAuNjM0OTUzNSAwLjY3ODM2OSwtMS44NDUxNjQxIDAsLTEuMjA0NzgzNTUgLTAuNjcyOTQyLC0xLjgzNDMxMDExIC0wLjY3Mjk0MiwtMC42Mjk1MjY1OSAtMS45NTkxMywtMC42Mjk1MjY1OSB6IG0gLTIuMDg5Mzc3LC0xLjU3OTI0MzMgaCAyLjIwMzM0MyBxIDEuODQ1MTY0LDAgMi43NDYwMzksMC4yNjU5MjA3IDAuOTA2MzAxLDAuMjYwNDkzNyAxLjU1MjEwOCwwLjg5MDAyMDMgMC41Njk4MywwLjU0ODEyMjMgMC44NDY2MDUsMS4yNjQ0ODAwNiAwLjI3Njc3NCwwLjcxNjM1NzgxIDAuMjc2Nzc0LDEuNjIyNjU4OTQgMCwwLjkxNzE1NTEgLTAuMjc2Nzc0LDEuNjM4OTM5OSAtMC4yNzY3NzUsMC43MTYzNTc4IC0wLjg0NjYwNSwxLjI2NDQ4IC0wLjY1MTIzNCwwLjYyOTUyNjYgLTEuNTYyOTYyLDAuODk1NDQ3MyAtMC45MTE3MjgsMC4yNjA0OTM3IC0yLjczNTE4NSwwLjI2MDQ5MzcgaCAtMi4yMDMzNDMgeiBtIC04LjE0NTg1NjUsMCBoIDMuNDY3ODIzIHEgMS41NDY2ODE2LDAgMi4zNzE1Nzg1LDAuNjg5MjIzIDAuODMwMzI0LDAuNjgzNzk2MSAwLjgzMDMyNCwxLjk1MzcwMzE0IDAsMS4yNzUzMzM5NyAtMC44MzAzMjQsMS45NjQ1NTcwNiBRIDkuOTg3MTk2MSwyLjI3NDkxNSA4LjQ0MDUxNDUsMi4yNzQ5MTUgSCA3LjA2MjA2ODQgViA1LjA4NjA3NjcgSCA0Ljk3MjY5MTUgWiBtIDIuMDg5Mzc2OSwxLjUxNDExOTkgdiAyLjI2MzAzOTQzIGggMS4xNTU5NDEgcSAwLjYwNzgxODgsMCAwLjkzODg2MjksLTAuMjkzMDU1NDcgMC4zMzEwNDQxLC0wLjI5ODQ4MjQxIDAuMzMxMDQ0MSwtMC44NDExNzc3MiAwLC0wLjU0MjY5NTMxIC0wLjMzMTA0NDEsLTAuODM1NzUwNzQgLTAuMzMxMDQ0MSwtMC4yOTMwNTU1IC0wLjkzODg2MjksLTAuMjkzMDU1NSB6IgovPgo8L3N2Zz4K);
  --jp-icon-python: url(data:image/svg+xml;base64,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);
  --jp-icon-r-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjE5NkYzIiBkPSJNNC40IDIuNWMxLjItLjEgMi45LS4zIDQuOS0uMyAyLjUgMCA0LjEuNCA1LjIgMS4zIDEgLjcgMS41IDEuOSAxLjUgMy41IDAgMi0xLjQgMy41LTIuOSA0LjEgMS4yLjQgMS43IDEuNiAyLjIgMyAuNiAxLjkgMSAzLjkgMS4zIDQuNmgtMy44Yy0uMy0uNC0uOC0xLjctMS4yLTMuN3MtMS4yLTIuNi0yLjYtMi42aC0uOXY2LjRINC40VjIuNXptMy43IDYuOWgxLjRjMS45IDAgMi45LS45IDIuOS0yLjNzLTEtMi4zLTIuOC0yLjNjLS43IDAtMS4zIDAtMS42LjJ2NC41aC4xdi0uMXoiLz4KPC9zdmc+Cg==);
  --jp-icon-react: url(data:image/svg+xml;base64,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);
  --jp-icon-redo: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgICA8cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZD0iTTE4LjQgMTAuNkMxNi41NSA4Ljk5IDE0LjE1IDggMTEuNSA4Yy00LjY1IDAtOC41OCAzLjAzLTkuOTYgNy4yMkwzLjkgMTZjMS4wNS0zLjE5IDQuMDUtNS41IDcuNi01LjUgMS45NSAwIDMuNzMuNzIgNS4xMiAxLjg4TDEzIDE2aDlWN2wtMy42IDMuNnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-refresh: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTkgMTMuNWMtMi40OSAwLTQuNS0yLjAxLTQuNS00LjVTNi41MSA0LjUgOSA0LjVjMS4yNCAwIDIuMzYuNTIgMy4xNyAxLjMzTDEwIDhoNVYzbC0xLjc2IDEuNzZDMTIuMTUgMy42OCAxMC42NiAzIDkgMyA1LjY5IDMgMy4wMSA1LjY5IDMuMDEgOVM1LjY5IDE1IDkgMTVjMi45NyAwIDUuNDMtMi4xNiA1LjktNWgtMS41MmMtLjQ2IDItMi4yNCAzLjUtNC4zOCAzLjV6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-regex: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbi1hY2NlbnQyIiBmaWxsPSIjRkZGIj4KICAgIDxjaXJjbGUgY2xhc3M9InN0MiIgY3g9IjUuNSIgY3k9IjE0LjUiIHI9IjEuNSIvPgogICAgPHJlY3QgeD0iMTIiIHk9IjQiIGNsYXNzPSJzdDIiIHdpZHRoPSIxIiBoZWlnaHQ9IjgiLz4KICAgIDxyZWN0IHg9IjguNSIgeT0iNy41IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjg2NiAtMC41IDAuNSAwLjg2NiAtMi4zMjU1IDcuMzIxOSkiIGNsYXNzPSJzdDIiIHdpZHRoPSI4IiBoZWlnaHQ9IjEiLz4KICAgIDxyZWN0IHg9IjEyIiB5PSI0IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjUgLTAuODY2IDAuODY2IDAuNSAtMC42Nzc5IDE0LjgyNTIpIiBjbGFzcz0ic3QyIiB3aWR0aD0iMSIgaGVpZ2h0PSI4Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-run: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTggNXYxNGwxMS03eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-running: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMjU2IDhDMTE5IDggOCAxMTkgOCAyNTZzMTExIDI0OCAyNDggMjQ4IDI0OC0xMTEgMjQ4LTI0OFMzOTMgOCAyNTYgOHptOTYgMzI4YzAgOC44LTcuMiAxNi0xNiAxNkgxNzZjLTguOCAwLTE2LTcuMi0xNi0xNlYxNzZjMC04LjggNy4yLTE2IDE2LTE2aDE2MGM4LjggMCAxNiA3LjIgMTYgMTZ2MTYweiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-save: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE3IDNINWMtMS4xMSAwLTIgLjktMiAydjE0YzAgMS4xLjg5IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjdsLTQtNHptLTUgMTZjLTEuNjYgMC0zLTEuMzQtMy0zczEuMzQtMyAzLTMgMyAxLjM0IDMgMy0xLjM0IDMtMyAzem0zLTEwSDVWNWgxMHY0eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-search: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjEsMTAuOWgtMC43bC0wLjItMC4yYzAuOC0wLjksMS4zLTIuMiwxLjMtMy41YzAtMy0yLjQtNS40LTUuNC01LjRTMS44LDQuMiwxLjgsNy4xczIuNCw1LjQsNS40LDUuNCBjMS4zLDAsMi41LTAuNSwzLjUtMS4zbDAuMiwwLjJ2MC43bDQuMSw0LjFsMS4yLTEuMkwxMi4xLDEwLjl6IE03LjEsMTAuOWMtMi4xLDAtMy43LTEuNy0zLjctMy43czEuNy0zLjcsMy43LTMuN3MzLjcsMS43LDMuNywzLjcgUzkuMiwxMC45LDcuMSwxMC45eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-settings: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuNDMgMTIuOThjLjA0LS4zMi4wNy0uNjQuMDctLjk4cy0uMDMtLjY2LS4wNy0uOThsMi4xMS0xLjY1Yy4xOS0uMTUuMjQtLjQyLjEyLS42NGwtMi0zLjQ2Yy0uMTItLjIyLS4zOS0uMy0uNjEtLjIybC0yLjQ5IDFjLS41Mi0uNC0xLjA4LS43My0xLjY5LS45OGwtLjM4LTIuNjVBLjQ4OC40ODggMCAwMDE0IDJoLTRjLS4yNSAwLS40Ni4xOC0uNDkuNDJsLS4zOCAyLjY1Yy0uNjEuMjUtMS4xNy41OS0xLjY5Ljk4bC0yLjQ5LTFjLS4yMy0uMDktLjQ5IDAtLjYxLjIybC0yIDMuNDZjLS4xMy4yMi0uMDcuNDkuMTIuNjRsMi4xMSAxLjY1Yy0uMDQuMzItLjA3LjY1LS4wNy45OHMuMDMuNjYuMDcuOThsLTIuMTEgMS42NWMtLjE5LjE1LS4yNC40Mi0uMTIuNjRsMiAzLjQ2Yy4xMi4yMi4zOS4zLjYxLjIybDIuNDktMWMuNTIuNCAxLjA4LjczIDEuNjkuOThsLjM4IDIuNjVjLjAzLjI0LjI0LjQyLjQ5LjQyaDRjLjI1IDAgLjQ2LS4xOC40OS0uNDJsLjM4LTIuNjVjLjYxLS4yNSAxLjE3LS41OSAxLjY5LS45OGwyLjQ5IDFjLjIzLjA5LjQ5IDAgLjYxLS4yMmwyLTMuNDZjLjEyLS4yMi4wNy0uNDktLjEyLS42NGwtMi4xMS0xLjY1ek0xMiAxNS41Yy0xLjkzIDAtMy41LTEuNTctMy41LTMuNXMxLjU3LTMuNSAzLjUtMy41IDMuNSAxLjU3IDMuNSAzLjUtMS41NyAzLjUtMy41IDMuNXoiLz4KPC9zdmc+Cg==);
  --jp-icon-share: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIHZpZXdCb3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTSAxOCAyIEMgMTYuMzU0OTkgMiAxNSAzLjM1NDk5MDQgMTUgNSBDIDE1IDUuMTkwOTUyOSAxNS4wMjE3OTEgNS4zNzcxMjI0IDE1LjA1NjY0MSA1LjU1ODU5MzggTCA3LjkyMTg3NSA5LjcyMDcwMzEgQyA3LjM5ODUzOTkgOS4yNzc4NTM5IDYuNzMyMDc3MSA5IDYgOSBDIDQuMzU0OTkwNCA5IDMgMTAuMzU0OTkgMyAxMiBDIDMgMTMuNjQ1MDEgNC4zNTQ5OTA0IDE1IDYgMTUgQyA2LjczMjA3NzEgMTUgNy4zOTg1Mzk5IDE0LjcyMjE0NiA3LjkyMTg3NSAxNC4yNzkyOTcgTCAxNS4wNTY2NDEgMTguNDM5NDUzIEMgMTUuMDIxNTU1IDE4LjYyMTUxNCAxNSAxOC44MDgzODYgMTUgMTkgQyAxNSAyMC42NDUwMSAxNi4zNTQ5OSAyMiAxOCAyMiBDIDE5LjY0NTAxIDIyIDIxIDIwLjY0NTAxIDIxIDE5IEMgMjEgMTcuMzU0OTkgMTkuNjQ1MDEgMTYgMTggMTYgQyAxNy4yNjc0OCAxNiAxNi42MDE1OTMgMTYuMjc5MzI4IDE2LjA3ODEyNSAxNi43MjI2NTYgTCA4Ljk0MzM1OTQgMTIuNTU4NTk0IEMgOC45NzgyMDk1IDEyLjM3NzEyMiA5IDEyLjE5MDk1MyA5IDEyIEMgOSAxMS44MDkwNDcgOC45NzgyMDk1IDExLjYyMjg3OCA4Ljk0MzM1OTQgMTEuNDQxNDA2IEwgMTYuMDc4MTI1IDcuMjc5Mjk2OSBDIDE2LjYwMTQ2IDcuNzIyMTQ2MSAxNy4yNjc5MjMgOCAxOCA4IEMgMTkuNjQ1MDEgOCAyMSA2LjY0NTAwOTYgMjEgNSBDIDIxIDMuMzU0OTkwNCAxOS42NDUwMSAyIDE4IDIgeiBNIDE4IDQgQyAxOC41NjQxMjkgNCAxOSA0LjQzNTg3MDYgMTkgNSBDIDE5IDUuNTY0MTI5NCAxOC41NjQxMjkgNiAxOCA2IEMgMTcuNDM1ODcxIDYgMTcgNS41NjQxMjk0IDE3IDUgQyAxNyA0LjQzNTg3MDYgMTcuNDM1ODcxIDQgMTggNCB6IE0gNiAxMSBDIDYuNTY0MTI5NCAxMSA3IDExLjQzNTg3MSA3IDEyIEMgNyAxMi41NjQxMjkgNi41NjQxMjk0IDEzIDYgMTMgQyA1LjQzNTg3MDYgMTMgNSAxMi41NjQxMjkgNSAxMiBDIDUgMTEuNDM1ODcxIDUuNDM1ODcwNiAxMSA2IDExIHogTSAxOCAxOCBDIDE4LjU2NDEyOSAxOCAxOSAxOC40MzU4NzEgMTkgMTkgQyAxOSAxOS41NjQxMjkgMTguNTY0MTI5IDIwIDE4IDIwIEMgMTcuNDM1ODcxIDIwIDE3IDE5LjU2NDEyOSAxNyAxOSBDIDE3IDE4LjQzNTg3MSAxNy40MzU4NzEgMTggMTggMTggeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-spreadsheet: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNENBRjUwIiBkPSJNMi4yIDIuMnYxNy42aDE3LjZWMi4ySDIuMnptMTUuNCA3LjdoLTUuNVY0LjRoNS41djUuNXpNOS45IDQuNHY1LjVINC40VjQuNGg1LjV6bS01LjUgNy43aDUuNXY1LjVINC40di01LjV6bTcuNyA1LjV2LTUuNWg1LjV2NS41aC01LjV6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-stop: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik02IDZoMTJ2MTJINnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-tab: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIxIDNIM2MtMS4xIDAtMiAuOS0yIDJ2MTRjMCAxLjEuOSAyIDIgMmgxOGMxLjEgMCAyLS45IDItMlY1YzAtMS4xLS45LTItMi0yem0wIDE2SDNWNWgxMHY0aDh2MTB6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-table-rows: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik0yMSw4SDNWNGgxOFY4eiBNMjEsMTBIM3Y0aDE4VjEweiBNMjEsMTZIM3Y0aDE4VjE2eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-tag: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjgiIGhlaWdodD0iMjgiIHZpZXdCb3g9IjAgMCA0MyAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTI4LjgzMzIgMTIuMzM0TDMyLjk5OTggMTYuNTAwN0wzNy4xNjY1IDEyLjMzNEgyOC44MzMyWiIvPgoJCTxwYXRoIGQ9Ik0xNi4yMDk1IDIxLjYxMDRDMTUuNjg3MyAyMi4xMjk5IDE0Ljg0NDMgMjIuMTI5OSAxNC4zMjQ4IDIxLjYxMDRMNi45ODI5IDE0LjcyNDVDNi41NzI0IDE0LjMzOTQgNi4wODMxMyAxMy42MDk4IDYuMDQ3ODYgMTMuMDQ4MkM1Ljk1MzQ3IDExLjUyODggNi4wMjAwMiA4LjYxOTQ0IDYuMDY2MjEgNy4wNzY5NUM2LjA4MjgxIDYuNTE0NzcgNi41NTU0OCA2LjA0MzQ3IDcuMTE4MDQgNi4wMzA1NUM5LjA4ODYzIDUuOTg0NzMgMTMuMjYzOCA1LjkzNTc5IDEzLjY1MTggNi4zMjQyNUwyMS43MzY5IDEzLjYzOUMyMi4yNTYgMTQuMTU4NSAyMS43ODUxIDE1LjQ3MjQgMjEuMjYyIDE1Ljk5NDZMMTYuMjA5NSAyMS42MTA0Wk05Ljc3NTg1IDguMjY1QzkuMzM1NTEgNy44MjU2NiA4LjYyMzUxIDcuODI1NjYgOC4xODI4IDguMjY1QzcuNzQzNDYgOC43MDU3MSA3Ljc0MzQ2IDkuNDE3MzMgOC4xODI4IDkuODU2NjdDOC42MjM4MiAxMC4yOTY0IDkuMzM1ODIgMTAuMjk2NCA5Ljc3NTg1IDkuODU2NjdDMTAuMjE1NiA5LjQxNzMzIDEwLjIxNTYgOC43MDUzMyA5Ljc3NTg1IDguMjY1WiIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-terminal: url(data:image/svg+xml;base64,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);
  --jp-icon-text-editor: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtdGV4dC1lZGl0b3ItaWNvbi1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xNSAxNUgzdjJoMTJ2LTJ6bTAtOEgzdjJoMTJWN3pNMyAxM2gxOHYtMkgzdjJ6bTAgOGgxOHYtMkgzdjJ6TTMgM3YyaDE4VjNIM3oiLz4KPC9zdmc+Cg==);
  --jp-icon-toc: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik03LDVIMjFWN0g3VjVNNywxM1YxMUgyMVYxM0g3TTQsNC41QTEuNSwxLjUgMCAwLDEgNS41LDZBMS41LDEuNSAwIDAsMSA0LDcuNUExLjUsMS41IDAgMCwxIDIuNSw2QTEuNSwxLjUgMCAwLDEgNCw0LjVNNCwxMC41QTEuNSwxLjUgMCAwLDEgNS41LDEyQTEuNSwxLjUgMCAwLDEgNCwxMy41QTEuNSwxLjUgMCAwLDEgMi41LDEyQTEuNSwxLjUgMCAwLDEgNCwxMC41TTcsMTlWMTdIMjFWMTlIN000LDE2LjVBMS41LDEuNSAwIDAsMSA1LjUsMThBMS41LDEuNSAwIDAsMSA0LDE5LjVBMS41LDEuNSAwIDAsMSAyLjUsMThBMS41LDEuNSAwIDAsMSA0LDE2LjVaIiAvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-tree-view: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik0yMiAxMVYzaC03djNIOVYzSDJ2OGg3VjhoMnYxMGg0djNoN3YtOGgtN3YzaC0yVjhoMnYzeiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDIgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMiAxNy4xODQ0IDIuOTY5NjggMTQuMzAzMiAxLjg2MDk0IDExLjQ0MDlaIi8+CiAgICA8cGF0aCBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiMzMzMzMzMiIHN0cm9rZT0iIzMzMzMzMyIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOCA5Ljg2NzE5KSIgZD0iTTIuODYwMTUgNC44NjUzNUwwLjcyNjU0OSAyLjk5OTU5TDAgMy42MzA0NUwyLjg2MDE1IDYuMTMxNTdMOCAwLjYzMDg3Mkw3LjI3ODU3IDBMMi44NjAxNSA0Ljg2NTM1WiIvPgo8L3N2Zz4K);
  --jp-icon-undo: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjUgOGMtMi42NSAwLTUuMDUuOTktNi45IDIuNkwyIDd2OWg5bC0zLjYyLTMuNjJjMS4zOS0xLjE2IDMuMTYtMS44OCA1LjEyLTEuODggMy41NCAwIDYuNTUgMi4zMSA3LjYgNS41bDIuMzctLjc4QzIxLjA4IDExLjAzIDE3LjE1IDggMTIuNSA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-user: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIHZpZXdCb3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE2IDdhNCA0IDAgMTEtOCAwIDQgNCAwIDAxOCAwek0xMiAxNGE3IDcgMCAwMC03IDdoMTRhNyA3IDAgMDAtNy03eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-users: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZlcnNpb249IjEuMSIgdmlld0JveD0iMCAwIDM2IDI0IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogPGcgY2xhc3M9ImpwLWljb24zIiB0cmFuc2Zvcm09Im1hdHJpeCgxLjczMjcgMCAwIDEuNzMyNyAtMy42MjgyIC4wOTk1NzcpIiBmaWxsPSIjNjE2MTYxIj4KICA8cGF0aCB0cmFuc2Zvcm09Im1hdHJpeCgxLjUsMCwwLDEuNSwwLC02KSIgZD0ibTEyLjE4NiA3LjUwOThjLTEuMDUzNSAwLTEuOTc1NyAwLjU2NjUtMi40Nzg1IDEuNDEwMiAwLjc1MDYxIDAuMzEyNzcgMS4zOTc0IDAuODI2NDggMS44NzMgMS40NzI3aDMuNDg2M2MwLTEuNTkyLTEuMjg4OS0yLjg4MjgtMi44ODA5LTIuODgyOHoiLz4KICA8cGF0aCBkPSJtMjAuNDY1IDIuMzg5NWEyLjE4ODUgMi4xODg1IDAgMCAxLTIuMTg4NCAyLjE4ODUgMi4xODg1IDIuMTg4NSAwIDAgMS0yLjE4ODUtMi4xODg1IDIuMTg4NSAyLjE4ODUgMCAwIDEgMi4xODg1LTIuMTg4NSAyLjE4ODUgMi4xODg1IDAgMCAxIDIuMTg4NCAyLjE4ODV6Ii8+CiAgPHBhdGggdHJhbnNmb3JtPSJtYXRyaXgoMS41LDAsMCwxLjUsMCwtNikiIGQ9Im0zLjU4OTggOC40MjE5Yy0xLjExMjYgMC0yLjAxMzcgMC45MDExMS0yLjAxMzcgMi4wMTM3aDIuODE0NWMwLjI2Nzk3LTAuMzczMDkgMC41OTA3LTAuNzA0MzUgMC45NTg5OC0wLjk3ODUyLTAuMzQ0MzMtMC42MTY4OC0xLjAwMzEtMS4wMzUyLTEuNzU5OC0xLjAzNTJ6Ii8+CiAgPHBhdGggZD0ibTYuOTE1NCA0LjYyM2ExLjUyOTQgMS41Mjk0IDAgMCAxLTEuNTI5NCAxLjUyOTQgMS41Mjk0IDEuNTI5NCAwIDAgMS0xLjUyOTQtMS41Mjk0IDEuNTI5NCAxLjUyOTQgMCAwIDEgMS41Mjk0LTEuNTI5NCAxLjUyOTQgMS41Mjk0IDAgMCAxIDEuNTI5NCAxLjUyOTR6Ii8+CiAgPHBhdGggZD0ibTYuMTM1IDEzLjUzNWMwLTMuMjM5MiAyLjYyNTktNS44NjUgNS44NjUtNS44NjUgMy4yMzkyIDAgNS44NjUgMi42MjU5IDUuODY1IDUuODY1eiIvPgogIDxjaXJjbGUgY3g9IjEyIiBjeT0iMy43Njg1IiByPSIyLjk2ODUiLz4KIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-vega: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbjEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjEyMTIxIj4KICAgIDxwYXRoIGQ9Ik0xMC42IDUuNGwyLjItMy4ySDIuMnY3LjNsNC02LjZ6Ii8+CiAgICA8cGF0aCBkPSJNMTUuOCAyLjJsLTQuNCA2LjZMNyA2LjNsLTQuOCA4djUuNWgxNy42VjIuMmgtNHptLTcgMTUuNEg1LjV2LTQuNGgzLjN2NC40em00LjQgMEg5LjhWOS44aDMuNHY3Ljh6bTQuNCAwaC0zLjRWNi41aDMuNHYxMS4xeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-word: url(data:image/svg+xml;base64,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);
  --jp-icon-yaml: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1jb250cmFzdDIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjRDgxQjYwIj4KICAgIDxwYXRoIGQ9Ik03LjIgMTguNnYtNS40TDMgNS42aDMuM2wxLjQgMy4xYy4zLjkuNiAxLjYgMSAyLjUuMy0uOC42LTEuNiAxLTIuNWwxLjQtMy4xaDMuNGwtNC40IDcuNnY1LjVsLTIuOS0uMXoiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxNi41IiByPSIyLjEiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxMSIgcj0iMi4xIi8+CiAgPC9nPgo8L3N2Zz4K);
}

/* Icon CSS class declarations */

.jp-AddAboveIcon {
  background-image: var(--jp-icon-add-above);
}

.jp-AddBelowIcon {
  background-image: var(--jp-icon-add-below);
}

.jp-AddIcon {
  background-image: var(--jp-icon-add);
}

.jp-BellIcon {
  background-image: var(--jp-icon-bell);
}

.jp-BugDotIcon {
  background-image: var(--jp-icon-bug-dot);
}

.jp-BugIcon {
  background-image: var(--jp-icon-bug);
}

.jp-BuildIcon {
  background-image: var(--jp-icon-build);
}

.jp-CaretDownEmptyIcon {
  background-image: var(--jp-icon-caret-down-empty);
}

.jp-CaretDownEmptyThinIcon {
  background-image: var(--jp-icon-caret-down-empty-thin);
}

.jp-CaretDownIcon {
  background-image: var(--jp-icon-caret-down);
}

.jp-CaretLeftIcon {
  background-image: var(--jp-icon-caret-left);
}

.jp-CaretRightIcon {
  background-image: var(--jp-icon-caret-right);
}

.jp-CaretUpEmptyThinIcon {
  background-image: var(--jp-icon-caret-up-empty-thin);
}

.jp-CaretUpIcon {
  background-image: var(--jp-icon-caret-up);
}

.jp-CaseSensitiveIcon {
  background-image: var(--jp-icon-case-sensitive);
}

.jp-CheckIcon {
  background-image: var(--jp-icon-check);
}

.jp-CircleEmptyIcon {
  background-image: var(--jp-icon-circle-empty);
}

.jp-CircleIcon {
  background-image: var(--jp-icon-circle);
}

.jp-ClearIcon {
  background-image: var(--jp-icon-clear);
}

.jp-CloseIcon {
  background-image: var(--jp-icon-close);
}

.jp-CodeCheckIcon {
  background-image: var(--jp-icon-code-check);
}

.jp-CodeIcon {
  background-image: var(--jp-icon-code);
}

.jp-CollapseAllIcon {
  background-image: var(--jp-icon-collapse-all);
}

.jp-ConsoleIcon {
  background-image: var(--jp-icon-console);
}

.jp-CopyIcon {
  background-image: var(--jp-icon-copy);
}

.jp-CopyrightIcon {
  background-image: var(--jp-icon-copyright);
}

.jp-CutIcon {
  background-image: var(--jp-icon-cut);
}

.jp-DeleteIcon {
  background-image: var(--jp-icon-delete);
}

.jp-DownloadIcon {
  background-image: var(--jp-icon-download);
}

.jp-DuplicateIcon {
  background-image: var(--jp-icon-duplicate);
}

.jp-EditIcon {
  background-image: var(--jp-icon-edit);
}

.jp-EllipsesIcon {
  background-image: var(--jp-icon-ellipses);
}

.jp-ErrorIcon {
  background-image: var(--jp-icon-error);
}

.jp-ExpandAllIcon {
  background-image: var(--jp-icon-expand-all);
}

.jp-ExtensionIcon {
  background-image: var(--jp-icon-extension);
}

.jp-FastForwardIcon {
  background-image: var(--jp-icon-fast-forward);
}

.jp-FileIcon {
  background-image: var(--jp-icon-file);
}

.jp-FileUploadIcon {
  background-image: var(--jp-icon-file-upload);
}

.jp-FilterDotIcon {
  background-image: var(--jp-icon-filter-dot);
}

.jp-FilterIcon {
  background-image: var(--jp-icon-filter);
}

.jp-FilterListIcon {
  background-image: var(--jp-icon-filter-list);
}

.jp-FolderFavoriteIcon {
  background-image: var(--jp-icon-folder-favorite);
}

.jp-FolderIcon {
  background-image: var(--jp-icon-folder);
}

.jp-HomeIcon {
  background-image: var(--jp-icon-home);
}

.jp-Html5Icon {
  background-image: var(--jp-icon-html5);
}

.jp-ImageIcon {
  background-image: var(--jp-icon-image);
}

.jp-InfoIcon {
  background-image: var(--jp-icon-info);
}

.jp-InspectorIcon {
  background-image: var(--jp-icon-inspector);
}

.jp-JsonIcon {
  background-image: var(--jp-icon-json);
}

.jp-JuliaIcon {
  background-image: var(--jp-icon-julia);
}

.jp-JupyterFaviconIcon {
  background-image: var(--jp-icon-jupyter-favicon);
}

.jp-JupyterIcon {
  background-image: var(--jp-icon-jupyter);
}

.jp-JupyterlabWordmarkIcon {
  background-image: var(--jp-icon-jupyterlab-wordmark);
}

.jp-KernelIcon {
  background-image: var(--jp-icon-kernel);
}

.jp-KeyboardIcon {
  background-image: var(--jp-icon-keyboard);
}

.jp-LaunchIcon {
  background-image: var(--jp-icon-launch);
}

.jp-LauncherIcon {
  background-image: var(--jp-icon-launcher);
}

.jp-LineFormIcon {
  background-image: var(--jp-icon-line-form);
}

.jp-LinkIcon {
  background-image: var(--jp-icon-link);
}

.jp-ListIcon {
  background-image: var(--jp-icon-list);
}

.jp-MarkdownIcon {
  background-image: var(--jp-icon-markdown);
}

.jp-MoveDownIcon {
  background-image: var(--jp-icon-move-down);
}

.jp-MoveUpIcon {
  background-image: var(--jp-icon-move-up);
}

.jp-NewFolderIcon {
  background-image: var(--jp-icon-new-folder);
}

.jp-NotTrustedIcon {
  background-image: var(--jp-icon-not-trusted);
}

.jp-NotebookIcon {
  background-image: var(--jp-icon-notebook);
}

.jp-NumberingIcon {
  background-image: var(--jp-icon-numbering);
}

.jp-OfflineBoltIcon {
  background-image: var(--jp-icon-offline-bolt);
}

.jp-PaletteIcon {
  background-image: var(--jp-icon-palette);
}

.jp-PasteIcon {
  background-image: var(--jp-icon-paste);
}

.jp-PdfIcon {
  background-image: var(--jp-icon-pdf);
}

.jp-PythonIcon {
  background-image: var(--jp-icon-python);
}

.jp-RKernelIcon {
  background-image: var(--jp-icon-r-kernel);
}

.jp-ReactIcon {
  background-image: var(--jp-icon-react);
}

.jp-RedoIcon {
  background-image: var(--jp-icon-redo);
}

.jp-RefreshIcon {
  background-image: var(--jp-icon-refresh);
}

.jp-RegexIcon {
  background-image: var(--jp-icon-regex);
}

.jp-RunIcon {
  background-image: var(--jp-icon-run);
}

.jp-RunningIcon {
  background-image: var(--jp-icon-running);
}

.jp-SaveIcon {
  background-image: var(--jp-icon-save);
}

.jp-SearchIcon {
  background-image: var(--jp-icon-search);
}

.jp-SettingsIcon {
  background-image: var(--jp-icon-settings);
}

.jp-ShareIcon {
  background-image: var(--jp-icon-share);
}

.jp-SpreadsheetIcon {
  background-image: var(--jp-icon-spreadsheet);
}

.jp-StopIcon {
  background-image: var(--jp-icon-stop);
}

.jp-TabIcon {
  background-image: var(--jp-icon-tab);
}

.jp-TableRowsIcon {
  background-image: var(--jp-icon-table-rows);
}

.jp-TagIcon {
  background-image: var(--jp-icon-tag);
}

.jp-TerminalIcon {
  background-image: var(--jp-icon-terminal);
}

.jp-TextEditorIcon {
  background-image: var(--jp-icon-text-editor);
}

.jp-TocIcon {
  background-image: var(--jp-icon-toc);
}

.jp-TreeViewIcon {
  background-image: var(--jp-icon-tree-view);
}

.jp-TrustedIcon {
  background-image: var(--jp-icon-trusted);
}

.jp-UndoIcon {
  background-image: var(--jp-icon-undo);
}

.jp-UserIcon {
  background-image: var(--jp-icon-user);
}

.jp-UsersIcon {
  background-image: var(--jp-icon-users);
}

.jp-VegaIcon {
  background-image: var(--jp-icon-vega);
}

.jp-WordIcon {
  background-image: var(--jp-icon-word);
}

.jp-YamlIcon {
  background-image: var(--jp-icon-yaml);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * (DEPRECATED) Support for consuming icons as CSS background images
 */

.jp-Icon,
.jp-MaterialIcon {
  background-position: center;
  background-repeat: no-repeat;
  background-size: 16px;
  min-width: 16px;
  min-height: 16px;
}

.jp-Icon-cover {
  background-position: center;
  background-repeat: no-repeat;
  background-size: cover;
}

/**
 * (DEPRECATED) Support for specific CSS icon sizes
 */

.jp-Icon-16 {
  background-size: 16px;
  min-width: 16px;
  min-height: 16px;
}

.jp-Icon-18 {
  background-size: 18px;
  min-width: 18px;
  min-height: 18px;
}

.jp-Icon-20 {
  background-size: 20px;
  min-width: 20px;
  min-height: 20px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.lm-TabBar .lm-TabBar-addButton {
  align-items: center;
  display: flex;
  padding: 4px;
  padding-bottom: 5px;
  margin-right: 1px;
  background-color: var(--jp-layout-color2);
}

.lm-TabBar .lm-TabBar-addButton:hover {
  background-color: var(--jp-layout-color1);
}

.lm-DockPanel-tabBar .lm-TabBar-tab {
  width: var(--jp-private-horizontal-tab-width);
}

.lm-DockPanel-tabBar .lm-TabBar-content {
  flex: unset;
}

.lm-DockPanel-tabBar[data-orientation='horizontal'] {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * Support for icons as inline SVG HTMLElements
 */

/* recolor the primary elements of an icon */
.jp-icon0[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon1[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon2[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon3[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon4[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon0[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon1[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon2[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon3[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon4[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/* recolor the accent elements of an icon */
.jp-icon-accent0[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-accent1[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-accent2[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-accent3[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-accent4[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-accent0[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-accent1[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-accent2[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-accent3[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-accent4[stroke] {
  stroke: var(--jp-layout-color4);
}

/* set the color of an icon to transparent */
.jp-icon-none[fill] {
  fill: none;
}

.jp-icon-none[stroke] {
  stroke: none;
}

/* brand icon colors. Same for light and dark */
.jp-icon-brand0[fill] {
  fill: var(--jp-brand-color0);
}

.jp-icon-brand1[fill] {
  fill: var(--jp-brand-color1);
}

.jp-icon-brand2[fill] {
  fill: var(--jp-brand-color2);
}

.jp-icon-brand3[fill] {
  fill: var(--jp-brand-color3);
}

.jp-icon-brand4[fill] {
  fill: var(--jp-brand-color4);
}

.jp-icon-brand0[stroke] {
  stroke: var(--jp-brand-color0);
}

.jp-icon-brand1[stroke] {
  stroke: var(--jp-brand-color1);
}

.jp-icon-brand2[stroke] {
  stroke: var(--jp-brand-color2);
}

.jp-icon-brand3[stroke] {
  stroke: var(--jp-brand-color3);
}

.jp-icon-brand4[stroke] {
  stroke: var(--jp-brand-color4);
}

/* warn icon colors. Same for light and dark */
.jp-icon-warn0[fill] {
  fill: var(--jp-warn-color0);
}

.jp-icon-warn1[fill] {
  fill: var(--jp-warn-color1);
}

.jp-icon-warn2[fill] {
  fill: var(--jp-warn-color2);
}

.jp-icon-warn3[fill] {
  fill: var(--jp-warn-color3);
}

.jp-icon-warn0[stroke] {
  stroke: var(--jp-warn-color0);
}

.jp-icon-warn1[stroke] {
  stroke: var(--jp-warn-color1);
}

.jp-icon-warn2[stroke] {
  stroke: var(--jp-warn-color2);
}

.jp-icon-warn3[stroke] {
  stroke: var(--jp-warn-color3);
}

/* icon colors that contrast well with each other and most backgrounds */
.jp-icon-contrast0[fill] {
  fill: var(--jp-icon-contrast-color0);
}

.jp-icon-contrast1[fill] {
  fill: var(--jp-icon-contrast-color1);
}

.jp-icon-contrast2[fill] {
  fill: var(--jp-icon-contrast-color2);
}

.jp-icon-contrast3[fill] {
  fill: var(--jp-icon-contrast-color3);
}

.jp-icon-contrast0[stroke] {
  stroke: var(--jp-icon-contrast-color0);
}

.jp-icon-contrast1[stroke] {
  stroke: var(--jp-icon-contrast-color1);
}

.jp-icon-contrast2[stroke] {
  stroke: var(--jp-icon-contrast-color2);
}

.jp-icon-contrast3[stroke] {
  stroke: var(--jp-icon-contrast-color3);
}

.jp-icon-dot[fill] {
  fill: var(--jp-warn-color0);
}

.jp-jupyter-icon-color[fill] {
  fill: var(--jp-jupyter-icon-color, var(--jp-warn-color0));
}

.jp-notebook-icon-color[fill] {
  fill: var(--jp-notebook-icon-color, var(--jp-warn-color0));
}

.jp-json-icon-color[fill] {
  fill: var(--jp-json-icon-color, var(--jp-warn-color1));
}

.jp-console-icon-color[fill] {
  fill: var(--jp-console-icon-color, white);
}

.jp-console-icon-background-color[fill] {
  fill: var(--jp-console-icon-background-color, var(--jp-brand-color1));
}

.jp-terminal-icon-color[fill] {
  fill: var(--jp-terminal-icon-color, var(--jp-layout-color2));
}

.jp-terminal-icon-background-color[fill] {
  fill: var(
    --jp-terminal-icon-background-color,
    var(--jp-inverse-layout-color2)
  );
}

.jp-text-editor-icon-color[fill] {
  fill: var(--jp-text-editor-icon-color, var(--jp-inverse-layout-color3));
}

.jp-inspector-icon-color[fill] {
  fill: var(--jp-inspector-icon-color, var(--jp-inverse-layout-color3));
}

/* CSS for icons in selected filebrowser listing items */
.jp-DirListing-item.jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}

.jp-DirListing-item.jp-mod-selected .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}

/* stylelint-disable selector-max-class, selector-max-compound-selectors */

/**
* TODO: come up with non css-hack solution for showing the busy icon on top
*  of the close icon
* CSS for complex behavior of close icon of tabs in the main area tabbar
*/
.lm-DockPanel-tabBar
  .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon3[fill] {
  fill: none;
}

.lm-DockPanel-tabBar
  .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon-busy[fill] {
  fill: var(--jp-inverse-layout-color3);
}

/* stylelint-enable selector-max-class, selector-max-compound-selectors */

/* CSS for icons in status bar */
#jp-main-statusbar .jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}

#jp-main-statusbar .jp-mod-selected .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}

/* special handling for splash icon CSS. While the theme CSS reloads during
   splash, the splash icon can loose theming. To prevent that, we set a
   default for its color variable */
:root {
  --jp-warn-color0: var(--md-orange-700);
}

/* not sure what to do with this one, used in filebrowser listing */
.jp-DragIcon {
  margin-right: 4px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * Support for alt colors for icons as inline SVG HTMLElements
 */

/* alt recolor the primary elements of an icon */
.jp-icon-alt .jp-icon0[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-alt .jp-icon1[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-alt .jp-icon2[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-alt .jp-icon3[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-alt .jp-icon4[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-alt .jp-icon0[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-alt .jp-icon1[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-alt .jp-icon2[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-alt .jp-icon3[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-alt .jp-icon4[stroke] {
  stroke: var(--jp-layout-color4);
}

/* alt recolor the accent elements of an icon */
.jp-icon-alt .jp-icon-accent0[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-alt .jp-icon-accent1[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-alt .jp-icon-accent2[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-alt .jp-icon-accent3[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-alt .jp-icon-accent4[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-alt .jp-icon-accent0[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-alt .jp-icon-accent1[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-alt .jp-icon-accent2[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-alt .jp-icon-accent3[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-alt .jp-icon-accent4[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-icon-hoverShow:not(:hover) .jp-icon-hoverShow-content {
  display: none !important;
}

/**
 * Support for hover colors for icons as inline SVG HTMLElements
 */

/**
 * regular colors
 */

/* recolor the primary elements of an icon */
.jp-icon-hover :hover .jp-icon0-hover[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-hover :hover .jp-icon1-hover[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-hover :hover .jp-icon2-hover[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-hover :hover .jp-icon3-hover[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-hover :hover .jp-icon4-hover[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-hover :hover .jp-icon0-hover[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-hover :hover .jp-icon1-hover[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-hover :hover .jp-icon2-hover[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-hover :hover .jp-icon3-hover[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-hover :hover .jp-icon4-hover[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/* recolor the accent elements of an icon */
.jp-icon-hover :hover .jp-icon-accent0-hover[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-hover :hover .jp-icon-accent1-hover[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-hover :hover .jp-icon-accent2-hover[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-hover :hover .jp-icon-accent3-hover[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-hover :hover .jp-icon-accent4-hover[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-hover :hover .jp-icon-accent0-hover[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-hover :hover .jp-icon-accent1-hover[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-hover :hover .jp-icon-accent2-hover[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-hover :hover .jp-icon-accent3-hover[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-hover :hover .jp-icon-accent4-hover[stroke] {
  stroke: var(--jp-layout-color4);
}

/* set the color of an icon to transparent */
.jp-icon-hover :hover .jp-icon-none-hover[fill] {
  fill: none;
}

.jp-icon-hover :hover .jp-icon-none-hover[stroke] {
  stroke: none;
}

/**
 * inverse colors
 */

/* inverse recolor the primary elements of an icon */
.jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[stroke] {
  stroke: var(--jp-layout-color4);
}

/* inverse recolor the accent elements of an icon */
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-IFrame {
  width: 100%;
  height: 100%;
}

.jp-IFrame > iframe {
  border: none;
}

/*
When drag events occur, `lm-mod-override-cursor` is added to the body.
Because iframes steal all cursor events, the following two rules are necessary
to suppress pointer events while resize drags are occurring. There may be a
better solution to this problem.
*/
body.lm-mod-override-cursor .jp-IFrame {
  position: relative;
}

body.lm-mod-override-cursor .jp-IFrame::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-HoverBox {
  position: fixed;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-FormGroup-content fieldset {
  border: none;
  padding: 0;
  min-width: 0;
  width: 100%;
}

/* stylelint-disable selector-max-type */

.jp-FormGroup-content fieldset .jp-inputFieldWrapper input,
.jp-FormGroup-content fieldset .jp-inputFieldWrapper select,
.jp-FormGroup-content fieldset .jp-inputFieldWrapper textarea {
  font-size: var(--jp-content-font-size2);
  border-color: var(--jp-input-border-color);
  border-style: solid;
  border-radius: var(--jp-border-radius);
  border-width: 1px;
  padding: 6px 8px;
  background: none;
  color: var(--jp-ui-font-color0);
  height: inherit;
}

.jp-FormGroup-content fieldset input[type='checkbox'] {
  position: relative;
  top: 2px;
  margin-left: 0;
}

.jp-FormGroup-content button.jp-mod-styled {
  cursor: pointer;
}

.jp-FormGroup-content .checkbox label {
  cursor: pointer;
  font-size: var(--jp-content-font-size1);
}

.jp-FormGroup-content .jp-root > fieldset > legend {
  display: none;
}

.jp-FormGroup-content .jp-root > fieldset > p {
  display: none;
}

/** copy of `input.jp-mod-styled:focus` style */
.jp-FormGroup-content fieldset input:focus,
.jp-FormGroup-content fieldset select:focus {
  -moz-outline-radius: unset;
  outline: var(--jp-border-width) solid var(--md-blue-500);
  outline-offset: -1px;
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-FormGroup-content fieldset input:hover:not(:focus),
.jp-FormGroup-content fieldset select:hover:not(:focus) {
  background-color: var(--jp-border-color2);
}

/* stylelint-enable selector-max-type */

.jp-FormGroup-content .checkbox .field-description {
  /* Disable default description field for checkbox:
   because other widgets do not have description fields,
   we add descriptions to each widget on the field level.
  */
  display: none;
}

.jp-FormGroup-content #root__description {
  display: none;
}

.jp-FormGroup-content .jp-modifiedIndicator {
  width: 5px;
  background-color: var(--jp-brand-color2);
  margin-top: 0;
  margin-left: calc(var(--jp-private-settingeditor-modifier-indent) * -1);
  flex-shrink: 0;
}

.jp-FormGroup-content .jp-modifiedIndicator.jp-errorIndicator {
  background-color: var(--jp-error-color0);
  margin-right: 0.5em;
}

/* RJSF ARRAY style */

.jp-arrayFieldWrapper legend {
  font-size: var(--jp-content-font-size2);
  color: var(--jp-ui-font-color0);
  flex-basis: 100%;
  padding: 4px 0;
  font-weight: var(--jp-content-heading-font-weight);
  border-bottom: 1px solid var(--jp-border-color2);
}

.jp-arrayFieldWrapper .field-description {
  padding: 4px 0;
  white-space: pre-wrap;
}

.jp-arrayFieldWrapper .array-item {
  width: 100%;
  border: 1px solid var(--jp-border-color2);
  border-radius: 4px;
  margin: 4px;
}

.jp-ArrayOperations {
  display: flex;
  margin-left: 8px;
}

.jp-ArrayOperationsButton {
  margin: 2px;
}

.jp-ArrayOperationsButton .jp-icon3[fill] {
  fill: var(--jp-ui-font-color0);
}

button.jp-ArrayOperationsButton.jp-mod-styled:disabled {
  cursor: not-allowed;
  opacity: 0.5;
}

/* RJSF form validation error */

.jp-FormGroup-content .validationErrors {
  color: var(--jp-error-color0);
}

/* Hide panel level error as duplicated the field level error */
.jp-FormGroup-content .panel.errors {
  display: none;
}

/* RJSF normal content (settings-editor) */

.jp-FormGroup-contentNormal {
  display: flex;
  align-items: center;
  flex-wrap: wrap;
}

.jp-FormGroup-contentNormal .jp-FormGroup-contentItem {
  margin-left: 7px;
  color: var(--jp-ui-font-color0);
}

.jp-FormGroup-contentNormal .jp-FormGroup-description {
  flex-basis: 100%;
  padding: 4px 7px;
}

.jp-FormGroup-contentNormal .jp-FormGroup-default {
  flex-basis: 100%;
  padding: 4px 7px;
}

.jp-FormGroup-contentNormal .jp-FormGroup-fieldLabel {
  font-size: var(--jp-content-font-size1);
  font-weight: normal;
  min-width: 120px;
}

.jp-FormGroup-contentNormal fieldset:not(:first-child) {
  margin-left: 7px;
}

.jp-FormGroup-contentNormal .field-array-of-string .array-item {
  /* Display `jp-ArrayOperations` buttons side-by-side with content except
    for small screens where flex-wrap will place them one below the other.
  */
  display: flex;
  align-items: center;
  flex-wrap: wrap;
}

.jp-FormGroup-contentNormal .jp-objectFieldWrapper .form-group {
  padding: 2px 8px 2px var(--jp-private-settingeditor-modifier-indent);
  margin-top: 2px;
}

/* RJSF compact content (metadata-form) */

.jp-FormGroup-content.jp-FormGroup-contentCompact {
  width: 100%;
}

.jp-FormGroup-contentCompact .form-group {
  display: flex;
  padding: 0.5em 0.2em 0.5em 0;
}

.jp-FormGroup-contentCompact
  .jp-FormGroup-compactTitle
  .jp-FormGroup-description {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color2);
}

.jp-FormGroup-contentCompact .jp-FormGroup-fieldLabel {
  padding-bottom: 0.3em;
}

.jp-FormGroup-contentCompact .jp-inputFieldWrapper .form-control {
  width: 100%;
  box-sizing: border-box;
}

.jp-FormGroup-contentCompact .jp-arrayFieldWrapper .jp-FormGroup-compactTitle {
  padding-bottom: 7px;
}

.jp-FormGroup-contentCompact
  .jp-objectFieldWrapper
  .jp-objectFieldWrapper
  .form-group {
  padding: 2px 8px 2px var(--jp-private-settingeditor-modifier-indent);
  margin-top: 2px;
}

.jp-FormGroup-contentCompact ul.error-detail {
  margin-block-start: 0.5em;
  margin-block-end: 0.5em;
  padding-inline-start: 1em;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-SidePanel {
  display: flex;
  flex-direction: column;
  min-width: var(--jp-sidebar-min-width);
  overflow-y: auto;
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);
  font-size: var(--jp-ui-font-size1);
}

.jp-SidePanel-header {
  flex: 0 0 auto;
  display: flex;
  border-bottom: var(--jp-border-width) solid var(--jp-border-color2);
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  letter-spacing: 1px;
  margin: 0;
  padding: 2px;
  text-transform: uppercase;
}

.jp-SidePanel-toolbar {
  flex: 0 0 auto;
}

.jp-SidePanel-content {
  flex: 1 1 auto;
}

.jp-SidePanel-toolbar,
.jp-AccordionPanel-toolbar {
  height: var(--jp-private-toolbar-height);
}

.jp-SidePanel-toolbar.jp-Toolbar-micro {
  display: none;
}

.lm-AccordionPanel .jp-AccordionPanel-title {
  box-sizing: border-box;
  line-height: 25px;
  margin: 0;
  display: flex;
  align-items: center;
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  font-size: var(--jp-ui-font-size0);
}

.jp-AccordionPanel-title {
  cursor: pointer;
  user-select: none;
  -moz-user-select: none;
  -webkit-user-select: none;
  text-transform: uppercase;
}

.lm-AccordionPanel[data-orientation='horizontal'] > .jp-AccordionPanel-title {
  /* Title is rotated for horizontal accordion panel using CSS */
  display: block;
  transform-origin: top left;
  transform: rotate(-90deg) translate(-100%);
}

.jp-AccordionPanel-title .lm-AccordionPanel-titleLabel {
  user-select: none;
  text-overflow: ellipsis;
  white-space: nowrap;
  overflow: hidden;
}

.jp-AccordionPanel-title .lm-AccordionPanel-titleCollapser {
  transform: rotate(-90deg);
  margin: auto 0;
  height: 16px;
}

.jp-AccordionPanel-title.lm-mod-expanded .lm-AccordionPanel-titleCollapser {
  transform: rotate(0deg);
}

.lm-AccordionPanel .jp-AccordionPanel-toolbar {
  background: none;
  box-shadow: none;
  border: none;
  margin-left: auto;
}

.lm-AccordionPanel .lm-SplitPanel-handle:hover {
  background: var(--jp-layout-color3);
}

.jp-text-truncated {
  overflow: hidden;
  text-overflow: ellipsis;
  white-space: nowrap;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Spinner {
  position: absolute;
  display: flex;
  justify-content: center;
  align-items: center;
  z-index: 10;
  left: 0;
  top: 0;
  width: 100%;
  height: 100%;
  background: var(--jp-layout-color0);
  outline: none;
}

.jp-SpinnerContent {
  font-size: 10px;
  margin: 50px auto;
  text-indent: -9999em;
  width: 3em;
  height: 3em;
  border-radius: 50%;
  background: var(--jp-brand-color3);
  background: linear-gradient(
    to right,
    #f37626 10%,
    rgba(255, 255, 255, 0) 42%
  );
  position: relative;
  animation: load3 1s infinite linear, fadeIn 1s;
}

.jp-SpinnerContent::before {
  width: 50%;
  height: 50%;
  background: #f37626;
  border-radius: 100% 0 0;
  position: absolute;
  top: 0;
  left: 0;
  content: '';
}

.jp-SpinnerContent::after {
  background: var(--jp-layout-color0);
  width: 75%;
  height: 75%;
  border-radius: 50%;
  content: '';
  margin: auto;
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  right: 0;
}

@keyframes fadeIn {
  0% {
    opacity: 0;
  }

  100% {
    opacity: 1;
  }
}

@keyframes load3 {
  0% {
    transform: rotate(0deg);
  }

  100% {
    transform: rotate(360deg);
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

button.jp-mod-styled {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  border: none;
  box-sizing: border-box;
  text-align: center;
  line-height: 32px;
  height: 32px;
  padding: 0 12px;
  letter-spacing: 0.8px;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

input.jp-mod-styled {
  background: var(--jp-input-background);
  height: 28px;
  box-sizing: border-box;
  border: var(--jp-border-width) solid var(--jp-border-color1);
  padding-left: 7px;
  padding-right: 7px;
  font-size: var(--jp-ui-font-size2);
  color: var(--jp-ui-font-color0);
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

input[type='checkbox'].jp-mod-styled {
  appearance: checkbox;
  -webkit-appearance: checkbox;
  -moz-appearance: checkbox;
  height: auto;
}

input.jp-mod-styled:focus {
  border: var(--jp-border-width) solid var(--md-blue-500);
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-select-wrapper {
  display: flex;
  position: relative;
  flex-direction: column;
  padding: 1px;
  background-color: var(--jp-layout-color1);
  box-sizing: border-box;
  margin-bottom: 12px;
}

.jp-select-wrapper:not(.multiple) {
  height: 28px;
}

.jp-select-wrapper.jp-mod-focused select.jp-mod-styled {
  border: var(--jp-border-width) solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
  background-color: var(--jp-input-active-background);
}

select.jp-mod-styled:hover {
  cursor: pointer;
  color: var(--jp-ui-font-color0);
  background-color: var(--jp-input-hover-background);
  box-shadow: inset 0 0 1px rgba(0, 0, 0, 0.5);
}

select.jp-mod-styled {
  flex: 1 1 auto;
  width: 100%;
  font-size: var(--jp-ui-font-size2);
  background: var(--jp-input-background);
  color: var(--jp-ui-font-color0);
  padding: 0 25px 0 8px;
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  border-radius: 0;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

select.jp-mod-styled:not([multiple]) {
  height: 32px;
}

select.jp-mod-styled[multiple] {
  max-height: 200px;
  overflow-y: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-switch {
  display: flex;
  align-items: center;
  padding-left: 4px;
  padding-right: 4px;
  font-size: var(--jp-ui-font-size1);
  background-color: transparent;
  color: var(--jp-ui-font-color1);
  border: none;
  height: 20px;
}

.jp-switch:hover {
  background-color: var(--jp-layout-color2);
}

.jp-switch-label {
  margin-right: 5px;
  font-family: var(--jp-ui-font-family);
}

.jp-switch-track {
  cursor: pointer;
  background-color: var(--jp-switch-color, var(--jp-border-color1));
  -webkit-transition: 0.4s;
  transition: 0.4s;
  border-radius: 34px;
  height: 16px;
  width: 35px;
  position: relative;
}

.jp-switch-track::before {
  content: '';
  position: absolute;
  height: 10px;
  width: 10px;
  margin: 3px;
  left: 0;
  background-color: var(--jp-ui-inverse-font-color1);
  -webkit-transition: 0.4s;
  transition: 0.4s;
  border-radius: 50%;
}

.jp-switch[aria-checked='true'] .jp-switch-track {
  background-color: var(--jp-switch-true-position-color, var(--jp-warn-color0));
}

.jp-switch[aria-checked='true'] .jp-switch-track::before {
  /* track width (35) - margins (3 + 3) - thumb width (10) */
  left: 19px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

:root {
  --jp-private-toolbar-height: calc(
    28px + var(--jp-border-width)
  ); /* leave 28px for content */
}

.jp-Toolbar {
  color: var(--jp-ui-font-color1);
  flex: 0 0 auto;
  display: flex;
  flex-direction: row;
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  background: var(--jp-toolbar-background);
  min-height: var(--jp-toolbar-micro-height);
  padding: 2px;
  z-index: 8;
  overflow-x: hidden;
}

/* Toolbar items */

.jp-Toolbar > .jp-Toolbar-item.jp-Toolbar-spacer {
  flex-grow: 1;
  flex-shrink: 1;
}

.jp-Toolbar-item.jp-Toolbar-kernelStatus {
  display: inline-block;
  width: 32px;
  background-repeat: no-repeat;
  background-position: center;
  background-size: 16px;
}

.jp-Toolbar > .jp-Toolbar-item {
  flex: 0 0 auto;
  display: flex;
  padding-left: 1px;
  padding-right: 1px;
  font-size: var(--jp-ui-font-size1);
  line-height: var(--jp-private-toolbar-height);
  height: 100%;
}

/* Toolbar buttons */

/* This is the div we use to wrap the react component into a Widget */
div.jp-ToolbarButton {
  color: transparent;
  border: none;
  box-sizing: border-box;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0;
  margin: 0;
}

button.jp-ToolbarButtonComponent {
  background: var(--jp-layout-color1);
  border: none;
  box-sizing: border-box;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0 6px;
  margin: 0;
  height: 24px;
  border-radius: var(--jp-border-radius);
  display: flex;
  align-items: center;
  text-align: center;
  font-size: 14px;
  min-width: unset;
  min-height: unset;
}

button.jp-ToolbarButtonComponent:disabled {
  opacity: 0.4;
}

button.jp-ToolbarButtonComponent > span {
  padding: 0;
  flex: 0 0 auto;
}

button.jp-ToolbarButtonComponent .jp-ToolbarButtonComponent-label {
  font-size: var(--jp-ui-font-size1);
  line-height: 100%;
  padding-left: 2px;
  color: var(--jp-ui-font-color1);
  font-family: var(--jp-ui-font-family);
}

#jp-main-dock-panel[data-mode='single-document']
  .jp-MainAreaWidget
  > .jp-Toolbar.jp-Toolbar-micro {
  padding: 0;
  min-height: 0;
}

#jp-main-dock-panel[data-mode='single-document']
  .jp-MainAreaWidget
  > .jp-Toolbar {
  border: none;
  box-shadow: none;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-WindowedPanel-outer {
  position: relative;
  overflow-y: auto;
}

.jp-WindowedPanel-inner {
  position: relative;
}

.jp-WindowedPanel-window {
  position: absolute;
  left: 0;
  right: 0;
  overflow: visible;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* Sibling imports */

body {
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
}

/* Disable native link decoration styles everywhere outside of dialog boxes */
a {
  text-decoration: unset;
  color: unset;
}

a:hover {
  text-decoration: unset;
  color: unset;
}

/* Accessibility for links inside dialog box text */
.jp-Dialog-content a {
  text-decoration: revert;
  color: var(--jp-content-link-color);
}

.jp-Dialog-content a:hover {
  text-decoration: revert;
}

/* Styles for ui-components */
.jp-Button {
  color: var(--jp-ui-font-color2);
  border-radius: var(--jp-border-radius);
  padding: 0 12px;
  font-size: var(--jp-ui-font-size1);

  /* Copy from blueprint 3 */
  display: inline-flex;
  flex-direction: row;
  border: none;
  cursor: pointer;
  align-items: center;
  justify-content: center;
  text-align: left;
  vertical-align: middle;
  min-height: 30px;
  min-width: 30px;
}

.jp-Button:disabled {
  cursor: not-allowed;
}

.jp-Button:empty {
  padding: 0 !important;
}

.jp-Button.jp-mod-small {
  min-height: 24px;
  min-width: 24px;
  font-size: 12px;
  padding: 0 7px;
}

/* Use our own theme for hover styles */
.jp-Button.jp-mod-minimal:hover {
  background-color: var(--jp-layout-color2);
}

.jp-Button.jp-mod-minimal {
  background: none;
}

.jp-InputGroup {
  display: block;
  position: relative;
}

.jp-InputGroup input {
  box-sizing: border-box;
  border: none;
  border-radius: 0;
  background-color: transparent;
  color: var(--jp-ui-font-color0);
  box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color);
  padding-bottom: 0;
  padding-top: 0;
  padding-left: 10px;
  padding-right: 28px;
  position: relative;
  width: 100%;
  -webkit-appearance: none;
  -moz-appearance: none;
  appearance: none;
  font-size: 14px;
  font-weight: 400;
  height: 30px;
  line-height: 30px;
  outline: none;
  vertical-align: middle;
}

.jp-InputGroup input:focus {
  box-shadow: inset 0 0 0 var(--jp-border-width)
      var(--jp-input-active-box-shadow-color),
    inset 0 0 0 3px var(--jp-input-active-box-shadow-color);
}

.jp-InputGroup input:disabled {
  cursor: not-allowed;
  resize: block;
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color2);
}

.jp-InputGroup input:disabled ~ span {
  cursor: not-allowed;
  color: var(--jp-ui-font-color2);
}

.jp-InputGroup input::placeholder,
input::placeholder {
  color: var(--jp-ui-font-color2);
}

.jp-InputGroupAction {
  position: absolute;
  bottom: 1px;
  right: 0;
  padding: 6px;
}

.jp-HTMLSelect.jp-DefaultStyle select {
  background-color: initial;
  border: none;
  border-radius: 0;
  box-shadow: none;
  color: var(--jp-ui-font-color0);
  display: block;
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  height: 24px;
  line-height: 14px;
  padding: 0 25px 0 10px;
  text-align: left;
  -moz-appearance: none;
  -webkit-appearance: none;
}

.jp-HTMLSelect.jp-DefaultStyle select:disabled {
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color2);
  cursor: not-allowed;
  resize: block;
}

.jp-HTMLSelect.jp-DefaultStyle select:disabled ~ span {
  cursor: not-allowed;
}

/* Use our own theme for hover and option styles */
/* stylelint-disable-next-line selector-max-type */
.jp-HTMLSelect.jp-DefaultStyle select:hover,
.jp-HTMLSelect.jp-DefaultStyle select > option {
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color0);
}

select {
  box-sizing: border-box;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-StatusBar-Widget {
  display: flex;
  align-items: center;
  background: var(--jp-layout-color2);
  min-height: var(--jp-statusbar-height);
  justify-content: space-between;
  padding: 0 10px;
}

.jp-StatusBar-Left {
  display: flex;
  align-items: center;
  flex-direction: row;
}

.jp-StatusBar-Middle {
  display: flex;
  align-items: center;
}

.jp-StatusBar-Right {
  display: flex;
  align-items: center;
  flex-direction: row-reverse;
}

.jp-StatusBar-Item {
  max-height: var(--jp-statusbar-height);
  margin: 0 2px;
  height: var(--jp-statusbar-height);
  white-space: nowrap;
  text-overflow: ellipsis;
  color: var(--jp-ui-font-color1);
  padding: 0 6px;
}

.jp-mod-highlighted:hover {
  background-color: var(--jp-layout-color3);
}

.jp-mod-clicked {
  background-color: var(--jp-brand-color1);
}

.jp-mod-clicked:hover {
  background-color: var(--jp-brand-color0);
}

.jp-mod-clicked .jp-StatusBar-TextItem {
  color: var(--jp-ui-inverse-font-color1);
}

.jp-StatusBar-HoverItem {
  box-shadow: '0px 4px 4px rgba(0, 0, 0, 0.25)';
}

.jp-StatusBar-TextItem {
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  line-height: 24px;
  color: var(--jp-ui-font-color1);
}

.jp-StatusBar-GroupItem {
  display: flex;
  align-items: center;
  flex-direction: row;
}

.jp-Statusbar-ProgressCircle svg {
  display: block;
  margin: 0 auto;
  width: 16px;
  height: 24px;
  align-self: normal;
}

.jp-Statusbar-ProgressCircle path {
  fill: var(--jp-inverse-layout-color3);
}

.jp-Statusbar-ProgressBar-progress-bar {
  height: 10px;
  width: 100px;
  border: solid 0.25px var(--jp-brand-color2);
  border-radius: 3px;
  overflow: hidden;
  align-self: center;
}

.jp-Statusbar-ProgressBar-progress-bar > div {
  background-color: var(--jp-brand-color2);
  background-image: linear-gradient(
    -45deg,
    rgba(255, 255, 255, 0.2) 25%,
    transparent 25%,
    transparent 50%,
    rgba(255, 255, 255, 0.2) 50%,
    rgba(255, 255, 255, 0.2) 75%,
    transparent 75%,
    transparent
  );
  background-size: 40px 40px;
  float: left;
  width: 0%;
  height: 100%;
  font-size: 12px;
  line-height: 14px;
  color: #fff;
  text-align: center;
  animation: jp-Statusbar-ExecutionTime-progress-bar 2s linear infinite;
}

.jp-Statusbar-ProgressBar-progress-bar p {
  color: var(--jp-ui-font-color1);
  font-family: var(--jp-ui-font-family);
  font-size: var(--jp-ui-font-size1);
  line-height: 10px;
  width: 100px;
}

@keyframes jp-Statusbar-ExecutionTime-progress-bar {
  0% {
    background-position: 0 0;
  }

  100% {
    background-position: 40px 40px;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-commandpalette-search-height: 28px;
}

/*-----------------------------------------------------------------------------
| Overall styles
|----------------------------------------------------------------------------*/

.lm-CommandPalette {
  padding-bottom: 0;
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);

  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
}

/*-----------------------------------------------------------------------------
| Modal variant
|----------------------------------------------------------------------------*/

.jp-ModalCommandPalette {
  position: absolute;
  z-index: 10000;
  top: 38px;
  left: 30%;
  margin: 0;
  padding: 4px;
  width: 40%;
  box-shadow: var(--jp-elevation-z4);
  border-radius: 4px;
  background: var(--jp-layout-color0);
}

.jp-ModalCommandPalette .lm-CommandPalette {
  max-height: 40vh;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-close-icon::after {
  display: none;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-CommandPalette-header {
  display: none;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-CommandPalette-item {
  margin-left: 4px;
  margin-right: 4px;
}

.jp-ModalCommandPalette
  .lm-CommandPalette
  .lm-CommandPalette-item.lm-mod-disabled {
  display: none;
}

/*-----------------------------------------------------------------------------
| Search
|----------------------------------------------------------------------------*/

.lm-CommandPalette-search {
  padding: 4px;
  background-color: var(--jp-layout-color1);
  z-index: 2;
}

.lm-CommandPalette-wrapper {
  overflow: overlay;
  padding: 0 9px;
  background-color: var(--jp-input-active-background);
  height: 30px;
  box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color);
}

.lm-CommandPalette.lm-mod-focused .lm-CommandPalette-wrapper {
  box-shadow: inset 0 0 0 1px var(--jp-input-active-box-shadow-color),
    inset 0 0 0 3px var(--jp-input-active-box-shadow-color);
}

.jp-SearchIconGroup {
  color: white;
  background-color: var(--jp-brand-color1);
  position: absolute;
  top: 4px;
  right: 4px;
  padding: 5px 5px 1px;
}

.jp-SearchIconGroup svg {
  height: 20px;
  width: 20px;
}

.jp-SearchIconGroup .jp-icon3[fill] {
  fill: var(--jp-layout-color0);
}

.lm-CommandPalette-input {
  background: transparent;
  width: calc(100% - 18px);
  float: left;
  border: none;
  outline: none;
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  line-height: var(--jp-private-commandpalette-search-height);
}

.lm-CommandPalette-input::-webkit-input-placeholder,
.lm-CommandPalette-input::-moz-placeholder,
.lm-CommandPalette-input:-ms-input-placeholder {
  color: var(--jp-ui-font-color2);
  font-size: var(--jp-ui-font-size1);
}

/*-----------------------------------------------------------------------------
| Results
|----------------------------------------------------------------------------*/

.lm-CommandPalette-header:first-child {
  margin-top: 0;
}

.lm-CommandPalette-header {
  border-bottom: solid var(--jp-border-width) var(--jp-border-color2);
  color: var(--jp-ui-font-color1);
  cursor: pointer;
  display: flex;
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  letter-spacing: 1px;
  margin-top: 8px;
  padding: 8px 0 8px 12px;
  text-transform: uppercase;
}

.lm-CommandPalette-header.lm-mod-active {
  background: var(--jp-layout-color2);
}

.lm-CommandPalette-header > mark {
  background-color: transparent;
  font-weight: bold;
  color: var(--jp-ui-font-color1);
}

.lm-CommandPalette-item {
  padding: 4px 12px 4px 4px;
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  font-weight: 400;
  display: flex;
}

.lm-CommandPalette-item.lm-mod-disabled {
  color: var(--jp-ui-font-color2);
}

.lm-CommandPalette-item.lm-mod-active {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.lm-CommandPalette-item.lm-mod-active .lm-CommandPalette-itemLabel > mark {
  color: var(--jp-ui-inverse-font-color0);
}

.lm-CommandPalette-item.lm-mod-active .jp-icon-selectable[fill] {
  fill: var(--jp-layout-color0);
}

.lm-CommandPalette-item.lm-mod-active:hover:not(.lm-mod-disabled) {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.lm-CommandPalette-item:hover:not(.lm-mod-active):not(.lm-mod-disabled) {
  background: var(--jp-layout-color2);
}

.lm-CommandPalette-itemContent {
  overflow: hidden;
}

.lm-CommandPalette-itemLabel > mark {
  color: var(--jp-ui-font-color0);
  background-color: transparent;
  font-weight: bold;
}

.lm-CommandPalette-item.lm-mod-disabled mark {
  color: var(--jp-ui-font-color2);
}

.lm-CommandPalette-item .lm-CommandPalette-itemIcon {
  margin: 0 4px 0 0;
  position: relative;
  width: 16px;
  top: 2px;
  flex: 0 0 auto;
}

.lm-CommandPalette-item.lm-mod-disabled .lm-CommandPalette-itemIcon {
  opacity: 0.6;
}

.lm-CommandPalette-item .lm-CommandPalette-itemShortcut {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemCaption {
  display: none;
}

.lm-CommandPalette-content {
  background-color: var(--jp-layout-color1);
}

.lm-CommandPalette-content:empty::after {
  content: 'No results';
  margin: auto;
  margin-top: 20px;
  width: 100px;
  display: block;
  font-size: var(--jp-ui-font-size2);
  font-family: var(--jp-ui-font-family);
  font-weight: lighter;
}

.lm-CommandPalette-emptyMessage {
  text-align: center;
  margin-top: 24px;
  line-height: 1.32;
  padding: 0 8px;
  color: var(--jp-content-font-color3);
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Dialog {
  position: absolute;
  z-index: 10000;
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  top: 0;
  left: 0;
  margin: 0;
  padding: 0;
  width: 100%;
  height: 100%;
  background: var(--jp-dialog-background);
}

.jp-Dialog-content {
  display: flex;
  flex-direction: column;
  margin-left: auto;
  margin-right: auto;
  background: var(--jp-layout-color1);
  padding: 24px 24px 12px;
  min-width: 300px;
  min-height: 150px;
  max-width: 1000px;
  max-height: 500px;
  box-sizing: border-box;
  box-shadow: var(--jp-elevation-z20);
  word-wrap: break-word;
  border-radius: var(--jp-border-radius);

  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color1);
  resize: both;
}

.jp-Dialog-content.jp-Dialog-content-small {
  max-width: 500px;
}

.jp-Dialog-button {
  overflow: visible;
}

button.jp-Dialog-button:focus {
  outline: 1px solid var(--jp-brand-color1);
  outline-offset: 4px;
  -moz-outline-radius: 0;
}

button.jp-Dialog-button:focus::-moz-focus-inner {
  border: 0;
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-accept:focus,
button.jp-Dialog-button.jp-mod-styled.jp-mod-warn:focus,
button.jp-Dialog-button.jp-mod-styled.jp-mod-reject:focus {
  outline-offset: 4px;
  -moz-outline-radius: 0;
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-accept:focus {
  outline: 1px solid var(--jp-accept-color-normal, var(--jp-brand-color1));
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-warn:focus {
  outline: 1px solid var(--jp-warn-color-normal, var(--jp-error-color1));
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-reject:focus {
  outline: 1px solid var(--jp-reject-color-normal, var(--md-grey-600));
}

button.jp-Dialog-close-button {
  padding: 0;
  height: 100%;
  min-width: unset;
  min-height: unset;
}

.jp-Dialog-header {
  display: flex;
  justify-content: space-between;
  flex: 0 0 auto;
  padding-bottom: 12px;
  font-size: var(--jp-ui-font-size3);
  font-weight: 400;
  color: var(--jp-ui-font-color1);
}

.jp-Dialog-body {
  display: flex;
  flex-direction: column;
  flex: 1 1 auto;
  font-size: var(--jp-ui-font-size1);
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  overflow: auto;
}

.jp-Dialog-footer {
  display: flex;
  flex-direction: row;
  justify-content: flex-end;
  align-items: center;
  flex: 0 0 auto;
  margin-left: -12px;
  margin-right: -12px;
  padding: 12px;
}

.jp-Dialog-checkbox {
  padding-right: 5px;
}

.jp-Dialog-checkbox > input:focus-visible {
  outline: 1px solid var(--jp-input-active-border-color);
  outline-offset: 1px;
}

.jp-Dialog-spacer {
  flex: 1 1 auto;
}

.jp-Dialog-title {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.jp-Dialog-body > .jp-select-wrapper {
  width: 100%;
}

.jp-Dialog-body > button {
  padding: 0 16px;
}

.jp-Dialog-body > label {
  line-height: 1.4;
  color: var(--jp-ui-font-color0);
}

.jp-Dialog-button.jp-mod-styled:not(:last-child) {
  margin-right: 12px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-Input-Boolean-Dialog {
  flex-direction: row-reverse;
  align-items: end;
  width: 100%;
}

.jp-Input-Boolean-Dialog > label {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-MainAreaWidget > :focus {
  outline: none;
}

.jp-MainAreaWidget .jp-MainAreaWidget-error {
  padding: 6px;
}

.jp-MainAreaWidget .jp-MainAreaWidget-error > pre {
  width: auto;
  padding: 10px;
  background: var(--jp-error-color3);
  border: var(--jp-border-width) solid var(--jp-error-color1);
  border-radius: var(--jp-border-radius);
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  white-space: pre-wrap;
  word-wrap: break-word;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/**
 * google-material-color v1.2.6
 * https://github.com/danlevan/google-material-color
 */
:root {
  --md-red-50: #ffebee;
  --md-red-100: #ffcdd2;
  --md-red-200: #ef9a9a;
  --md-red-300: #e57373;
  --md-red-400: #ef5350;
  --md-red-500: #f44336;
  --md-red-600: #e53935;
  --md-red-700: #d32f2f;
  --md-red-800: #c62828;
  --md-red-900: #b71c1c;
  --md-red-A100: #ff8a80;
  --md-red-A200: #ff5252;
  --md-red-A400: #ff1744;
  --md-red-A700: #d50000;
  --md-pink-50: #fce4ec;
  --md-pink-100: #f8bbd0;
  --md-pink-200: #f48fb1;
  --md-pink-300: #f06292;
  --md-pink-400: #ec407a;
  --md-pink-500: #e91e63;
  --md-pink-600: #d81b60;
  --md-pink-700: #c2185b;
  --md-pink-800: #ad1457;
  --md-pink-900: #880e4f;
  --md-pink-A100: #ff80ab;
  --md-pink-A200: #ff4081;
  --md-pink-A400: #f50057;
  --md-pink-A700: #c51162;
  --md-purple-50: #f3e5f5;
  --md-purple-100: #e1bee7;
  --md-purple-200: #ce93d8;
  --md-purple-300: #ba68c8;
  --md-purple-400: #ab47bc;
  --md-purple-500: #9c27b0;
  --md-purple-600: #8e24aa;
  --md-purple-700: #7b1fa2;
  --md-purple-800: #6a1b9a;
  --md-purple-900: #4a148c;
  --md-purple-A100: #ea80fc;
  --md-purple-A200: #e040fb;
  --md-purple-A400: #d500f9;
  --md-purple-A700: #a0f;
  --md-deep-purple-50: #ede7f6;
  --md-deep-purple-100: #d1c4e9;
  --md-deep-purple-200: #b39ddb;
  --md-deep-purple-300: #9575cd;
  --md-deep-purple-400: #7e57c2;
  --md-deep-purple-500: #673ab7;
  --md-deep-purple-600: #5e35b1;
  --md-deep-purple-700: #512da8;
  --md-deep-purple-800: #4527a0;
  --md-deep-purple-900: #311b92;
  --md-deep-purple-A100: #b388ff;
  --md-deep-purple-A200: #7c4dff;
  --md-deep-purple-A400: #651fff;
  --md-deep-purple-A700: #6200ea;
  --md-indigo-50: #e8eaf6;
  --md-indigo-100: #c5cae9;
  --md-indigo-200: #9fa8da;
  --md-indigo-300: #7986cb;
  --md-indigo-400: #5c6bc0;
  --md-indigo-500: #3f51b5;
  --md-indigo-600: #3949ab;
  --md-indigo-700: #303f9f;
  --md-indigo-800: #283593;
  --md-indigo-900: #1a237e;
  --md-indigo-A100: #8c9eff;
  --md-indigo-A200: #536dfe;
  --md-indigo-A400: #3d5afe;
  --md-indigo-A700: #304ffe;
  --md-blue-50: #e3f2fd;
  --md-blue-100: #bbdefb;
  --md-blue-200: #90caf9;
  --md-blue-300: #64b5f6;
  --md-blue-400: #42a5f5;
  --md-blue-500: #2196f3;
  --md-blue-600: #1e88e5;
  --md-blue-700: #1976d2;
  --md-blue-800: #1565c0;
  --md-blue-900: #0d47a1;
  --md-blue-A100: #82b1ff;
  --md-blue-A200: #448aff;
  --md-blue-A400: #2979ff;
  --md-blue-A700: #2962ff;
  --md-light-blue-50: #e1f5fe;
  --md-light-blue-100: #b3e5fc;
  --md-light-blue-200: #81d4fa;
  --md-light-blue-300: #4fc3f7;
  --md-light-blue-400: #29b6f6;
  --md-light-blue-500: #03a9f4;
  --md-light-blue-600: #039be5;
  --md-light-blue-700: #0288d1;
  --md-light-blue-800: #0277bd;
  --md-light-blue-900: #01579b;
  --md-light-blue-A100: #80d8ff;
  --md-light-blue-A200: #40c4ff;
  --md-light-blue-A400: #00b0ff;
  --md-light-blue-A700: #0091ea;
  --md-cyan-50: #e0f7fa;
  --md-cyan-100: #b2ebf2;
  --md-cyan-200: #80deea;
  --md-cyan-300: #4dd0e1;
  --md-cyan-400: #26c6da;
  --md-cyan-500: #00bcd4;
  --md-cyan-600: #00acc1;
  --md-cyan-700: #0097a7;
  --md-cyan-800: #00838f;
  --md-cyan-900: #006064;
  --md-cyan-A100: #84ffff;
  --md-cyan-A200: #18ffff;
  --md-cyan-A400: #00e5ff;
  --md-cyan-A700: #00b8d4;
  --md-teal-50: #e0f2f1;
  --md-teal-100: #b2dfdb;
  --md-teal-200: #80cbc4;
  --md-teal-300: #4db6ac;
  --md-teal-400: #26a69a;
  --md-teal-500: #009688;
  --md-teal-600: #00897b;
  --md-teal-700: #00796b;
  --md-teal-800: #00695c;
  --md-teal-900: #004d40;
  --md-teal-A100: #a7ffeb;
  --md-teal-A200: #64ffda;
  --md-teal-A400: #1de9b6;
  --md-teal-A700: #00bfa5;
  --md-green-50: #e8f5e9;
  --md-green-100: #c8e6c9;
  --md-green-200: #a5d6a7;
  --md-green-300: #81c784;
  --md-green-400: #66bb6a;
  --md-green-500: #4caf50;
  --md-green-600: #43a047;
  --md-green-700: #388e3c;
  --md-green-800: #2e7d32;
  --md-green-900: #1b5e20;
  --md-green-A100: #b9f6ca;
  --md-green-A200: #69f0ae;
  --md-green-A400: #00e676;
  --md-green-A700: #00c853;
  --md-light-green-50: #f1f8e9;
  --md-light-green-100: #dcedc8;
  --md-light-green-200: #c5e1a5;
  --md-light-green-300: #aed581;
  --md-light-green-400: #9ccc65;
  --md-light-green-500: #8bc34a;
  --md-light-green-600: #7cb342;
  --md-light-green-700: #689f38;
  --md-light-green-800: #558b2f;
  --md-light-green-900: #33691e;
  --md-light-green-A100: #ccff90;
  --md-light-green-A200: #b2ff59;
  --md-light-green-A400: #76ff03;
  --md-light-green-A700: #64dd17;
  --md-lime-50: #f9fbe7;
  --md-lime-100: #f0f4c3;
  --md-lime-200: #e6ee9c;
  --md-lime-300: #dce775;
  --md-lime-400: #d4e157;
  --md-lime-500: #cddc39;
  --md-lime-600: #c0ca33;
  --md-lime-700: #afb42b;
  --md-lime-800: #9e9d24;
  --md-lime-900: #827717;
  --md-lime-A100: #f4ff81;
  --md-lime-A200: #eeff41;
  --md-lime-A400: #c6ff00;
  --md-lime-A700: #aeea00;
  --md-yellow-50: #fffde7;
  --md-yellow-100: #fff9c4;
  --md-yellow-200: #fff59d;
  --md-yellow-300: #fff176;
  --md-yellow-400: #ffee58;
  --md-yellow-500: #ffeb3b;
  --md-yellow-600: #fdd835;
  --md-yellow-700: #fbc02d;
  --md-yellow-800: #f9a825;
  --md-yellow-900: #f57f17;
  --md-yellow-A100: #ffff8d;
  --md-yellow-A200: #ff0;
  --md-yellow-A400: #ffea00;
  --md-yellow-A700: #ffd600;
  --md-amber-50: #fff8e1;
  --md-amber-100: #ffecb3;
  --md-amber-200: #ffe082;
  --md-amber-300: #ffd54f;
  --md-amber-400: #ffca28;
  --md-amber-500: #ffc107;
  --md-amber-600: #ffb300;
  --md-amber-700: #ffa000;
  --md-amber-800: #ff8f00;
  --md-amber-900: #ff6f00;
  --md-amber-A100: #ffe57f;
  --md-amber-A200: #ffd740;
  --md-amber-A400: #ffc400;
  --md-amber-A700: #ffab00;
  --md-orange-50: #fff3e0;
  --md-orange-100: #ffe0b2;
  --md-orange-200: #ffcc80;
  --md-orange-300: #ffb74d;
  --md-orange-400: #ffa726;
  --md-orange-500: #ff9800;
  --md-orange-600: #fb8c00;
  --md-orange-700: #f57c00;
  --md-orange-800: #ef6c00;
  --md-orange-900: #e65100;
  --md-orange-A100: #ffd180;
  --md-orange-A200: #ffab40;
  --md-orange-A400: #ff9100;
  --md-orange-A700: #ff6d00;
  --md-deep-orange-50: #fbe9e7;
  --md-deep-orange-100: #ffccbc;
  --md-deep-orange-200: #ffab91;
  --md-deep-orange-300: #ff8a65;
  --md-deep-orange-400: #ff7043;
  --md-deep-orange-500: #ff5722;
  --md-deep-orange-600: #f4511e;
  --md-deep-orange-700: #e64a19;
  --md-deep-orange-800: #d84315;
  --md-deep-orange-900: #bf360c;
  --md-deep-orange-A100: #ff9e80;
  --md-deep-orange-A200: #ff6e40;
  --md-deep-orange-A400: #ff3d00;
  --md-deep-orange-A700: #dd2c00;
  --md-brown-50: #efebe9;
  --md-brown-100: #d7ccc8;
  --md-brown-200: #bcaaa4;
  --md-brown-300: #a1887f;
  --md-brown-400: #8d6e63;
  --md-brown-500: #795548;
  --md-brown-600: #6d4c41;
  --md-brown-700: #5d4037;
  --md-brown-800: #4e342e;
  --md-brown-900: #3e2723;
  --md-grey-50: #fafafa;
  --md-grey-100: #f5f5f5;
  --md-grey-200: #eee;
  --md-grey-300: #e0e0e0;
  --md-grey-400: #bdbdbd;
  --md-grey-500: #9e9e9e;
  --md-grey-600: #757575;
  --md-grey-700: #616161;
  --md-grey-800: #424242;
  --md-grey-900: #212121;
  --md-blue-grey-50: #eceff1;
  --md-blue-grey-100: #cfd8dc;
  --md-blue-grey-200: #b0bec5;
  --md-blue-grey-300: #90a4ae;
  --md-blue-grey-400: #78909c;
  --md-blue-grey-500: #607d8b;
  --md-blue-grey-600: #546e7a;
  --md-blue-grey-700: #455a64;
  --md-blue-grey-800: #37474f;
  --md-blue-grey-900: #263238;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| RenderedText
|----------------------------------------------------------------------------*/

:root {
  /* This is the padding value to fill the gaps between lines containing spans with background color. */
  --jp-private-code-span-padding: calc(
    (var(--jp-code-line-height) - 1) * var(--jp-code-font-size) / 2
  );
}

.jp-RenderedText {
  text-align: left;
  padding-left: var(--jp-code-padding);
  line-height: var(--jp-code-line-height);
  font-family: var(--jp-code-font-family);
}

.jp-RenderedText pre,
.jp-RenderedJavaScript pre,
.jp-RenderedHTMLCommon pre {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-code-font-size);
  border: none;
  margin: 0;
  padding: 0;
}

.jp-RenderedText pre a:link {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

.jp-RenderedText pre a:hover {
  text-decoration: underline;
  color: var(--jp-content-link-color);
}

.jp-RenderedText pre a:visited {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

/* console foregrounds and backgrounds */
.jp-RenderedText pre .ansi-black-fg {
  color: #3e424d;
}

.jp-RenderedText pre .ansi-red-fg {
  color: #e75c58;
}

.jp-RenderedText pre .ansi-green-fg {
  color: #00a250;
}

.jp-RenderedText pre .ansi-yellow-fg {
  color: #ddb62b;
}

.jp-RenderedText pre .ansi-blue-fg {
  color: #208ffb;
}

.jp-RenderedText pre .ansi-magenta-fg {
  color: #d160c4;
}

.jp-RenderedText pre .ansi-cyan-fg {
  color: #60c6c8;
}

.jp-RenderedText pre .ansi-white-fg {
  color: #c5c1b4;
}

.jp-RenderedText pre .ansi-black-bg {
  background-color: #3e424d;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-red-bg {
  background-color: #e75c58;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-green-bg {
  background-color: #00a250;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-yellow-bg {
  background-color: #ddb62b;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-blue-bg {
  background-color: #208ffb;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-magenta-bg {
  background-color: #d160c4;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-cyan-bg {
  background-color: #60c6c8;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-white-bg {
  background-color: #c5c1b4;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-black-intense-fg {
  color: #282c36;
}

.jp-RenderedText pre .ansi-red-intense-fg {
  color: #b22b31;
}

.jp-RenderedText pre .ansi-green-intense-fg {
  color: #007427;
}

.jp-RenderedText pre .ansi-yellow-intense-fg {
  color: #b27d12;
}

.jp-RenderedText pre .ansi-blue-intense-fg {
  color: #0065ca;
}

.jp-RenderedText pre .ansi-magenta-intense-fg {
  color: #a03196;
}

.jp-RenderedText pre .ansi-cyan-intense-fg {
  color: #258f8f;
}

.jp-RenderedText pre .ansi-white-intense-fg {
  color: #a1a6b2;
}

.jp-RenderedText pre .ansi-black-intense-bg {
  background-color: #282c36;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-red-intense-bg {
  background-color: #b22b31;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-green-intense-bg {
  background-color: #007427;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-yellow-intense-bg {
  background-color: #b27d12;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-blue-intense-bg {
  background-color: #0065ca;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-magenta-intense-bg {
  background-color: #a03196;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-cyan-intense-bg {
  background-color: #258f8f;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-white-intense-bg {
  background-color: #a1a6b2;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-default-inverse-fg {
  color: var(--jp-ui-inverse-font-color0);
}

.jp-RenderedText pre .ansi-default-inverse-bg {
  background-color: var(--jp-inverse-layout-color0);
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-bold {
  font-weight: bold;
}

.jp-RenderedText pre .ansi-underline {
  text-decoration: underline;
}

.jp-RenderedText[data-mime-type='application/vnd.jupyter.stderr'] {
  background: var(--jp-rendermime-error-background);
  padding-top: var(--jp-code-padding);
}

/*-----------------------------------------------------------------------------
| RenderedLatex
|----------------------------------------------------------------------------*/

.jp-RenderedLatex {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-content-font-size1);
  line-height: var(--jp-content-line-height);
}

/* Left-justify outputs.*/
.jp-OutputArea-output.jp-RenderedLatex {
  padding: var(--jp-code-padding);
  text-align: left;
}

/*-----------------------------------------------------------------------------
| RenderedHTML
|----------------------------------------------------------------------------*/

.jp-RenderedHTMLCommon {
  color: var(--jp-content-font-color1);
  font-family: var(--jp-content-font-family);
  font-size: var(--jp-content-font-size1);
  line-height: var(--jp-content-line-height);

  /* Give a bit more R padding on Markdown text to keep line lengths reasonable */
  padding-right: 20px;
}

.jp-RenderedHTMLCommon em {
  font-style: italic;
}

.jp-RenderedHTMLCommon strong {
  font-weight: bold;
}

.jp-RenderedHTMLCommon u {
  text-decoration: underline;
}

.jp-RenderedHTMLCommon a:link {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

.jp-RenderedHTMLCommon a:hover {
  text-decoration: underline;
  color: var(--jp-content-link-color);
}

.jp-RenderedHTMLCommon a:visited {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

/* Headings */

.jp-RenderedHTMLCommon h1,
.jp-RenderedHTMLCommon h2,
.jp-RenderedHTMLCommon h3,
.jp-RenderedHTMLCommon h4,
.jp-RenderedHTMLCommon h5,
.jp-RenderedHTMLCommon h6 {
  line-height: var(--jp-content-heading-line-height);
  font-weight: var(--jp-content-heading-font-weight);
  font-style: normal;
  margin: var(--jp-content-heading-margin-top) 0
    var(--jp-content-heading-margin-bottom) 0;
}

.jp-RenderedHTMLCommon h1:first-child,
.jp-RenderedHTMLCommon h2:first-child,
.jp-RenderedHTMLCommon h3:first-child,
.jp-RenderedHTMLCommon h4:first-child,
.jp-RenderedHTMLCommon h5:first-child,
.jp-RenderedHTMLCommon h6:first-child {
  margin-top: calc(0.5 * var(--jp-content-heading-margin-top));
}

.jp-RenderedHTMLCommon h1:last-child,
.jp-RenderedHTMLCommon h2:last-child,
.jp-RenderedHTMLCommon h3:last-child,
.jp-RenderedHTMLCommon h4:last-child,
.jp-RenderedHTMLCommon h5:last-child,
.jp-RenderedHTMLCommon h6:last-child {
  margin-bottom: calc(0.5 * var(--jp-content-heading-margin-bottom));
}

.jp-RenderedHTMLCommon h1 {
  font-size: var(--jp-content-font-size5);
}

.jp-RenderedHTMLCommon h2 {
  font-size: var(--jp-content-font-size4);
}

.jp-RenderedHTMLCommon h3 {
  font-size: var(--jp-content-font-size3);
}

.jp-RenderedHTMLCommon h4 {
  font-size: var(--jp-content-font-size2);
}

.jp-RenderedHTMLCommon h5 {
  font-size: var(--jp-content-font-size1);
}

.jp-RenderedHTMLCommon h6 {
  font-size: var(--jp-content-font-size0);
}

/* Lists */

/* stylelint-disable selector-max-type, selector-max-compound-selectors */

.jp-RenderedHTMLCommon ul:not(.list-inline),
.jp-RenderedHTMLCommon ol:not(.list-inline) {
  padding-left: 2em;
}

.jp-RenderedHTMLCommon ul {
  list-style: disc;
}

.jp-RenderedHTMLCommon ul ul {
  list-style: square;
}

.jp-RenderedHTMLCommon ul ul ul {
  list-style: circle;
}

.jp-RenderedHTMLCommon ol {
  list-style: decimal;
}

.jp-RenderedHTMLCommon ol ol {
  list-style: upper-alpha;
}

.jp-RenderedHTMLCommon ol ol ol {
  list-style: lower-alpha;
}

.jp-RenderedHTMLCommon ol ol ol ol {
  list-style: lower-roman;
}

.jp-RenderedHTMLCommon ol ol ol ol ol {
  list-style: decimal;
}

.jp-RenderedHTMLCommon ol,
.jp-RenderedHTMLCommon ul {
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon ul ul,
.jp-RenderedHTMLCommon ul ol,
.jp-RenderedHTMLCommon ol ul,
.jp-RenderedHTMLCommon ol ol {
  margin-bottom: 0;
}

/* stylelint-enable selector-max-type, selector-max-compound-selectors */

.jp-RenderedHTMLCommon hr {
  color: var(--jp-border-color2);
  background-color: var(--jp-border-color1);
  margin-top: 1em;
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon > pre {
  margin: 1.5em 2em;
}

.jp-RenderedHTMLCommon pre,
.jp-RenderedHTMLCommon code {
  border: 0;
  background-color: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
  font-family: var(--jp-code-font-family);
  font-size: inherit;
  line-height: var(--jp-code-line-height);
  padding: 0;
  white-space: pre-wrap;
}

.jp-RenderedHTMLCommon :not(pre) > code {
  background-color: var(--jp-layout-color2);
  padding: 1px 5px;
}

/* Tables */

.jp-RenderedHTMLCommon table {
  border-collapse: collapse;
  border-spacing: 0;
  border: none;
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  table-layout: fixed;
  margin-left: auto;
  margin-bottom: 1em;
  margin-right: auto;
}

.jp-RenderedHTMLCommon thead {
  border-bottom: var(--jp-border-width) solid var(--jp-border-color1);
  vertical-align: bottom;
}

.jp-RenderedHTMLCommon td,
.jp-RenderedHTMLCommon th,
.jp-RenderedHTMLCommon tr {
  vertical-align: middle;
  padding: 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}

.jp-RenderedMarkdown.jp-RenderedHTMLCommon td,
.jp-RenderedMarkdown.jp-RenderedHTMLCommon th {
  max-width: none;
}

:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon td,
:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon th,
:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon tr {
  text-align: right;
}

.jp-RenderedHTMLCommon th {
  font-weight: bold;
}

.jp-RenderedHTMLCommon tbody tr:nth-child(odd) {
  background: var(--jp-layout-color0);
}

.jp-RenderedHTMLCommon tbody tr:nth-child(even) {
  background: var(--jp-rendermime-table-row-background);
}

.jp-RenderedHTMLCommon tbody tr:hover {
  background: var(--jp-rendermime-table-row-hover-background);
}

.jp-RenderedHTMLCommon p {
  text-align: left;
  margin: 0;
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon img {
  -moz-force-broken-image-icon: 1;
}

/* Restrict to direct children as other images could be nested in other content. */
.jp-RenderedHTMLCommon > img {
  display: block;
  margin-left: 0;
  margin-right: 0;
  margin-bottom: 1em;
}

/* Change color behind transparent images if they need it... */
[data-jp-theme-light='false'] .jp-RenderedImage img.jp-needs-light-background {
  background-color: var(--jp-inverse-layout-color1);
}

[data-jp-theme-light='true'] .jp-RenderedImage img.jp-needs-dark-background {
  background-color: var(--jp-inverse-layout-color1);
}

.jp-RenderedHTMLCommon img,
.jp-RenderedImage img,
.jp-RenderedHTMLCommon svg,
.jp-RenderedSVG svg {
  max-width: 100%;
  height: auto;
}

.jp-RenderedHTMLCommon img.jp-mod-unconfined,
.jp-RenderedImage img.jp-mod-unconfined,
.jp-RenderedHTMLCommon svg.jp-mod-unconfined,
.jp-RenderedSVG svg.jp-mod-unconfined {
  max-width: none;
}

.jp-RenderedHTMLCommon .alert {
  padding: var(--jp-notebook-padding);
  border: var(--jp-border-width) solid transparent;
  border-radius: var(--jp-border-radius);
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon .alert-info {
  color: var(--jp-info-color0);
  background-color: var(--jp-info-color3);
  border-color: var(--jp-info-color2);
}

.jp-RenderedHTMLCommon .alert-info hr {
  border-color: var(--jp-info-color3);
}

.jp-RenderedHTMLCommon .alert-info > p:last-child,
.jp-RenderedHTMLCommon .alert-info > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-warning {
  color: var(--jp-warn-color0);
  background-color: var(--jp-warn-color3);
  border-color: var(--jp-warn-color2);
}

.jp-RenderedHTMLCommon .alert-warning hr {
  border-color: var(--jp-warn-color3);
}

.jp-RenderedHTMLCommon .alert-warning > p:last-child,
.jp-RenderedHTMLCommon .alert-warning > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-success {
  color: var(--jp-success-color0);
  background-color: var(--jp-success-color3);
  border-color: var(--jp-success-color2);
}

.jp-RenderedHTMLCommon .alert-success hr {
  border-color: var(--jp-success-color3);
}

.jp-RenderedHTMLCommon .alert-success > p:last-child,
.jp-RenderedHTMLCommon .alert-success > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-danger {
  color: var(--jp-error-color0);
  background-color: var(--jp-error-color3);
  border-color: var(--jp-error-color2);
}

.jp-RenderedHTMLCommon .alert-danger hr {
  border-color: var(--jp-error-color3);
}

.jp-RenderedHTMLCommon .alert-danger > p:last-child,
.jp-RenderedHTMLCommon .alert-danger > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon blockquote {
  margin: 1em 2em;
  padding: 0 1em;
  border-left: 5px solid var(--jp-border-color2);
}

a.jp-InternalAnchorLink {
  visibility: hidden;
  margin-left: 8px;
  color: var(--md-blue-800);
}

h1:hover .jp-InternalAnchorLink,
h2:hover .jp-InternalAnchorLink,
h3:hover .jp-InternalAnchorLink,
h4:hover .jp-InternalAnchorLink,
h5:hover .jp-InternalAnchorLink,
h6:hover .jp-InternalAnchorLink {
  visibility: visible;
}

.jp-RenderedHTMLCommon kbd {
  background-color: var(--jp-rendermime-table-row-background);
  border: 1px solid var(--jp-border-color0);
  border-bottom-color: var(--jp-border-color2);
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.25);
  display: inline-block;
  font-size: var(--jp-ui-font-size0);
  line-height: 1em;
  padding: 0.2em 0.5em;
}

/* Most direct children of .jp-RenderedHTMLCommon have a margin-bottom of 1.0.
 * At the bottom of cells this is a bit too much as there is also spacing
 * between cells. Going all the way to 0 gets too tight between markdown and
 * code cells.
 */
.jp-RenderedHTMLCommon > *:last-child {
  margin-bottom: 0.5em;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-cursor-backdrop {
  position: fixed;
  width: 200px;
  height: 200px;
  margin-top: -100px;
  margin-left: -100px;
  will-change: transform;
  z-index: 100;
}

.lm-mod-drag-image {
  will-change: transform;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-lineFormSearch {
  padding: 4px 12px;
  background-color: var(--jp-layout-color2);
  box-shadow: var(--jp-toolbar-box-shadow);
  z-index: 2;
  font-size: var(--jp-ui-font-size1);
}

.jp-lineFormCaption {
  font-size: var(--jp-ui-font-size0);
  line-height: var(--jp-ui-font-size1);
  margin-top: 4px;
  color: var(--jp-ui-font-color0);
}

.jp-baseLineForm {
  border: none;
  border-radius: 0;
  position: absolute;
  background-size: 16px;
  background-repeat: no-repeat;
  background-position: center;
  outline: none;
}

.jp-lineFormButtonContainer {
  top: 4px;
  right: 8px;
  height: 24px;
  padding: 0 12px;
  width: 12px;
}

.jp-lineFormButtonIcon {
  top: 0;
  right: 0;
  background-color: var(--jp-brand-color1);
  height: 100%;
  width: 100%;
  box-sizing: border-box;
  padding: 4px 6px;
}

.jp-lineFormButton {
  top: 0;
  right: 0;
  background-color: transparent;
  height: 100%;
  width: 100%;
  box-sizing: border-box;
}

.jp-lineFormWrapper {
  overflow: hidden;
  padding: 0 8px;
  border: 1px solid var(--jp-border-color0);
  background-color: var(--jp-input-active-background);
  height: 22px;
}

.jp-lineFormWrapperFocusWithin {
  border: var(--jp-border-width) solid var(--md-blue-500);
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-lineFormInput {
  background: transparent;
  width: 200px;
  height: 100%;
  border: none;
  outline: none;
  color: var(--jp-ui-font-color0);
  line-height: 28px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-JSONEditor {
  display: flex;
  flex-direction: column;
  width: 100%;
}

.jp-JSONEditor-host {
  flex: 1 1 auto;
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  border-radius: 0;
  background: var(--jp-layout-color0);
  min-height: 50px;
  padding: 1px;
}

.jp-JSONEditor.jp-mod-error .jp-JSONEditor-host {
  border-color: red;
  outline-color: red;
}

.jp-JSONEditor-header {
  display: flex;
  flex: 1 0 auto;
  padding: 0 0 0 12px;
}

.jp-JSONEditor-header label {
  flex: 0 0 auto;
}

.jp-JSONEditor-commitButton {
  height: 16px;
  width: 16px;
  background-size: 18px;
  background-repeat: no-repeat;
  background-position: center;
}

.jp-JSONEditor-host.jp-mod-focused {
  background-color: var(--jp-input-active-background);
  border: 1px solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
}

.jp-Editor.jp-mod-dropTarget {
  border: var(--jp-border-width) solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/
.jp-DocumentSearch-input {
  border: none;
  outline: none;
  color: var(--jp-ui-font-color0);
  font-size: var(--jp-ui-font-size1);
  background-color: var(--jp-layout-color0);
  font-family: var(--jp-ui-font-family);
  padding: 2px 1px;
  resize: none;
}

.jp-DocumentSearch-overlay {
  position: absolute;
  background-color: var(--jp-toolbar-background);
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  border-left: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  top: 0;
  right: 0;
  z-index: 7;
  min-width: 405px;
  padding: 2px;
  font-size: var(--jp-ui-font-size1);

  --jp-private-document-search-button-height: 20px;
}

.jp-DocumentSearch-overlay button {
  background-color: var(--jp-toolbar-background);
  outline: 0;
}

.jp-DocumentSearch-overlay button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-overlay button:active {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-overlay-row {
  display: flex;
  align-items: center;
  margin-bottom: 2px;
}

.jp-DocumentSearch-button-content {
  display: inline-block;
  cursor: pointer;
  box-sizing: border-box;
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-button-content svg {
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-input-wrapper {
  border: var(--jp-border-width) solid var(--jp-border-color0);
  display: flex;
  background-color: var(--jp-layout-color0);
  margin: 2px;
}

.jp-DocumentSearch-input-wrapper:focus-within {
  border-color: var(--jp-cell-editor-active-border-color);
}

.jp-DocumentSearch-toggle-wrapper,
.jp-DocumentSearch-button-wrapper {
  all: initial;
  overflow: hidden;
  display: inline-block;
  border: none;
  box-sizing: border-box;
}

.jp-DocumentSearch-toggle-wrapper {
  width: 14px;
  height: 14px;
}

.jp-DocumentSearch-button-wrapper {
  width: var(--jp-private-document-search-button-height);
  height: var(--jp-private-document-search-button-height);
}

.jp-DocumentSearch-toggle-wrapper:focus,
.jp-DocumentSearch-button-wrapper:focus {
  outline: var(--jp-border-width) solid
    var(--jp-cell-editor-active-border-color);
  outline-offset: -1px;
}

.jp-DocumentSearch-toggle-wrapper,
.jp-DocumentSearch-button-wrapper,
.jp-DocumentSearch-button-content:focus {
  outline: none;
}

.jp-DocumentSearch-toggle-placeholder {
  width: 5px;
}

.jp-DocumentSearch-input-button::before {
  display: block;
  padding-top: 100%;
}

.jp-DocumentSearch-input-button-off {
  opacity: var(--jp-search-toggle-off-opacity);
}

.jp-DocumentSearch-input-button-off:hover {
  opacity: var(--jp-search-toggle-hover-opacity);
}

.jp-DocumentSearch-input-button-on {
  opacity: var(--jp-search-toggle-on-opacity);
}

.jp-DocumentSearch-index-counter {
  padding-left: 10px;
  padding-right: 10px;
  user-select: none;
  min-width: 35px;
  display: inline-block;
}

.jp-DocumentSearch-up-down-wrapper {
  display: inline-block;
  padding-right: 2px;
  margin-left: auto;
  white-space: nowrap;
}

.jp-DocumentSearch-spacer {
  margin-left: auto;
}

.jp-DocumentSearch-up-down-wrapper button {
  outline: 0;
  border: none;
  width: var(--jp-private-document-search-button-height);
  height: var(--jp-private-document-search-button-height);
  vertical-align: middle;
  margin: 1px 5px 2px;
}

.jp-DocumentSearch-up-down-button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-up-down-button:active {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-filter-button {
  border-radius: var(--jp-border-radius);
}

.jp-DocumentSearch-filter-button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-filter-button-enabled {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-filter-button-enabled:hover {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-search-options {
  padding: 0 8px;
  margin-left: 3px;
  width: 100%;
  display: grid;
  justify-content: start;
  grid-template-columns: 1fr 1fr;
  align-items: center;
  justify-items: stretch;
}

.jp-DocumentSearch-search-filter-disabled {
  color: var(--jp-ui-font-color2);
}

.jp-DocumentSearch-search-filter {
  display: flex;
  align-items: center;
  user-select: none;
}

.jp-DocumentSearch-regex-error {
  color: var(--jp-error-color0);
}

.jp-DocumentSearch-replace-button-wrapper {
  overflow: hidden;
  display: inline-block;
  box-sizing: border-box;
  border: var(--jp-border-width) solid var(--jp-border-color0);
  margin: auto 2px;
  padding: 1px 4px;
  height: calc(var(--jp-private-document-search-button-height) + 2px);
}

.jp-DocumentSearch-replace-button-wrapper:focus {
  border: var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);
}

.jp-DocumentSearch-replace-button {
  display: inline-block;
  text-align: center;
  cursor: pointer;
  box-sizing: border-box;
  color: var(--jp-ui-font-color1);

  /* height - 2 * (padding of wrapper) */
  line-height: calc(var(--jp-private-document-search-button-height) - 2px);
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-replace-button:focus {
  outline: none;
}

.jp-DocumentSearch-replace-wrapper-class {
  margin-left: 14px;
  display: flex;
}

.jp-DocumentSearch-replace-toggle {
  border: none;
  background-color: var(--jp-toolbar-background);
  border-radius: var(--jp-border-radius);
}

.jp-DocumentSearch-replace-toggle:hover {
  background-color: var(--jp-layout-color2);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.cm-editor {
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  font-family: var(--jp-code-font-family);
  border: 0;
  border-radius: 0;
  height: auto;

  /* Changed to auto to autogrow */
}

.cm-editor pre {
  padding: 0 var(--jp-code-padding);
}

.jp-CodeMirrorEditor[data-type='inline'] .cm-dialog {
  background-color: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
}

.jp-CodeMirrorEditor {
  cursor: text;
}

/* When zoomed out 67% and 33% on a screen of 1440 width x 900 height */
@media screen and (min-width: 2138px) and (max-width: 4319px) {
  .jp-CodeMirrorEditor[data-type='inline'] .cm-cursor {
    border-left: var(--jp-code-cursor-width1) solid
      var(--jp-editor-cursor-color);
  }
}

/* When zoomed out less than 33% */
@media screen and (min-width: 4320px) {
  .jp-CodeMirrorEditor[data-type='inline'] .cm-cursor {
    border-left: var(--jp-code-cursor-width2) solid
      var(--jp-editor-cursor-color);
  }
}

.cm-editor.jp-mod-readOnly .cm-cursor {
  display: none;
}

.jp-CollaboratorCursor {
  border-left: 5px solid transparent;
  border-right: 5px solid transparent;
  border-top: none;
  border-bottom: 3px solid;
  background-clip: content-box;
  margin-left: -5px;
  margin-right: -5px;
}

.cm-searching,
.cm-searching span {
  /* `.cm-searching span`: we need to override syntax highlighting */
  background-color: var(--jp-search-unselected-match-background-color);
  color: var(--jp-search-unselected-match-color);
}

.cm-searching::selection,
.cm-searching span::selection {
  background-color: var(--jp-search-unselected-match-background-color);
  color: var(--jp-search-unselected-match-color);
}

.jp-current-match > .cm-searching,
.jp-current-match > .cm-searching span,
.cm-searching > .jp-current-match,
.cm-searching > .jp-current-match span {
  background-color: var(--jp-search-selected-match-background-color);
  color: var(--jp-search-selected-match-color);
}

.jp-current-match > .cm-searching::selection,
.cm-searching > .jp-current-match::selection,
.jp-current-match > .cm-searching span::selection {
  background-color: var(--jp-search-selected-match-background-color);
  color: var(--jp-search-selected-match-color);
}

.cm-trailingspace {
  background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAYAAAB4ka1VAAAAsElEQVQIHQGlAFr/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA7+r3zKmT0/+pk9P/7+r3zAAAAAAAAAAABAAAAAAAAAAA6OPzM+/q9wAAAAAA6OPzMwAAAAAAAAAAAgAAAAAAAAAAGR8NiRQaCgAZIA0AGR8NiQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQyoYJ/SY80UAAAAASUVORK5CYII=);
  background-position: center left;
  background-repeat: repeat-x;
}

.jp-CollaboratorCursor-hover {
  position: absolute;
  z-index: 1;
  transform: translateX(-50%);
  color: white;
  border-radius: 3px;
  padding-left: 4px;
  padding-right: 4px;
  padding-top: 1px;
  padding-bottom: 1px;
  text-align: center;
  font-size: var(--jp-ui-font-size1);
  white-space: nowrap;
}

.jp-CodeMirror-ruler {
  border-left: 1px dashed var(--jp-border-color2);
}

/* Styles for shared cursors (remote cursor locations and selected ranges) */
.jp-CodeMirrorEditor .cm-ySelectionCaret {
  position: relative;
  border-left: 1px solid black;
  margin-left: -1px;
  margin-right: -1px;
  box-sizing: border-box;
}

.jp-CodeMirrorEditor .cm-ySelectionCaret > .cm-ySelectionInfo {
  white-space: nowrap;
  position: absolute;
  top: -1.15em;
  padding-bottom: 0.05em;
  left: -1px;
  font-size: 0.95em;
  font-family: var(--jp-ui-font-family);
  font-weight: bold;
  line-height: normal;
  user-select: none;
  color: white;
  padding-left: 2px;
  padding-right: 2px;
  z-index: 101;
  transition: opacity 0.3s ease-in-out;
}

.jp-CodeMirrorEditor .cm-ySelectionInfo {
  transition-delay: 0.7s;
  opacity: 0;
}

.jp-CodeMirrorEditor .cm-ySelectionCaret:hover > .cm-ySelectionInfo {
  opacity: 1;
  transition-delay: 0s;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-MimeDocument {
  outline: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-filebrowser-button-height: 28px;
  --jp-private-filebrowser-button-width: 48px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-FileBrowser .jp-SidePanel-content {
  display: flex;
  flex-direction: column;
}

.jp-FileBrowser-toolbar.jp-Toolbar {
  flex-wrap: wrap;
  row-gap: 12px;
  border-bottom: none;
  height: auto;
  margin: 8px 12px 0;
  box-shadow: none;
  padding: 0;
  justify-content: flex-start;
}

.jp-FileBrowser-Panel {
  flex: 1 1 auto;
  display: flex;
  flex-direction: column;
}

.jp-BreadCrumbs {
  flex: 0 0 auto;
  margin: 8px 12px;
}

.jp-BreadCrumbs-item {
  margin: 0 2px;
  padding: 0 2px;
  border-radius: var(--jp-border-radius);
  cursor: pointer;
}

.jp-BreadCrumbs-item:hover {
  background-color: var(--jp-layout-color2);
}

.jp-BreadCrumbs-item:first-child {
  margin-left: 0;
}

.jp-BreadCrumbs-item.jp-mod-dropTarget {
  background-color: var(--jp-brand-color2);
  opacity: 0.7;
}

/*-----------------------------------------------------------------------------
| Buttons
|----------------------------------------------------------------------------*/

.jp-FileBrowser-toolbar > .jp-Toolbar-item {
  flex: 0 0 auto;
  padding-left: 0;
  padding-right: 2px;
  align-items: center;
  height: unset;
}

.jp-FileBrowser-toolbar > .jp-Toolbar-item .jp-ToolbarButtonComponent {
  width: 40px;
}

/*-----------------------------------------------------------------------------
| Other styles
|----------------------------------------------------------------------------*/

.jp-FileDialog.jp-mod-conflict input {
  color: var(--jp-error-color1);
}

.jp-FileDialog .jp-new-name-title {
  margin-top: 12px;
}

.jp-LastModified-hidden {
  display: none;
}

.jp-FileSize-hidden {
  display: none;
}

.jp-FileBrowser .lm-AccordionPanel > h3:first-child {
  display: none;
}

/*-----------------------------------------------------------------------------
| DirListing
|----------------------------------------------------------------------------*/

.jp-DirListing {
  flex: 1 1 auto;
  display: flex;
  flex-direction: column;
  outline: 0;
}

.jp-DirListing-header {
  flex: 0 0 auto;
  display: flex;
  flex-direction: row;
  align-items: center;
  overflow: hidden;
  border-top: var(--jp-border-width) solid var(--jp-border-color2);
  border-bottom: var(--jp-border-width) solid var(--jp-border-color1);
  box-shadow: var(--jp-toolbar-box-shadow);
  z-index: 2;
}

.jp-DirListing-headerItem {
  padding: 4px 12px 2px;
  font-weight: 500;
}

.jp-DirListing-headerItem:hover {
  background: var(--jp-layout-color2);
}

.jp-DirListing-headerItem.jp-id-name {
  flex: 1 0 84px;
}

.jp-DirListing-headerItem.jp-id-modified {
  flex: 0 0 112px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
}

.jp-DirListing-headerItem.jp-id-filesize {
  flex: 0 0 75px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
}

.jp-id-narrow {
  display: none;
  flex: 0 0 5px;
  padding: 4px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
  color: var(--jp-border-color2);
}

.jp-DirListing-narrow .jp-id-narrow {
  display: block;
}

.jp-DirListing-narrow .jp-id-modified,
.jp-DirListing-narrow .jp-DirListing-itemModified {
  display: none;
}

.jp-DirListing-headerItem.jp-mod-selected {
  font-weight: 600;
}

/* increase specificity to override bundled default */
.jp-DirListing-content {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  list-style-type: none;
  overflow: auto;
  background-color: var(--jp-layout-color1);
}

.jp-DirListing-content mark {
  color: var(--jp-ui-font-color0);
  background-color: transparent;
  font-weight: bold;
}

.jp-DirListing-content .jp-DirListing-item.jp-mod-selected mark {
  color: var(--jp-ui-inverse-font-color0);
}

/* Style the directory listing content when a user drops a file to upload */
.jp-DirListing.jp-mod-native-drop .jp-DirListing-content {
  outline: 5px dashed rgba(128, 128, 128, 0.5);
  outline-offset: -10px;
  cursor: copy;
}

.jp-DirListing-item {
  display: flex;
  flex-direction: row;
  align-items: center;
  padding: 4px 12px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-DirListing-checkboxWrapper {
  /* Increases hit area of checkbox. */
  padding: 4px;
}

.jp-DirListing-header
  .jp-DirListing-checkboxWrapper
  + .jp-DirListing-headerItem {
  padding-left: 4px;
}

.jp-DirListing-content .jp-DirListing-checkboxWrapper {
  position: relative;
  left: -4px;
  margin: -4px 0 -4px -8px;
}

.jp-DirListing-checkboxWrapper.jp-mod-visible {
  visibility: visible;
}

/* For devices that support hovering, hide checkboxes until hovered, selected...
*/
@media (hover: hover) {
  .jp-DirListing-checkboxWrapper {
    visibility: hidden;
  }

  .jp-DirListing-item:hover .jp-DirListing-checkboxWrapper,
  .jp-DirListing-item.jp-mod-selected .jp-DirListing-checkboxWrapper {
    visibility: visible;
  }
}

.jp-DirListing-item[data-is-dot] {
  opacity: 75%;
}

.jp-DirListing-item.jp-mod-selected {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.jp-DirListing-item.jp-mod-dropTarget {
  background: var(--jp-brand-color3);
}

.jp-DirListing-item:hover:not(.jp-mod-selected) {
  background: var(--jp-layout-color2);
}

.jp-DirListing-itemIcon {
  flex: 0 0 20px;
  margin-right: 4px;
}

.jp-DirListing-itemText {
  flex: 1 0 64px;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  user-select: none;
}

.jp-DirListing-itemText:focus {
  outline-width: 2px;
  outline-color: var(--jp-inverse-layout-color1);
  outline-style: solid;
  outline-offset: 1px;
}

.jp-DirListing-item.jp-mod-selected .jp-DirListing-itemText:focus {
  outline-color: var(--jp-layout-color1);
}

.jp-DirListing-itemModified {
  flex: 0 0 125px;
  text-align: right;
}

.jp-DirListing-itemFileSize {
  flex: 0 0 90px;
  text-align: right;
}

.jp-DirListing-editor {
  flex: 1 0 64px;
  outline: none;
  border: none;
  color: var(--jp-ui-font-color1);
  background-color: var(--jp-layout-color1);
}

.jp-DirListing-item.jp-mod-running .jp-DirListing-itemIcon::before {
  color: var(--jp-success-color1);
  content: '\25CF';
  font-size: 8px;
  position: absolute;
  left: -8px;
}

.jp-DirListing-item.jp-mod-running.jp-mod-selected
  .jp-DirListing-itemIcon::before {
  color: var(--jp-ui-inverse-font-color1);
}

.jp-DirListing-item.lm-mod-drag-image,
.jp-DirListing-item.jp-mod-selected.lm-mod-drag-image {
  font-size: var(--jp-ui-font-size1);
  padding-left: 4px;
  margin-left: 4px;
  width: 160px;
  background-color: var(--jp-ui-inverse-font-color2);
  box-shadow: var(--jp-elevation-z2);
  border-radius: 0;
  color: var(--jp-ui-font-color1);
  transform: translateX(-40%) translateY(-58%);
}

.jp-Document {
  min-width: 120px;
  min-height: 120px;
  outline: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Main OutputArea
| OutputArea has a list of Outputs
|----------------------------------------------------------------------------*/

.jp-OutputArea {
  overflow-y: auto;
}

.jp-OutputArea-child {
  display: table;
  table-layout: fixed;
  width: 100%;
  overflow: hidden;
}

.jp-OutputPrompt {
  width: var(--jp-cell-prompt-width);
  color: var(--jp-cell-outprompt-font-color);
  font-family: var(--jp-cell-prompt-font-family);
  padding: var(--jp-code-padding);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
  opacity: var(--jp-cell-prompt-opacity);

  /* Right align prompt text, don't wrap to handle large prompt numbers */
  text-align: right;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;

  /* Disable text selection */
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-OutputArea-prompt {
  display: table-cell;
  vertical-align: top;
}

.jp-OutputArea-output {
  display: table-cell;
  width: 100%;
  height: auto;
  overflow: auto;
  user-select: text;
  -moz-user-select: text;
  -webkit-user-select: text;
  -ms-user-select: text;
}

.jp-OutputArea .jp-RenderedText {
  padding-left: 1ch;
}

/**
 * Prompt overlay.
 */

.jp-OutputArea-promptOverlay {
  position: absolute;
  top: 0;
  width: var(--jp-cell-prompt-width);
  height: 100%;
  opacity: 0.5;
}

.jp-OutputArea-promptOverlay:hover {
  background: var(--jp-layout-color2);
  box-shadow: inset 0 0 1px var(--jp-inverse-layout-color0);
  cursor: zoom-out;
}

.jp-mod-outputsScrolled .jp-OutputArea-promptOverlay:hover {
  cursor: zoom-in;
}

/**
 * Isolated output.
 */
.jp-OutputArea-output.jp-mod-isolated {
  width: 100%;
  display: block;
}

/*
When drag events occur, `lm-mod-override-cursor` is added to the body.
Because iframes steal all cursor events, the following two rules are necessary
to suppress pointer events while resize drags are occurring. There may be a
better solution to this problem.
*/
body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated {
  position: relative;
}

body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: transparent;
}

/* pre */

.jp-OutputArea-output pre {
  border: none;
  margin: 0;
  padding: 0;
  overflow-x: auto;
  overflow-y: auto;
  word-break: break-all;
  word-wrap: break-word;
  white-space: pre-wrap;
}

/* tables */

.jp-OutputArea-output.jp-RenderedHTMLCommon table {
  margin-left: 0;
  margin-right: 0;
}

/* description lists */

.jp-OutputArea-output dl,
.jp-OutputArea-output dt,
.jp-OutputArea-output dd {
  display: block;
}

.jp-OutputArea-output dl {
  width: 100%;
  overflow: hidden;
  padding: 0;
  margin: 0;
}

.jp-OutputArea-output dt {
  font-weight: bold;
  float: left;
  width: 20%;
  padding: 0;
  margin: 0;
}

.jp-OutputArea-output dd {
  float: left;
  width: 80%;
  padding: 0;
  margin: 0;
}

.jp-TrimmedOutputs pre {
  background: var(--jp-layout-color3);
  font-size: calc(var(--jp-code-font-size) * 1.4);
  text-align: center;
  text-transform: uppercase;
}

/* Hide the gutter in case of
 *  - nested output areas (e.g. in the case of output widgets)
 *  - mirrored output areas
 */
.jp-OutputArea .jp-OutputArea .jp-OutputArea-prompt {
  display: none;
}

/* Hide empty lines in the output area, for instance due to cleared widgets */
.jp-OutputArea-prompt:empty {
  padding: 0;
  border: 0;
}

/*-----------------------------------------------------------------------------
| executeResult is added to any Output-result for the display of the object
| returned by a cell
|----------------------------------------------------------------------------*/

.jp-OutputArea-output.jp-OutputArea-executeResult {
  margin-left: 0;
  width: 100%;
}

/* Text output with the Out[] prompt needs a top padding to match the
 * alignment of the Out[] prompt itself.
 */
.jp-OutputArea-executeResult .jp-RenderedText.jp-OutputArea-output {
  padding-top: var(--jp-code-padding);
  border-top: var(--jp-border-width) solid transparent;
}

/*-----------------------------------------------------------------------------
| The Stdin output
|----------------------------------------------------------------------------*/

.jp-Stdin-prompt {
  color: var(--jp-content-font-color0);
  padding-right: var(--jp-code-padding);
  vertical-align: baseline;
  flex: 0 0 auto;
}

.jp-Stdin-input {
  font-family: var(--jp-code-font-family);
  font-size: inherit;
  color: inherit;
  background-color: inherit;
  width: 42%;
  min-width: 200px;

  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;

  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0 0.25em;
  margin: 0 0.25em;
  flex: 0 0 70%;
}

.jp-Stdin-input::placeholder {
  opacity: 0;
}

.jp-Stdin-input:focus {
  box-shadow: none;
}

.jp-Stdin-input:focus::placeholder {
  opacity: 1;
}

/*-----------------------------------------------------------------------------
| Output Area View
|----------------------------------------------------------------------------*/

.jp-LinkedOutputView .jp-OutputArea {
  height: 100%;
  display: block;
}

.jp-LinkedOutputView .jp-OutputArea-output:only-child {
  height: 100%;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

@media print {
  .jp-OutputArea-child {
    break-inside: avoid-page;
  }
}

/*-----------------------------------------------------------------------------
| Mobile
|----------------------------------------------------------------------------*/
@media only screen and (max-width: 760px) {
  .jp-OutputPrompt {
    display: table-row;
    text-align: left;
  }

  .jp-OutputArea-child .jp-OutputArea-output {
    display: table-row;
    margin-left: var(--jp-notebook-padding);
  }
}

/* Trimmed outputs warning */
.jp-TrimmedOutputs > a {
  margin: 10px;
  text-decoration: none;
  cursor: pointer;
}

.jp-TrimmedOutputs > a:hover {
  text-decoration: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Table of Contents
|----------------------------------------------------------------------------*/

:root {
  --jp-private-toc-active-width: 4px;
}

.jp-TableOfContents {
  display: flex;
  flex-direction: column;
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  height: 100%;
}

.jp-TableOfContents-placeholder {
  text-align: center;
}

.jp-TableOfContents-placeholderContent {
  color: var(--jp-content-font-color2);
  padding: 8px;
}

.jp-TableOfContents-placeholderContent > h3 {
  margin-bottom: var(--jp-content-heading-margin-bottom);
}

.jp-TableOfContents .jp-SidePanel-content {
  overflow-y: auto;
}

.jp-TableOfContents-tree {
  margin: 4px;
}

.jp-TableOfContents ol {
  list-style-type: none;
}

/* stylelint-disable-next-line selector-max-type */
.jp-TableOfContents li > ol {
  /* Align left border with triangle icon center */
  padding-left: 11px;
}

.jp-TableOfContents-content {
  /* left margin for the active heading indicator */
  margin: 0 0 0 var(--jp-private-toc-active-width);
  padding: 0;
  background-color: var(--jp-layout-color1);
}

.jp-tocItem {
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-tocItem-heading {
  display: flex;
  cursor: pointer;
}

.jp-tocItem-heading:hover {
  background-color: var(--jp-layout-color2);
}

.jp-tocItem-content {
  display: block;
  padding: 4px 0;
  white-space: nowrap;
  text-overflow: ellipsis;
  overflow-x: hidden;
}

.jp-tocItem-collapser {
  height: 20px;
  margin: 2px 2px 0;
  padding: 0;
  background: none;
  border: none;
  cursor: pointer;
}

.jp-tocItem-collapser:hover {
  background-color: var(--jp-layout-color3);
}

/* Active heading indicator */

.jp-tocItem-heading::before {
  content: ' ';
  background: transparent;
  width: var(--jp-private-toc-active-width);
  height: 24px;
  position: absolute;
  left: 0;
  border-radius: var(--jp-border-radius);
}

.jp-tocItem-heading.jp-tocItem-active::before {
  background-color: var(--jp-brand-color1);
}

.jp-tocItem-heading:hover.jp-tocItem-active::before {
  background: var(--jp-brand-color0);
  opacity: 1;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Collapser {
  flex: 0 0 var(--jp-cell-collapser-width);
  padding: 0;
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
  border-radius: var(--jp-border-radius);
  opacity: 1;
}

.jp-Collapser-child {
  display: block;
  width: 100%;
  box-sizing: border-box;

  /* height: 100% doesn't work because the height of its parent is computed from content */
  position: absolute;
  top: 0;
  bottom: 0;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

/*
Hiding collapsers in print mode.

Note: input and output wrappers have "display: block" propery in print mode.
*/

@media print {
  .jp-Collapser {
    display: none;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Header/Footer
|----------------------------------------------------------------------------*/

/* Hidden by zero height by default */
.jp-CellHeader,
.jp-CellFooter {
  height: 0;
  width: 100%;
  padding: 0;
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Input
|----------------------------------------------------------------------------*/

/* All input areas */
.jp-InputArea {
  display: table;
  table-layout: fixed;
  width: 100%;
  overflow: hidden;
}

.jp-InputArea-editor {
  display: table-cell;
  overflow: hidden;
  vertical-align: top;

  /* This is the non-active, default styling */
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  border-radius: 0;
  background: var(--jp-cell-editor-background);
}

.jp-InputPrompt {
  display: table-cell;
  vertical-align: top;
  width: var(--jp-cell-prompt-width);
  color: var(--jp-cell-inprompt-font-color);
  font-family: var(--jp-cell-prompt-font-family);
  padding: var(--jp-code-padding);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  opacity: var(--jp-cell-prompt-opacity);
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;

  /* Right align prompt text, don't wrap to handle large prompt numbers */
  text-align: right;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;

  /* Disable text selection */
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

/*-----------------------------------------------------------------------------
| Mobile
|----------------------------------------------------------------------------*/
@media only screen and (max-width: 760px) {
  .jp-InputArea-editor {
    display: table-row;
    margin-left: var(--jp-notebook-padding);
  }

  .jp-InputPrompt {
    display: table-row;
    text-align: left;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Placeholder
|----------------------------------------------------------------------------*/

.jp-Placeholder {
  display: table;
  table-layout: fixed;
  width: 100%;
}

.jp-Placeholder-prompt {
  display: table-cell;
  box-sizing: border-box;
}

.jp-Placeholder-content {
  display: table-cell;
  padding: 4px 6px;
  border: 1px solid transparent;
  border-radius: 0;
  background: none;
  box-sizing: border-box;
  cursor: pointer;
}

.jp-Placeholder-contentContainer {
  display: flex;
}

.jp-Placeholder-content:hover,
.jp-InputPlaceholder > .jp-Placeholder-content:hover {
  border-color: var(--jp-layout-color3);
}

.jp-Placeholder-content .jp-MoreHorizIcon {
  width: 32px;
  height: 16px;
  border: 1px solid transparent;
  border-radius: var(--jp-border-radius);
}

.jp-Placeholder-content .jp-MoreHorizIcon:hover {
  border: 1px solid var(--jp-border-color1);
  box-shadow: 0 0 2px 0 rgba(0, 0, 0, 0.25);
  background-color: var(--jp-layout-color0);
}

.jp-PlaceholderText {
  white-space: nowrap;
  overflow-x: hidden;
  color: var(--jp-inverse-layout-color3);
  font-family: var(--jp-code-font-family);
}

.jp-InputPlaceholder > .jp-Placeholder-content {
  border-color: var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Private CSS variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-cell-scrolling-output-offset: 5px;
}

/*-----------------------------------------------------------------------------
| Cell
|----------------------------------------------------------------------------*/

.jp-Cell {
  padding: var(--jp-cell-padding);
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Common input/output
|----------------------------------------------------------------------------*/

.jp-Cell-inputWrapper,
.jp-Cell-outputWrapper {
  display: flex;
  flex-direction: row;
  padding: 0;
  margin: 0;

  /* Added to reveal the box-shadow on the input and output collapsers. */
  overflow: visible;
}

/* Only input/output areas inside cells */
.jp-Cell-inputArea,
.jp-Cell-outputArea {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Collapser
|----------------------------------------------------------------------------*/

/* Make the output collapser disappear when there is not output, but do so
 * in a manner that leaves it in the layout and preserves its width.
 */
.jp-Cell.jp-mod-noOutputs .jp-Cell-outputCollapser {
  border: none !important;
  background: transparent !important;
}

.jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputCollapser {
  min-height: var(--jp-cell-collapser-min-height);
}

/*-----------------------------------------------------------------------------
| Output
|----------------------------------------------------------------------------*/

/* Put a space between input and output when there IS output */
.jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputWrapper {
  margin-top: 5px;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea {
  overflow-y: auto;
  max-height: 24em;
  margin-left: var(--jp-private-cell-scrolling-output-offset);
  resize: vertical;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea[style*='height'] {
  max-height: unset;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea::after {
  content: ' ';
  box-shadow: inset 0 0 6px 2px rgb(0 0 0 / 30%);
  width: 100%;
  height: 100%;
  position: sticky;
  bottom: 0;
  top: 0;
  margin-top: -50%;
  float: left;
  display: block;
  pointer-events: none;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-child {
  padding-top: 6px;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-prompt {
  width: calc(
    var(--jp-cell-prompt-width) - var(--jp-private-cell-scrolling-output-offset)
  );
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-promptOverlay {
  left: calc(-1 * var(--jp-private-cell-scrolling-output-offset));
}

/*-----------------------------------------------------------------------------
| CodeCell
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| MarkdownCell
|----------------------------------------------------------------------------*/

.jp-MarkdownOutput {
  display: table-cell;
  width: 100%;
  margin-top: 0;
  margin-bottom: 0;
  padding-left: var(--jp-code-padding);
}

.jp-MarkdownOutput.jp-RenderedHTMLCommon {
  overflow: auto;
}

/* collapseHeadingButton (show always if hiddenCellsButton is _not_ shown) */
.jp-collapseHeadingButton {
  display: flex;
  min-height: var(--jp-cell-collapser-min-height);
  font-size: var(--jp-code-font-size);
  position: absolute;
  background-color: transparent;
  background-size: 25px;
  background-repeat: no-repeat;
  background-position-x: center;
  background-position-y: top;
  background-image: var(--jp-icon-caret-down);
  right: 0;
  top: 0;
  bottom: 0;
}

.jp-collapseHeadingButton.jp-mod-collapsed {
  background-image: var(--jp-icon-caret-right);
}

/*
 set the container font size to match that of content
 so that the nested collapse buttons have the right size
*/
.jp-MarkdownCell .jp-InputPrompt {
  font-size: var(--jp-content-font-size1);
}

/*
  Align collapseHeadingButton with cell top header
  The font sizes are identical to the ones in packages/rendermime/style/base.css
*/
.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='1'] {
  font-size: var(--jp-content-font-size5);
  background-position-y: calc(0.3 * var(--jp-content-font-size5));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='2'] {
  font-size: var(--jp-content-font-size4);
  background-position-y: calc(0.3 * var(--jp-content-font-size4));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='3'] {
  font-size: var(--jp-content-font-size3);
  background-position-y: calc(0.3 * var(--jp-content-font-size3));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='4'] {
  font-size: var(--jp-content-font-size2);
  background-position-y: calc(0.3 * var(--jp-content-font-size2));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='5'] {
  font-size: var(--jp-content-font-size1);
  background-position-y: top;
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='6'] {
  font-size: var(--jp-content-font-size0);
  background-position-y: top;
}

/* collapseHeadingButton (show only on (hover,active) if hiddenCellsButton is shown) */
.jp-Notebook.jp-mod-showHiddenCellsButton .jp-collapseHeadingButton {
  display: none;
}

.jp-Notebook.jp-mod-showHiddenCellsButton
  :is(.jp-MarkdownCell:hover, .jp-mod-active)
  .jp-collapseHeadingButton {
  display: flex;
}

/* showHiddenCellsButton (only show if jp-mod-showHiddenCellsButton is set, which
is a consequence of the showHiddenCellsButton option in Notebook Settings)*/
.jp-Notebook.jp-mod-showHiddenCellsButton .jp-showHiddenCellsButton {
  margin-left: calc(var(--jp-cell-prompt-width) + 2 * var(--jp-code-padding));
  margin-top: var(--jp-code-padding);
  border: 1px solid var(--jp-border-color2);
  background-color: var(--jp-border-color3) !important;
  color: var(--jp-content-font-color0) !important;
  display: flex;
}

.jp-Notebook.jp-mod-showHiddenCellsButton .jp-showHiddenCellsButton:hover {
  background-color: var(--jp-border-color2) !important;
}

.jp-showHiddenCellsButton {
  display: none;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

/*
Using block instead of flex to allow the use of the break-inside CSS property for
cell outputs.
*/

@media print {
  .jp-Cell-inputWrapper,
  .jp-Cell-outputWrapper {
    display: block;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-notebook-toolbar-padding: 2px 5px 2px 2px;
}

/*-----------------------------------------------------------------------------

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-NotebookPanel-toolbar {
  padding: var(--jp-notebook-toolbar-padding);

  /* disable paint containment from lumino 2.0 default strict CSS containment */
  contain: style size !important;
}

.jp-Toolbar-item.jp-Notebook-toolbarCellType .jp-select-wrapper.jp-mod-focused {
  border: none;
  box-shadow: none;
}

.jp-Notebook-toolbarCellTypeDropdown select {
  height: 24px;
  font-size: var(--jp-ui-font-size1);
  line-height: 14px;
  border-radius: 0;
  display: block;
}

.jp-Notebook-toolbarCellTypeDropdown span {
  top: 5px !important;
}

.jp-Toolbar-responsive-popup {
  position: absolute;
  height: fit-content;
  display: flex;
  flex-direction: row;
  flex-wrap: wrap;
  justify-content: flex-end;
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  background: var(--jp-toolbar-background);
  min-height: var(--jp-toolbar-micro-height);
  padding: var(--jp-notebook-toolbar-padding);
  z-index: 1;
  right: 0;
  top: 0;
}

.jp-Toolbar > .jp-Toolbar-responsive-opener {
  margin-left: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-Notebook-ExecutionIndicator {
  position: relative;
  display: inline-block;
  height: 100%;
  z-index: 9997;
}

.jp-Notebook-ExecutionIndicator-tooltip {
  visibility: hidden;
  height: auto;
  width: max-content;
  width: -moz-max-content;
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color1);
  text-align: justify;
  border-radius: 6px;
  padding: 0 5px;
  position: fixed;
  display: table;
}

.jp-Notebook-ExecutionIndicator-tooltip.up {
  transform: translateX(-50%) translateY(-100%) translateY(-32px);
}

.jp-Notebook-ExecutionIndicator-tooltip.down {
  transform: translateX(calc(-100% + 16px)) translateY(5px);
}

.jp-Notebook-ExecutionIndicator-tooltip.hidden {
  display: none;
}

.jp-Notebook-ExecutionIndicator:hover .jp-Notebook-ExecutionIndicator-tooltip {
  visibility: visible;
}

.jp-Notebook-ExecutionIndicator span {
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  color: var(--jp-ui-font-color1);
  line-height: 24px;
  display: block;
}

.jp-Notebook-ExecutionIndicator-progress-bar {
  display: flex;
  justify-content: center;
  height: 100%;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*
 * Execution indicator
 */
.jp-tocItem-content::after {
  content: '';

  /* Must be identical to form a circle */
  width: 12px;
  height: 12px;
  background: none;
  border: none;
  position: absolute;
  right: 0;
}

.jp-tocItem-content[data-running='0']::after {
  border-radius: 50%;
  border: var(--jp-border-width) solid var(--jp-inverse-layout-color3);
  background: none;
}

.jp-tocItem-content[data-running='1']::after {
  border-radius: 50%;
  border: var(--jp-border-width) solid var(--jp-inverse-layout-color3);
  background-color: var(--jp-inverse-layout-color3);
}

.jp-tocItem-content[data-running='0'],
.jp-tocItem-content[data-running='1'] {
  margin-right: 12px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-Notebook-footer {
  height: 27px;
  margin-left: calc(
    var(--jp-cell-prompt-width) + var(--jp-cell-collapser-width) +
      var(--jp-cell-padding)
  );
  width: calc(
    100% -
      (
        var(--jp-cell-prompt-width) + var(--jp-cell-collapser-width) +
          var(--jp-cell-padding) + var(--jp-cell-padding)
      )
  );
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  color: var(--jp-ui-font-color3);
  margin-top: 6px;
  background: none;
  cursor: pointer;
}

.jp-Notebook-footer:focus {
  border-color: var(--jp-cell-editor-active-border-color);
}

/* For devices that support hovering, hide footer until hover */
@media (hover: hover) {
  .jp-Notebook-footer {
    opacity: 0;
  }

  .jp-Notebook-footer:focus,
  .jp-Notebook-footer:hover {
    opacity: 1;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Imports
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| CSS variables
|----------------------------------------------------------------------------*/

:root {
  --jp-side-by-side-output-size: 1fr;
  --jp-side-by-side-resized-cell: var(--jp-side-by-side-output-size);
  --jp-private-notebook-dragImage-width: 304px;
  --jp-private-notebook-dragImage-height: 36px;
  --jp-private-notebook-selected-color: var(--md-blue-400);
  --jp-private-notebook-active-color: var(--md-green-400);
}

/*-----------------------------------------------------------------------------
| Notebook
|----------------------------------------------------------------------------*/

/* stylelint-disable selector-max-class */

.jp-NotebookPanel {
  display: block;
  height: 100%;
}

.jp-NotebookPanel.jp-Document {
  min-width: 240px;
  min-height: 120px;
}

.jp-Notebook {
  padding: var(--jp-notebook-padding);
  outline: none;
  overflow: auto;
  background: var(--jp-layout-color0);
}

.jp-Notebook.jp-mod-scrollPastEnd::after {
  display: block;
  content: '';
  min-height: var(--jp-notebook-scroll-padding);
}

.jp-MainAreaWidget-ContainStrict .jp-Notebook * {
  contain: strict;
}

.jp-Notebook .jp-Cell {
  overflow: visible;
}

.jp-Notebook .jp-Cell .jp-InputPrompt {
  cursor: move;
}

/*-----------------------------------------------------------------------------
| Notebook state related styling
|
| The notebook and cells each have states, here are the possibilities:
|
| - Notebook
|   - Command
|   - Edit
| - Cell
|   - None
|   - Active (only one can be active)
|   - Selected (the cells actions are applied to)
|   - Multiselected (when multiple selected, the cursor)
|   - No outputs
|----------------------------------------------------------------------------*/

/* Command or edit modes */

.jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-InputPrompt {
  opacity: var(--jp-cell-prompt-not-active-opacity);
  color: var(--jp-cell-prompt-not-active-font-color);
}

.jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-OutputPrompt {
  opacity: var(--jp-cell-prompt-not-active-opacity);
  color: var(--jp-cell-prompt-not-active-font-color);
}

/* cell is active */
.jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser {
  background: var(--jp-brand-color1);
}

/* cell is dirty */
.jp-Notebook .jp-Cell.jp-mod-dirty .jp-InputPrompt {
  color: var(--jp-warn-color1);
}

.jp-Notebook .jp-Cell.jp-mod-dirty .jp-InputPrompt::before {
  color: var(--jp-warn-color1);
  content: '•';
}

.jp-Notebook .jp-Cell.jp-mod-active.jp-mod-dirty .jp-Collapser {
  background: var(--jp-warn-color1);
}

/* collapser is hovered */
.jp-Notebook .jp-Cell .jp-Collapser:hover {
  box-shadow: var(--jp-elevation-z2);
  background: var(--jp-brand-color1);
  opacity: var(--jp-cell-collapser-not-active-hover-opacity);
}

/* cell is active and collapser is hovered */
.jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser:hover {
  background: var(--jp-brand-color0);
  opacity: 1;
}

/* Command mode */

.jp-Notebook.jp-mod-commandMode .jp-Cell.jp-mod-selected {
  background: var(--jp-notebook-multiselected-color);
}

.jp-Notebook.jp-mod-commandMode
  .jp-Cell.jp-mod-active.jp-mod-selected:not(.jp-mod-multiSelected) {
  background: transparent;
}

/* Edit mode */

.jp-Notebook.jp-mod-editMode .jp-Cell.jp-mod-active .jp-InputArea-editor {
  border: var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
  background-color: var(--jp-cell-editor-active-background);
}

/*-----------------------------------------------------------------------------
| Notebook drag and drop
|----------------------------------------------------------------------------*/

.jp-Notebook-cell.jp-mod-dropSource {
  opacity: 0.5;
}

.jp-Notebook-cell.jp-mod-dropTarget,
.jp-Notebook.jp-mod-commandMode
  .jp-Notebook-cell.jp-mod-active.jp-mod-selected.jp-mod-dropTarget {
  border-top-color: var(--jp-private-notebook-selected-color);
  border-top-style: solid;
  border-top-width: 2px;
}

.jp-dragImage {
  display: block;
  flex-direction: row;
  width: var(--jp-private-notebook-dragImage-width);
  height: var(--jp-private-notebook-dragImage-height);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background);
  overflow: visible;
}

.jp-dragImage-singlePrompt {
  box-shadow: 2px 2px 4px 0 rgba(0, 0, 0, 0.12);
}

.jp-dragImage .jp-dragImage-content {
  flex: 1 1 auto;
  z-index: 2;
  font-size: var(--jp-code-font-size);
  font-family: var(--jp-code-font-family);
  line-height: var(--jp-code-line-height);
  padding: var(--jp-code-padding);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background-color);
  color: var(--jp-content-font-color3);
  text-align: left;
  margin: 4px 4px 4px 0;
}

.jp-dragImage .jp-dragImage-prompt {
  flex: 0 0 auto;
  min-width: 36px;
  color: var(--jp-cell-inprompt-font-color);
  padding: var(--jp-code-padding);
  padding-left: 12px;
  font-family: var(--jp-cell-prompt-font-family);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  line-height: 1.9;
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
}

.jp-dragImage-multipleBack {
  z-index: -1;
  position: absolute;
  height: 32px;
  width: 300px;
  top: 8px;
  left: 8px;
  background: var(--jp-layout-color2);
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  box-shadow: 2px 2px 4px 0 rgba(0, 0, 0, 0.12);
}

/*-----------------------------------------------------------------------------
| Cell toolbar
|----------------------------------------------------------------------------*/

.jp-NotebookTools {
  display: block;
  min-width: var(--jp-sidebar-min-width);
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);

  /* This is needed so that all font sizing of children done in ems is
    * relative to this base size */
  font-size: var(--jp-ui-font-size1);
  overflow: auto;
}

.jp-ActiveCellTool {
  padding: 12px 0;
  display: flex;
}

.jp-ActiveCellTool-Content {
  flex: 1 1 auto;
}

.jp-ActiveCellTool .jp-ActiveCellTool-CellContent {
  background: var(--jp-cell-editor-background);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  border-radius: 0;
  min-height: 29px;
}

.jp-ActiveCellTool .jp-InputPrompt {
  min-width: calc(var(--jp-cell-prompt-width) * 0.75);
}

.jp-ActiveCellTool-CellContent > pre {
  padding: 5px 4px;
  margin: 0;
  white-space: normal;
}

.jp-MetadataEditorTool {
  flex-direction: column;
  padding: 12px 0;
}

.jp-RankedPanel > :not(:first-child) {
  margin-top: 12px;
}

.jp-KeySelector select.jp-mod-styled {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  border: var(--jp-border-width) solid var(--jp-border-color1);
}

.jp-KeySelector label,
.jp-MetadataEditorTool label,
.jp-NumberSetter label {
  line-height: 1.4;
}

.jp-NotebookTools .jp-select-wrapper {
  margin-top: 4px;
  margin-bottom: 0;
}

.jp-NumberSetter input {
  width: 100%;
  margin-top: 4px;
}

.jp-NotebookTools .jp-Collapse {
  margin-top: 16px;
}

/*-----------------------------------------------------------------------------
| Presentation Mode (.jp-mod-presentationMode)
|----------------------------------------------------------------------------*/

.jp-mod-presentationMode .jp-Notebook {
  --jp-content-font-size1: var(--jp-content-presentation-font-size1);
  --jp-code-font-size: var(--jp-code-presentation-font-size);
}

.jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-InputPrompt,
.jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-OutputPrompt {
  flex: 0 0 110px;
}

/*-----------------------------------------------------------------------------
| Side-by-side Mode (.jp-mod-sideBySide)
|----------------------------------------------------------------------------*/
.jp-mod-sideBySide.jp-Notebook .jp-Notebook-cell {
  margin-top: 3em;
  margin-bottom: 3em;
  margin-left: 5%;
  margin-right: 5%;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell {
  display: grid;
  grid-template-columns: minmax(0, 1fr) min-content minmax(
      0,
      var(--jp-side-by-side-output-size)
    );
  grid-template-rows: auto minmax(0, 1fr) auto;
  grid-template-areas:
    'header header header'
    'input handle output'
    'footer footer footer';
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell.jp-mod-resizedCell {
  grid-template-columns: minmax(0, 1fr) min-content minmax(
      0,
      var(--jp-side-by-side-resized-cell)
    );
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellHeader {
  grid-area: header;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-Cell-inputWrapper {
  grid-area: input;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-Cell-outputWrapper {
  /* overwrite the default margin (no vertical separation needed in side by side move */
  margin-top: 0;
  grid-area: output;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellFooter {
  grid-area: footer;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellResizeHandle {
  grid-area: handle;
  user-select: none;
  display: block;
  height: 100%;
  cursor: ew-resize;
  padding: 0 var(--jp-cell-padding);
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellResizeHandle::after {
  content: '';
  display: block;
  background: var(--jp-border-color2);
  height: 100%;
  width: 5px;
}

.jp-mod-sideBySide.jp-Notebook
  .jp-CodeCell.jp-mod-resizedCell
  .jp-CellResizeHandle::after {
  background: var(--jp-border-color0);
}

.jp-CellResizeHandle {
  display: none;
}

/*-----------------------------------------------------------------------------
| Placeholder
|----------------------------------------------------------------------------*/

.jp-Cell-Placeholder {
  padding-left: 55px;
}

.jp-Cell-Placeholder-wrapper {
  background: #fff;
  border: 1px solid;
  border-color: #e5e6e9 #dfe0e4 #d0d1d5;
  border-radius: 4px;
  -webkit-border-radius: 4px;
  margin: 10px 15px;
}

.jp-Cell-Placeholder-wrapper-inner {
  padding: 15px;
  position: relative;
}

.jp-Cell-Placeholder-wrapper-body {
  background-repeat: repeat;
  background-size: 50% auto;
}

.jp-Cell-Placeholder-wrapper-body div {
  background: #f6f7f8;
  background-image: -webkit-linear-gradient(
    left,
    #f6f7f8 0%,
    #edeef1 20%,
    #f6f7f8 40%,
    #f6f7f8 100%
  );
  background-repeat: no-repeat;
  background-size: 800px 104px;
  height: 104px;
  position: absolute;
  right: 15px;
  left: 15px;
  top: 15px;
}

div.jp-Cell-Placeholder-h1 {
  top: 20px;
  height: 20px;
  left: 15px;
  width: 150px;
}

div.jp-Cell-Placeholder-h2 {
  left: 15px;
  top: 50px;
  height: 10px;
  width: 100px;
}

div.jp-Cell-Placeholder-content-1,
div.jp-Cell-Placeholder-content-2,
div.jp-Cell-Placeholder-content-3 {
  left: 15px;
  right: 15px;
  height: 10px;
}

div.jp-Cell-Placeholder-content-1 {
  top: 100px;
}

div.jp-Cell-Placeholder-content-2 {
  top: 120px;
}

div.jp-Cell-Placeholder-content-3 {
  top: 140px;
}

</style>
<style type="text/css">
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*
The following CSS variables define the main, public API for styling JupyterLab.
These variables should be used by all plugins wherever possible. In other
words, plugins should not define custom colors, sizes, etc unless absolutely
necessary. This enables users to change the visual theme of JupyterLab
by changing these variables.

Many variables appear in an ordered sequence (0,1,2,3). These sequences
are designed to work well together, so for example, `--jp-border-color1` should
be used with `--jp-layout-color1`. The numbers have the following meanings:

* 0: super-primary, reserved for special emphasis
* 1: primary, most important under normal situations
* 2: secondary, next most important under normal situations
* 3: tertiary, next most important under normal situations

Throughout JupyterLab, we are mostly following principles from Google's
Material Design when selecting colors. We are not, however, following
all of MD as it is not optimized for dense, information rich UIs.
*/

:root {
  /* Elevation
   *
   * We style box-shadows using Material Design's idea of elevation. These particular numbers are taken from here:
   *
   * https://github.com/material-components/material-components-web
   * https://material-components-web.appspot.com/elevation.html
   */

  --jp-shadow-base-lightness: 0;
  --jp-shadow-umbra-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.2
  );
  --jp-shadow-penumbra-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.14
  );
  --jp-shadow-ambient-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.12
  );
  --jp-elevation-z0: none;
  --jp-elevation-z1: 0 2px 1px -1px var(--jp-shadow-umbra-color),
    0 1px 1px 0 var(--jp-shadow-penumbra-color),
    0 1px 3px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z2: 0 3px 1px -2px var(--jp-shadow-umbra-color),
    0 2px 2px 0 var(--jp-shadow-penumbra-color),
    0 1px 5px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z4: 0 2px 4px -1px var(--jp-shadow-umbra-color),
    0 4px 5px 0 var(--jp-shadow-penumbra-color),
    0 1px 10px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z6: 0 3px 5px -1px var(--jp-shadow-umbra-color),
    0 6px 10px 0 var(--jp-shadow-penumbra-color),
    0 1px 18px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z8: 0 5px 5px -3px var(--jp-shadow-umbra-color),
    0 8px 10px 1px var(--jp-shadow-penumbra-color),
    0 3px 14px 2px var(--jp-shadow-ambient-color);
  --jp-elevation-z12: 0 7px 8px -4px var(--jp-shadow-umbra-color),
    0 12px 17px 2px var(--jp-shadow-penumbra-color),
    0 5px 22px 4px var(--jp-shadow-ambient-color);
  --jp-elevation-z16: 0 8px 10px -5px var(--jp-shadow-umbra-color),
    0 16px 24px 2px var(--jp-shadow-penumbra-color),
    0 6px 30px 5px var(--jp-shadow-ambient-color);
  --jp-elevation-z20: 0 10px 13px -6px var(--jp-shadow-umbra-color),
    0 20px 31px 3px var(--jp-shadow-penumbra-color),
    0 8px 38px 7px var(--jp-shadow-ambient-color);
  --jp-elevation-z24: 0 11px 15px -7px var(--jp-shadow-umbra-color),
    0 24px 38px 3px var(--jp-shadow-penumbra-color),
    0 9px 46px 8px var(--jp-shadow-ambient-color);

  /* Borders
   *
   * The following variables, specify the visual styling of borders in JupyterLab.
   */

  --jp-border-width: 1px;
  --jp-border-color0: var(--md-grey-400);
  --jp-border-color1: var(--md-grey-400);
  --jp-border-color2: var(--md-grey-300);
  --jp-border-color3: var(--md-grey-200);
  --jp-inverse-border-color: var(--md-grey-600);
  --jp-border-radius: 2px;

  /* UI Fonts
   *
   * The UI font CSS variables are used for the typography all of the JupyterLab
   * user interface elements that are not directly user generated content.
   *
   * The font sizing here is done assuming that the body font size of --jp-ui-font-size1
   * is applied to a parent element. When children elements, such as headings, are sized
   * in em all things will be computed relative to that body size.
   */

  --jp-ui-font-scale-factor: 1.2;
  --jp-ui-font-size0: 0.83333em;
  --jp-ui-font-size1: 13px; /* Base font size */
  --jp-ui-font-size2: 1.2em;
  --jp-ui-font-size3: 1.44em;
  --jp-ui-font-family: system-ui, -apple-system, blinkmacsystemfont, 'Segoe UI',
    helvetica, arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji',
    'Segoe UI Symbol';

  /*
   * Use these font colors against the corresponding main layout colors.
   * In a light theme, these go from dark to light.
   */

  /* Defaults use Material Design specification */
  --jp-ui-font-color0: rgba(0, 0, 0, 1);
  --jp-ui-font-color1: rgba(0, 0, 0, 0.87);
  --jp-ui-font-color2: rgba(0, 0, 0, 0.54);
  --jp-ui-font-color3: rgba(0, 0, 0, 0.38);

  /*
   * Use these against the brand/accent/warn/error colors.
   * These will typically go from light to darker, in both a dark and light theme.
   */

  --jp-ui-inverse-font-color0: rgba(255, 255, 255, 1);
  --jp-ui-inverse-font-color1: rgba(255, 255, 255, 1);
  --jp-ui-inverse-font-color2: rgba(255, 255, 255, 0.7);
  --jp-ui-inverse-font-color3: rgba(255, 255, 255, 0.5);

  /* Content Fonts
   *
   * Content font variables are used for typography of user generated content.
   *
   * The font sizing here is done assuming that the body font size of --jp-content-font-size1
   * is applied to a parent element. When children elements, such as headings, are sized
   * in em all things will be computed relative to that body size.
   */

  --jp-content-line-height: 1.6;
  --jp-content-font-scale-factor: 1.2;
  --jp-content-font-size0: 0.83333em;
  --jp-content-font-size1: 14px; /* Base font size */
  --jp-content-font-size2: 1.2em;
  --jp-content-font-size3: 1.44em;
  --jp-content-font-size4: 1.728em;
  --jp-content-font-size5: 2.0736em;

  /* This gives a magnification of about 125% in presentation mode over normal. */
  --jp-content-presentation-font-size1: 17px;
  --jp-content-heading-line-height: 1;
  --jp-content-heading-margin-top: 1.2em;
  --jp-content-heading-margin-bottom: 0.8em;
  --jp-content-heading-font-weight: 500;

  /* Defaults use Material Design specification */
  --jp-content-font-color0: rgba(0, 0, 0, 1);
  --jp-content-font-color1: rgba(0, 0, 0, 0.87);
  --jp-content-font-color2: rgba(0, 0, 0, 0.54);
  --jp-content-font-color3: rgba(0, 0, 0, 0.38);
  --jp-content-link-color: var(--md-blue-900);
  --jp-content-font-family: system-ui, -apple-system, blinkmacsystemfont,
    'Segoe UI', helvetica, arial, sans-serif, 'Apple Color Emoji',
    'Segoe UI Emoji', 'Segoe UI Symbol';

  /*
   * Code Fonts
   *
   * Code font variables are used for typography of code and other monospaces content.
   */

  --jp-code-font-size: 13px;
  --jp-code-line-height: 1.3077; /* 17px for 13px base */
  --jp-code-padding: 5px; /* 5px for 13px base, codemirror highlighting needs integer px value */
  --jp-code-font-family-default: menlo, consolas, 'DejaVu Sans Mono', monospace;
  --jp-code-font-family: var(--jp-code-font-family-default);

  /* This gives a magnification of about 125% in presentation mode over normal. */
  --jp-code-presentation-font-size: 16px;

  /* may need to tweak cursor width if you change font size */
  --jp-code-cursor-width0: 1.4px;
  --jp-code-cursor-width1: 2px;
  --jp-code-cursor-width2: 4px;

  /* Layout
   *
   * The following are the main layout colors use in JupyterLab. In a light
   * theme these would go from light to dark.
   */

  --jp-layout-color0: white;
  --jp-layout-color1: white;
  --jp-layout-color2: var(--md-grey-200);
  --jp-layout-color3: var(--md-grey-400);
  --jp-layout-color4: var(--md-grey-600);

  /* Inverse Layout
   *
   * The following are the inverse layout colors use in JupyterLab. In a light
   * theme these would go from dark to light.
   */

  --jp-inverse-layout-color0: #111;
  --jp-inverse-layout-color1: var(--md-grey-900);
  --jp-inverse-layout-color2: var(--md-grey-800);
  --jp-inverse-layout-color3: var(--md-grey-700);
  --jp-inverse-layout-color4: var(--md-grey-600);

  /* Brand/accent */

  --jp-brand-color0: var(--md-blue-900);
  --jp-brand-color1: var(--md-blue-700);
  --jp-brand-color2: var(--md-blue-300);
  --jp-brand-color3: var(--md-blue-100);
  --jp-brand-color4: var(--md-blue-50);
  --jp-accent-color0: var(--md-green-900);
  --jp-accent-color1: var(--md-green-700);
  --jp-accent-color2: var(--md-green-300);
  --jp-accent-color3: var(--md-green-100);

  /* State colors (warn, error, success, info) */

  --jp-warn-color0: var(--md-orange-900);
  --jp-warn-color1: var(--md-orange-700);
  --jp-warn-color2: var(--md-orange-300);
  --jp-warn-color3: var(--md-orange-100);
  --jp-error-color0: var(--md-red-900);
  --jp-error-color1: var(--md-red-700);
  --jp-error-color2: var(--md-red-300);
  --jp-error-color3: var(--md-red-100);
  --jp-success-color0: var(--md-green-900);
  --jp-success-color1: var(--md-green-700);
  --jp-success-color2: var(--md-green-300);
  --jp-success-color3: var(--md-green-100);
  --jp-info-color0: var(--md-cyan-900);
  --jp-info-color1: var(--md-cyan-700);
  --jp-info-color2: var(--md-cyan-300);
  --jp-info-color3: var(--md-cyan-100);

  /* Cell specific styles */

  --jp-cell-padding: 5px;
  --jp-cell-collapser-width: 8px;
  --jp-cell-collapser-min-height: 20px;
  --jp-cell-collapser-not-active-hover-opacity: 0.6;
  --jp-cell-editor-background: var(--md-grey-100);
  --jp-cell-editor-border-color: var(--md-grey-300);
  --jp-cell-editor-box-shadow: inset 0 0 2px var(--md-blue-300);
  --jp-cell-editor-active-background: var(--jp-layout-color0);
  --jp-cell-editor-active-border-color: var(--jp-brand-color1);
  --jp-cell-prompt-width: 64px;
  --jp-cell-prompt-font-family: var(--jp-code-font-family-default);
  --jp-cell-prompt-letter-spacing: 0;
  --jp-cell-prompt-opacity: 1;
  --jp-cell-prompt-not-active-opacity: 0.5;
  --jp-cell-prompt-not-active-font-color: var(--md-grey-700);

  /* A custom blend of MD grey and blue 600
   * See https://meyerweb.com/eric/tools/color-blend/#546E7A:1E88E5:5:hex */
  --jp-cell-inprompt-font-color: #307fc1;

  /* A custom blend of MD grey and orange 600
   * https://meyerweb.com/eric/tools/color-blend/#546E7A:F4511E:5:hex */
  --jp-cell-outprompt-font-color: #bf5b3d;

  /* Notebook specific styles */

  --jp-notebook-padding: 10px;
  --jp-notebook-select-background: var(--jp-layout-color1);
  --jp-notebook-multiselected-color: var(--md-blue-50);

  /* The scroll padding is calculated to fill enough space at the bottom of the
  notebook to show one single-line cell (with appropriate padding) at the top
  when the notebook is scrolled all the way to the bottom. We also subtract one
  pixel so that no scrollbar appears if we have just one single-line cell in the
  notebook. This padding is to enable a 'scroll past end' feature in a notebook.
  */
  --jp-notebook-scroll-padding: calc(
    100% - var(--jp-code-font-size) * var(--jp-code-line-height) -
      var(--jp-code-padding) - var(--jp-cell-padding) - 1px
  );

  /* Rendermime styles */

  --jp-rendermime-error-background: #fdd;
  --jp-rendermime-table-row-background: var(--md-grey-100);
  --jp-rendermime-table-row-hover-background: var(--md-light-blue-50);

  /* Dialog specific styles */

  --jp-dialog-background: rgba(0, 0, 0, 0.25);

  /* Console specific styles */

  --jp-console-padding: 10px;

  /* Toolbar specific styles */

  --jp-toolbar-border-color: var(--jp-border-color1);
  --jp-toolbar-micro-height: 8px;
  --jp-toolbar-background: var(--jp-layout-color1);
  --jp-toolbar-box-shadow: 0 0 2px 0 rgba(0, 0, 0, 0.24);
  --jp-toolbar-header-margin: 4px 4px 0 4px;
  --jp-toolbar-active-background: var(--md-grey-300);

  /* Statusbar specific styles */

  --jp-statusbar-height: 24px;

  /* Input field styles */

  --jp-input-box-shadow: inset 0 0 2px var(--md-blue-300);
  --jp-input-active-background: var(--jp-layout-color1);
  --jp-input-hover-background: var(--jp-layout-color1);
  --jp-input-background: var(--md-grey-100);
  --jp-input-border-color: var(--jp-inverse-border-color);
  --jp-input-active-border-color: var(--jp-brand-color1);
  --jp-input-active-box-shadow-color: rgba(19, 124, 189, 0.3);

  /* General editor styles */

  --jp-editor-selected-background: #d9d9d9;
  --jp-editor-selected-focused-background: #d7d4f0;
  --jp-editor-cursor-color: var(--jp-ui-font-color0);

  /* Code mirror specific styles */

  --jp-mirror-editor-keyword-color: #008000;
  --jp-mirror-editor-atom-color: #88f;
  --jp-mirror-editor-number-color: #080;
  --jp-mirror-editor-def-color: #00f;
  --jp-mirror-editor-variable-color: var(--md-grey-900);
  --jp-mirror-editor-variable-2-color: rgb(0, 54, 109);
  --jp-mirror-editor-variable-3-color: #085;
  --jp-mirror-editor-punctuation-color: #05a;
  --jp-mirror-editor-property-color: #05a;
  --jp-mirror-editor-operator-color: #a2f;
  --jp-mirror-editor-comment-color: #408080;
  --jp-mirror-editor-string-color: #ba2121;
  --jp-mirror-editor-string-2-color: #708;
  --jp-mirror-editor-meta-color: #a2f;
  --jp-mirror-editor-qualifier-color: #555;
  --jp-mirror-editor-builtin-color: #008000;
  --jp-mirror-editor-bracket-color: #997;
  --jp-mirror-editor-tag-color: #170;
  --jp-mirror-editor-attribute-color: #00c;
  --jp-mirror-editor-header-color: blue;
  --jp-mirror-editor-quote-color: #090;
  --jp-mirror-editor-link-color: #00c;
  --jp-mirror-editor-error-color: #f00;
  --jp-mirror-editor-hr-color: #999;

  /*
    RTC user specific colors.
    These colors are used for the cursor, username in the editor,
    and the icon of the user.
  */

  --jp-collaborator-color1: #ffad8e;
  --jp-collaborator-color2: #dac83d;
  --jp-collaborator-color3: #72dd76;
  --jp-collaborator-color4: #00e4d0;
  --jp-collaborator-color5: #45d4ff;
  --jp-collaborator-color6: #e2b1ff;
  --jp-collaborator-color7: #ff9de6;

  /* Vega extension styles */

  --jp-vega-background: white;

  /* Sidebar-related styles */

  --jp-sidebar-min-width: 250px;

  /* Search-related styles */

  --jp-search-toggle-off-opacity: 0.5;
  --jp-search-toggle-hover-opacity: 0.8;
  --jp-search-toggle-on-opacity: 1;
  --jp-search-selected-match-background-color: rgb(245, 200, 0);
  --jp-search-selected-match-color: black;
  --jp-search-unselected-match-background-color: var(
    --jp-inverse-layout-color0
  );
  --jp-search-unselected-match-color: var(--jp-ui-inverse-font-color0);

  /* Icon colors that work well with light or dark backgrounds */
  --jp-icon-contrast-color0: var(--md-purple-600);
  --jp-icon-contrast-color1: var(--md-green-600);
  --jp-icon-contrast-color2: var(--md-pink-600);
  --jp-icon-contrast-color3: var(--md-blue-600);

  /* Button colors */
  --jp-accept-color-normal: var(--md-blue-700);
  --jp-accept-color-hover: var(--md-blue-800);
  --jp-accept-color-active: var(--md-blue-900);
  --jp-warn-color-normal: var(--md-red-700);
  --jp-warn-color-hover: var(--md-red-800);
  --jp-warn-color-active: var(--md-red-900);
  --jp-reject-color-normal: var(--md-grey-600);
  --jp-reject-color-hover: var(--md-grey-700);
  --jp-reject-color-active: var(--md-grey-800);

  /* File or activity icons and switch semantic variables */
  --jp-jupyter-icon-color: #f37626;
  --jp-notebook-icon-color: #f37626;
  --jp-json-icon-color: var(--md-orange-700);
  --jp-console-icon-background-color: var(--md-blue-700);
  --jp-console-icon-color: white;
  --jp-terminal-icon-background-color: var(--md-grey-800);
  --jp-terminal-icon-color: var(--md-grey-200);
  --jp-text-editor-icon-color: var(--md-grey-700);
  --jp-inspector-icon-color: var(--md-grey-700);
  --jp-switch-color: var(--md-grey-400);
  --jp-switch-true-position-color: var(--md-orange-900);
}
</style>
<style type="text/css">
/* Force rendering true colors when outputing to pdf */
* {
  -webkit-print-color-adjust: exact;
}

/* Misc */
a.anchor-link {
  display: none;
}

/* Input area styling */
.jp-InputArea {
  overflow: hidden;
}

.jp-InputArea-editor {
  overflow: hidden;
}

.cm-editor.cm-s-jupyter .highlight pre {
/* weird, but --jp-code-padding defined to be 5px but 4px horizontal padding is hardcoded for pre.cm-line */
  padding: var(--jp-code-padding) 4px;
  margin: 0;

  font-family: inherit;
  font-size: inherit;
  line-height: inherit;
  color: inherit;

}

.jp-OutputArea-output pre {
  line-height: inherit;
  font-family: inherit;
}

.jp-RenderedText pre {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-code-font-size);
}

/* Hiding the collapser by default */
.jp-Collapser {
  display: none;
}

@page {
    margin: 0.5in; /* Margin for each printed piece of paper */
}

@media print {
  .jp-Cell-inputWrapper,
  .jp-Cell-outputWrapper {
    display: block;
  }
}
</style>
<!-- Load mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML-full,Safe"> </script>
<!-- MathJax configuration -->
<script type="text/x-mathjax-config">
    init_mathjax = function() {
        if (window.MathJax) {
        // MathJax loaded
            MathJax.Hub.Config({
                TeX: {
                    equationNumbers: {
                    autoNumber: "AMS",
                    useLabelIds: true
                    }
                },
                tex2jax: {
                    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                    processEscapes: true,
                    processEnvironments: true
                },
                displayAlign: 'center',
                CommonHTML: {
                    linebreaks: {
                    automatic: true
                    }
                }
            });

            MathJax.Hub.Queue(["Typeset", MathJax.Hub]);
        }
    }
    init_mathjax();
    </script>
<!-- End of mathjax configuration --><script type="module">
  document.addEventListener("DOMContentLoaded", async () => {
    const diagrams = document.querySelectorAll(".jp-Mermaid > pre.mermaid");
    // do not load mermaidjs if not needed
    if (!diagrams.length) {
      return;
    }
    const mermaid = (await import("https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.6.0/mermaid.esm.min.mjs")).default;
    const parser = new DOMParser();

    mermaid.initialize({
      maxTextSize: 100000,
      startOnLoad: false,
      fontFamily: window
        .getComputedStyle(document.body)
        .getPropertyValue("--jp-ui-font-family"),
      theme: document.querySelector("body[data-jp-theme-light='true']")
        ? "default"
        : "dark",
    });

    let _nextMermaidId = 0;

    function makeMermaidImage(svg) {
      const img = document.createElement("img");
      const doc = parser.parseFromString(svg, "image/svg+xml");
      const svgEl = doc.querySelector("svg");
      const { maxWidth } = svgEl?.style || {};
      const firstTitle = doc.querySelector("title");
      const firstDesc = doc.querySelector("desc");

      img.setAttribute("src", `data:image/svg+xml,${encodeURIComponent(svg)}`);
      if (maxWidth) {
        img.width = parseInt(maxWidth);
      }
      if (firstTitle) {
        img.setAttribute("alt", firstTitle.textContent);
      }
      if (firstDesc) {
        const caption = document.createElement("figcaption");
        caption.className = "sr-only";
        caption.textContent = firstDesc.textContent;
        return [img, caption];
      }
      return [img];
    }

    async function makeMermaidError(text) {
      let errorMessage = "";
      try {
        await mermaid.parse(text);
      } catch (err) {
        errorMessage = `${err}`;
      }

      const result = document.createElement("details");
      result.className = 'jp-RenderedMermaid-Details';
      const summary = document.createElement("summary");
      summary.className = 'jp-RenderedMermaid-Summary';
      const pre = document.createElement("pre");
      const code = document.createElement("code");
      code.innerText = text;
      pre.appendChild(code);
      summary.appendChild(pre);
      result.appendChild(summary);

      const warning = document.createElement("pre");
      warning.innerText = errorMessage;
      result.appendChild(warning);
      return [result];
    }

    async function renderOneMarmaid(src) {
      const id = `jp-mermaid-${_nextMermaidId++}`;
      const parent = src.parentNode;
      let raw = src.textContent.trim();
      const el = document.createElement("div");
      el.style.visibility = "hidden";
      document.body.appendChild(el);
      let results = null;
      let output = null;
      try {
        const { svg } = await mermaid.render(id, raw, el);
        results = makeMermaidImage(svg);
        output = document.createElement("figure");
        results.map(output.appendChild, output);
      } catch (err) {
        parent.classList.add("jp-mod-warning");
        results = await makeMermaidError(raw);
        output = results[0];
      } finally {
        el.remove();
      }
      parent.classList.add("jp-RenderedMermaid");
      parent.appendChild(output);
    }

    void Promise.all([...diagrams].map(renderOneMarmaid));
  });
</script>
<style>
  .jp-Mermaid:not(.jp-RenderedMermaid) {
    display: none;
  }

  .jp-RenderedMermaid {
    overflow: auto;
    display: flex;
  }

  .jp-RenderedMermaid.jp-mod-warning {
    width: auto;
    padding: 0.5em;
    margin-top: 0.5em;
    border: var(--jp-border-width) solid var(--jp-warn-color2);
    border-radius: var(--jp-border-radius);
    color: var(--jp-ui-font-color1);
    font-size: var(--jp-ui-font-size1);
    white-space: pre-wrap;
    word-wrap: break-word;
  }

  .jp-RenderedMermaid figure {
    margin: 0;
    overflow: auto;
    max-width: 100%;
  }

  .jp-RenderedMermaid img {
    max-width: 100%;
  }

  .jp-RenderedMermaid-Details > pre {
    margin-top: 1em;
  }

  .jp-RenderedMermaid-Summary {
    color: var(--jp-warn-color2);
  }

  .jp-RenderedMermaid:not(.jp-mod-warning) pre {
    display: none;
  }

  .jp-RenderedMermaid-Summary > pre {
    display: inline-block;
    white-space: normal;
  }
</style>
<!-- End of mermaid configuration --></head>
<body class="jp-Notebook" data-jp-theme-light="true" data-jp-theme-name="JupyterLab Light">
<main>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=38d056bb-061f-460c-a2eb-7226fe89027f">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="Banking-Binary-Classification-Problem">Banking Binary Classification Problem<a class="anchor-link" href="#Banking-Binary-Classification-Problem">¶</a></h2>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=fa5b55c3-7fb0-4f80-ae6b-48620875032a">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Problem">Problem<a class="anchor-link" href="#Problem">¶</a></h3><p>We would like to predict if given a bank client, are they likely to make a term deposit or not at the bank.</p>
<p>Dataset: bankingInfo.csv
     - if output == yes, then the client has made a term deposit at this bank, else they have not.</p>
<p>Project was built using the bc_ml_breast_cancer_pred.ipynb and the deep_learning.ipynb files provided by Dr. Sambriddhi Mainali along with the <em>A hands-on introduction to feed-forward neural networks using Tensorflow and Keras</em> Github Repository by Dr. Badri Adhikari found <a href="https://badriadhikari.github.io/AI-2022spring/NN-using-TF.html">here.</a></p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=05813c1f-4424-4604-a872-e1b8d7252173">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Phase-4:-Feature-Importance-and-Reduction">Phase 4: Feature Importance and Reduction<a class="anchor-link" href="#Phase-4:-Feature-Importance-and-Reduction">¶</a></h3><hr/>
<h4 id="4.1-Import-Data">4.1 Import Data<a class="anchor-link" href="#4.1-Import-Data">¶</a></h4>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=020f6e3b-943f-4513-9f06-cbc2333260e1">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [1]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="o">%</span><span class="k">matplotlib</span> inline

<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">precision_score</span><span class="p">,</span> <span class="n">recall_score</span><span class="p">,</span> <span class="n">f1_score</span>
<span class="kn">import</span> <span class="nn">keras</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> 
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.callbacks</span> <span class="kn">import</span> <span class="n">EarlyStopping</span><span class="p">,</span> <span class="n">ModelCheckpoint</span>
<span class="kn">import</span> <span class="nn">os</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=68479ec1-9d39-4d9e-8c9d-b5499f75c812">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [2]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1">#low_memory=False used to remove the warning for mixed data types in each columns</span>
<span class="n">banking_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'./cleaned_banking_data.csv'</span><span class="p">,</span> <span class="n">low_memory</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">banking_data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[2]:</div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html" tabindex="0">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>age</th>
<th>education</th>
<th>housing</th>
<th>loan</th>
<th>campaign</th>
<th>pdays</th>
<th>previous</th>
<th>poutcome</th>
<th>emp.var.rate</th>
<th>cons.price.idx</th>
<th>...</th>
<th>entrepreneur</th>
<th>housemaid</th>
<th>management</th>
<th>retired</th>
<th>self-employed</th>
<th>services</th>
<th>student</th>
<th>technician</th>
<th>unemployed</th>
<th>output</th>
</tr>
</thead>
<tbody>
<tr>
<th>0</th>
<td>36</td>
<td>2</td>
<td>0</td>
<td>0</td>
<td>9</td>
<td>999</td>
<td>0</td>
<td>0</td>
<td>1.4</td>
<td>93.444</td>
<td>...</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<th>1</th>
<td>43</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>2</td>
<td>999</td>
<td>0</td>
<td>0</td>
<td>1.1</td>
<td>93.994</td>
<td>...</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<th>2</th>
<td>27</td>
<td>4</td>
<td>1</td>
<td>0</td>
<td>3</td>
<td>999</td>
<td>0</td>
<td>0</td>
<td>1.4</td>
<td>93.918</td>
<td>...</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<th>3</th>
<td>28</td>
<td>3</td>
<td>1</td>
<td>0</td>
<td>3</td>
<td>999</td>
<td>0</td>
<td>0</td>
<td>1.1</td>
<td>93.994</td>
<td>...</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<th>4</th>
<td>25</td>
<td>4</td>
<td>1</td>
<td>0</td>
<td>5</td>
<td>999</td>
<td>0</td>
<td>0</td>
<td>1.4</td>
<td>93.918</td>
<td>...</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>5 rows × 27 columns</p>
</div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=315a683f-126a-402f-a2c8-0d65b4344f91">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [3]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">banking_data</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[3]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>(8516, 27)</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=dbafb1ed-b321-45f9-baa4-dff7bb662ce2">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="4.2-Find-and-Evaluate-the-Accuracy-of-the-Model-for-each-Feature">4.2 Find and Evaluate the Accuracy of the Model for each Feature<a class="anchor-link" href="#4.2-Find-and-Evaluate-the-Accuracy-of-the-Model-for-each-Feature">¶</a></h4><p>To determine which features might not be good features for our model and could be adding bias or noise I will first build 26 models, each with only one input feature to see what the model's validation accuracy is for that feature.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=fe4e2784-2803-40ad-8f7e-54d458da1407">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [4]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Shuffle Data</span>
<span class="n">banking_data</span> <span class="o">=</span> <span class="n">banking_data</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">frac</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1">#Get output data</span>
<span class="n">X</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">banking_data</span><span class="p">[</span><span class="s1">'output'</span><span class="p">]</span>
<span class="n">index_20</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.2</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">index</span><span class="p">))</span>  
<span class="c1"># Split y into validation (20%) and training (80%)</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:</span><span class="n">index_20</span><span class="p">]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">index_20</span><span class="p">:]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">" y_train  has shape: </span><span class="si">{</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"y_test has shape: </span><span class="si">{</span><span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre> y_train  has shape: (6813,)
y_test has shape: (1703,)
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=9bc9cdfd-bc79-455f-82f0-def5f6e50727">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [8]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">'Feature'</span><span class="p">,</span> <span class="s1">'Validation_Accuracy'</span><span class="p">])</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">banking_data</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">'output'</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="s1">'columns'</span><span class="p">)</span>

<span class="k">for</span> <span class="n">feature</span> <span class="ow">in</span> <span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Processing feature: </span><span class="si">{</span><span class="n">feature</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="n">feature_X</span> <span class="o">=</span> <span class="n">banking_data</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="p">[</span><span class="n">feature</span><span class="p">]]</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"data frame x has shape: </span><span class="si">{</span><span class="n">feature_X</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    
    <span class="c1"># Split x into validation (20%) and training (80%)</span>
    <span class="n">X_train</span> <span class="o">=</span> <span class="n">feature_X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">index_20</span><span class="p">:,</span> <span class="p">:]</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="n">feature_X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:</span><span class="n">index_20</span><span class="p">,</span> <span class="p">:]</span>
    <span class="nb">min</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> 
    <span class="nb">max</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> 
    <span class="n">X_train</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_train</span> <span class="o">-</span> <span class="nb">min</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="nb">max</span> <span class="o">-</span> <span class="nb">min</span><span class="p">)</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_test</span> <span class="o">-</span> <span class="nb">min</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="nb">max</span> <span class="o">-</span> <span class="nb">min</span><span class="p">)</span>

    <span class="n">feature_regression_model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
    <span class="n">feature_regression_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">columns</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'sigmoid'</span><span class="p">))</span>

    <span class="n">feature_regression_model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">'binary_crossentropy'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s1">'adam'</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'accuracy'</span><span class="p">])</span>
    <span class="n">callback_a</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">filepath</span> <span class="o">=</span> <span class="s1">'feature_best.hdf5'</span><span class="p">,</span> <span class="n">monitor</span><span class="o">=</span><span class="s1">'val_loss'</span><span class="p">,</span> <span class="n">save_best_only</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">save_weights_only</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">verbose</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">callback_b</span> <span class="o">=</span> <span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s1">'val_loss'</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">'min'</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">history_regression_feature</span> <span class="o">=</span> <span class="n">feature_regression_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">callback_a</span><span class="p">,</span> <span class="n">callback_b</span><span class="p">])</span>
    <span class="n">feature_regression_model</span><span class="o">.</span><span class="n">load_weights</span><span class="p">(</span><span class="s1">'feature_best.hdf5'</span><span class="p">)</span>
    <span class="n">regression_model_scores</span> <span class="o">=</span>  <span class="n">feature_regression_model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="p">(</span><span class="n">regression_model_scores</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">results</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">index</span><span class="p">)]</span> <span class="o">=</span> <span class="p">[</span><span class="n">feature</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">]</span> 
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Processing feature: age
data frame x has shape: (8516, 1)
Epoch 1/60
653/682 [===========================&gt;..] - ETA: 0s - loss: 0.6944 - accuracy: 0.5181
Epoch 1: val_loss improved from inf to 0.69332, saving model to feature_best.hdf5
682/682 [==============================] - 1s 1ms/step - loss: 0.6944 - accuracy: 0.5196 - val_loss: 0.6933 - val_accuracy: 0.5549
Epoch 2/60
648/682 [===========================&gt;..] - ETA: 0s - loss: 0.6938 - accuracy: 0.5116
Epoch 2: val_loss improved from 0.69332 to 0.69326, saving model to feature_best.hdf5
682/682 [==============================] - 1s 1ms/step - loss: 0.6938 - accuracy: 0.5096 - val_loss: 0.6933 - val_accuracy: 0.4833
Epoch 3/60
682/682 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4876
Epoch 3: val_loss improved from 0.69326 to 0.69325, saving model to feature_best.hdf5
682/682 [==============================] - 1s 1ms/step - loss: 0.6936 - accuracy: 0.4876 - val_loss: 0.6933 - val_accuracy: 0.4809
Epoch 4/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.6934 - accuracy: 0.4873
Epoch 4: val_loss improved from 0.69325 to 0.69320, saving model to feature_best.hdf5
682/682 [==============================] - 1s 1ms/step - loss: 0.6934 - accuracy: 0.4870 - val_loss: 0.6932 - val_accuracy: 0.4991
Epoch 5/60
649/682 [===========================&gt;..] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000
Epoch 5: val_loss improved from 0.69320 to 0.69319, saving model to feature_best.hdf5
682/682 [==============================] - 1s 1ms/step - loss: 0.6933 - accuracy: 0.5002 - val_loss: 0.6932 - val_accuracy: 0.4991
Epoch 6/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5021
Epoch 6: val_loss improved from 0.69319 to 0.69317, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6931 - accuracy: 0.5021 - val_loss: 0.6932 - val_accuracy: 0.4985
Epoch 7/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.6930 - accuracy: 0.4909
Epoch 7: val_loss did not improve from 0.69317
682/682 [==============================] - 1s 1ms/step - loss: 0.6930 - accuracy: 0.4905 - val_loss: 0.6932 - val_accuracy: 0.4885
Epoch 8/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.6929 - accuracy: 0.4835
Epoch 8: val_loss improved from 0.69317 to 0.69314, saving model to feature_best.hdf5
682/682 [==============================] - 1s 1ms/step - loss: 0.6929 - accuracy: 0.4844 - val_loss: 0.6931 - val_accuracy: 0.4427
Epoch 9/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.6928 - accuracy: 0.4837
Epoch 9: val_loss did not improve from 0.69314
682/682 [==============================] - 1s 1ms/step - loss: 0.6928 - accuracy: 0.4825 - val_loss: 0.6931 - val_accuracy: 0.4563
Epoch 10/60
653/682 [===========================&gt;..] - ETA: 0s - loss: 0.6927 - accuracy: 0.4779
Epoch 10: val_loss did not improve from 0.69314
682/682 [==============================] - 1s 1ms/step - loss: 0.6927 - accuracy: 0.4776 - val_loss: 0.6932 - val_accuracy: 0.4451
Epoch 11/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.6926 - accuracy: 0.4886
Epoch 11: val_loss did not improve from 0.69314
682/682 [==============================] - 1s 1ms/step - loss: 0.6926 - accuracy: 0.4882 - val_loss: 0.6932 - val_accuracy: 0.4427
Epoch 12/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.6925 - accuracy: 0.4832
Epoch 12: val_loss did not improve from 0.69314
682/682 [==============================] - 1s 1ms/step - loss: 0.6925 - accuracy: 0.4817 - val_loss: 0.6932 - val_accuracy: 0.4427
Epoch 13/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.6925 - accuracy: 0.4843
Epoch 13: val_loss did not improve from 0.69314
682/682 [==============================] - 1s 1ms/step - loss: 0.6925 - accuracy: 0.4850 - val_loss: 0.6932 - val_accuracy: 0.4645
Epoch 14/60
649/682 [===========================&gt;..] - ETA: 0s - loss: 0.6924 - accuracy: 0.4847
Epoch 14: val_loss did not improve from 0.69314
682/682 [==============================] - 1s 1ms/step - loss: 0.6924 - accuracy: 0.4847 - val_loss: 0.6932 - val_accuracy: 0.4756
Epoch 15/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.6924 - accuracy: 0.4931
Epoch 15: val_loss did not improve from 0.69314
682/682 [==============================] - 1s 1ms/step - loss: 0.6924 - accuracy: 0.4951 - val_loss: 0.6932 - val_accuracy: 0.4885
Epoch 16/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.6923 - accuracy: 0.4928
Epoch 16: val_loss did not improve from 0.69314
682/682 [==============================] - 1s 1ms/step - loss: 0.6923 - accuracy: 0.4930 - val_loss: 0.6933 - val_accuracy: 0.4610
Epoch 17/60
654/682 [===========================&gt;..] - ETA: 0s - loss: 0.6923 - accuracy: 0.4951
Epoch 17: val_loss did not improve from 0.69314
682/682 [==============================] - 1s 2ms/step - loss: 0.6923 - accuracy: 0.4951 - val_loss: 0.6934 - val_accuracy: 0.4427
Epoch 18/60
641/682 [===========================&gt;..] - ETA: 0s - loss: 0.6923 - accuracy: 0.4819
Epoch 18: val_loss did not improve from 0.69314
682/682 [==============================] - 1s 2ms/step - loss: 0.6923 - accuracy: 0.4838 - val_loss: 0.6934 - val_accuracy: 0.4610
Epoch 19/60
654/682 [===========================&gt;..] - ETA: 0s - loss: 0.6923 - accuracy: 0.4864
Epoch 19: val_loss did not improve from 0.69314
682/682 [==============================] - 1s 1ms/step - loss: 0.6922 - accuracy: 0.4870 - val_loss: 0.6934 - val_accuracy: 0.4645
Epoch 20/60
642/682 [===========================&gt;..] - ETA: 0s - loss: 0.6921 - accuracy: 0.4952
Epoch 20: val_loss did not improve from 0.69314
682/682 [==============================] - 1s 1ms/step - loss: 0.6922 - accuracy: 0.4938 - val_loss: 0.6934 - val_accuracy: 0.4610
Epoch 21/60
640/682 [===========================&gt;..] - ETA: 0s - loss: 0.6920 - accuracy: 0.4863
Epoch 21: val_loss did not improve from 0.69314
682/682 [==============================] - 1s 1ms/step - loss: 0.6922 - accuracy: 0.4854 - val_loss: 0.6934 - val_accuracy: 0.4645
Epoch 22/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.6921 - accuracy: 0.4913
Epoch 22: val_loss did not improve from 0.69314
682/682 [==============================] - 1s 2ms/step - loss: 0.6922 - accuracy: 0.4914 - val_loss: 0.6935 - val_accuracy: 0.4610
Epoch 23/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.6921 - accuracy: 0.4883
Epoch 23: val_loss did not improve from 0.69314
682/682 [==============================] - 1s 2ms/step - loss: 0.6921 - accuracy: 0.4880 - val_loss: 0.6935 - val_accuracy: 0.4885
Epoch 24/60
682/682 [==============================] - ETA: 0s - loss: 0.6921 - accuracy: 0.4961
Epoch 24: val_loss did not improve from 0.69314
682/682 [==============================] - 1s 1ms/step - loss: 0.6921 - accuracy: 0.4961 - val_loss: 0.6935 - val_accuracy: 0.4756
Epoch 25/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.6921 - accuracy: 0.4914
Epoch 25: val_loss did not improve from 0.69314
682/682 [==============================] - 1s 1ms/step - loss: 0.6921 - accuracy: 0.4911 - val_loss: 0.6935 - val_accuracy: 0.4756
Epoch 26/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.6921 - accuracy: 0.4836
Epoch 26: val_loss did not improve from 0.69314
682/682 [==============================] - 1s 1ms/step - loss: 0.6921 - accuracy: 0.4836 - val_loss: 0.6935 - val_accuracy: 0.4885
Epoch 27/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.6920 - accuracy: 0.4930
Epoch 27: val_loss did not improve from 0.69314
682/682 [==============================] - 1s 1ms/step - loss: 0.6921 - accuracy: 0.4919 - val_loss: 0.6936 - val_accuracy: 0.4885
Epoch 28/60
642/682 [===========================&gt;..] - ETA: 0s - loss: 0.6921 - accuracy: 0.5023
Epoch 28: val_loss did not improve from 0.69314
682/682 [==============================] - 1s 1ms/step - loss: 0.6920 - accuracy: 0.5026 - val_loss: 0.6936 - val_accuracy: 0.4645
Epoch 28: early stopping
54/54 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.4427
Processing feature: education
data frame x has shape: (8516, 1)
Epoch 1/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.6940 - accuracy: 0.5011
Epoch 1: val_loss improved from inf to 0.68992, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6938 - accuracy: 0.5029 - val_loss: 0.6899 - val_accuracy: 0.5161
Epoch 2/60
646/682 [===========================&gt;..] - ETA: 0s - loss: 0.6906 - accuracy: 0.5356
Epoch 2: val_loss improved from 0.68992 to 0.68937, saving model to feature_best.hdf5
682/682 [==============================] - 1s 1ms/step - loss: 0.6906 - accuracy: 0.5362 - val_loss: 0.6894 - val_accuracy: 0.5490
Epoch 3/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.6905 - accuracy: 0.5372
Epoch 3: val_loss improved from 0.68937 to 0.68913, saving model to feature_best.hdf5
682/682 [==============================] - 1s 1ms/step - loss: 0.6904 - accuracy: 0.5387 - val_loss: 0.6891 - val_accuracy: 0.5490
Epoch 4/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.6901 - accuracy: 0.5386
Epoch 4: val_loss improved from 0.68913 to 0.68891, saving model to feature_best.hdf5
682/682 [==============================] - 1s 1ms/step - loss: 0.6901 - accuracy: 0.5375 - val_loss: 0.6889 - val_accuracy: 0.5590
Epoch 5/60
648/682 [===========================&gt;..] - ETA: 0s - loss: 0.6901 - accuracy: 0.5485
Epoch 5: val_loss improved from 0.68891 to 0.68874, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6901 - accuracy: 0.5490 - val_loss: 0.6887 - val_accuracy: 0.5590
Epoch 6/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.6900 - accuracy: 0.5375
Epoch 6: val_loss improved from 0.68874 to 0.68856, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6899 - accuracy: 0.5379 - val_loss: 0.6886 - val_accuracy: 0.5590
Epoch 7/60
651/682 [===========================&gt;..] - ETA: 0s - loss: 0.6896 - accuracy: 0.5481
Epoch 7: val_loss improved from 0.68856 to 0.68840, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6899 - accuracy: 0.5462 - val_loss: 0.6884 - val_accuracy: 0.5590
Epoch 8/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.6899 - accuracy: 0.5479
Epoch 8: val_loss improved from 0.68840 to 0.68825, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6898 - accuracy: 0.5491 - val_loss: 0.6883 - val_accuracy: 0.5590
Epoch 9/60
649/682 [===========================&gt;..] - ETA: 0s - loss: 0.6893 - accuracy: 0.5455
Epoch 9: val_loss improved from 0.68825 to 0.68813, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6897 - accuracy: 0.5445 - val_loss: 0.6881 - val_accuracy: 0.5590
Epoch 10/60
652/682 [===========================&gt;..] - ETA: 0s - loss: 0.6897 - accuracy: 0.5396
Epoch 10: val_loss improved from 0.68813 to 0.68802, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6896 - accuracy: 0.5404 - val_loss: 0.6880 - val_accuracy: 0.5590
Epoch 11/60
642/682 [===========================&gt;..] - ETA: 0s - loss: 0.6893 - accuracy: 0.5512
Epoch 11: val_loss improved from 0.68802 to 0.68790, saving model to feature_best.hdf5
682/682 [==============================] - 1s 1ms/step - loss: 0.6896 - accuracy: 0.5503 - val_loss: 0.6879 - val_accuracy: 0.5590
Epoch 12/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.6895 - accuracy: 0.5482
Epoch 12: val_loss improved from 0.68790 to 0.68788, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6895 - accuracy: 0.5478 - val_loss: 0.6879 - val_accuracy: 0.5490
Epoch 13/60
650/682 [===========================&gt;..] - ETA: 0s - loss: 0.6893 - accuracy: 0.5508
Epoch 13: val_loss improved from 0.68788 to 0.68773, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6894 - accuracy: 0.5506 - val_loss: 0.6877 - val_accuracy: 0.5590
Epoch 14/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.6894 - accuracy: 0.5429
Epoch 14: val_loss improved from 0.68773 to 0.68766, saving model to feature_best.hdf5
682/682 [==============================] - 1s 1ms/step - loss: 0.6894 - accuracy: 0.5429 - val_loss: 0.6877 - val_accuracy: 0.5590
Epoch 15/60
648/682 [===========================&gt;..] - ETA: 0s - loss: 0.6897 - accuracy: 0.5522
Epoch 15: val_loss improved from 0.68766 to 0.68758, saving model to feature_best.hdf5
682/682 [==============================] - 1s 1ms/step - loss: 0.6894 - accuracy: 0.5522 - val_loss: 0.6876 - val_accuracy: 0.5590
Epoch 16/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.6894 - accuracy: 0.5519
Epoch 16: val_loss improved from 0.68758 to 0.68757, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6893 - accuracy: 0.5514 - val_loss: 0.6876 - val_accuracy: 0.5490
Epoch 17/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.6893 - accuracy: 0.5510
Epoch 17: val_loss improved from 0.68757 to 0.68745, saving model to feature_best.hdf5
682/682 [==============================] - 1s 1ms/step - loss: 0.6893 - accuracy: 0.5513 - val_loss: 0.6874 - val_accuracy: 0.5590
Epoch 18/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.6893 - accuracy: 0.5488
Epoch 18: val_loss improved from 0.68745 to 0.68738, saving model to feature_best.hdf5
682/682 [==============================] - 1s 1ms/step - loss: 0.6893 - accuracy: 0.5488 - val_loss: 0.6874 - val_accuracy: 0.5590
Epoch 19/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.6891 - accuracy: 0.5498
Epoch 19: val_loss improved from 0.68738 to 0.68733, saving model to feature_best.hdf5
682/682 [==============================] - 1s 1ms/step - loss: 0.6893 - accuracy: 0.5487 - val_loss: 0.6873 - val_accuracy: 0.5590
Epoch 20/60
642/682 [===========================&gt;..] - ETA: 0s - loss: 0.6891 - accuracy: 0.5533
Epoch 20: val_loss improved from 0.68733 to 0.68730, saving model to feature_best.hdf5
682/682 [==============================] - 1s 1ms/step - loss: 0.6892 - accuracy: 0.5507 - val_loss: 0.6873 - val_accuracy: 0.5590
Epoch 21/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.6891 - accuracy: 0.5495
Epoch 21: val_loss improved from 0.68730 to 0.68724, saving model to feature_best.hdf5
682/682 [==============================] - 1s 1ms/step - loss: 0.6892 - accuracy: 0.5482 - val_loss: 0.6872 - val_accuracy: 0.5590
Epoch 22/60
682/682 [==============================] - ETA: 0s - loss: 0.6892 - accuracy: 0.5482
Epoch 22: val_loss improved from 0.68724 to 0.68720, saving model to feature_best.hdf5
682/682 [==============================] - 1s 1ms/step - loss: 0.6892 - accuracy: 0.5482 - val_loss: 0.6872 - val_accuracy: 0.5590
Epoch 23/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.6891 - accuracy: 0.5528
Epoch 23: val_loss improved from 0.68720 to 0.68717, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6892 - accuracy: 0.5528 - val_loss: 0.6872 - val_accuracy: 0.5590
Epoch 24/60
648/682 [===========================&gt;..] - ETA: 0s - loss: 0.6893 - accuracy: 0.5502
Epoch 24: val_loss improved from 0.68717 to 0.68713, saving model to feature_best.hdf5
682/682 [==============================] - 1s 1ms/step - loss: 0.6892 - accuracy: 0.5522 - val_loss: 0.6871 - val_accuracy: 0.5590
Epoch 25/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.6891 - accuracy: 0.5523
Epoch 25: val_loss did not improve from 0.68713
682/682 [==============================] - 1s 2ms/step - loss: 0.6891 - accuracy: 0.5522 - val_loss: 0.6871 - val_accuracy: 0.5590
Epoch 26/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.6889 - accuracy: 0.5501
Epoch 26: val_loss improved from 0.68713 to 0.68707, saving model to feature_best.hdf5
682/682 [==============================] - 1s 1ms/step - loss: 0.6892 - accuracy: 0.5484 - val_loss: 0.6871 - val_accuracy: 0.5590
Epoch 27/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.6890 - accuracy: 0.5551
Epoch 27: val_loss did not improve from 0.68707
682/682 [==============================] - 1s 2ms/step - loss: 0.6890 - accuracy: 0.5548 - val_loss: 0.6872 - val_accuracy: 0.5490
Epoch 28/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.6893 - accuracy: 0.5452
Epoch 28: val_loss improved from 0.68707 to 0.68702, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6892 - accuracy: 0.5466 - val_loss: 0.6870 - val_accuracy: 0.5590
Epoch 29/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.6889 - accuracy: 0.5531
Epoch 29: val_loss improved from 0.68702 to 0.68700, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6891 - accuracy: 0.5522 - val_loss: 0.6870 - val_accuracy: 0.5590
Epoch 30/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.6891 - accuracy: 0.5515
Epoch 30: val_loss improved from 0.68700 to 0.68698, saving model to feature_best.hdf5
682/682 [==============================] - 1s 1ms/step - loss: 0.6891 - accuracy: 0.5506 - val_loss: 0.6870 - val_accuracy: 0.5590
Epoch 31/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.6892 - accuracy: 0.5525
Epoch 31: val_loss improved from 0.68698 to 0.68696, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6891 - accuracy: 0.5522 - val_loss: 0.6870 - val_accuracy: 0.5590
Epoch 32/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.6890 - accuracy: 0.5530
Epoch 32: val_loss improved from 0.68696 to 0.68694, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6891 - accuracy: 0.5522 - val_loss: 0.6869 - val_accuracy: 0.5590
Epoch 33/60
653/682 [===========================&gt;..] - ETA: 0s - loss: 0.6888 - accuracy: 0.5528
Epoch 33: val_loss improved from 0.68694 to 0.68692, saving model to feature_best.hdf5
682/682 [==============================] - 1s 1ms/step - loss: 0.6891 - accuracy: 0.5522 - val_loss: 0.6869 - val_accuracy: 0.5590
Epoch 34/60
652/682 [===========================&gt;..] - ETA: 0s - loss: 0.6885 - accuracy: 0.5563
Epoch 34: val_loss improved from 0.68692 to 0.68691, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6891 - accuracy: 0.5522 - val_loss: 0.6869 - val_accuracy: 0.5590
Epoch 35/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.6892 - accuracy: 0.5516
Epoch 35: val_loss improved from 0.68691 to 0.68690, saving model to feature_best.hdf5
682/682 [==============================] - 1s 1ms/step - loss: 0.6891 - accuracy: 0.5522 - val_loss: 0.6869 - val_accuracy: 0.5590
Epoch 36/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.6891 - accuracy: 0.5520
Epoch 36: val_loss improved from 0.68690 to 0.68688, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6891 - accuracy: 0.5522 - val_loss: 0.6869 - val_accuracy: 0.5590
Epoch 37/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.6891 - accuracy: 0.5497
Epoch 37: val_loss improved from 0.68688 to 0.68687, saving model to feature_best.hdf5
682/682 [==============================] - 1s 1ms/step - loss: 0.6891 - accuracy: 0.5506 - val_loss: 0.6869 - val_accuracy: 0.5590
Epoch 38/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.6893 - accuracy: 0.5512
Epoch 38: val_loss improved from 0.68687 to 0.68686, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6891 - accuracy: 0.5525 - val_loss: 0.6869 - val_accuracy: 0.5590
Epoch 39/60
649/682 [===========================&gt;..] - ETA: 0s - loss: 0.6890 - accuracy: 0.5541
Epoch 39: val_loss improved from 0.68686 to 0.68684, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6891 - accuracy: 0.5522 - val_loss: 0.6868 - val_accuracy: 0.5590
Epoch 40/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.6894 - accuracy: 0.5505
Epoch 40: val_loss improved from 0.68684 to 0.68684, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6891 - accuracy: 0.5522 - val_loss: 0.6868 - val_accuracy: 0.5590
Epoch 41/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.6891 - accuracy: 0.5484
Epoch 41: val_loss did not improve from 0.68684
682/682 [==============================] - 1s 2ms/step - loss: 0.6891 - accuracy: 0.5482 - val_loss: 0.6869 - val_accuracy: 0.5590
Epoch 42/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.6892 - accuracy: 0.5510
Epoch 42: val_loss improved from 0.68684 to 0.68683, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6891 - accuracy: 0.5522 - val_loss: 0.6868 - val_accuracy: 0.5590
Epoch 43/60
648/682 [===========================&gt;..] - ETA: 0s - loss: 0.6889 - accuracy: 0.5537
Epoch 43: val_loss improved from 0.68683 to 0.68681, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6891 - accuracy: 0.5522 - val_loss: 0.6868 - val_accuracy: 0.5590
Epoch 44/60
649/682 [===========================&gt;..] - ETA: 0s - loss: 0.6888 - accuracy: 0.5522
Epoch 44: val_loss improved from 0.68681 to 0.68680, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6891 - accuracy: 0.5522 - val_loss: 0.6868 - val_accuracy: 0.5590
Epoch 45/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.6892 - accuracy: 0.5501
Epoch 45: val_loss improved from 0.68680 to 0.68679, saving model to feature_best.hdf5
682/682 [==============================] - 1s 1ms/step - loss: 0.6891 - accuracy: 0.5506 - val_loss: 0.6868 - val_accuracy: 0.5590
Epoch 46/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.6894 - accuracy: 0.5502
Epoch 46: val_loss did not improve from 0.68679
682/682 [==============================] - 1s 1ms/step - loss: 0.6891 - accuracy: 0.5522 - val_loss: 0.6868 - val_accuracy: 0.5590
Epoch 47/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.6889 - accuracy: 0.5534
Epoch 47: val_loss improved from 0.68679 to 0.68679, saving model to feature_best.hdf5
682/682 [==============================] - 1s 1ms/step - loss: 0.6891 - accuracy: 0.5522 - val_loss: 0.6868 - val_accuracy: 0.5590
Epoch 48/60
652/682 [===========================&gt;..] - ETA: 0s - loss: 0.6890 - accuracy: 0.5520
Epoch 48: val_loss improved from 0.68679 to 0.68678, saving model to feature_best.hdf5
682/682 [==============================] - 1s 1ms/step - loss: 0.6891 - accuracy: 0.5522 - val_loss: 0.6868 - val_accuracy: 0.5590
Epoch 49/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.6890 - accuracy: 0.5519
Epoch 49: val_loss improved from 0.68678 to 0.68677, saving model to feature_best.hdf5
682/682 [==============================] - 1s 1ms/step - loss: 0.6890 - accuracy: 0.5522 - val_loss: 0.6868 - val_accuracy: 0.5590
Epoch 50/60
645/682 [===========================&gt;..] - ETA: 0s - loss: 0.6891 - accuracy: 0.5522
Epoch 50: val_loss improved from 0.68677 to 0.68676, saving model to feature_best.hdf5
682/682 [==============================] - 1s 1ms/step - loss: 0.6891 - accuracy: 0.5522 - val_loss: 0.6868 - val_accuracy: 0.5590
Epoch 51/60
644/682 [===========================&gt;..] - ETA: 0s - loss: 0.6888 - accuracy: 0.5542
Epoch 51: val_loss did not improve from 0.68676
682/682 [==============================] - 1s 2ms/step - loss: 0.6891 - accuracy: 0.5522 - val_loss: 0.6868 - val_accuracy: 0.5590
Epoch 52/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.6891 - accuracy: 0.5458
Epoch 52: val_loss improved from 0.68676 to 0.68676, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6891 - accuracy: 0.5467 - val_loss: 0.6868 - val_accuracy: 0.5590
Epoch 53/60
653/682 [===========================&gt;..] - ETA: 0s - loss: 0.6889 - accuracy: 0.5492
Epoch 53: val_loss improved from 0.68676 to 0.68675, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6891 - accuracy: 0.5495 - val_loss: 0.6868 - val_accuracy: 0.5590
Epoch 54/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.6894 - accuracy: 0.5500
Epoch 54: val_loss did not improve from 0.68675
682/682 [==============================] - 2s 2ms/step - loss: 0.6891 - accuracy: 0.5522 - val_loss: 0.6868 - val_accuracy: 0.5590
Epoch 55/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.6889 - accuracy: 0.5534
Epoch 55: val_loss did not improve from 0.68675
682/682 [==============================] - 2s 2ms/step - loss: 0.6891 - accuracy: 0.5522 - val_loss: 0.6868 - val_accuracy: 0.5590
Epoch 56/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.6890 - accuracy: 0.5529
Epoch 56: val_loss improved from 0.68675 to 0.68674, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6891 - accuracy: 0.5522 - val_loss: 0.6867 - val_accuracy: 0.5590
Epoch 57/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.6888 - accuracy: 0.5536
Epoch 57: val_loss did not improve from 0.68674
682/682 [==============================] - 1s 2ms/step - loss: 0.6891 - accuracy: 0.5522 - val_loss: 0.6867 - val_accuracy: 0.5590
Epoch 58/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.6888 - accuracy: 0.5516
Epoch 58: val_loss improved from 0.68674 to 0.68673, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6891 - accuracy: 0.5513 - val_loss: 0.6867 - val_accuracy: 0.5590
Epoch 59/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.6894 - accuracy: 0.5510
Epoch 59: val_loss did not improve from 0.68673
682/682 [==============================] - 2s 2ms/step - loss: 0.6891 - accuracy: 0.5522 - val_loss: 0.6867 - val_accuracy: 0.5590
Epoch 60/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.6892 - accuracy: 0.5495
Epoch 60: val_loss did not improve from 0.68673
682/682 [==============================] - 2s 2ms/step - loss: 0.6891 - accuracy: 0.5504 - val_loss: 0.6867 - val_accuracy: 0.5590
54/54 [==============================] - 0s 2ms/step - loss: 0.6867 - accuracy: 0.5590
Processing feature: housing
data frame x has shape: (8516, 1)
Epoch 1/60
682/682 [==============================] - ETA: 0s - loss: 0.6984 - accuracy: 0.4891
Epoch 1: val_loss improved from inf to 0.69514, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6984 - accuracy: 0.4891 - val_loss: 0.6951 - val_accuracy: 0.4927
Epoch 2/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.6950 - accuracy: 0.4886
Epoch 2: val_loss improved from 0.69514 to 0.69410, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6949 - accuracy: 0.4897 - val_loss: 0.6941 - val_accuracy: 0.4927
Epoch 3/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.6941 - accuracy: 0.4887
Epoch 3: val_loss improved from 0.69410 to 0.69366, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6941 - accuracy: 0.4886 - val_loss: 0.6937 - val_accuracy: 0.4927
Epoch 4/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.6937 - accuracy: 0.4972
Epoch 4: val_loss improved from 0.69366 to 0.69342, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6937 - accuracy: 0.4971 - val_loss: 0.6934 - val_accuracy: 0.4991
Epoch 5/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.6935 - accuracy: 0.4920
Epoch 5: val_loss improved from 0.69342 to 0.69325, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6934 - accuracy: 0.4941 - val_loss: 0.6932 - val_accuracy: 0.4991
Epoch 6/60
654/682 [===========================&gt;..] - ETA: 0s - loss: 0.6933 - accuracy: 0.4920
Epoch 6: val_loss improved from 0.69325 to 0.69314, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6933 - accuracy: 0.4941 - val_loss: 0.6931 - val_accuracy: 0.4991
Epoch 7/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.6932 - accuracy: 0.5005
Epoch 7: val_loss improved from 0.69314 to 0.69311, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.4999 - val_loss: 0.6931 - val_accuracy: 0.5073
Epoch 8/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.6932 - accuracy: 0.5047
Epoch 8: val_loss did not improve from 0.69311
682/682 [==============================] - 1s 2ms/step - loss: 0.6931 - accuracy: 0.5062 - val_loss: 0.6931 - val_accuracy: 0.5073
Epoch 9/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5089
Epoch 9: val_loss improved from 0.69311 to 0.69304, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6931 - accuracy: 0.5092 - val_loss: 0.6930 - val_accuracy: 0.5073
Epoch 10/60
654/682 [===========================&gt;..] - ETA: 0s - loss: 0.6931 - accuracy: 0.5078
Epoch 10: val_loss did not improve from 0.69304
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5089 - val_loss: 0.6931 - val_accuracy: 0.5073
Epoch 11/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.6931 - accuracy: 0.5108
Epoch 11: val_loss improved from 0.69304 to 0.69304, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5103 - val_loss: 0.6930 - val_accuracy: 0.5073
Epoch 12/60
652/682 [===========================&gt;..] - ETA: 0s - loss: 0.6930 - accuracy: 0.5123
Epoch 12: val_loss did not improve from 0.69304
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5103 - val_loss: 0.6931 - val_accuracy: 0.5073
Epoch 13/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5097
Epoch 13: val_loss improved from 0.69304 to 0.69303, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6931 - accuracy: 0.5103 - val_loss: 0.6930 - val_accuracy: 0.5073
Epoch 14/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5107
Epoch 14: val_loss did not improve from 0.69303
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5103 - val_loss: 0.6930 - val_accuracy: 0.5073
Epoch 15/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5104
Epoch 15: val_loss did not improve from 0.69303
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5103 - val_loss: 0.6930 - val_accuracy: 0.5073
Epoch 16/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5106
Epoch 16: val_loss did not improve from 0.69303
682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.5103 - val_loss: 0.6930 - val_accuracy: 0.5073
Epoch 17/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.6931 - accuracy: 0.5099
Epoch 17: val_loss did not improve from 0.69303
682/682 [==============================] - 1s 2ms/step - loss: 0.6931 - accuracy: 0.5103 - val_loss: 0.6930 - val_accuracy: 0.5073
Epoch 18/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5111
Epoch 18: val_loss did not improve from 0.69303
682/682 [==============================] - 1s 2ms/step - loss: 0.6930 - accuracy: 0.5103 - val_loss: 0.6930 - val_accuracy: 0.5073
Epoch 19/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5094
Epoch 19: val_loss did not improve from 0.69303
682/682 [==============================] - 1s 2ms/step - loss: 0.6930 - accuracy: 0.5103 - val_loss: 0.6930 - val_accuracy: 0.5073
Epoch 20/60
682/682 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5103
Epoch 20: val_loss did not improve from 0.69303
682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.5103 - val_loss: 0.6931 - val_accuracy: 0.5073
Epoch 21/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5103
Epoch 21: val_loss did not improve from 0.69303
682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.5103 - val_loss: 0.6931 - val_accuracy: 0.5073
Epoch 22/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5103
Epoch 22: val_loss did not improve from 0.69303
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5103 - val_loss: 0.6931 - val_accuracy: 0.5073
Epoch 23/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.6931 - accuracy: 0.5081
Epoch 23: val_loss did not improve from 0.69303
682/682 [==============================] - 1s 2ms/step - loss: 0.6930 - accuracy: 0.5103 - val_loss: 0.6930 - val_accuracy: 0.5073
Epoch 24/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5060
Epoch 24: val_loss did not improve from 0.69303
682/682 [==============================] - 1s 2ms/step - loss: 0.6930 - accuracy: 0.5067 - val_loss: 0.6932 - val_accuracy: 0.5073
Epoch 25/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5108
Epoch 25: val_loss did not improve from 0.69303
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5103 - val_loss: 0.6931 - val_accuracy: 0.5073
Epoch 26/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5108
Epoch 26: val_loss did not improve from 0.69303
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5103 - val_loss: 0.6930 - val_accuracy: 0.5073
Epoch 27/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.6931 - accuracy: 0.5090
Epoch 27: val_loss did not improve from 0.69303
682/682 [==============================] - 1s 2ms/step - loss: 0.6931 - accuracy: 0.5103 - val_loss: 0.6930 - val_accuracy: 0.5073
Epoch 28/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5072
Epoch 28: val_loss did not improve from 0.69303
682/682 [==============================] - 1s 2ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6931 - val_accuracy: 0.5073
Epoch 29/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5094
Epoch 29: val_loss did not improve from 0.69303
682/682 [==============================] - 1s 2ms/step - loss: 0.6930 - accuracy: 0.5103 - val_loss: 0.6930 - val_accuracy: 0.5073
Epoch 30/60
654/682 [===========================&gt;..] - ETA: 0s - loss: 0.6929 - accuracy: 0.5135
Epoch 30: val_loss did not improve from 0.69303
682/682 [==============================] - 1s 2ms/step - loss: 0.6930 - accuracy: 0.5103 - val_loss: 0.6931 - val_accuracy: 0.5073
Epoch 31/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.6930 - accuracy: 0.5123
Epoch 31: val_loss did not improve from 0.69303
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5103 - val_loss: 0.6931 - val_accuracy: 0.5073
Epoch 32/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.6930 - accuracy: 0.5115
Epoch 32: val_loss did not improve from 0.69303
682/682 [==============================] - 1s 2ms/step - loss: 0.6930 - accuracy: 0.5103 - val_loss: 0.6930 - val_accuracy: 0.5073
Epoch 33/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5106
Epoch 33: val_loss did not improve from 0.69303
682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.5103 - val_loss: 0.6931 - val_accuracy: 0.5073
Epoch 33: early stopping
54/54 [==============================] - 0s 2ms/step - loss: 0.6930 - accuracy: 0.5073
Processing feature: loan
data frame x has shape: (8516, 1)
Epoch 1/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.7045 - accuracy: 0.5000
Epoch 1: val_loss improved from inf to 0.70221, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.7044 - accuracy: 0.5004 - val_loss: 0.7022 - val_accuracy: 0.4979
Epoch 2/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.6986 - accuracy: 0.5000
Epoch 2: val_loss improved from 0.70221 to 0.69805, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6989 - accuracy: 0.4992 - val_loss: 0.6981 - val_accuracy: 0.4979
Epoch 3/60
653/682 [===========================&gt;..] - ETA: 0s - loss: 0.6961 - accuracy: 0.4998
Epoch 3: val_loss improved from 0.69805 to 0.69579, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6963 - accuracy: 0.4992 - val_loss: 0.6958 - val_accuracy: 0.4979
Epoch 4/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.6949 - accuracy: 0.4987
Epoch 4: val_loss improved from 0.69579 to 0.69465, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6949 - accuracy: 0.4992 - val_loss: 0.6946 - val_accuracy: 0.4979
Epoch 5/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.6940 - accuracy: 0.5006
Epoch 5: val_loss improved from 0.69465 to 0.69401, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6941 - accuracy: 0.4992 - val_loss: 0.6940 - val_accuracy: 0.4979
Epoch 6/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.6937 - accuracy: 0.4991
Epoch 6: val_loss improved from 0.69401 to 0.69363, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6937 - accuracy: 0.4992 - val_loss: 0.6936 - val_accuracy: 0.4979
Epoch 7/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.6935 - accuracy: 0.4987
Epoch 7: val_loss improved from 0.69363 to 0.69342, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6935 - accuracy: 0.4992 - val_loss: 0.6934 - val_accuracy: 0.4979
Epoch 8/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.6934 - accuracy: 0.4961
Epoch 8: val_loss improved from 0.69342 to 0.69329, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6934 - accuracy: 0.4961 - val_loss: 0.6933 - val_accuracy: 0.4979
Epoch 9/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.6933 - accuracy: 0.4957
Epoch 9: val_loss improved from 0.69329 to 0.69324, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6933 - accuracy: 0.4955 - val_loss: 0.6932 - val_accuracy: 0.4979
Epoch 10/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.6933 - accuracy: 0.4979
Epoch 10: val_loss improved from 0.69324 to 0.69320, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6933 - accuracy: 0.4982 - val_loss: 0.6932 - val_accuracy: 0.4979
Epoch 11/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.6933 - accuracy: 0.4901
Epoch 11: val_loss did not improve from 0.69320
682/682 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.4897 - val_loss: 0.6932 - val_accuracy: 0.4991
Epoch 12/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4916
Epoch 12: val_loss improved from 0.69320 to 0.69319, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6933 - accuracy: 0.4914 - val_loss: 0.6932 - val_accuracy: 0.4991
Epoch 13/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4874
Epoch 13: val_loss improved from 0.69319 to 0.69316, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4872 - val_loss: 0.6932 - val_accuracy: 0.4991
Epoch 14/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4899
Epoch 14: val_loss improved from 0.69316 to 0.69314, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.4910 - val_loss: 0.6931 - val_accuracy: 0.5009
Epoch 15/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4939
Epoch 15: val_loss improved from 0.69314 to 0.69314, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.4941 - val_loss: 0.6931 - val_accuracy: 0.5009
Epoch 16/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4946
Epoch 16: val_loss improved from 0.69314 to 0.69314, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.4943 - val_loss: 0.6931 - val_accuracy: 0.5009
Epoch 17/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4972
Epoch 17: val_loss did not improve from 0.69314
682/682 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.4968 - val_loss: 0.6931 - val_accuracy: 0.5009
Epoch 18/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.5006
Epoch 18: val_loss did not improve from 0.69314
682/682 [==============================] - 1s 2ms/step - loss: 0.6933 - accuracy: 0.4986 - val_loss: 0.6931 - val_accuracy: 0.5021
Epoch 19/60
651/682 [===========================&gt;..] - ETA: 0s - loss: 0.6932 - accuracy: 0.4886
Epoch 19: val_loss did not improve from 0.69314
682/682 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.4883 - val_loss: 0.6932 - val_accuracy: 0.4991
Epoch 20/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4987
Epoch 20: val_loss improved from 0.69314 to 0.69313, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4982 - val_loss: 0.6931 - val_accuracy: 0.5021
Epoch 21/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4968
Epoch 21: val_loss did not improve from 0.69313
682/682 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.4952 - val_loss: 0.6931 - val_accuracy: 0.5021
Epoch 22/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.6933 - accuracy: 0.4898
Epoch 22: val_loss did not improve from 0.69313
682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4908 - val_loss: 0.6931 - val_accuracy: 0.5009
Epoch 23/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.6933 - accuracy: 0.4897
Epoch 23: val_loss did not improve from 0.69313
682/682 [==============================] - 1s 2ms/step - loss: 0.6933 - accuracy: 0.4889 - val_loss: 0.6931 - val_accuracy: 0.5021
Epoch 24/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.6933 - accuracy: 0.4866
Epoch 24: val_loss improved from 0.69313 to 0.69313, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6933 - accuracy: 0.4860 - val_loss: 0.6931 - val_accuracy: 0.5021
Epoch 25/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4893
Epoch 25: val_loss did not improve from 0.69313
682/682 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.4888 - val_loss: 0.6931 - val_accuracy: 0.5021
Epoch 26/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.6933 - accuracy: 0.4859
Epoch 26: val_loss did not improve from 0.69313
682/682 [==============================] - 1s 2ms/step - loss: 0.6933 - accuracy: 0.4850 - val_loss: 0.6931 - val_accuracy: 0.5021
Epoch 27/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4973
Epoch 27: val_loss did not improve from 0.69313
682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4968 - val_loss: 0.6931 - val_accuracy: 0.5021
Epoch 28/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.6933 - accuracy: 0.4925
Epoch 28: val_loss did not improve from 0.69313
682/682 [==============================] - 1s 2ms/step - loss: 0.6933 - accuracy: 0.4930 - val_loss: 0.6931 - val_accuracy: 0.5021
Epoch 29/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4966
Epoch 29: val_loss did not improve from 0.69313
682/682 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.4967 - val_loss: 0.6931 - val_accuracy: 0.5009
Epoch 30/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.6932 - accuracy: 0.4901
Epoch 30: val_loss did not improve from 0.69313
682/682 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.4897 - val_loss: 0.6931 - val_accuracy: 0.5021
Epoch 31/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.6933 - accuracy: 0.4877
Epoch 31: val_loss did not improve from 0.69313
682/682 [==============================] - 2s 2ms/step - loss: 0.6933 - accuracy: 0.4873 - val_loss: 0.6931 - val_accuracy: 0.5021
Epoch 32/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.6932 - accuracy: 0.4948
Epoch 32: val_loss did not improve from 0.69313
682/682 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.4970 - val_loss: 0.6932 - val_accuracy: 0.4991
Epoch 33/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4988
Epoch 33: val_loss did not improve from 0.69313
682/682 [==============================] - 1s 2ms/step - loss: 0.6933 - accuracy: 0.4980 - val_loss: 0.6932 - val_accuracy: 0.4991
Epoch 34/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.5013
Epoch 34: val_loss did not improve from 0.69313
682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.5014 - val_loss: 0.6931 - val_accuracy: 0.5009
Epoch 35/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4925
Epoch 35: val_loss did not improve from 0.69313
682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4926 - val_loss: 0.6931 - val_accuracy: 0.5021
Epoch 36/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4984
Epoch 36: val_loss did not improve from 0.69313
682/682 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.4986 - val_loss: 0.6932 - val_accuracy: 0.4991
Epoch 37/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4891
Epoch 37: val_loss did not improve from 0.69313
682/682 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.4888 - val_loss: 0.6931 - val_accuracy: 0.5021
Epoch 38/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.6933 - accuracy: 0.4939
Epoch 38: val_loss did not improve from 0.69313
682/682 [==============================] - 1s 2ms/step - loss: 0.6933 - accuracy: 0.4948 - val_loss: 0.6931 - val_accuracy: 0.5009
Epoch 39/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4973
Epoch 39: val_loss did not improve from 0.69313
682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4974 - val_loss: 0.6931 - val_accuracy: 0.5021
Epoch 40/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4925
Epoch 40: val_loss did not improve from 0.69313
682/682 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.4929 - val_loss: 0.6931 - val_accuracy: 0.5021
Epoch 41/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.5010
Epoch 41: val_loss did not improve from 0.69313
682/682 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.5007 - val_loss: 0.6931 - val_accuracy: 0.5021
Epoch 42/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.6933 - accuracy: 0.4959
Epoch 42: val_loss did not improve from 0.69313
682/682 [==============================] - 1s 2ms/step - loss: 0.6933 - accuracy: 0.4954 - val_loss: 0.6931 - val_accuracy: 0.5021
Epoch 43/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4895
Epoch 43: val_loss did not improve from 0.69313
682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4891 - val_loss: 0.6931 - val_accuracy: 0.5009
Epoch 44/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4945
Epoch 44: val_loss did not improve from 0.69313
682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4939 - val_loss: 0.6931 - val_accuracy: 0.5021
Epoch 44: early stopping
54/54 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5021
Processing feature: campaign
data frame x has shape: (8516, 1)
Epoch 1/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.7003 - accuracy: 0.4592
Epoch 1: val_loss improved from inf to 0.69897, saving model to feature_best.hdf5
682/682 [==============================] - 2s 3ms/step - loss: 0.7004 - accuracy: 0.4584 - val_loss: 0.6990 - val_accuracy: 0.4686
Epoch 2/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.6990 - accuracy: 0.4602
Epoch 2: val_loss improved from 0.69897 to 0.69781, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6990 - accuracy: 0.4596 - val_loss: 0.6978 - val_accuracy: 0.4692
Epoch 3/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.6979 - accuracy: 0.4567
Epoch 3: val_loss improved from 0.69781 to 0.69682, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6978 - accuracy: 0.4584 - val_loss: 0.6968 - val_accuracy: 0.4692
Epoch 4/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.6967 - accuracy: 0.4595
Epoch 4: val_loss improved from 0.69682 to 0.69588, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6968 - accuracy: 0.4587 - val_loss: 0.6959 - val_accuracy: 0.4692
Epoch 5/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.6957 - accuracy: 0.4588
Epoch 5: val_loss improved from 0.69588 to 0.69497, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6957 - accuracy: 0.4590 - val_loss: 0.6950 - val_accuracy: 0.4662
Epoch 6/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.6948 - accuracy: 0.4617
Epoch 6: val_loss improved from 0.69497 to 0.69413, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6948 - accuracy: 0.4616 - val_loss: 0.6941 - val_accuracy: 0.4662
Epoch 7/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.6939 - accuracy: 0.4742
Epoch 7: val_loss improved from 0.69413 to 0.69337, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6939 - accuracy: 0.4748 - val_loss: 0.6934 - val_accuracy: 0.4780
Epoch 8/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.4991
Epoch 8: val_loss improved from 0.69337 to 0.69265, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4995 - val_loss: 0.6927 - val_accuracy: 0.5314
Epoch 9/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.6923 - accuracy: 0.5125
Epoch 9: val_loss improved from 0.69265 to 0.69201, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6923 - accuracy: 0.5130 - val_loss: 0.6920 - val_accuracy: 0.5308
Epoch 10/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.6917 - accuracy: 0.5267
Epoch 10: val_loss improved from 0.69201 to 0.69141, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6917 - accuracy: 0.5274 - val_loss: 0.6914 - val_accuracy: 0.5308
Epoch 11/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.6910 - accuracy: 0.5343
Epoch 11: val_loss improved from 0.69141 to 0.69082, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5350 - val_loss: 0.6908 - val_accuracy: 0.5308
Epoch 12/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.6904 - accuracy: 0.5421
Epoch 12: val_loss improved from 0.69082 to 0.69029, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6904 - accuracy: 0.5410 - val_loss: 0.6903 - val_accuracy: 0.5308
Epoch 13/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.6898 - accuracy: 0.5360
Epoch 13: val_loss improved from 0.69029 to 0.68982, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6899 - accuracy: 0.5360 - val_loss: 0.6898 - val_accuracy: 0.5308
Epoch 14/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.6894 - accuracy: 0.5355
Epoch 14: val_loss improved from 0.68982 to 0.68938, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6894 - accuracy: 0.5354 - val_loss: 0.6894 - val_accuracy: 0.5308
Epoch 15/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.6889 - accuracy: 0.5425
Epoch 15: val_loss improved from 0.68938 to 0.68898, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6889 - accuracy: 0.5426 - val_loss: 0.6890 - val_accuracy: 0.5308
Epoch 16/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.6884 - accuracy: 0.5424
Epoch 16: val_loss improved from 0.68898 to 0.68861, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6885 - accuracy: 0.5416 - val_loss: 0.6886 - val_accuracy: 0.5308
Epoch 17/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.6882 - accuracy: 0.5411
Epoch 17: val_loss improved from 0.68861 to 0.68827, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6881 - accuracy: 0.5416 - val_loss: 0.6883 - val_accuracy: 0.5308
Epoch 18/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.6877 - accuracy: 0.5389
Epoch 18: val_loss improved from 0.68827 to 0.68796, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6878 - accuracy: 0.5387 - val_loss: 0.6880 - val_accuracy: 0.5308
Epoch 19/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.6874 - accuracy: 0.5408
Epoch 19: val_loss improved from 0.68796 to 0.68769, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6875 - accuracy: 0.5416 - val_loss: 0.6877 - val_accuracy: 0.5308
Epoch 20/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.6872 - accuracy: 0.5419
Epoch 20: val_loss improved from 0.68769 to 0.68746, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6872 - accuracy: 0.5416 - val_loss: 0.6875 - val_accuracy: 0.5308
Epoch 21/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.6872 - accuracy: 0.5399
Epoch 21: val_loss improved from 0.68746 to 0.68722, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6870 - accuracy: 0.5416 - val_loss: 0.6872 - val_accuracy: 0.5308
Epoch 22/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.6868 - accuracy: 0.5409
Epoch 22: val_loss improved from 0.68722 to 0.68702, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6867 - accuracy: 0.5416 - val_loss: 0.6870 - val_accuracy: 0.5308
Epoch 23/60
654/682 [===========================&gt;..] - ETA: 0s - loss: 0.6862 - accuracy: 0.5442
Epoch 23: val_loss improved from 0.68702 to 0.68681, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6865 - accuracy: 0.5416 - val_loss: 0.6868 - val_accuracy: 0.5308
Epoch 24/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.6864 - accuracy: 0.5415
Epoch 24: val_loss improved from 0.68681 to 0.68664, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6863 - accuracy: 0.5416 - val_loss: 0.6866 - val_accuracy: 0.5308
Epoch 25/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.6862 - accuracy: 0.5405
Epoch 25: val_loss improved from 0.68664 to 0.68650, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6862 - accuracy: 0.5406 - val_loss: 0.6865 - val_accuracy: 0.5308
Epoch 26/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.6861 - accuracy: 0.5408
Epoch 26: val_loss improved from 0.68650 to 0.68635, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6860 - accuracy: 0.5416 - val_loss: 0.6863 - val_accuracy: 0.5308
Epoch 27/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.6858 - accuracy: 0.5423
Epoch 27: val_loss improved from 0.68635 to 0.68625, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6858 - accuracy: 0.5415 - val_loss: 0.6862 - val_accuracy: 0.5308
Epoch 28/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.6857 - accuracy: 0.5417
Epoch 28: val_loss improved from 0.68625 to 0.68611, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6857 - accuracy: 0.5416 - val_loss: 0.6861 - val_accuracy: 0.5308
Epoch 29/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.6858 - accuracy: 0.5408
Epoch 29: val_loss improved from 0.68611 to 0.68601, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6857 - accuracy: 0.5416 - val_loss: 0.6860 - val_accuracy: 0.5308
Epoch 30/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.6856 - accuracy: 0.5419
Epoch 30: val_loss improved from 0.68601 to 0.68591, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6855 - accuracy: 0.5416 - val_loss: 0.6859 - val_accuracy: 0.5308
Epoch 31/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.6855 - accuracy: 0.5405
Epoch 31: val_loss improved from 0.68591 to 0.68582, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6855 - accuracy: 0.5416 - val_loss: 0.6858 - val_accuracy: 0.5308
Epoch 32/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.6854 - accuracy: 0.5416
Epoch 32: val_loss improved from 0.68582 to 0.68574, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6854 - accuracy: 0.5416 - val_loss: 0.6857 - val_accuracy: 0.5308
Epoch 33/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.6853 - accuracy: 0.5410
Epoch 33: val_loss improved from 0.68574 to 0.68567, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6853 - accuracy: 0.5416 - val_loss: 0.6857 - val_accuracy: 0.5308
Epoch 34/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.6853 - accuracy: 0.5412
Epoch 34: val_loss improved from 0.68567 to 0.68560, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6852 - accuracy: 0.5416 - val_loss: 0.6856 - val_accuracy: 0.5308
Epoch 35/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.6854 - accuracy: 0.5409
Epoch 35: val_loss improved from 0.68560 to 0.68554, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6851 - accuracy: 0.5416 - val_loss: 0.6855 - val_accuracy: 0.5308
Epoch 36/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.6850 - accuracy: 0.5431
Epoch 36: val_loss improved from 0.68554 to 0.68549, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6852 - accuracy: 0.5416 - val_loss: 0.6855 - val_accuracy: 0.5308
Epoch 37/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.6849 - accuracy: 0.5423
Epoch 37: val_loss improved from 0.68549 to 0.68544, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6851 - accuracy: 0.5416 - val_loss: 0.6854 - val_accuracy: 0.5308
Epoch 38/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.6850 - accuracy: 0.5420
Epoch 38: val_loss improved from 0.68544 to 0.68540, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6850 - accuracy: 0.5416 - val_loss: 0.6854 - val_accuracy: 0.5308
Epoch 39/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.6850 - accuracy: 0.5408
Epoch 39: val_loss improved from 0.68540 to 0.68538, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6850 - accuracy: 0.5416 - val_loss: 0.6854 - val_accuracy: 0.5308
Epoch 40/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.6847 - accuracy: 0.5414
Epoch 40: val_loss improved from 0.68538 to 0.68532, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6849 - accuracy: 0.5416 - val_loss: 0.6853 - val_accuracy: 0.5308
Epoch 41/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.6849 - accuracy: 0.5413
Epoch 41: val_loss improved from 0.68532 to 0.68528, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6849 - accuracy: 0.5416 - val_loss: 0.6853 - val_accuracy: 0.5308
Epoch 42/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.6848 - accuracy: 0.5390
Epoch 42: val_loss improved from 0.68528 to 0.68526, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6849 - accuracy: 0.5410 - val_loss: 0.6853 - val_accuracy: 0.5308
Epoch 43/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.6848 - accuracy: 0.5423
Epoch 43: val_loss improved from 0.68526 to 0.68522, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6848 - accuracy: 0.5416 - val_loss: 0.6852 - val_accuracy: 0.5308
Epoch 44/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.6849 - accuracy: 0.5423
Epoch 44: val_loss improved from 0.68522 to 0.68519, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6848 - accuracy: 0.5416 - val_loss: 0.6852 - val_accuracy: 0.5308
Epoch 45/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.6850 - accuracy: 0.5401
Epoch 45: val_loss improved from 0.68519 to 0.68517, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6848 - accuracy: 0.5416 - val_loss: 0.6852 - val_accuracy: 0.5308
Epoch 46/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.6845 - accuracy: 0.5419
Epoch 46: val_loss improved from 0.68517 to 0.68515, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6848 - accuracy: 0.5416 - val_loss: 0.6851 - val_accuracy: 0.5308
Epoch 47/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.6848 - accuracy: 0.5412
Epoch 47: val_loss improved from 0.68515 to 0.68512, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6848 - accuracy: 0.5416 - val_loss: 0.6851 - val_accuracy: 0.5308
Epoch 48/60
654/682 [===========================&gt;..] - ETA: 0s - loss: 0.6849 - accuracy: 0.5419
Epoch 48: val_loss improved from 0.68512 to 0.68512, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6847 - accuracy: 0.5416 - val_loss: 0.6851 - val_accuracy: 0.5308
Epoch 49/60
652/682 [===========================&gt;..] - ETA: 0s - loss: 0.6846 - accuracy: 0.5402
Epoch 49: val_loss improved from 0.68512 to 0.68509, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6847 - accuracy: 0.5416 - val_loss: 0.6851 - val_accuracy: 0.5308
Epoch 50/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.6847 - accuracy: 0.5412
Epoch 50: val_loss improved from 0.68509 to 0.68507, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6847 - accuracy: 0.5416 - val_loss: 0.6851 - val_accuracy: 0.5308
Epoch 51/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.6846 - accuracy: 0.5417
Epoch 51: val_loss improved from 0.68507 to 0.68506, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6847 - accuracy: 0.5416 - val_loss: 0.6851 - val_accuracy: 0.5308
Epoch 52/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.6847 - accuracy: 0.5415
Epoch 52: val_loss improved from 0.68506 to 0.68504, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6847 - accuracy: 0.5416 - val_loss: 0.6850 - val_accuracy: 0.5308
Epoch 53/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.6846 - accuracy: 0.5419
Epoch 53: val_loss improved from 0.68504 to 0.68503, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6847 - accuracy: 0.5416 - val_loss: 0.6850 - val_accuracy: 0.5308
Epoch 54/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.6845 - accuracy: 0.5426
Epoch 54: val_loss improved from 0.68503 to 0.68501, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6847 - accuracy: 0.5416 - val_loss: 0.6850 - val_accuracy: 0.5308
Epoch 55/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.6847 - accuracy: 0.5399
Epoch 55: val_loss improved from 0.68501 to 0.68501, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6847 - accuracy: 0.5416 - val_loss: 0.6850 - val_accuracy: 0.5308
Epoch 56/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.6844 - accuracy: 0.5437
Epoch 56: val_loss did not improve from 0.68501
682/682 [==============================] - 1s 2ms/step - loss: 0.6847 - accuracy: 0.5416 - val_loss: 0.6850 - val_accuracy: 0.5308
Epoch 57/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.6844 - accuracy: 0.5426
Epoch 57: val_loss improved from 0.68501 to 0.68501, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6846 - accuracy: 0.5416 - val_loss: 0.6850 - val_accuracy: 0.5308
Epoch 58/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.6845 - accuracy: 0.5418
Epoch 58: val_loss improved from 0.68501 to 0.68497, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6846 - accuracy: 0.5416 - val_loss: 0.6850 - val_accuracy: 0.5308
Epoch 59/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.6846 - accuracy: 0.5418
Epoch 59: val_loss improved from 0.68497 to 0.68496, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6846 - accuracy: 0.5416 - val_loss: 0.6850 - val_accuracy: 0.5308
Epoch 60/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.6846 - accuracy: 0.5416
Epoch 60: val_loss improved from 0.68496 to 0.68496, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6846 - accuracy: 0.5416 - val_loss: 0.6850 - val_accuracy: 0.5308
54/54 [==============================] - 0s 2ms/step - loss: 0.6850 - accuracy: 0.5308
Processing feature: pdays
data frame x has shape: (8516, 1)
Epoch 1/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.6837 - accuracy: 0.5951
Epoch 1: val_loss improved from inf to 0.68131, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6838 - accuracy: 0.5943 - val_loss: 0.6813 - val_accuracy: 0.5907
Epoch 2/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.6788 - accuracy: 0.5956
Epoch 2: val_loss improved from 0.68131 to 0.67786, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6789 - accuracy: 0.5945 - val_loss: 0.6779 - val_accuracy: 0.5907
Epoch 3/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.6757 - accuracy: 0.5935
Epoch 3: val_loss improved from 0.67786 to 0.67486, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6755 - accuracy: 0.5945 - val_loss: 0.6749 - val_accuracy: 0.5907
Epoch 4/60
653/682 [===========================&gt;..] - ETA: 0s - loss: 0.6727 - accuracy: 0.5939
Epoch 4: val_loss improved from 0.67486 to 0.67208, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6724 - accuracy: 0.5945 - val_loss: 0.6721 - val_accuracy: 0.5907
Epoch 5/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.6695 - accuracy: 0.5957
Epoch 5: val_loss improved from 0.67208 to 0.66963, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6696 - accuracy: 0.5945 - val_loss: 0.6696 - val_accuracy: 0.5907
Epoch 6/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.6673 - accuracy: 0.5936
Epoch 6: val_loss improved from 0.66963 to 0.66746, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6671 - accuracy: 0.5945 - val_loss: 0.6675 - val_accuracy: 0.5907
Epoch 7/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.6647 - accuracy: 0.5948
Epoch 7: val_loss improved from 0.66746 to 0.66547, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6649 - accuracy: 0.5945 - val_loss: 0.6655 - val_accuracy: 0.5907
Epoch 8/60
652/682 [===========================&gt;..] - ETA: 0s - loss: 0.6632 - accuracy: 0.5939
Epoch 8: val_loss improved from 0.66547 to 0.66372, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6629 - accuracy: 0.5945 - val_loss: 0.6637 - val_accuracy: 0.5907
Epoch 9/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.6609 - accuracy: 0.5945
Epoch 9: val_loss improved from 0.66372 to 0.66213, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6610 - accuracy: 0.5945 - val_loss: 0.6621 - val_accuracy: 0.5907
Epoch 10/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.6593 - accuracy: 0.5942
Epoch 10: val_loss improved from 0.66213 to 0.66069, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6594 - accuracy: 0.5945 - val_loss: 0.6607 - val_accuracy: 0.5907
Epoch 11/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.6575 - accuracy: 0.5957
Epoch 11: val_loss improved from 0.66069 to 0.65942, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6579 - accuracy: 0.5945 - val_loss: 0.6594 - val_accuracy: 0.5907
Epoch 12/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.6565 - accuracy: 0.5945
Epoch 12: val_loss improved from 0.65942 to 0.65825, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6565 - accuracy: 0.5945 - val_loss: 0.6582 - val_accuracy: 0.5907
Epoch 13/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.6554 - accuracy: 0.5939
Epoch 13: val_loss improved from 0.65825 to 0.65720, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6552 - accuracy: 0.5945 - val_loss: 0.6572 - val_accuracy: 0.5907
Epoch 14/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.6539 - accuracy: 0.5955
Epoch 14: val_loss improved from 0.65720 to 0.65625, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6541 - accuracy: 0.5945 - val_loss: 0.6563 - val_accuracy: 0.5907
Epoch 15/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.6531 - accuracy: 0.5942
Epoch 15: val_loss improved from 0.65625 to 0.65537, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6531 - accuracy: 0.5945 - val_loss: 0.6554 - val_accuracy: 0.5907
Epoch 16/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.6513 - accuracy: 0.5963
Epoch 16: val_loss improved from 0.65537 to 0.65461, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6522 - accuracy: 0.5945 - val_loss: 0.6546 - val_accuracy: 0.5907
Epoch 17/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.6511 - accuracy: 0.5946
Epoch 17: val_loss improved from 0.65461 to 0.65391, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6513 - accuracy: 0.5945 - val_loss: 0.6539 - val_accuracy: 0.5907
Epoch 18/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.6507 - accuracy: 0.5932
Epoch 18: val_loss improved from 0.65391 to 0.65326, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6505 - accuracy: 0.5945 - val_loss: 0.6533 - val_accuracy: 0.5907
Epoch 19/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.6501 - accuracy: 0.5942
Epoch 19: val_loss improved from 0.65326 to 0.65267, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6499 - accuracy: 0.5945 - val_loss: 0.6527 - val_accuracy: 0.5907
Epoch 20/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.6490 - accuracy: 0.5955
Epoch 20: val_loss improved from 0.65267 to 0.65216, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6491 - accuracy: 0.5945 - val_loss: 0.6522 - val_accuracy: 0.5907
Epoch 21/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.6486 - accuracy: 0.5943
Epoch 21: val_loss improved from 0.65216 to 0.65165, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6485 - accuracy: 0.5945 - val_loss: 0.6517 - val_accuracy: 0.5907
Epoch 22/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.6480 - accuracy: 0.5938
Epoch 22: val_loss improved from 0.65165 to 0.65118, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6480 - accuracy: 0.5945 - val_loss: 0.6512 - val_accuracy: 0.5907
Epoch 23/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.6475 - accuracy: 0.5946
Epoch 23: val_loss improved from 0.65118 to 0.65078, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6475 - accuracy: 0.5945 - val_loss: 0.6508 - val_accuracy: 0.5907
Epoch 24/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.6471 - accuracy: 0.5935
Epoch 24: val_loss improved from 0.65078 to 0.65043, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6470 - accuracy: 0.5945 - val_loss: 0.6504 - val_accuracy: 0.5907
Epoch 25/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.6462 - accuracy: 0.5941
Epoch 25: val_loss improved from 0.65043 to 0.65014, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6465 - accuracy: 0.5945 - val_loss: 0.6501 - val_accuracy: 0.5907
Epoch 26/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.6453 - accuracy: 0.5982
Epoch 26: val_loss improved from 0.65014 to 0.64977, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6461 - accuracy: 0.5945 - val_loss: 0.6498 - val_accuracy: 0.5907
Epoch 27/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.6456 - accuracy: 0.5936
Epoch 27: val_loss improved from 0.64977 to 0.64945, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6458 - accuracy: 0.5945 - val_loss: 0.6495 - val_accuracy: 0.5907
Epoch 28/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.6461 - accuracy: 0.5934
Epoch 28: val_loss improved from 0.64945 to 0.64917, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6454 - accuracy: 0.5945 - val_loss: 0.6492 - val_accuracy: 0.5907
Epoch 29/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.6457 - accuracy: 0.5940
Epoch 29: val_loss improved from 0.64917 to 0.64891, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6450 - accuracy: 0.5945 - val_loss: 0.6489 - val_accuracy: 0.5907
Epoch 30/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.6446 - accuracy: 0.5942
Epoch 30: val_loss improved from 0.64891 to 0.64867, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6448 - accuracy: 0.5945 - val_loss: 0.6487 - val_accuracy: 0.5907
Epoch 31/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.6446 - accuracy: 0.5938
Epoch 31: val_loss improved from 0.64867 to 0.64847, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6445 - accuracy: 0.5945 - val_loss: 0.6485 - val_accuracy: 0.5907
Epoch 32/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.6442 - accuracy: 0.5960
Epoch 32: val_loss improved from 0.64847 to 0.64831, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6442 - accuracy: 0.5945 - val_loss: 0.6483 - val_accuracy: 0.5907
Epoch 33/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.6437 - accuracy: 0.5950
Epoch 33: val_loss improved from 0.64831 to 0.64808, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6439 - accuracy: 0.5945 - val_loss: 0.6481 - val_accuracy: 0.5907
Epoch 34/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.6438 - accuracy: 0.5947
Epoch 34: val_loss improved from 0.64808 to 0.64797, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6437 - accuracy: 0.5945 - val_loss: 0.6480 - val_accuracy: 0.5907
Epoch 35/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.6431 - accuracy: 0.5961
Epoch 35: val_loss improved from 0.64797 to 0.64781, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6435 - accuracy: 0.5945 - val_loss: 0.6478 - val_accuracy: 0.5907
Epoch 36/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.6434 - accuracy: 0.5932
Epoch 36: val_loss improved from 0.64781 to 0.64764, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6433 - accuracy: 0.5945 - val_loss: 0.6476 - val_accuracy: 0.5907
Epoch 37/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.6436 - accuracy: 0.5931
Epoch 37: val_loss improved from 0.64764 to 0.64751, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6431 - accuracy: 0.5945 - val_loss: 0.6475 - val_accuracy: 0.5907
Epoch 38/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.6427 - accuracy: 0.5941
Epoch 38: val_loss improved from 0.64751 to 0.64735, saving model to feature_best.hdf5
682/682 [==============================] - 2s 3ms/step - loss: 0.6429 - accuracy: 0.5945 - val_loss: 0.6474 - val_accuracy: 0.5907
Epoch 39/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.6418 - accuracy: 0.5958
Epoch 39: val_loss improved from 0.64735 to 0.64724, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6427 - accuracy: 0.5945 - val_loss: 0.6472 - val_accuracy: 0.5907
Epoch 40/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.6424 - accuracy: 0.5943
Epoch 40: val_loss improved from 0.64724 to 0.64714, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6426 - accuracy: 0.5945 - val_loss: 0.6471 - val_accuracy: 0.5907
Epoch 41/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.6422 - accuracy: 0.5943
Epoch 41: val_loss improved from 0.64714 to 0.64703, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6424 - accuracy: 0.5945 - val_loss: 0.6470 - val_accuracy: 0.5907
Epoch 42/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.6423 - accuracy: 0.5940
Epoch 42: val_loss improved from 0.64703 to 0.64698, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6422 - accuracy: 0.5945 - val_loss: 0.6470 - val_accuracy: 0.5907
Epoch 43/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.6423 - accuracy: 0.5944
Epoch 43: val_loss improved from 0.64698 to 0.64686, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6422 - accuracy: 0.5945 - val_loss: 0.6469 - val_accuracy: 0.5907
Epoch 44/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.6420 - accuracy: 0.5943
Epoch 44: val_loss improved from 0.64686 to 0.64677, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6420 - accuracy: 0.5945 - val_loss: 0.6468 - val_accuracy: 0.5907
Epoch 45/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.6421 - accuracy: 0.5944
Epoch 45: val_loss improved from 0.64677 to 0.64670, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6419 - accuracy: 0.5945 - val_loss: 0.6467 - val_accuracy: 0.5907
Epoch 46/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.6417 - accuracy: 0.5948
Epoch 46: val_loss improved from 0.64670 to 0.64662, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6418 - accuracy: 0.5945 - val_loss: 0.6466 - val_accuracy: 0.5907
Epoch 47/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.6414 - accuracy: 0.5958
Epoch 47: val_loss improved from 0.64662 to 0.64658, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6417 - accuracy: 0.5945 - val_loss: 0.6466 - val_accuracy: 0.5907
Epoch 48/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.6415 - accuracy: 0.5945
Epoch 48: val_loss improved from 0.64658 to 0.64651, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6415 - accuracy: 0.5945 - val_loss: 0.6465 - val_accuracy: 0.5907
Epoch 49/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.6410 - accuracy: 0.5952
Epoch 49: val_loss improved from 0.64651 to 0.64647, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6415 - accuracy: 0.5945 - val_loss: 0.6465 - val_accuracy: 0.5907
Epoch 50/60
652/682 [===========================&gt;..] - ETA: 0s - loss: 0.6415 - accuracy: 0.5951
Epoch 50: val_loss improved from 0.64647 to 0.64638, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6414 - accuracy: 0.5945 - val_loss: 0.6464 - val_accuracy: 0.5907
Epoch 51/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.6415 - accuracy: 0.5949
Epoch 51: val_loss improved from 0.64638 to 0.64633, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6413 - accuracy: 0.5945 - val_loss: 0.6463 - val_accuracy: 0.5907
Epoch 52/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.6408 - accuracy: 0.5966
Epoch 52: val_loss improved from 0.64633 to 0.64628, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6412 - accuracy: 0.5945 - val_loss: 0.6463 - val_accuracy: 0.5907
Epoch 53/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.6412 - accuracy: 0.5950
Epoch 53: val_loss improved from 0.64628 to 0.64625, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6411 - accuracy: 0.5945 - val_loss: 0.6463 - val_accuracy: 0.5907
Epoch 54/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.6411 - accuracy: 0.5951
Epoch 54: val_loss improved from 0.64625 to 0.64620, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6411 - accuracy: 0.5945 - val_loss: 0.6462 - val_accuracy: 0.5907
Epoch 55/60
654/682 [===========================&gt;..] - ETA: 0s - loss: 0.6409 - accuracy: 0.5956
Epoch 55: val_loss improved from 0.64620 to 0.64616, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6410 - accuracy: 0.5945 - val_loss: 0.6462 - val_accuracy: 0.5907
Epoch 56/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.6413 - accuracy: 0.5930
Epoch 56: val_loss improved from 0.64616 to 0.64612, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6409 - accuracy: 0.5945 - val_loss: 0.6461 - val_accuracy: 0.5907
Epoch 57/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.6414 - accuracy: 0.5941
Epoch 57: val_loss did not improve from 0.64612
682/682 [==============================] - 2s 2ms/step - loss: 0.6408 - accuracy: 0.5945 - val_loss: 0.6462 - val_accuracy: 0.5907
Epoch 58/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.6409 - accuracy: 0.5950
Epoch 58: val_loss improved from 0.64612 to 0.64607, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6408 - accuracy: 0.5945 - val_loss: 0.6461 - val_accuracy: 0.5907
Epoch 59/60
651/682 [===========================&gt;..] - ETA: 0s - loss: 0.6411 - accuracy: 0.5931
Epoch 59: val_loss improved from 0.64607 to 0.64606, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6407 - accuracy: 0.5945 - val_loss: 0.6461 - val_accuracy: 0.5907
Epoch 60/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.6403 - accuracy: 0.5953
Epoch 60: val_loss improved from 0.64606 to 0.64602, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6407 - accuracy: 0.5945 - val_loss: 0.6460 - val_accuracy: 0.5907
54/54 [==============================] - 0s 2ms/step - loss: 0.6460 - accuracy: 0.5907
Processing feature: previous
data frame x has shape: (8516, 1)
Epoch 1/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.6836 - accuracy: 0.5643
Epoch 1: val_loss improved from inf to 0.68261, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6836 - accuracy: 0.5658 - val_loss: 0.6826 - val_accuracy: 0.5972
Epoch 2/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.6801 - accuracy: 0.5996
Epoch 2: val_loss improved from 0.68261 to 0.67966, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6801 - accuracy: 0.5994 - val_loss: 0.6797 - val_accuracy: 0.5972
Epoch 3/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.6773 - accuracy: 0.5979
Epoch 3: val_loss improved from 0.67966 to 0.67715, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6771 - accuracy: 0.5994 - val_loss: 0.6771 - val_accuracy: 0.5972
Epoch 4/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.6747 - accuracy: 0.5968
Epoch 4: val_loss improved from 0.67715 to 0.67495, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6746 - accuracy: 0.5994 - val_loss: 0.6749 - val_accuracy: 0.5972
Epoch 5/60
682/682 [==============================] - ETA: 0s - loss: 0.6723 - accuracy: 0.5994
Epoch 5: val_loss improved from 0.67495 to 0.67303, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6723 - accuracy: 0.5994 - val_loss: 0.6730 - val_accuracy: 0.5972
Epoch 6/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.6704 - accuracy: 0.5996
Epoch 6: val_loss improved from 0.67303 to 0.67138, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6703 - accuracy: 0.5994 - val_loss: 0.6714 - val_accuracy: 0.5972
Epoch 7/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.6687 - accuracy: 0.5996
Epoch 7: val_loss improved from 0.67138 to 0.66996, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6687 - accuracy: 0.5994 - val_loss: 0.6700 - val_accuracy: 0.5972
Epoch 8/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.6675 - accuracy: 0.5987
Epoch 8: val_loss improved from 0.66996 to 0.66876, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6672 - accuracy: 0.5994 - val_loss: 0.6688 - val_accuracy: 0.5972
Epoch 9/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.6661 - accuracy: 0.5991
Epoch 9: val_loss improved from 0.66876 to 0.66772, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6660 - accuracy: 0.5994 - val_loss: 0.6677 - val_accuracy: 0.5972
Epoch 10/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.6646 - accuracy: 0.5994
Epoch 10: val_loss improved from 0.66772 to 0.66682, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6649 - accuracy: 0.5994 - val_loss: 0.6668 - val_accuracy: 0.5972
Epoch 11/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.6641 - accuracy: 0.6006
Epoch 11: val_loss improved from 0.66682 to 0.66608, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6640 - accuracy: 0.5994 - val_loss: 0.6661 - val_accuracy: 0.5972
Epoch 12/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.6635 - accuracy: 0.5985
Epoch 12: val_loss improved from 0.66608 to 0.66544, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6632 - accuracy: 0.5994 - val_loss: 0.6654 - val_accuracy: 0.5972
Epoch 13/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.6627 - accuracy: 0.5997
Epoch 13: val_loss improved from 0.66544 to 0.66488, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6625 - accuracy: 0.5994 - val_loss: 0.6649 - val_accuracy: 0.5972
Epoch 14/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.6619 - accuracy: 0.6005
Epoch 14: val_loss improved from 0.66488 to 0.66441, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6620 - accuracy: 0.5994 - val_loss: 0.6644 - val_accuracy: 0.5972
Epoch 15/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.6619 - accuracy: 0.5992
Epoch 15: val_loss improved from 0.66441 to 0.66404, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6614 - accuracy: 0.5994 - val_loss: 0.6640 - val_accuracy: 0.5972
Epoch 16/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.6614 - accuracy: 0.5994
Epoch 16: val_loss improved from 0.66404 to 0.66368, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6610 - accuracy: 0.5994 - val_loss: 0.6637 - val_accuracy: 0.5972
Epoch 17/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.6606 - accuracy: 0.5996
Epoch 17: val_loss improved from 0.66368 to 0.66339, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6606 - accuracy: 0.5994 - val_loss: 0.6634 - val_accuracy: 0.5972
Epoch 18/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.6605 - accuracy: 0.5982
Epoch 18: val_loss improved from 0.66339 to 0.66315, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6604 - accuracy: 0.5994 - val_loss: 0.6631 - val_accuracy: 0.5972
Epoch 19/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.6603 - accuracy: 0.5993
Epoch 19: val_loss improved from 0.66315 to 0.66295, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6601 - accuracy: 0.5994 - val_loss: 0.6629 - val_accuracy: 0.5972
Epoch 20/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.6598 - accuracy: 0.5991
Epoch 20: val_loss improved from 0.66295 to 0.66277, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6598 - accuracy: 0.5994 - val_loss: 0.6628 - val_accuracy: 0.5972
Epoch 21/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.6588 - accuracy: 0.6018
Epoch 21: val_loss improved from 0.66277 to 0.66261, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6597 - accuracy: 0.5994 - val_loss: 0.6626 - val_accuracy: 0.5972
Epoch 22/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.6598 - accuracy: 0.5982
Epoch 22: val_loss improved from 0.66261 to 0.66251, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6595 - accuracy: 0.5994 - val_loss: 0.6625 - val_accuracy: 0.5972
Epoch 23/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.6598 - accuracy: 0.5979
Epoch 23: val_loss improved from 0.66251 to 0.66240, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6594 - accuracy: 0.5994 - val_loss: 0.6624 - val_accuracy: 0.5972
Epoch 24/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.6593 - accuracy: 0.5998
Epoch 24: val_loss improved from 0.66240 to 0.66231, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6592 - accuracy: 0.5994 - val_loss: 0.6623 - val_accuracy: 0.5972
Epoch 25/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.6593 - accuracy: 0.5986
Epoch 25: val_loss improved from 0.66231 to 0.66223, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6591 - accuracy: 0.5994 - val_loss: 0.6622 - val_accuracy: 0.5972
Epoch 26/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.6598 - accuracy: 0.5988
Epoch 26: val_loss improved from 0.66223 to 0.66215, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6590 - accuracy: 0.5994 - val_loss: 0.6622 - val_accuracy: 0.5972
Epoch 27/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.6586 - accuracy: 0.5997
Epoch 27: val_loss improved from 0.66215 to 0.66211, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6589 - accuracy: 0.5994 - val_loss: 0.6621 - val_accuracy: 0.5972
Epoch 28/60
651/682 [===========================&gt;..] - ETA: 0s - loss: 0.6585 - accuracy: 0.6012
Epoch 28: val_loss improved from 0.66211 to 0.66208, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6589 - accuracy: 0.5994 - val_loss: 0.6621 - val_accuracy: 0.5972
Epoch 29/60
682/682 [==============================] - ETA: 0s - loss: 0.6588 - accuracy: 0.5994
Epoch 29: val_loss improved from 0.66208 to 0.66202, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6588 - accuracy: 0.5994 - val_loss: 0.6620 - val_accuracy: 0.5972
Epoch 30/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.6596 - accuracy: 0.5978
Epoch 30: val_loss improved from 0.66202 to 0.66198, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6588 - accuracy: 0.5994 - val_loss: 0.6620 - val_accuracy: 0.5972
Epoch 31/60
682/682 [==============================] - ETA: 0s - loss: 0.6587 - accuracy: 0.5994
Epoch 31: val_loss improved from 0.66198 to 0.66195, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6587 - accuracy: 0.5994 - val_loss: 0.6619 - val_accuracy: 0.5972
Epoch 32/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.6587 - accuracy: 0.5991
Epoch 32: val_loss improved from 0.66195 to 0.66193, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6587 - accuracy: 0.5994 - val_loss: 0.6619 - val_accuracy: 0.5972
Epoch 33/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.6578 - accuracy: 0.6014
Epoch 33: val_loss did not improve from 0.66193
682/682 [==============================] - 2s 2ms/step - loss: 0.6587 - accuracy: 0.5994 - val_loss: 0.6619 - val_accuracy: 0.5972
Epoch 34/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.6587 - accuracy: 0.5996
Epoch 34: val_loss improved from 0.66193 to 0.66189, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6586 - accuracy: 0.5994 - val_loss: 0.6619 - val_accuracy: 0.5972
Epoch 35/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.6585 - accuracy: 0.5990
Epoch 35: val_loss improved from 0.66189 to 0.66187, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6586 - accuracy: 0.5994 - val_loss: 0.6619 - val_accuracy: 0.5972
Epoch 36/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.6584 - accuracy: 0.5999
Epoch 36: val_loss improved from 0.66187 to 0.66186, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6586 - accuracy: 0.5994 - val_loss: 0.6619 - val_accuracy: 0.5972
Epoch 37/60
654/682 [===========================&gt;..] - ETA: 0s - loss: 0.6592 - accuracy: 0.5989
Epoch 37: val_loss improved from 0.66186 to 0.66185, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6585 - accuracy: 0.5994 - val_loss: 0.6618 - val_accuracy: 0.5972
Epoch 38/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.6584 - accuracy: 0.5997
Epoch 38: val_loss improved from 0.66185 to 0.66184, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6585 - accuracy: 0.5994 - val_loss: 0.6618 - val_accuracy: 0.5972
Epoch 39/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.6586 - accuracy: 0.5984
Epoch 39: val_loss did not improve from 0.66184
682/682 [==============================] - 2s 2ms/step - loss: 0.6585 - accuracy: 0.5994 - val_loss: 0.6619 - val_accuracy: 0.5972
Epoch 40/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.6577 - accuracy: 0.6006
Epoch 40: val_loss did not improve from 0.66184
682/682 [==============================] - 2s 2ms/step - loss: 0.6585 - accuracy: 0.5994 - val_loss: 0.6618 - val_accuracy: 0.5972
Epoch 41/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.6584 - accuracy: 0.6000
Epoch 41: val_loss improved from 0.66184 to 0.66183, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6585 - accuracy: 0.5994 - val_loss: 0.6618 - val_accuracy: 0.5972
Epoch 42/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.6585 - accuracy: 0.5993
Epoch 42: val_loss did not improve from 0.66183
682/682 [==============================] - 2s 2ms/step - loss: 0.6584 - accuracy: 0.5994 - val_loss: 0.6618 - val_accuracy: 0.5972
Epoch 43/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.6584 - accuracy: 0.5991
Epoch 43: val_loss did not improve from 0.66183
682/682 [==============================] - 1s 2ms/step - loss: 0.6585 - accuracy: 0.5994 - val_loss: 0.6618 - val_accuracy: 0.5972
Epoch 44/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.6584 - accuracy: 0.5999
Epoch 44: val_loss did not improve from 0.66183
682/682 [==============================] - 2s 2ms/step - loss: 0.6585 - accuracy: 0.5994 - val_loss: 0.6618 - val_accuracy: 0.5972
Epoch 45/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.6589 - accuracy: 0.5983
Epoch 45: val_loss improved from 0.66183 to 0.66181, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6585 - accuracy: 0.5994 - val_loss: 0.6618 - val_accuracy: 0.5972
Epoch 46/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.6578 - accuracy: 0.5997
Epoch 46: val_loss did not improve from 0.66181
682/682 [==============================] - 2s 2ms/step - loss: 0.6584 - accuracy: 0.5994 - val_loss: 0.6618 - val_accuracy: 0.5972
Epoch 47/60
682/682 [==============================] - ETA: 0s - loss: 0.6585 - accuracy: 0.5994
Epoch 47: val_loss did not improve from 0.66181
682/682 [==============================] - 1s 2ms/step - loss: 0.6585 - accuracy: 0.5994 - val_loss: 0.6618 - val_accuracy: 0.5972
Epoch 48/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.6587 - accuracy: 0.5999
Epoch 48: val_loss did not improve from 0.66181
682/682 [==============================] - 2s 2ms/step - loss: 0.6584 - accuracy: 0.5994 - val_loss: 0.6618 - val_accuracy: 0.5972
Epoch 49/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.6585 - accuracy: 0.5996
Epoch 49: val_loss did not improve from 0.66181
682/682 [==============================] - 1s 2ms/step - loss: 0.6584 - accuracy: 0.5994 - val_loss: 0.6618 - val_accuracy: 0.5972
Epoch 50/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.6576 - accuracy: 0.6009
Epoch 50: val_loss improved from 0.66181 to 0.66180, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6584 - accuracy: 0.5994 - val_loss: 0.6618 - val_accuracy: 0.5972
Epoch 51/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.6572 - accuracy: 0.6011
Epoch 51: val_loss did not improve from 0.66180
682/682 [==============================] - 2s 2ms/step - loss: 0.6584 - accuracy: 0.5994 - val_loss: 0.6618 - val_accuracy: 0.5972
Epoch 52/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.6584 - accuracy: 0.5998
Epoch 52: val_loss did not improve from 0.66180
682/682 [==============================] - 2s 2ms/step - loss: 0.6584 - accuracy: 0.5994 - val_loss: 0.6618 - val_accuracy: 0.5972
Epoch 53/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.6587 - accuracy: 0.5997
Epoch 53: val_loss did not improve from 0.66180
682/682 [==============================] - 2s 2ms/step - loss: 0.6584 - accuracy: 0.5994 - val_loss: 0.6618 - val_accuracy: 0.5972
Epoch 54/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.6583 - accuracy: 0.5989
Epoch 54: val_loss did not improve from 0.66180
682/682 [==============================] - 1s 2ms/step - loss: 0.6584 - accuracy: 0.5994 - val_loss: 0.6618 - val_accuracy: 0.5972
Epoch 55/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.6585 - accuracy: 0.5993
Epoch 55: val_loss did not improve from 0.66180
682/682 [==============================] - 2s 2ms/step - loss: 0.6584 - accuracy: 0.5994 - val_loss: 0.6618 - val_accuracy: 0.5972
Epoch 56/60
652/682 [===========================&gt;..] - ETA: 0s - loss: 0.6590 - accuracy: 0.5980
Epoch 56: val_loss did not improve from 0.66180
682/682 [==============================] - 2s 2ms/step - loss: 0.6584 - accuracy: 0.5994 - val_loss: 0.6618 - val_accuracy: 0.5972
Epoch 57/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.6584 - accuracy: 0.5997
Epoch 57: val_loss did not improve from 0.66180
682/682 [==============================] - 2s 2ms/step - loss: 0.6584 - accuracy: 0.5994 - val_loss: 0.6618 - val_accuracy: 0.5972
Epoch 58/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.6598 - accuracy: 0.5976
Epoch 58: val_loss did not improve from 0.66180
682/682 [==============================] - 1s 2ms/step - loss: 0.6584 - accuracy: 0.5994 - val_loss: 0.6618 - val_accuracy: 0.5972
Epoch 59/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.6576 - accuracy: 0.6018
Epoch 59: val_loss did not improve from 0.66180
682/682 [==============================] - 2s 2ms/step - loss: 0.6584 - accuracy: 0.5994 - val_loss: 0.6618 - val_accuracy: 0.5972
Epoch 60/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.6582 - accuracy: 0.5997
Epoch 60: val_loss did not improve from 0.66180
682/682 [==============================] - 2s 2ms/step - loss: 0.6584 - accuracy: 0.5994 - val_loss: 0.6618 - val_accuracy: 0.5972
54/54 [==============================] - 0s 2ms/step - loss: 0.6618 - accuracy: 0.5972
Processing feature: poutcome
data frame x has shape: (8516, 1)
Epoch 1/60
650/682 [===========================&gt;..] - ETA: 0s - loss: 0.6505 - accuracy: 0.5894
Epoch 1: val_loss improved from inf to 0.65059, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6511 - accuracy: 0.5874 - val_loss: 0.6506 - val_accuracy: 0.5872
Epoch 2/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.6477 - accuracy: 0.5876
Epoch 2: val_loss improved from 0.65059 to 0.64850, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6478 - accuracy: 0.5874 - val_loss: 0.6485 - val_accuracy: 0.5872
Epoch 3/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.6458 - accuracy: 0.5879
Epoch 3: val_loss improved from 0.64850 to 0.64742, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6461 - accuracy: 0.5874 - val_loss: 0.6474 - val_accuracy: 0.5872
Epoch 4/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.6450 - accuracy: 0.5876
Epoch 4: val_loss improved from 0.64742 to 0.64690, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6451 - accuracy: 0.5874 - val_loss: 0.6469 - val_accuracy: 0.5872
Epoch 5/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.6446 - accuracy: 0.5873
Epoch 5: val_loss improved from 0.64690 to 0.64657, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6445 - accuracy: 0.5874 - val_loss: 0.6466 - val_accuracy: 0.5872
Epoch 6/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.6434 - accuracy: 0.5862
Epoch 6: val_loss improved from 0.64657 to 0.64639, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6442 - accuracy: 0.5874 - val_loss: 0.6464 - val_accuracy: 0.5872
Epoch 7/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.6436 - accuracy: 0.5883
Epoch 7: val_loss improved from 0.64639 to 0.64630, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6440 - accuracy: 0.5874 - val_loss: 0.6463 - val_accuracy: 0.5872
Epoch 8/60
653/682 [===========================&gt;..] - ETA: 0s - loss: 0.6439 - accuracy: 0.5896
Epoch 8: val_loss improved from 0.64630 to 0.64628, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6438 - accuracy: 0.5874 - val_loss: 0.6463 - val_accuracy: 0.5872
Epoch 9/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.6428 - accuracy: 0.5889
Epoch 9: val_loss improved from 0.64628 to 0.64624, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6437 - accuracy: 0.5874 - val_loss: 0.6462 - val_accuracy: 0.5872
Epoch 10/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.6436 - accuracy: 0.5874
Epoch 10: val_loss did not improve from 0.64624
682/682 [==============================] - 1s 2ms/step - loss: 0.6436 - accuracy: 0.5874 - val_loss: 0.6463 - val_accuracy: 0.5872
Epoch 11/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.6430 - accuracy: 0.5871
Epoch 11: val_loss did not improve from 0.64624
682/682 [==============================] - 2s 2ms/step - loss: 0.6436 - accuracy: 0.5874 - val_loss: 0.6463 - val_accuracy: 0.5872
Epoch 12/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.6438 - accuracy: 0.5877
Epoch 12: val_loss did not improve from 0.64624
682/682 [==============================] - 2s 2ms/step - loss: 0.6436 - accuracy: 0.5874 - val_loss: 0.6463 - val_accuracy: 0.5872
Epoch 13/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.6439 - accuracy: 0.5871
Epoch 13: val_loss did not improve from 0.64624
682/682 [==============================] - 2s 2ms/step - loss: 0.6435 - accuracy: 0.5874 - val_loss: 0.6463 - val_accuracy: 0.5872
Epoch 14/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.6438 - accuracy: 0.5882
Epoch 14: val_loss did not improve from 0.64624
682/682 [==============================] - 1s 2ms/step - loss: 0.6435 - accuracy: 0.5874 - val_loss: 0.6463 - val_accuracy: 0.5872
Epoch 15/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.6434 - accuracy: 0.5880
Epoch 15: val_loss did not improve from 0.64624
682/682 [==============================] - 2s 2ms/step - loss: 0.6435 - accuracy: 0.5874 - val_loss: 0.6463 - val_accuracy: 0.5872
Epoch 16/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.6437 - accuracy: 0.5859
Epoch 16: val_loss did not improve from 0.64624
682/682 [==============================] - 2s 2ms/step - loss: 0.6435 - accuracy: 0.5874 - val_loss: 0.6463 - val_accuracy: 0.5872
Epoch 17/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.6430 - accuracy: 0.5870
Epoch 17: val_loss did not improve from 0.64624
682/682 [==============================] - 1s 2ms/step - loss: 0.6435 - accuracy: 0.5874 - val_loss: 0.6463 - val_accuracy: 0.5872
Epoch 18/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.6438 - accuracy: 0.5860
Epoch 18: val_loss did not improve from 0.64624
682/682 [==============================] - 2s 2ms/step - loss: 0.6435 - accuracy: 0.5874 - val_loss: 0.6463 - val_accuracy: 0.5872
Epoch 19/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.6433 - accuracy: 0.5877
Epoch 19: val_loss did not improve from 0.64624
682/682 [==============================] - 2s 2ms/step - loss: 0.6435 - accuracy: 0.5874 - val_loss: 0.6463 - val_accuracy: 0.5872
Epoch 20/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.6435 - accuracy: 0.5883
Epoch 20: val_loss did not improve from 0.64624
682/682 [==============================] - 1s 2ms/step - loss: 0.6435 - accuracy: 0.5874 - val_loss: 0.6464 - val_accuracy: 0.5872
Epoch 21/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.6435 - accuracy: 0.5876
Epoch 21: val_loss did not improve from 0.64624
682/682 [==============================] - 1s 2ms/step - loss: 0.6435 - accuracy: 0.5874 - val_loss: 0.6464 - val_accuracy: 0.5872
Epoch 22/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.6442 - accuracy: 0.5866
Epoch 22: val_loss did not improve from 0.64624
682/682 [==============================] - 2s 2ms/step - loss: 0.6435 - accuracy: 0.5874 - val_loss: 0.6464 - val_accuracy: 0.5872
Epoch 23/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.6436 - accuracy: 0.5871
Epoch 23: val_loss did not improve from 0.64624
682/682 [==============================] - 2s 2ms/step - loss: 0.6435 - accuracy: 0.5874 - val_loss: 0.6464 - val_accuracy: 0.5872
Epoch 24/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.6432 - accuracy: 0.5867
Epoch 24: val_loss did not improve from 0.64624
682/682 [==============================] - 2s 2ms/step - loss: 0.6435 - accuracy: 0.5874 - val_loss: 0.6464 - val_accuracy: 0.5872
Epoch 25/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.6435 - accuracy: 0.5876
Epoch 25: val_loss did not improve from 0.64624
682/682 [==============================] - 2s 2ms/step - loss: 0.6435 - accuracy: 0.5874 - val_loss: 0.6464 - val_accuracy: 0.5872
Epoch 26/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.6440 - accuracy: 0.5850
Epoch 26: val_loss did not improve from 0.64624
682/682 [==============================] - 2s 2ms/step - loss: 0.6435 - accuracy: 0.5874 - val_loss: 0.6464 - val_accuracy: 0.5872
Epoch 27/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.6435 - accuracy: 0.5875
Epoch 27: val_loss did not improve from 0.64624
682/682 [==============================] - 2s 2ms/step - loss: 0.6435 - accuracy: 0.5874 - val_loss: 0.6464 - val_accuracy: 0.5872
Epoch 28/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.6430 - accuracy: 0.5878
Epoch 28: val_loss did not improve from 0.64624
682/682 [==============================] - 2s 2ms/step - loss: 0.6435 - accuracy: 0.5874 - val_loss: 0.6464 - val_accuracy: 0.5872
Epoch 29/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.6435 - accuracy: 0.5866
Epoch 29: val_loss did not improve from 0.64624
682/682 [==============================] - 1s 2ms/step - loss: 0.6435 - accuracy: 0.5874 - val_loss: 0.6464 - val_accuracy: 0.5872
Epoch 29: early stopping
54/54 [==============================] - 0s 2ms/step - loss: 0.6462 - accuracy: 0.5872
Processing feature: emp.var.rate
data frame x has shape: (8516, 1)
Epoch 1/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.6675 - accuracy: 0.5783
Epoch 1: val_loss improved from inf to 0.63609, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6670 - accuracy: 0.5780 - val_loss: 0.6361 - val_accuracy: 0.6025
Epoch 2/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.6305 - accuracy: 0.6712
Epoch 2: val_loss improved from 0.63609 to 0.62274, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6306 - accuracy: 0.6712 - val_loss: 0.6227 - val_accuracy: 0.7158
Epoch 3/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.6225 - accuracy: 0.7139
Epoch 3: val_loss improved from 0.62274 to 0.61732, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6226 - accuracy: 0.7132 - val_loss: 0.6173 - val_accuracy: 0.7158
Epoch 4/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.6180 - accuracy: 0.7130
Epoch 4: val_loss improved from 0.61732 to 0.61318, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6180 - accuracy: 0.7132 - val_loss: 0.6132 - val_accuracy: 0.7158
Epoch 5/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.6145 - accuracy: 0.7134
Epoch 5: val_loss improved from 0.61318 to 0.60980, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6143 - accuracy: 0.7132 - val_loss: 0.6098 - val_accuracy: 0.7158
Epoch 6/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.6123 - accuracy: 0.7121
Epoch 6: val_loss improved from 0.60980 to 0.60707, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6115 - accuracy: 0.7132 - val_loss: 0.6071 - val_accuracy: 0.7158
Epoch 7/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.6095 - accuracy: 0.7126
Epoch 7: val_loss improved from 0.60707 to 0.60485, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6092 - accuracy: 0.7132 - val_loss: 0.6049 - val_accuracy: 0.7158
Epoch 8/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.6080 - accuracy: 0.7116
Epoch 8: val_loss improved from 0.60485 to 0.60313, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6073 - accuracy: 0.7132 - val_loss: 0.6031 - val_accuracy: 0.7158
Epoch 9/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.6067 - accuracy: 0.7115
Epoch 9: val_loss improved from 0.60313 to 0.60173, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6058 - accuracy: 0.7132 - val_loss: 0.6017 - val_accuracy: 0.7158
Epoch 10/60
654/682 [===========================&gt;..] - ETA: 0s - loss: 0.6041 - accuracy: 0.7144
Epoch 10: val_loss improved from 0.60173 to 0.60059, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6047 - accuracy: 0.7132 - val_loss: 0.6006 - val_accuracy: 0.7158
Epoch 11/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.6037 - accuracy: 0.7132
Epoch 11: val_loss improved from 0.60059 to 0.59978, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6037 - accuracy: 0.7132 - val_loss: 0.5998 - val_accuracy: 0.7158
Epoch 12/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.6027 - accuracy: 0.7139
Epoch 12: val_loss improved from 0.59978 to 0.59907, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6031 - accuracy: 0.7132 - val_loss: 0.5991 - val_accuracy: 0.7158
Epoch 13/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.6026 - accuracy: 0.7133
Epoch 13: val_loss improved from 0.59907 to 0.59849, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6025 - accuracy: 0.7132 - val_loss: 0.5985 - val_accuracy: 0.7158
Epoch 14/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.6021 - accuracy: 0.7131
Epoch 14: val_loss improved from 0.59849 to 0.59804, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6020 - accuracy: 0.7132 - val_loss: 0.5980 - val_accuracy: 0.7158
Epoch 15/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.6010 - accuracy: 0.7136
Epoch 15: val_loss improved from 0.59804 to 0.59765, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6016 - accuracy: 0.7132 - val_loss: 0.5976 - val_accuracy: 0.7158
Epoch 16/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.6006 - accuracy: 0.7148
Epoch 16: val_loss improved from 0.59765 to 0.59732, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6013 - accuracy: 0.7132 - val_loss: 0.5973 - val_accuracy: 0.7158
Epoch 17/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.6020 - accuracy: 0.7126
Epoch 17: val_loss improved from 0.59732 to 0.59708, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6011 - accuracy: 0.7132 - val_loss: 0.5971 - val_accuracy: 0.7158
Epoch 18/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.6007 - accuracy: 0.7135
Epoch 18: val_loss improved from 0.59708 to 0.59685, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6009 - accuracy: 0.7132 - val_loss: 0.5969 - val_accuracy: 0.7158
Epoch 19/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.6011 - accuracy: 0.7135
Epoch 19: val_loss improved from 0.59685 to 0.59669, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6007 - accuracy: 0.7132 - val_loss: 0.5967 - val_accuracy: 0.7158
Epoch 20/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.6010 - accuracy: 0.7128
Epoch 20: val_loss improved from 0.59669 to 0.59655, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6006 - accuracy: 0.7132 - val_loss: 0.5966 - val_accuracy: 0.7158
Epoch 21/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.6011 - accuracy: 0.7130
Epoch 21: val_loss improved from 0.59655 to 0.59645, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6005 - accuracy: 0.7132 - val_loss: 0.5964 - val_accuracy: 0.7158
Epoch 22/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.5991 - accuracy: 0.7144
Epoch 22: val_loss improved from 0.59645 to 0.59635, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6004 - accuracy: 0.7132 - val_loss: 0.5964 - val_accuracy: 0.7158
Epoch 23/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.6004 - accuracy: 0.7131
Epoch 23: val_loss improved from 0.59635 to 0.59631, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6003 - accuracy: 0.7132 - val_loss: 0.5963 - val_accuracy: 0.7158
Epoch 24/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.6012 - accuracy: 0.7127
Epoch 24: val_loss improved from 0.59631 to 0.59624, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6003 - accuracy: 0.7132 - val_loss: 0.5962 - val_accuracy: 0.7158
Epoch 25/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.5990 - accuracy: 0.7148
Epoch 25: val_loss improved from 0.59624 to 0.59614, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6002 - accuracy: 0.7132 - val_loss: 0.5961 - val_accuracy: 0.7158
Epoch 26/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.6001 - accuracy: 0.7132
Epoch 26: val_loss improved from 0.59614 to 0.59611, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6002 - accuracy: 0.7132 - val_loss: 0.5961 - val_accuracy: 0.7158
Epoch 27/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.6007 - accuracy: 0.7131
Epoch 27: val_loss improved from 0.59611 to 0.59605, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6001 - accuracy: 0.7132 - val_loss: 0.5961 - val_accuracy: 0.7158
Epoch 28/60
682/682 [==============================] - ETA: 0s - loss: 0.6002 - accuracy: 0.7132
Epoch 28: val_loss improved from 0.59605 to 0.59601, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6002 - accuracy: 0.7132 - val_loss: 0.5960 - val_accuracy: 0.7158
Epoch 29/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.6001 - accuracy: 0.7131
Epoch 29: val_loss improved from 0.59601 to 0.59600, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6001 - accuracy: 0.7132 - val_loss: 0.5960 - val_accuracy: 0.7158
Epoch 30/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.6004 - accuracy: 0.7130
Epoch 30: val_loss did not improve from 0.59600
682/682 [==============================] - 2s 2ms/step - loss: 0.6000 - accuracy: 0.7132 - val_loss: 0.5962 - val_accuracy: 0.7158
Epoch 31/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.6002 - accuracy: 0.7133
Epoch 31: val_loss improved from 0.59600 to 0.59597, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6002 - accuracy: 0.7132 - val_loss: 0.5960 - val_accuracy: 0.7158
Epoch 32/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.6005 - accuracy: 0.7124
Epoch 32: val_loss did not improve from 0.59597
682/682 [==============================] - 2s 2ms/step - loss: 0.6001 - accuracy: 0.7132 - val_loss: 0.5960 - val_accuracy: 0.7158
Epoch 33/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.6005 - accuracy: 0.7127
Epoch 33: val_loss improved from 0.59597 to 0.59592, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6001 - accuracy: 0.7132 - val_loss: 0.5959 - val_accuracy: 0.7158
Epoch 34/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.5992 - accuracy: 0.7139
Epoch 34: val_loss improved from 0.59592 to 0.59592, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6001 - accuracy: 0.7132 - val_loss: 0.5959 - val_accuracy: 0.7158
Epoch 35/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.5995 - accuracy: 0.7137
Epoch 35: val_loss improved from 0.59592 to 0.59590, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6001 - accuracy: 0.7132 - val_loss: 0.5959 - val_accuracy: 0.7158
Epoch 36/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.6007 - accuracy: 0.7126
Epoch 36: val_loss improved from 0.59590 to 0.59590, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6001 - accuracy: 0.7132 - val_loss: 0.5959 - val_accuracy: 0.7158
Epoch 37/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.5994 - accuracy: 0.7142
Epoch 37: val_loss improved from 0.59590 to 0.59589, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6000 - accuracy: 0.7132 - val_loss: 0.5959 - val_accuracy: 0.7158
Epoch 38/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.6001 - accuracy: 0.7131
Epoch 38: val_loss improved from 0.59589 to 0.59588, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6000 - accuracy: 0.7132 - val_loss: 0.5959 - val_accuracy: 0.7158
Epoch 39/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.5999 - accuracy: 0.7139
Epoch 39: val_loss improved from 0.59588 to 0.59587, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6001 - accuracy: 0.7132 - val_loss: 0.5959 - val_accuracy: 0.7158
Epoch 40/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.6000 - accuracy: 0.7134
Epoch 40: val_loss did not improve from 0.59587
682/682 [==============================] - 2s 2ms/step - loss: 0.6000 - accuracy: 0.7132 - val_loss: 0.5959 - val_accuracy: 0.7158
Epoch 41/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.6008 - accuracy: 0.7123
Epoch 41: val_loss improved from 0.59587 to 0.59586, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6000 - accuracy: 0.7132 - val_loss: 0.5959 - val_accuracy: 0.7158
Epoch 42/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.6011 - accuracy: 0.7122
Epoch 42: val_loss did not improve from 0.59586
682/682 [==============================] - 1s 2ms/step - loss: 0.6000 - accuracy: 0.7132 - val_loss: 0.5959 - val_accuracy: 0.7158
Epoch 43/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.6001 - accuracy: 0.7133
Epoch 43: val_loss did not improve from 0.59586
682/682 [==============================] - 1s 2ms/step - loss: 0.6000 - accuracy: 0.7132 - val_loss: 0.5959 - val_accuracy: 0.7158
Epoch 44/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.6003 - accuracy: 0.7124
Epoch 44: val_loss improved from 0.59586 to 0.59585, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6001 - accuracy: 0.7132 - val_loss: 0.5958 - val_accuracy: 0.7158
Epoch 45/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.5999 - accuracy: 0.7134
Epoch 45: val_loss improved from 0.59585 to 0.59585, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6000 - accuracy: 0.7132 - val_loss: 0.5958 - val_accuracy: 0.7158
Epoch 46/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.5983 - accuracy: 0.7150
Epoch 46: val_loss improved from 0.59585 to 0.59585, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6000 - accuracy: 0.7132 - val_loss: 0.5958 - val_accuracy: 0.7158
Epoch 47/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.6006 - accuracy: 0.7126
Epoch 47: val_loss did not improve from 0.59585
682/682 [==============================] - 2s 2ms/step - loss: 0.6000 - accuracy: 0.7132 - val_loss: 0.5959 - val_accuracy: 0.7158
Epoch 48/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.6002 - accuracy: 0.7130
Epoch 48: val_loss improved from 0.59585 to 0.59585, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6000 - accuracy: 0.7132 - val_loss: 0.5958 - val_accuracy: 0.7158
Epoch 49/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.6013 - accuracy: 0.7115
Epoch 49: val_loss improved from 0.59585 to 0.59584, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6000 - accuracy: 0.7132 - val_loss: 0.5958 - val_accuracy: 0.7158
Epoch 50/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.5999 - accuracy: 0.7134
Epoch 50: val_loss did not improve from 0.59584
682/682 [==============================] - 2s 2ms/step - loss: 0.6000 - accuracy: 0.7132 - val_loss: 0.5959 - val_accuracy: 0.7158
Epoch 51/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.5993 - accuracy: 0.7138
Epoch 51: val_loss improved from 0.59584 to 0.59584, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6000 - accuracy: 0.7132 - val_loss: 0.5958 - val_accuracy: 0.7158
Epoch 52/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.6000 - accuracy: 0.7133
Epoch 52: val_loss did not improve from 0.59584
682/682 [==============================] - 1s 2ms/step - loss: 0.6000 - accuracy: 0.7132 - val_loss: 0.5959 - val_accuracy: 0.7158
Epoch 53/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.5996 - accuracy: 0.7134
Epoch 53: val_loss improved from 0.59584 to 0.59584, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6000 - accuracy: 0.7132 - val_loss: 0.5958 - val_accuracy: 0.7158
Epoch 54/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.6003 - accuracy: 0.7130
Epoch 54: val_loss did not improve from 0.59584
682/682 [==============================] - 2s 2ms/step - loss: 0.6000 - accuracy: 0.7132 - val_loss: 0.5959 - val_accuracy: 0.7158
Epoch 55/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.6001 - accuracy: 0.7130
Epoch 55: val_loss improved from 0.59584 to 0.59583, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6000 - accuracy: 0.7132 - val_loss: 0.5958 - val_accuracy: 0.7158
Epoch 56/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.6005 - accuracy: 0.7127
Epoch 56: val_loss did not improve from 0.59583
682/682 [==============================] - 2s 2ms/step - loss: 0.6000 - accuracy: 0.7132 - val_loss: 0.5958 - val_accuracy: 0.7158
Epoch 57/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.6015 - accuracy: 0.7116
Epoch 57: val_loss did not improve from 0.59583
682/682 [==============================] - 2s 2ms/step - loss: 0.6000 - accuracy: 0.7132 - val_loss: 0.5958 - val_accuracy: 0.7158
Epoch 58/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.6016 - accuracy: 0.7120
Epoch 58: val_loss did not improve from 0.59583
682/682 [==============================] - 2s 2ms/step - loss: 0.6000 - accuracy: 0.7132 - val_loss: 0.5958 - val_accuracy: 0.7158
Epoch 59/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.6006 - accuracy: 0.7125
Epoch 59: val_loss improved from 0.59583 to 0.59583, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6000 - accuracy: 0.7132 - val_loss: 0.5958 - val_accuracy: 0.7158
Epoch 60/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.6007 - accuracy: 0.7126
Epoch 60: val_loss did not improve from 0.59583
682/682 [==============================] - 2s 2ms/step - loss: 0.6000 - accuracy: 0.7132 - val_loss: 0.5959 - val_accuracy: 0.7158
54/54 [==============================] - 0s 2ms/step - loss: 0.5958 - accuracy: 0.7158
Processing feature: cons.price.idx
data frame x has shape: (8516, 1)
Epoch 1/60
682/682 [==============================] - ETA: 0s - loss: 0.7053 - accuracy: 0.4361
Epoch 1: val_loss improved from inf to 0.70063, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.7053 - accuracy: 0.4361 - val_loss: 0.7006 - val_accuracy: 0.3893
Epoch 2/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.6976 - accuracy: 0.4747
Epoch 2: val_loss improved from 0.70063 to 0.69615, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6975 - accuracy: 0.4750 - val_loss: 0.6962 - val_accuracy: 0.5103
Epoch 3/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.6944 - accuracy: 0.5007
Epoch 3: val_loss improved from 0.69615 to 0.69268, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6944 - accuracy: 0.5010 - val_loss: 0.6927 - val_accuracy: 0.5009
Epoch 4/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.6917 - accuracy: 0.5050
Epoch 4: val_loss improved from 0.69268 to 0.68969, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6917 - accuracy: 0.5052 - val_loss: 0.6897 - val_accuracy: 0.5531
Epoch 5/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.6895 - accuracy: 0.5723
Epoch 5: val_loss improved from 0.68969 to 0.68715, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6895 - accuracy: 0.5733 - val_loss: 0.6871 - val_accuracy: 0.6271
Epoch 6/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.6875 - accuracy: 0.5974
Epoch 6: val_loss improved from 0.68715 to 0.68478, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6875 - accuracy: 0.5986 - val_loss: 0.6848 - val_accuracy: 0.6271
Epoch 7/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.6858 - accuracy: 0.5964
Epoch 7: val_loss improved from 0.68478 to 0.68282, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6858 - accuracy: 0.5961 - val_loss: 0.6828 - val_accuracy: 0.5984
Epoch 8/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.6841 - accuracy: 0.5860
Epoch 8: val_loss improved from 0.68282 to 0.68102, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6844 - accuracy: 0.5840 - val_loss: 0.6810 - val_accuracy: 0.5984
Epoch 9/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.6832 - accuracy: 0.5818
Epoch 9: val_loss improved from 0.68102 to 0.67955, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6831 - accuracy: 0.5820 - val_loss: 0.6795 - val_accuracy: 0.5984
Epoch 10/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.6821 - accuracy: 0.5824
Epoch 10: val_loss improved from 0.67955 to 0.67824, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6821 - accuracy: 0.5827 - val_loss: 0.6782 - val_accuracy: 0.6107
Epoch 11/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.6808 - accuracy: 0.5958
Epoch 11: val_loss improved from 0.67824 to 0.67702, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6812 - accuracy: 0.5945 - val_loss: 0.6770 - val_accuracy: 0.5984
Epoch 12/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.6803 - accuracy: 0.5937
Epoch 12: val_loss improved from 0.67702 to 0.67605, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6804 - accuracy: 0.5930 - val_loss: 0.6761 - val_accuracy: 0.6107
Epoch 13/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.6796 - accuracy: 0.5920
Epoch 13: val_loss improved from 0.67605 to 0.67523, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6798 - accuracy: 0.5911 - val_loss: 0.6752 - val_accuracy: 0.6107
Epoch 14/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.6791 - accuracy: 0.5881
Epoch 14: val_loss improved from 0.67523 to 0.67436, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6792 - accuracy: 0.5876 - val_loss: 0.6744 - val_accuracy: 0.6107
Epoch 15/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.6786 - accuracy: 0.5964
Epoch 15: val_loss improved from 0.67436 to 0.67374, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6787 - accuracy: 0.5964 - val_loss: 0.6737 - val_accuracy: 0.6107
Epoch 16/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.6782 - accuracy: 0.5889
Epoch 16: val_loss improved from 0.67374 to 0.67320, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6783 - accuracy: 0.5881 - val_loss: 0.6732 - val_accuracy: 0.5784
Epoch 17/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.6781 - accuracy: 0.5738
Epoch 17: val_loss improved from 0.67320 to 0.67265, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6780 - accuracy: 0.5739 - val_loss: 0.6727 - val_accuracy: 0.6107
Epoch 18/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.6775 - accuracy: 0.5819
Epoch 18: val_loss improved from 0.67265 to 0.67212, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6777 - accuracy: 0.5805 - val_loss: 0.6721 - val_accuracy: 0.6107
Epoch 19/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.6774 - accuracy: 0.5786
Epoch 19: val_loss improved from 0.67212 to 0.67173, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6774 - accuracy: 0.5793 - val_loss: 0.6717 - val_accuracy: 0.6107
Epoch 20/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.6772 - accuracy: 0.5739
Epoch 20: val_loss improved from 0.67173 to 0.67135, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6772 - accuracy: 0.5742 - val_loss: 0.6714 - val_accuracy: 0.6107
Epoch 21/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.6771 - accuracy: 0.5647
Epoch 21: val_loss improved from 0.67135 to 0.67099, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6770 - accuracy: 0.5660 - val_loss: 0.6710 - val_accuracy: 0.6107
Epoch 22/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.6769 - accuracy: 0.5779
Epoch 22: val_loss improved from 0.67099 to 0.67074, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6769 - accuracy: 0.5785 - val_loss: 0.6707 - val_accuracy: 0.6107
Epoch 23/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.6769 - accuracy: 0.5711
Epoch 23: val_loss improved from 0.67074 to 0.67056, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6768 - accuracy: 0.5708 - val_loss: 0.6706 - val_accuracy: 0.5784
Epoch 24/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.6766 - accuracy: 0.5633
Epoch 24: val_loss improved from 0.67056 to 0.67032, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6766 - accuracy: 0.5632 - val_loss: 0.6703 - val_accuracy: 0.5784
Epoch 25/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.6764 - accuracy: 0.5749
Epoch 25: val_loss improved from 0.67032 to 0.67016, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6765 - accuracy: 0.5748 - val_loss: 0.6702 - val_accuracy: 0.5784
Epoch 26/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.6764 - accuracy: 0.5674
Epoch 26: val_loss improved from 0.67016 to 0.66995, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6764 - accuracy: 0.5669 - val_loss: 0.6699 - val_accuracy: 0.5784
Epoch 27/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.6761 - accuracy: 0.5759
Epoch 27: val_loss did not improve from 0.66995
682/682 [==============================] - 2s 2ms/step - loss: 0.6763 - accuracy: 0.5746 - val_loss: 0.6700 - val_accuracy: 0.5784
Epoch 28/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.6764 - accuracy: 0.5568
Epoch 28: val_loss improved from 0.66995 to 0.66972, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6763 - accuracy: 0.5572 - val_loss: 0.6697 - val_accuracy: 0.5784
Epoch 29/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.6770 - accuracy: 0.5583
Epoch 29: val_loss improved from 0.66972 to 0.66954, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6762 - accuracy: 0.5598 - val_loss: 0.6695 - val_accuracy: 0.5784
Epoch 30/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.6761 - accuracy: 0.5588
Epoch 30: val_loss improved from 0.66954 to 0.66937, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6762 - accuracy: 0.5586 - val_loss: 0.6694 - val_accuracy: 0.5784
Epoch 31/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.6762 - accuracy: 0.5609
Epoch 31: val_loss improved from 0.66937 to 0.66926, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6761 - accuracy: 0.5614 - val_loss: 0.6693 - val_accuracy: 0.5784
Epoch 32/60
682/682 [==============================] - ETA: 0s - loss: 0.6761 - accuracy: 0.5572
Epoch 32: val_loss improved from 0.66926 to 0.66917, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6761 - accuracy: 0.5572 - val_loss: 0.6692 - val_accuracy: 0.5784
Epoch 33/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.6762 - accuracy: 0.5573
Epoch 33: val_loss improved from 0.66917 to 0.66910, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6761 - accuracy: 0.5572 - val_loss: 0.6691 - val_accuracy: 0.5784
Epoch 34/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.6760 - accuracy: 0.5609
Epoch 34: val_loss improved from 0.66910 to 0.66894, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6760 - accuracy: 0.5604 - val_loss: 0.6689 - val_accuracy: 0.5784
Epoch 35/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.6757 - accuracy: 0.5702
Epoch 35: val_loss did not improve from 0.66894
682/682 [==============================] - 2s 2ms/step - loss: 0.6760 - accuracy: 0.5694 - val_loss: 0.6690 - val_accuracy: 0.5784
Epoch 36/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.6758 - accuracy: 0.5623
Epoch 36: val_loss did not improve from 0.66894
682/682 [==============================] - 2s 2ms/step - loss: 0.6760 - accuracy: 0.5622 - val_loss: 0.6690 - val_accuracy: 0.5784
Epoch 37/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.6762 - accuracy: 0.5571
Epoch 37: val_loss improved from 0.66894 to 0.66887, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6760 - accuracy: 0.5572 - val_loss: 0.6689 - val_accuracy: 0.5784
Epoch 38/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.6755 - accuracy: 0.5586
Epoch 38: val_loss improved from 0.66887 to 0.66875, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6760 - accuracy: 0.5572 - val_loss: 0.6688 - val_accuracy: 0.5784
Epoch 39/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.6763 - accuracy: 0.5560
Epoch 39: val_loss improved from 0.66875 to 0.66871, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6759 - accuracy: 0.5572 - val_loss: 0.6687 - val_accuracy: 0.5784
Epoch 40/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.6755 - accuracy: 0.5588
Epoch 40: val_loss improved from 0.66871 to 0.66863, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6760 - accuracy: 0.5572 - val_loss: 0.6686 - val_accuracy: 0.5784
Epoch 41/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.6758 - accuracy: 0.5582
Epoch 41: val_loss improved from 0.66863 to 0.66860, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6759 - accuracy: 0.5572 - val_loss: 0.6686 - val_accuracy: 0.5784
Epoch 42/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.6758 - accuracy: 0.5572
Epoch 42: val_loss did not improve from 0.66860
682/682 [==============================] - 2s 2ms/step - loss: 0.6759 - accuracy: 0.5572 - val_loss: 0.6686 - val_accuracy: 0.5784
Epoch 43/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.6761 - accuracy: 0.5572
Epoch 43: val_loss improved from 0.66860 to 0.66853, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6759 - accuracy: 0.5572 - val_loss: 0.6685 - val_accuracy: 0.5784
Epoch 44/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.6756 - accuracy: 0.5577
Epoch 44: val_loss improved from 0.66853 to 0.66842, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6759 - accuracy: 0.5570 - val_loss: 0.6684 - val_accuracy: 0.5784
Epoch 45/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.6756 - accuracy: 0.5593
Epoch 45: val_loss did not improve from 0.66842
682/682 [==============================] - 1s 2ms/step - loss: 0.6759 - accuracy: 0.5583 - val_loss: 0.6685 - val_accuracy: 0.5784
Epoch 46/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.6762 - accuracy: 0.5566
Epoch 46: val_loss improved from 0.66842 to 0.66840, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6759 - accuracy: 0.5572 - val_loss: 0.6684 - val_accuracy: 0.5784
Epoch 47/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.6754 - accuracy: 0.5588
Epoch 47: val_loss did not improve from 0.66840
682/682 [==============================] - 1s 2ms/step - loss: 0.6759 - accuracy: 0.5575 - val_loss: 0.6685 - val_accuracy: 0.5784
Epoch 48/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.6759 - accuracy: 0.5595
Epoch 48: val_loss improved from 0.66840 to 0.66837, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6759 - accuracy: 0.5598 - val_loss: 0.6684 - val_accuracy: 0.5784
Epoch 49/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.6754 - accuracy: 0.5673
Epoch 49: val_loss did not improve from 0.66837
682/682 [==============================] - 1s 2ms/step - loss: 0.6759 - accuracy: 0.5657 - val_loss: 0.6685 - val_accuracy: 0.5784
Epoch 50/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.6760 - accuracy: 0.5566
Epoch 50: val_loss did not improve from 0.66837
682/682 [==============================] - 2s 2ms/step - loss: 0.6759 - accuracy: 0.5572 - val_loss: 0.6684 - val_accuracy: 0.5784
Epoch 51/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.6761 - accuracy: 0.5576
Epoch 51: val_loss improved from 0.66837 to 0.66834, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6759 - accuracy: 0.5572 - val_loss: 0.6683 - val_accuracy: 0.5784
Epoch 52/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.6758 - accuracy: 0.5655
Epoch 52: val_loss did not improve from 0.66834
682/682 [==============================] - 2s 2ms/step - loss: 0.6759 - accuracy: 0.5657 - val_loss: 0.6684 - val_accuracy: 0.5784
Epoch 53/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.6760 - accuracy: 0.5565
Epoch 53: val_loss improved from 0.66834 to 0.66829, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6759 - accuracy: 0.5572 - val_loss: 0.6683 - val_accuracy: 0.5784
Epoch 54/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.6758 - accuracy: 0.5579
Epoch 54: val_loss did not improve from 0.66829
682/682 [==============================] - 2s 2ms/step - loss: 0.6759 - accuracy: 0.5572 - val_loss: 0.6683 - val_accuracy: 0.5784
Epoch 55/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.6763 - accuracy: 0.5567
Epoch 55: val_loss improved from 0.66829 to 0.66828, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6759 - accuracy: 0.5572 - val_loss: 0.6683 - val_accuracy: 0.5784
Epoch 56/60
682/682 [==============================] - ETA: 0s - loss: 0.6758 - accuracy: 0.5573
Epoch 56: val_loss improved from 0.66828 to 0.66816, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6758 - accuracy: 0.5573 - val_loss: 0.6682 - val_accuracy: 0.5784
Epoch 57/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.6757 - accuracy: 0.5580
Epoch 57: val_loss improved from 0.66816 to 0.66815, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6759 - accuracy: 0.5576 - val_loss: 0.6681 - val_accuracy: 0.5784
Epoch 58/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.6764 - accuracy: 0.5571
Epoch 58: val_loss did not improve from 0.66815
682/682 [==============================] - 1s 2ms/step - loss: 0.6759 - accuracy: 0.5572 - val_loss: 0.6682 - val_accuracy: 0.5784
Epoch 59/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.6760 - accuracy: 0.5572
Epoch 59: val_loss did not improve from 0.66815
682/682 [==============================] - 1s 2ms/step - loss: 0.6759 - accuracy: 0.5572 - val_loss: 0.6682 - val_accuracy: 0.5784
Epoch 60/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.6762 - accuracy: 0.5565
Epoch 60: val_loss did not improve from 0.66815
682/682 [==============================] - 2s 2ms/step - loss: 0.6759 - accuracy: 0.5572 - val_loss: 0.6682 - val_accuracy: 0.5784
54/54 [==============================] - 0s 2ms/step - loss: 0.6681 - accuracy: 0.5784
Processing feature: cons.conf.idx
data frame x has shape: (8516, 1)
Epoch 1/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.7236 - accuracy: 0.4714
Epoch 1: val_loss improved from inf to 0.69876, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.7236 - accuracy: 0.4715 - val_loss: 0.6988 - val_accuracy: 0.4651
Epoch 2/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.6951 - accuracy: 0.5111
Epoch 2: val_loss improved from 0.69876 to 0.69109, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6951 - accuracy: 0.5130 - val_loss: 0.6911 - val_accuracy: 0.5825
Epoch 3/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.6925 - accuracy: 0.5438
Epoch 3: val_loss improved from 0.69109 to 0.69052, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6925 - accuracy: 0.5441 - val_loss: 0.6905 - val_accuracy: 0.5567
Epoch 4/60
682/682 [==============================] - ETA: 0s - loss: 0.6921 - accuracy: 0.5302
Epoch 4: val_loss improved from 0.69052 to 0.69034, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6921 - accuracy: 0.5302 - val_loss: 0.6903 - val_accuracy: 0.5314
Epoch 5/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.6918 - accuracy: 0.5350
Epoch 5: val_loss improved from 0.69034 to 0.69025, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6920 - accuracy: 0.5343 - val_loss: 0.6903 - val_accuracy: 0.5208
Epoch 6/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.6920 - accuracy: 0.5185
Epoch 6: val_loss improved from 0.69025 to 0.69014, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6919 - accuracy: 0.5187 - val_loss: 0.6901 - val_accuracy: 0.5208
Epoch 7/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.6920 - accuracy: 0.5178
Epoch 7: val_loss improved from 0.69014 to 0.69005, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6917 - accuracy: 0.5197 - val_loss: 0.6901 - val_accuracy: 0.5314
Epoch 8/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.6913 - accuracy: 0.5250
Epoch 8: val_loss improved from 0.69005 to 0.68999, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6916 - accuracy: 0.5228 - val_loss: 0.6900 - val_accuracy: 0.5314
Epoch 9/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.6915 - accuracy: 0.5233
Epoch 9: val_loss improved from 0.68999 to 0.68992, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6915 - accuracy: 0.5233 - val_loss: 0.6899 - val_accuracy: 0.5314
Epoch 10/60
651/682 [===========================&gt;..] - ETA: 0s - loss: 0.6908 - accuracy: 0.5158
Epoch 10: val_loss improved from 0.68992 to 0.68988, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6914 - accuracy: 0.5139 - val_loss: 0.6899 - val_accuracy: 0.5420
Epoch 11/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5401
Epoch 11: val_loss improved from 0.68988 to 0.68985, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6913 - accuracy: 0.5390 - val_loss: 0.6899 - val_accuracy: 0.5208
Epoch 12/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5208
Epoch 12: val_loss improved from 0.68985 to 0.68982, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5202 - val_loss: 0.6898 - val_accuracy: 0.5314
Epoch 13/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5383
Epoch 13: val_loss improved from 0.68982 to 0.68981, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5379 - val_loss: 0.6898 - val_accuracy: 0.5208
Epoch 14/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.6914 - accuracy: 0.5202
Epoch 14: val_loss improved from 0.68981 to 0.68977, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5206 - val_loss: 0.6898 - val_accuracy: 0.5314
Epoch 15/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5195
Epoch 15: val_loss improved from 0.68977 to 0.68975, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6911 - accuracy: 0.5211 - val_loss: 0.6898 - val_accuracy: 0.5420
Epoch 16/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.6907 - accuracy: 0.5373
Epoch 16: val_loss did not improve from 0.68975
682/682 [==============================] - 1s 2ms/step - loss: 0.6911 - accuracy: 0.5352 - val_loss: 0.6898 - val_accuracy: 0.5208
Epoch 17/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.6913 - accuracy: 0.5143
Epoch 17: val_loss improved from 0.68975 to 0.68974, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6911 - accuracy: 0.5161 - val_loss: 0.6897 - val_accuracy: 0.5420
Epoch 18/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.6913 - accuracy: 0.5267
Epoch 18: val_loss did not improve from 0.68974
682/682 [==============================] - 2s 2ms/step - loss: 0.6911 - accuracy: 0.5275 - val_loss: 0.6897 - val_accuracy: 0.5602
Epoch 19/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.6914 - accuracy: 0.5441
Epoch 19: val_loss improved from 0.68974 to 0.68973, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5456 - val_loss: 0.6897 - val_accuracy: 0.5420
Epoch 20/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.6913 - accuracy: 0.5321
Epoch 20: val_loss did not improve from 0.68973
682/682 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5337 - val_loss: 0.6897 - val_accuracy: 0.5314
Epoch 21/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5141
Epoch 21: val_loss did not improve from 0.68973
682/682 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5159 - val_loss: 0.6897 - val_accuracy: 0.5602
Epoch 22/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.6909 - accuracy: 0.5501
Epoch 22: val_loss did not improve from 0.68973
682/682 [==============================] - 1s 2ms/step - loss: 0.6910 - accuracy: 0.5494 - val_loss: 0.6897 - val_accuracy: 0.5314
Epoch 23/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.6910 - accuracy: 0.5371
Epoch 23: val_loss did not improve from 0.68973
682/682 [==============================] - 1s 2ms/step - loss: 0.6909 - accuracy: 0.5368 - val_loss: 0.6897 - val_accuracy: 0.5314
Epoch 24/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.6910 - accuracy: 0.5112
Epoch 24: val_loss did not improve from 0.68973
682/682 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5128 - val_loss: 0.6897 - val_accuracy: 0.5602
Epoch 25/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.6909 - accuracy: 0.5469
Epoch 25: val_loss did not improve from 0.68973
682/682 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5469 - val_loss: 0.6897 - val_accuracy: 0.5567
Epoch 26/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.6909 - accuracy: 0.5527
Epoch 26: val_loss did not improve from 0.68973
682/682 [==============================] - 2s 2ms/step - loss: 0.6909 - accuracy: 0.5529 - val_loss: 0.6897 - val_accuracy: 0.5314
Epoch 27/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.6910 - accuracy: 0.5329
Epoch 27: val_loss did not improve from 0.68973
682/682 [==============================] - 2s 2ms/step - loss: 0.6909 - accuracy: 0.5322 - val_loss: 0.6898 - val_accuracy: 0.5208
Epoch 28/60
653/682 [===========================&gt;..] - ETA: 0s - loss: 0.6912 - accuracy: 0.5190
Epoch 28: val_loss did not improve from 0.68973
682/682 [==============================] - 1s 2ms/step - loss: 0.6909 - accuracy: 0.5208 - val_loss: 0.6897 - val_accuracy: 0.5420
Epoch 29/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.6910 - accuracy: 0.5175
Epoch 29: val_loss did not improve from 0.68973
682/682 [==============================] - 2s 2ms/step - loss: 0.6909 - accuracy: 0.5184 - val_loss: 0.6897 - val_accuracy: 0.5602
Epoch 30/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.6909 - accuracy: 0.5292
Epoch 30: val_loss did not improve from 0.68973
682/682 [==============================] - 2s 2ms/step - loss: 0.6909 - accuracy: 0.5303 - val_loss: 0.6897 - val_accuracy: 0.5567
Epoch 31/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.6909 - accuracy: 0.5409
Epoch 31: val_loss did not improve from 0.68973
682/682 [==============================] - 2s 2ms/step - loss: 0.6909 - accuracy: 0.5407 - val_loss: 0.6897 - val_accuracy: 0.5420
Epoch 32/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.6908 - accuracy: 0.5413
Epoch 32: val_loss did not improve from 0.68973
682/682 [==============================] - 2s 2ms/step - loss: 0.6909 - accuracy: 0.5407 - val_loss: 0.6898 - val_accuracy: 0.5420
Epoch 33/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5186
Epoch 33: val_loss did not improve from 0.68973
682/682 [==============================] - 2s 2ms/step - loss: 0.6909 - accuracy: 0.5202 - val_loss: 0.6898 - val_accuracy: 0.5602
Epoch 34/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.6909 - accuracy: 0.5324
Epoch 34: val_loss did not improve from 0.68973
682/682 [==============================] - 2s 2ms/step - loss: 0.6909 - accuracy: 0.5328 - val_loss: 0.6898 - val_accuracy: 0.5602
Epoch 35/60
652/682 [===========================&gt;..] - ETA: 0s - loss: 0.6907 - accuracy: 0.5267
Epoch 35: val_loss did not improve from 0.68973
682/682 [==============================] - 2s 2ms/step - loss: 0.6909 - accuracy: 0.5261 - val_loss: 0.6898 - val_accuracy: 0.5602
Epoch 36/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.6909 - accuracy: 0.5448
Epoch 36: val_loss did not improve from 0.68973
682/682 [==============================] - 2s 2ms/step - loss: 0.6909 - accuracy: 0.5448 - val_loss: 0.6898 - val_accuracy: 0.5602
Epoch 37/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.6911 - accuracy: 0.5314
Epoch 37: val_loss did not improve from 0.68973
682/682 [==============================] - 2s 2ms/step - loss: 0.6909 - accuracy: 0.5330 - val_loss: 0.6898 - val_accuracy: 0.5602
Epoch 38/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.6910 - accuracy: 0.5330
Epoch 38: val_loss did not improve from 0.68973
682/682 [==============================] - 2s 2ms/step - loss: 0.6909 - accuracy: 0.5335 - val_loss: 0.6898 - val_accuracy: 0.5602
Epoch 39/60
650/682 [===========================&gt;..] - ETA: 0s - loss: 0.6910 - accuracy: 0.5486
Epoch 39: val_loss did not improve from 0.68973
682/682 [==============================] - 1s 2ms/step - loss: 0.6909 - accuracy: 0.5482 - val_loss: 0.6898 - val_accuracy: 0.5567
Epoch 39: early stopping
54/54 [==============================] - 0s 2ms/step - loss: 0.6897 - accuracy: 0.5420
Processing feature: euribor3m
data frame x has shape: (8516, 1)
Epoch 1/60
682/682 [==============================] - ETA: 0s - loss: 0.6399 - accuracy: 0.6800
Epoch 1: val_loss improved from inf to 0.62941, saving model to feature_best.hdf5
682/682 [==============================] - 2s 3ms/step - loss: 0.6399 - accuracy: 0.6800 - val_loss: 0.6294 - val_accuracy: 0.7158
Epoch 2/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.6238 - accuracy: 0.7139
Epoch 2: val_loss improved from 0.62941 to 0.61746, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6238 - accuracy: 0.7132 - val_loss: 0.6175 - val_accuracy: 0.7158
Epoch 3/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.6139 - accuracy: 0.7141
Epoch 3: val_loss improved from 0.61746 to 0.60960, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6143 - accuracy: 0.7132 - val_loss: 0.6096 - val_accuracy: 0.7158
Epoch 4/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.6076 - accuracy: 0.7132
Epoch 4: val_loss improved from 0.60960 to 0.60405, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6077 - accuracy: 0.7132 - val_loss: 0.6040 - val_accuracy: 0.7158
Epoch 5/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.6032 - accuracy: 0.7132
Epoch 5: val_loss improved from 0.60405 to 0.60018, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6032 - accuracy: 0.7132 - val_loss: 0.6002 - val_accuracy: 0.7158
Epoch 6/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.6006 - accuracy: 0.7124
Epoch 6: val_loss improved from 0.60018 to 0.59748, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6001 - accuracy: 0.7132 - val_loss: 0.5975 - val_accuracy: 0.7158
Epoch 7/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.5978 - accuracy: 0.7134
Epoch 7: val_loss improved from 0.59748 to 0.59546, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5978 - accuracy: 0.7132 - val_loss: 0.5955 - val_accuracy: 0.7158
Epoch 8/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.5961 - accuracy: 0.7134
Epoch 8: val_loss improved from 0.59546 to 0.59403, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5962 - accuracy: 0.7132 - val_loss: 0.5940 - val_accuracy: 0.7158
Epoch 9/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.5948 - accuracy: 0.7136
Epoch 9: val_loss improved from 0.59403 to 0.59308, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5951 - accuracy: 0.7132 - val_loss: 0.5931 - val_accuracy: 0.7158
Epoch 10/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.5954 - accuracy: 0.7123
Epoch 10: val_loss improved from 0.59308 to 0.59239, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5944 - accuracy: 0.7132 - val_loss: 0.5924 - val_accuracy: 0.7158
Epoch 11/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.5943 - accuracy: 0.7124
Epoch 11: val_loss improved from 0.59239 to 0.59191, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5938 - accuracy: 0.7132 - val_loss: 0.5919 - val_accuracy: 0.7158
Epoch 12/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.5937 - accuracy: 0.7129
Epoch 12: val_loss improved from 0.59191 to 0.59153, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5935 - accuracy: 0.7132 - val_loss: 0.5915 - val_accuracy: 0.7158
Epoch 13/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.5928 - accuracy: 0.7137
Epoch 13: val_loss improved from 0.59153 to 0.59127, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5932 - accuracy: 0.7132 - val_loss: 0.5913 - val_accuracy: 0.7158
Epoch 14/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.5935 - accuracy: 0.7124
Epoch 14: val_loss improved from 0.59127 to 0.59112, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5930 - accuracy: 0.7132 - val_loss: 0.5911 - val_accuracy: 0.7158
Epoch 15/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.5926 - accuracy: 0.7136
Epoch 15: val_loss improved from 0.59112 to 0.59096, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.5929 - accuracy: 0.7132 - val_loss: 0.5910 - val_accuracy: 0.7158
Epoch 16/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.5920 - accuracy: 0.7142
Epoch 16: val_loss improved from 0.59096 to 0.59086, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5928 - accuracy: 0.7132 - val_loss: 0.5909 - val_accuracy: 0.7158
Epoch 17/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.5935 - accuracy: 0.7123
Epoch 17: val_loss improved from 0.59086 to 0.59079, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5928 - accuracy: 0.7132 - val_loss: 0.5908 - val_accuracy: 0.7158
Epoch 18/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.5933 - accuracy: 0.7125
Epoch 18: val_loss improved from 0.59079 to 0.59074, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5927 - accuracy: 0.7132 - val_loss: 0.5907 - val_accuracy: 0.7158
Epoch 19/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.5939 - accuracy: 0.7119
Epoch 19: val_loss improved from 0.59074 to 0.59069, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5927 - accuracy: 0.7132 - val_loss: 0.5907 - val_accuracy: 0.7158
Epoch 20/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.5939 - accuracy: 0.7118
Epoch 20: val_loss improved from 0.59069 to 0.59066, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5907 - val_accuracy: 0.7158
Epoch 21/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.5936 - accuracy: 0.7123
Epoch 21: val_loss improved from 0.59066 to 0.59064, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5927 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158
Epoch 22/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.5928 - accuracy: 0.7133
Epoch 22: val_loss improved from 0.59064 to 0.59062, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158
Epoch 23/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.5926 - accuracy: 0.7132
Epoch 23: val_loss improved from 0.59062 to 0.59061, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158
Epoch 24/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.5930 - accuracy: 0.7128
Epoch 24: val_loss improved from 0.59061 to 0.59060, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158
Epoch 25/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.5926 - accuracy: 0.7131
Epoch 25: val_loss did not improve from 0.59060
682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158
Epoch 26/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.5926 - accuracy: 0.7135
Epoch 26: val_loss improved from 0.59060 to 0.59058, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158
Epoch 27/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.5917 - accuracy: 0.7142
Epoch 27: val_loss improved from 0.59058 to 0.59057, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158
Epoch 28/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.5914 - accuracy: 0.7143
Epoch 28: val_loss did not improve from 0.59057
682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158
Epoch 29/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.5923 - accuracy: 0.7134
Epoch 29: val_loss improved from 0.59057 to 0.59057, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158
Epoch 30/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.5935 - accuracy: 0.7121
Epoch 30: val_loss did not improve from 0.59057
682/682 [==============================] - 1s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158
Epoch 31/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.5916 - accuracy: 0.7143
Epoch 31: val_loss improved from 0.59057 to 0.59056, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158
Epoch 32/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.5915 - accuracy: 0.7146
Epoch 32: val_loss did not improve from 0.59056
682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158
Epoch 33/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.5926 - accuracy: 0.7132
Epoch 33: val_loss did not improve from 0.59056
682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158
Epoch 34/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.5917 - accuracy: 0.7141
Epoch 34: val_loss did not improve from 0.59056
682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158
Epoch 35/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.5925 - accuracy: 0.7134
Epoch 35: val_loss improved from 0.59056 to 0.59056, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158
Epoch 36/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.5926 - accuracy: 0.7130
Epoch 36: val_loss did not improve from 0.59056
682/682 [==============================] - 1s 2ms/step - loss: 0.5925 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158
Epoch 37/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.5922 - accuracy: 0.7136
Epoch 37: val_loss improved from 0.59056 to 0.59056, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158
Epoch 38/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.5925 - accuracy: 0.7133
Epoch 38: val_loss did not improve from 0.59056
682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158
Epoch 39/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.5927 - accuracy: 0.7131
Epoch 39: val_loss improved from 0.59056 to 0.59056, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158
Epoch 40/60
654/682 [===========================&gt;..] - ETA: 0s - loss: 0.5924 - accuracy: 0.7135
Epoch 40: val_loss improved from 0.59056 to 0.59055, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158
Epoch 41/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.5926 - accuracy: 0.7131
Epoch 41: val_loss did not improve from 0.59055
682/682 [==============================] - 2s 2ms/step - loss: 0.5925 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158
Epoch 42/60
652/682 [===========================&gt;..] - ETA: 0s - loss: 0.5933 - accuracy: 0.7126
Epoch 42: val_loss did not improve from 0.59055
682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158
Epoch 43/60
653/682 [===========================&gt;..] - ETA: 0s - loss: 0.5922 - accuracy: 0.7132
Epoch 43: val_loss did not improve from 0.59055
682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158
Epoch 44/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.5931 - accuracy: 0.7127
Epoch 44: val_loss did not improve from 0.59055
682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158
Epoch 45/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.5927 - accuracy: 0.7131
Epoch 45: val_loss did not improve from 0.59055
682/682 [==============================] - 1s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158
Epoch 46/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.5924 - accuracy: 0.7132
Epoch 46: val_loss did not improve from 0.59055
682/682 [==============================] - 1s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158
Epoch 47/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.5911 - accuracy: 0.7147
Epoch 47: val_loss improved from 0.59055 to 0.59055, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158
Epoch 48/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.5928 - accuracy: 0.7131
Epoch 48: val_loss did not improve from 0.59055
682/682 [==============================] - 1s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158
Epoch 49/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.5927 - accuracy: 0.7131
Epoch 49: val_loss did not improve from 0.59055
682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158
Epoch 50/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.5930 - accuracy: 0.7128
Epoch 50: val_loss did not improve from 0.59055
682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158
Epoch 51/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.5921 - accuracy: 0.7137
Epoch 51: val_loss did not improve from 0.59055
682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158
Epoch 52/60
653/682 [===========================&gt;..] - ETA: 0s - loss: 0.5925 - accuracy: 0.7133
Epoch 52: val_loss did not improve from 0.59055
682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7133 - val_loss: 0.5906 - val_accuracy: 0.7158
Epoch 53/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.5926 - accuracy: 0.7136
Epoch 53: val_loss did not improve from 0.59055
682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158
Epoch 54/60
654/682 [===========================&gt;..] - ETA: 0s - loss: 0.5916 - accuracy: 0.7144
Epoch 54: val_loss did not improve from 0.59055
682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158
Epoch 55/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.5931 - accuracy: 0.7127
Epoch 55: val_loss did not improve from 0.59055
682/682 [==============================] - 1s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158
Epoch 56/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.5928 - accuracy: 0.7131
Epoch 56: val_loss did not improve from 0.59055
682/682 [==============================] - 1s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158
Epoch 57/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.5907 - accuracy: 0.7152
Epoch 57: val_loss improved from 0.59055 to 0.59055, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158
Epoch 58/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.5929 - accuracy: 0.7129
Epoch 58: val_loss did not improve from 0.59055
682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158
Epoch 59/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.5934 - accuracy: 0.7122
Epoch 59: val_loss did not improve from 0.59055
682/682 [==============================] - 1s 2ms/step - loss: 0.5925 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158
Epoch 60/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.5912 - accuracy: 0.7148
Epoch 60: val_loss improved from 0.59055 to 0.59055, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.7132 - val_loss: 0.5906 - val_accuracy: 0.7158
54/54 [==============================] - 0s 2ms/step - loss: 0.5906 - accuracy: 0.7158
Processing feature: divorced
data frame x has shape: (8516, 1)
Epoch 1/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.7156 - accuracy: 0.5043
Epoch 1: val_loss improved from inf to 0.70783, saving model to feature_best.hdf5
682/682 [==============================] - 2s 3ms/step - loss: 0.7152 - accuracy: 0.5048 - val_loss: 0.7078 - val_accuracy: 0.5068
Epoch 2/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.7068 - accuracy: 0.5048
Epoch 2: val_loss improved from 0.70783 to 0.70189, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.7068 - accuracy: 0.5048 - val_loss: 0.7019 - val_accuracy: 0.5068
Epoch 3/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.7017 - accuracy: 0.5024
Epoch 3: val_loss improved from 0.70189 to 0.69819, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.7015 - accuracy: 0.5048 - val_loss: 0.6982 - val_accuracy: 0.5068
Epoch 4/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.6983 - accuracy: 0.5042
Epoch 4: val_loss improved from 0.69819 to 0.69586, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6981 - accuracy: 0.5048 - val_loss: 0.6959 - val_accuracy: 0.5068
Epoch 5/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.6959 - accuracy: 0.5050
Epoch 5: val_loss improved from 0.69586 to 0.69448, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6960 - accuracy: 0.5048 - val_loss: 0.6945 - val_accuracy: 0.5068
Epoch 6/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.6946 - accuracy: 0.5044
Epoch 6: val_loss improved from 0.69448 to 0.69368, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6948 - accuracy: 0.5048 - val_loss: 0.6937 - val_accuracy: 0.5068
Epoch 7/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.6940 - accuracy: 0.5052
Epoch 7: val_loss improved from 0.69368 to 0.69324, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6941 - accuracy: 0.5048 - val_loss: 0.6932 - val_accuracy: 0.5068
Epoch 8/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.6936 - accuracy: 0.5056
Epoch 8: val_loss improved from 0.69324 to 0.69304, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6936 - accuracy: 0.5048 - val_loss: 0.6930 - val_accuracy: 0.5068
Epoch 9/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.6934 - accuracy: 0.5052
Epoch 9: val_loss improved from 0.69304 to 0.69295, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6934 - accuracy: 0.5048 - val_loss: 0.6929 - val_accuracy: 0.5068
Epoch 10/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.6934 - accuracy: 0.5039
Epoch 10: val_loss improved from 0.69295 to 0.69289, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6933 - accuracy: 0.5048 - val_loss: 0.6929 - val_accuracy: 0.5068
Epoch 11/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.5046
Epoch 11: val_loss improved from 0.69289 to 0.69288, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.5048 - val_loss: 0.6929 - val_accuracy: 0.5068
Epoch 12/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.5050
Epoch 12: val_loss improved from 0.69288 to 0.69286, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.5048 - val_loss: 0.6929 - val_accuracy: 0.5068
Epoch 13/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.5046
Epoch 13: val_loss did not improve from 0.69286
682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.5048 - val_loss: 0.6929 - val_accuracy: 0.5068
Epoch 14/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.6931 - accuracy: 0.5046
Epoch 14: val_loss did not improve from 0.69286
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5048 - val_loss: 0.6929 - val_accuracy: 0.5068
Epoch 15/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5060
Epoch 15: val_loss did not improve from 0.69286
682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.5048 - val_loss: 0.6929 - val_accuracy: 0.5068
Epoch 16/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5066
Epoch 16: val_loss did not improve from 0.69286
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5048 - val_loss: 0.6929 - val_accuracy: 0.5068
Epoch 17/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5021
Epoch 17: val_loss did not improve from 0.69286
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5021 - val_loss: 0.6929 - val_accuracy: 0.5068
Epoch 18/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.4994
Epoch 18: val_loss did not improve from 0.69286
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4999 - val_loss: 0.6929 - val_accuracy: 0.5068
Epoch 19/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5050
Epoch 19: val_loss did not improve from 0.69286
682/682 [==============================] - 1s 2ms/step - loss: 0.6931 - accuracy: 0.5048 - val_loss: 0.6929 - val_accuracy: 0.5068
Epoch 20/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.6931 - accuracy: 0.5014
Epoch 20: val_loss did not improve from 0.69286
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5001 - val_loss: 0.6929 - val_accuracy: 0.5068
Epoch 21/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.4991
Epoch 21: val_loss did not improve from 0.69286
682/682 [==============================] - 1s 2ms/step - loss: 0.6931 - accuracy: 0.4992 - val_loss: 0.6929 - val_accuracy: 0.5068
Epoch 22/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5051
Epoch 22: val_loss did not improve from 0.69286
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5048 - val_loss: 0.6929 - val_accuracy: 0.5068
Epoch 23/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.6931 - accuracy: 0.4940
Epoch 23: val_loss did not improve from 0.69286
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4948 - val_loss: 0.6929 - val_accuracy: 0.5068
Epoch 24/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5007
Epoch 24: val_loss did not improve from 0.69286
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5008 - val_loss: 0.6929 - val_accuracy: 0.5068
Epoch 25/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.5046
Epoch 25: val_loss did not improve from 0.69286
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5048 - val_loss: 0.6929 - val_accuracy: 0.5068
Epoch 26/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5048
Epoch 26: val_loss did not improve from 0.69286
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5048 - val_loss: 0.6929 - val_accuracy: 0.5068
Epoch 27/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5051
Epoch 27: val_loss did not improve from 0.69286
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5057 - val_loss: 0.6929 - val_accuracy: 0.5068
Epoch 28/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.5018
Epoch 28: val_loss did not improve from 0.69286
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5023 - val_loss: 0.6929 - val_accuracy: 0.5068
Epoch 29/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5065
Epoch 29: val_loss did not improve from 0.69286
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5048 - val_loss: 0.6929 - val_accuracy: 0.5068
Epoch 30/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5060
Epoch 30: val_loss did not improve from 0.69286
682/682 [==============================] - 1s 2ms/step - loss: 0.6931 - accuracy: 0.5048 - val_loss: 0.6929 - val_accuracy: 0.5068
Epoch 31/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4955
Epoch 31: val_loss did not improve from 0.69286
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4963 - val_loss: 0.6929 - val_accuracy: 0.5068
Epoch 32/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5071
Epoch 32: val_loss did not improve from 0.69286
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5048 - val_loss: 0.6929 - val_accuracy: 0.5068
Epoch 32: early stopping
54/54 [==============================] - 0s 2ms/step - loss: 0.6929 - accuracy: 0.5068
Processing feature: married
data frame x has shape: (8516, 1)
Epoch 1/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.7740 - accuracy: 0.4736
Epoch 1: val_loss improved from inf to 0.73603, saving model to feature_best.hdf5
682/682 [==============================] - 2s 3ms/step - loss: 0.7740 - accuracy: 0.4737 - val_loss: 0.7360 - val_accuracy: 0.4680
Epoch 2/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.7215 - accuracy: 0.4736
Epoch 2: val_loss improved from 0.73603 to 0.71526, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.7215 - accuracy: 0.4737 - val_loss: 0.7153 - val_accuracy: 0.4680
Epoch 3/60
652/682 [===========================&gt;..] - ETA: 0s - loss: 0.7094 - accuracy: 0.4735
Epoch 3: val_loss improved from 0.71526 to 0.70727, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.7094 - accuracy: 0.4737 - val_loss: 0.7073 - val_accuracy: 0.4680
Epoch 4/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.7038 - accuracy: 0.4731
Epoch 4: val_loss improved from 0.70727 to 0.70204, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.7035 - accuracy: 0.4737 - val_loss: 0.7020 - val_accuracy: 0.4680
Epoch 5/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.6995 - accuracy: 0.4737
Epoch 5: val_loss improved from 0.70204 to 0.69850, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6996 - accuracy: 0.4737 - val_loss: 0.6985 - val_accuracy: 0.4680
Epoch 6/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.6968 - accuracy: 0.4758
Epoch 6: val_loss improved from 0.69850 to 0.69611, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6969 - accuracy: 0.4750 - val_loss: 0.6961 - val_accuracy: 0.4680
Epoch 7/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.6952 - accuracy: 0.4787
Epoch 7: val_loss improved from 0.69611 to 0.69444, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6951 - accuracy: 0.4814 - val_loss: 0.6944 - val_accuracy: 0.5009
Epoch 8/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.6939 - accuracy: 0.5002
Epoch 8: val_loss improved from 0.69444 to 0.69336, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6939 - accuracy: 0.4998 - val_loss: 0.6934 - val_accuracy: 0.5009
Epoch 9/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.6932 - accuracy: 0.4998
Epoch 9: val_loss improved from 0.69336 to 0.69272, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5008 - val_loss: 0.6927 - val_accuracy: 0.5320
Epoch 10/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.6927 - accuracy: 0.5258
Epoch 10: val_loss improved from 0.69272 to 0.69220, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6927 - accuracy: 0.5263 - val_loss: 0.6922 - val_accuracy: 0.5320
Epoch 11/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.6924 - accuracy: 0.5269
Epoch 11: val_loss improved from 0.69220 to 0.69186, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6924 - accuracy: 0.5263 - val_loss: 0.6919 - val_accuracy: 0.5320
Epoch 12/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.6922 - accuracy: 0.5264
Epoch 12: val_loss improved from 0.69186 to 0.69163, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6922 - accuracy: 0.5263 - val_loss: 0.6916 - val_accuracy: 0.5320
Epoch 13/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.6920 - accuracy: 0.5277
Epoch 13: val_loss improved from 0.69163 to 0.69148, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6921 - accuracy: 0.5263 - val_loss: 0.6915 - val_accuracy: 0.5320
Epoch 14/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.6920 - accuracy: 0.5264
Epoch 14: val_loss improved from 0.69148 to 0.69140, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6920 - accuracy: 0.5263 - val_loss: 0.6914 - val_accuracy: 0.5320
Epoch 15/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.6918 - accuracy: 0.5275
Epoch 15: val_loss improved from 0.69140 to 0.69136, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6919 - accuracy: 0.5263 - val_loss: 0.6914 - val_accuracy: 0.5320
Epoch 16/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.6919 - accuracy: 0.5261
Epoch 16: val_loss improved from 0.69136 to 0.69130, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6919 - accuracy: 0.5263 - val_loss: 0.6913 - val_accuracy: 0.5320
Epoch 17/60
654/682 [===========================&gt;..] - ETA: 0s - loss: 0.6918 - accuracy: 0.5266
Epoch 17: val_loss improved from 0.69130 to 0.69125, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6919 - accuracy: 0.5263 - val_loss: 0.6912 - val_accuracy: 0.5320
Epoch 18/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.6919 - accuracy: 0.5263
Epoch 18: val_loss improved from 0.69125 to 0.69123, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6919 - accuracy: 0.5263 - val_loss: 0.6912 - val_accuracy: 0.5320
Epoch 19/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.6919 - accuracy: 0.5259
Epoch 19: val_loss improved from 0.69123 to 0.69119, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6919 - accuracy: 0.5263 - val_loss: 0.6912 - val_accuracy: 0.5320
Epoch 20/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.6918 - accuracy: 0.5268
Epoch 20: val_loss improved from 0.69119 to 0.69118, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6912 - val_accuracy: 0.5320
Epoch 21/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.6918 - accuracy: 0.5269
Epoch 21: val_loss improved from 0.69118 to 0.69118, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6912 - val_accuracy: 0.5320
Epoch 22/60
653/682 [===========================&gt;..] - ETA: 0s - loss: 0.6919 - accuracy: 0.5260
Epoch 22: val_loss did not improve from 0.69118
682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6912 - val_accuracy: 0.5320
Epoch 23/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.6919 - accuracy: 0.5260
Epoch 23: val_loss did not improve from 0.69118
682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6912 - val_accuracy: 0.5320
Epoch 24/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.6918 - accuracy: 0.5267
Epoch 24: val_loss improved from 0.69118 to 0.69116, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6912 - val_accuracy: 0.5320
Epoch 25/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.6916 - accuracy: 0.5285
Epoch 25: val_loss improved from 0.69116 to 0.69113, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5320
Epoch 26/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.6918 - accuracy: 0.5267
Epoch 26: val_loss did not improve from 0.69113
682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5320
Epoch 27/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.6919 - accuracy: 0.5259
Epoch 27: val_loss did not improve from 0.69113
682/682 [==============================] - 2s 2ms/step - loss: 0.6919 - accuracy: 0.5263 - val_loss: 0.6912 - val_accuracy: 0.5320
Epoch 28/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.6918 - accuracy: 0.5266
Epoch 28: val_loss did not improve from 0.69113
682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5320
Epoch 29/60
654/682 [===========================&gt;..] - ETA: 0s - loss: 0.6917 - accuracy: 0.5274
Epoch 29: val_loss did not improve from 0.69113
682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6912 - val_accuracy: 0.5320
Epoch 30/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.6919 - accuracy: 0.5258
Epoch 30: val_loss did not improve from 0.69113
682/682 [==============================] - 2s 2ms/step - loss: 0.6919 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5320
Epoch 31/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.6918 - accuracy: 0.5257
Epoch 31: val_loss did not improve from 0.69113
682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6912 - val_accuracy: 0.5320
Epoch 32/60
682/682 [==============================] - ETA: 0s - loss: 0.6918 - accuracy: 0.5263
Epoch 32: val_loss did not improve from 0.69113
682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6912 - val_accuracy: 0.5320
Epoch 33/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.6919 - accuracy: 0.5261
Epoch 33: val_loss did not improve from 0.69113
682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6912 - val_accuracy: 0.5320
Epoch 34/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.6918 - accuracy: 0.5268
Epoch 34: val_loss did not improve from 0.69113
682/682 [==============================] - 2s 2ms/step - loss: 0.6919 - accuracy: 0.5263 - val_loss: 0.6912 - val_accuracy: 0.5320
Epoch 35/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.6919 - accuracy: 0.5263
Epoch 35: val_loss did not improve from 0.69113
682/682 [==============================] - 2s 2ms/step - loss: 0.6919 - accuracy: 0.5263 - val_loss: 0.6912 - val_accuracy: 0.5320
Epoch 36/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.6918 - accuracy: 0.5267
Epoch 36: val_loss did not improve from 0.69113
682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6912 - val_accuracy: 0.5320
Epoch 37/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.6919 - accuracy: 0.5264
Epoch 37: val_loss did not improve from 0.69113
682/682 [==============================] - 1s 2ms/step - loss: 0.6919 - accuracy: 0.5263 - val_loss: 0.6912 - val_accuracy: 0.5320
Epoch 38/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.6917 - accuracy: 0.5269
Epoch 38: val_loss improved from 0.69113 to 0.69112, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5320
Epoch 39/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.6918 - accuracy: 0.5261
Epoch 39: val_loss did not improve from 0.69112
682/682 [==============================] - 2s 2ms/step - loss: 0.6917 - accuracy: 0.5263 - val_loss: 0.6913 - val_accuracy: 0.5320
Epoch 40/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.6919 - accuracy: 0.5256
Epoch 40: val_loss did not improve from 0.69112
682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5320
Epoch 41/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.6919 - accuracy: 0.5254
Epoch 41: val_loss did not improve from 0.69112
682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5320
Epoch 42/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.6918 - accuracy: 0.5268
Epoch 42: val_loss did not improve from 0.69112
682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5320
Epoch 43/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.6918 - accuracy: 0.5263
Epoch 43: val_loss did not improve from 0.69112
682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5320
Epoch 44/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.6918 - accuracy: 0.5266
Epoch 44: val_loss did not improve from 0.69112
682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5320
Epoch 45/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.6918 - accuracy: 0.5266
Epoch 45: val_loss did not improve from 0.69112
682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5320
Epoch 46/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.6919 - accuracy: 0.5262
Epoch 46: val_loss did not improve from 0.69112
682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5320
Epoch 47/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.6916 - accuracy: 0.5280
Epoch 47: val_loss improved from 0.69112 to 0.69112, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5320
Epoch 48/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.6918 - accuracy: 0.5265
Epoch 48: val_loss improved from 0.69112 to 0.69112, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5320
Epoch 49/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.6916 - accuracy: 0.5281
Epoch 49: val_loss did not improve from 0.69112
682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5320
Epoch 50/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.6918 - accuracy: 0.5263
Epoch 50: val_loss improved from 0.69112 to 0.69111, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5320
Epoch 51/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.6919 - accuracy: 0.5261
Epoch 51: val_loss did not improve from 0.69111
682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5320
Epoch 52/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.6918 - accuracy: 0.5269
Epoch 52: val_loss did not improve from 0.69111
682/682 [==============================] - 2s 2ms/step - loss: 0.6919 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5320
Epoch 53/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.6918 - accuracy: 0.5267
Epoch 53: val_loss did not improve from 0.69111
682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6912 - val_accuracy: 0.5320
Epoch 54/60
682/682 [==============================] - ETA: 0s - loss: 0.6918 - accuracy: 0.5263
Epoch 54: val_loss did not improve from 0.69111
682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5320
Epoch 55/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.6920 - accuracy: 0.5250
Epoch 55: val_loss did not improve from 0.69111
682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5320
Epoch 56/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.6919 - accuracy: 0.5258
Epoch 56: val_loss did not improve from 0.69111
682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5320
Epoch 57/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.6919 - accuracy: 0.5261
Epoch 57: val_loss did not improve from 0.69111
682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5320
Epoch 58/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.6918 - accuracy: 0.5270
Epoch 58: val_loss did not improve from 0.69111
682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6912 - val_accuracy: 0.5320
Epoch 59/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.6919 - accuracy: 0.5261
Epoch 59: val_loss did not improve from 0.69111
682/682 [==============================] - 2s 2ms/step - loss: 0.6919 - accuracy: 0.5263 - val_loss: 0.6912 - val_accuracy: 0.5320
Epoch 60/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.6918 - accuracy: 0.5259
Epoch 60: val_loss did not improve from 0.69111
682/682 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5320
54/54 [==============================] - 0s 2ms/step - loss: 0.6911 - accuracy: 0.5320
Processing feature: single
data frame x has shape: (8516, 1)
Epoch 1/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.6923 - accuracy: 0.5310
Epoch 1: val_loss improved from inf to 0.68985, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6923 - accuracy: 0.5309 - val_loss: 0.6899 - val_accuracy: 0.5396
Epoch 2/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.6914 - accuracy: 0.5284
Epoch 2: val_loss improved from 0.68985 to 0.68964, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6911 - accuracy: 0.5309 - val_loss: 0.6896 - val_accuracy: 0.5396
Epoch 3/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.6911 - accuracy: 0.5302
Epoch 3: val_loss did not improve from 0.68964
682/682 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5309 - val_loss: 0.6897 - val_accuracy: 0.5396
Epoch 4/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.6910 - accuracy: 0.5311
Epoch 4: val_loss did not improve from 0.68964
682/682 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5309 - val_loss: 0.6897 - val_accuracy: 0.5396
Epoch 5/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.6909 - accuracy: 0.5314
Epoch 5: val_loss did not improve from 0.68964
682/682 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5309 - val_loss: 0.6897 - val_accuracy: 0.5396
Epoch 6/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.6909 - accuracy: 0.5319
Epoch 6: val_loss did not improve from 0.68964
682/682 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5309 - val_loss: 0.6897 - val_accuracy: 0.5396
Epoch 7/60
682/682 [==============================] - ETA: 0s - loss: 0.6910 - accuracy: 0.5309
Epoch 7: val_loss did not improve from 0.68964
682/682 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5309 - val_loss: 0.6897 - val_accuracy: 0.5396
Epoch 8/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.6909 - accuracy: 0.5313
Epoch 8: val_loss did not improve from 0.68964
682/682 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5309 - val_loss: 0.6897 - val_accuracy: 0.5396
Epoch 9/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.6909 - accuracy: 0.5310
Epoch 9: val_loss did not improve from 0.68964
682/682 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5309 - val_loss: 0.6897 - val_accuracy: 0.5396
Epoch 10/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.6909 - accuracy: 0.5316
Epoch 10: val_loss did not improve from 0.68964
682/682 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5309 - val_loss: 0.6897 - val_accuracy: 0.5396
Epoch 11/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.6909 - accuracy: 0.5315
Epoch 11: val_loss did not improve from 0.68964
682/682 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5309 - val_loss: 0.6897 - val_accuracy: 0.5396
Epoch 12/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.6911 - accuracy: 0.5304
Epoch 12: val_loss did not improve from 0.68964
682/682 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5309 - val_loss: 0.6897 - val_accuracy: 0.5396
Epoch 13/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.6911 - accuracy: 0.5308
Epoch 13: val_loss did not improve from 0.68964
682/682 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5309 - val_loss: 0.6897 - val_accuracy: 0.5396
Epoch 14/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.6909 - accuracy: 0.5318
Epoch 14: val_loss did not improve from 0.68964
682/682 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5309 - val_loss: 0.6897 - val_accuracy: 0.5396
Epoch 15/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.6909 - accuracy: 0.5311
Epoch 15: val_loss did not improve from 0.68964
682/682 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5309 - val_loss: 0.6898 - val_accuracy: 0.5396
Epoch 16/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.6910 - accuracy: 0.5306
Epoch 16: val_loss did not improve from 0.68964
682/682 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5309 - val_loss: 0.6897 - val_accuracy: 0.5396
Epoch 17/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.6910 - accuracy: 0.5305
Epoch 17: val_loss did not improve from 0.68964
682/682 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5309 - val_loss: 0.6898 - val_accuracy: 0.5396
Epoch 18/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.6909 - accuracy: 0.5314
Epoch 18: val_loss did not improve from 0.68964
682/682 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5309 - val_loss: 0.6897 - val_accuracy: 0.5396
Epoch 19/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.6909 - accuracy: 0.5310
Epoch 19: val_loss did not improve from 0.68964
682/682 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5309 - val_loss: 0.6897 - val_accuracy: 0.5396
Epoch 20/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.6909 - accuracy: 0.5313
Epoch 20: val_loss did not improve from 0.68964
682/682 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5309 - val_loss: 0.6898 - val_accuracy: 0.5396
Epoch 21/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.6909 - accuracy: 0.5312
Epoch 21: val_loss did not improve from 0.68964
682/682 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5309 - val_loss: 0.6898 - val_accuracy: 0.5396
Epoch 22/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.6910 - accuracy: 0.5307
Epoch 22: val_loss did not improve from 0.68964
682/682 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5309 - val_loss: 0.6898 - val_accuracy: 0.5396
Epoch 22: early stopping
54/54 [==============================] - 0s 2ms/step - loss: 0.6896 - accuracy: 0.5396
Processing feature: admin.
data frame x has shape: (8516, 1)
Epoch 1/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.7085 - accuracy: 0.5220
Epoch 1: val_loss improved from inf to 0.69728, saving model to feature_best.hdf5
682/682 [==============================] - 2s 3ms/step - loss: 0.7084 - accuracy: 0.5222 - val_loss: 0.6973 - val_accuracy: 0.5326
Epoch 2/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.6981 - accuracy: 0.5226
Epoch 2: val_loss improved from 0.69728 to 0.69305, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6981 - accuracy: 0.5222 - val_loss: 0.6931 - val_accuracy: 0.5326
Epoch 3/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.6950 - accuracy: 0.5219
Epoch 3: val_loss improved from 0.69305 to 0.69148, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6949 - accuracy: 0.5222 - val_loss: 0.6915 - val_accuracy: 0.5326
Epoch 4/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.6935 - accuracy: 0.5221
Epoch 4: val_loss improved from 0.69148 to 0.69086, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6935 - accuracy: 0.5222 - val_loss: 0.6909 - val_accuracy: 0.5326
Epoch 5/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.6926 - accuracy: 0.5233
Epoch 5: val_loss improved from 0.69086 to 0.69064, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6927 - accuracy: 0.5222 - val_loss: 0.6906 - val_accuracy: 0.5326
Epoch 6/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.6924 - accuracy: 0.5225
Epoch 6: val_loss improved from 0.69064 to 0.69058, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6924 - accuracy: 0.5222 - val_loss: 0.6906 - val_accuracy: 0.5326
Epoch 7/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.6920 - accuracy: 0.5238
Epoch 7: val_loss did not improve from 0.69058
682/682 [==============================] - 1s 2ms/step - loss: 0.6922 - accuracy: 0.5222 - val_loss: 0.6906 - val_accuracy: 0.5326
Epoch 8/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.6920 - accuracy: 0.5228
Epoch 8: val_loss did not improve from 0.69058
682/682 [==============================] - 2s 2ms/step - loss: 0.6921 - accuracy: 0.5222 - val_loss: 0.6906 - val_accuracy: 0.5326
Epoch 9/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.6921 - accuracy: 0.5220
Epoch 9: val_loss did not improve from 0.69058
682/682 [==============================] - 2s 2ms/step - loss: 0.6920 - accuracy: 0.5222 - val_loss: 0.6907 - val_accuracy: 0.5326
Epoch 10/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.6920 - accuracy: 0.5224
Epoch 10: val_loss did not improve from 0.69058
682/682 [==============================] - 2s 2ms/step - loss: 0.6920 - accuracy: 0.5222 - val_loss: 0.6907 - val_accuracy: 0.5326
Epoch 11/60
682/682 [==============================] - ETA: 0s - loss: 0.6920 - accuracy: 0.5222
Epoch 11: val_loss did not improve from 0.69058
682/682 [==============================] - 2s 2ms/step - loss: 0.6920 - accuracy: 0.5222 - val_loss: 0.6907 - val_accuracy: 0.5326
Epoch 12/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.6918 - accuracy: 0.5246
Epoch 12: val_loss did not improve from 0.69058
682/682 [==============================] - 2s 2ms/step - loss: 0.6920 - accuracy: 0.5222 - val_loss: 0.6908 - val_accuracy: 0.5326
Epoch 13/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.6920 - accuracy: 0.5235
Epoch 13: val_loss did not improve from 0.69058
682/682 [==============================] - 1s 2ms/step - loss: 0.6920 - accuracy: 0.5222 - val_loss: 0.6908 - val_accuracy: 0.5326
Epoch 14/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.6920 - accuracy: 0.5228
Epoch 14: val_loss did not improve from 0.69058
682/682 [==============================] - 2s 2ms/step - loss: 0.6920 - accuracy: 0.5222 - val_loss: 0.6908 - val_accuracy: 0.5326
Epoch 15/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.6919 - accuracy: 0.5227
Epoch 15: val_loss did not improve from 0.69058
682/682 [==============================] - 2s 2ms/step - loss: 0.6920 - accuracy: 0.5222 - val_loss: 0.6908 - val_accuracy: 0.5326
Epoch 16/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.6919 - accuracy: 0.5229
Epoch 16: val_loss did not improve from 0.69058
682/682 [==============================] - 2s 2ms/step - loss: 0.6920 - accuracy: 0.5222 - val_loss: 0.6908 - val_accuracy: 0.5326
Epoch 17/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.6920 - accuracy: 0.5219
Epoch 17: val_loss did not improve from 0.69058
682/682 [==============================] - 2s 2ms/step - loss: 0.6920 - accuracy: 0.5222 - val_loss: 0.6908 - val_accuracy: 0.5326
Epoch 18/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.6920 - accuracy: 0.5231
Epoch 18: val_loss did not improve from 0.69058
682/682 [==============================] - 2s 2ms/step - loss: 0.6920 - accuracy: 0.5222 - val_loss: 0.6908 - val_accuracy: 0.5326
Epoch 19/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.6919 - accuracy: 0.5228
Epoch 19: val_loss did not improve from 0.69058
682/682 [==============================] - 2s 2ms/step - loss: 0.6920 - accuracy: 0.5222 - val_loss: 0.6909 - val_accuracy: 0.5326
Epoch 20/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.6918 - accuracy: 0.5236
Epoch 20: val_loss did not improve from 0.69058
682/682 [==============================] - 2s 2ms/step - loss: 0.6920 - accuracy: 0.5222 - val_loss: 0.6908 - val_accuracy: 0.5326
Epoch 21/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.6918 - accuracy: 0.5244
Epoch 21: val_loss did not improve from 0.69058
682/682 [==============================] - 2s 2ms/step - loss: 0.6920 - accuracy: 0.5222 - val_loss: 0.6908 - val_accuracy: 0.5326
Epoch 22/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.6920 - accuracy: 0.5221
Epoch 22: val_loss did not improve from 0.69058
682/682 [==============================] - 2s 2ms/step - loss: 0.6920 - accuracy: 0.5222 - val_loss: 0.6908 - val_accuracy: 0.5326
Epoch 23/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.6920 - accuracy: 0.5223
Epoch 23: val_loss did not improve from 0.69058
682/682 [==============================] - 2s 2ms/step - loss: 0.6920 - accuracy: 0.5222 - val_loss: 0.6908 - val_accuracy: 0.5326
Epoch 24/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.6920 - accuracy: 0.5215
Epoch 24: val_loss did not improve from 0.69058
682/682 [==============================] - 2s 2ms/step - loss: 0.6920 - accuracy: 0.5222 - val_loss: 0.6908 - val_accuracy: 0.5326
Epoch 25/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.6921 - accuracy: 0.5208
Epoch 25: val_loss did not improve from 0.69058
682/682 [==============================] - 2s 2ms/step - loss: 0.6920 - accuracy: 0.5222 - val_loss: 0.6908 - val_accuracy: 0.5326
Epoch 26/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.6921 - accuracy: 0.5225
Epoch 26: val_loss did not improve from 0.69058
682/682 [==============================] - 2s 2ms/step - loss: 0.6920 - accuracy: 0.5222 - val_loss: 0.6908 - val_accuracy: 0.5326
Epoch 26: early stopping
54/54 [==============================] - 0s 2ms/step - loss: 0.6906 - accuracy: 0.5326
Processing feature: blue-collar
data frame x has shape: (8516, 1)
Epoch 1/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.6884 - accuracy: 0.5337
Epoch 1: val_loss improved from inf to 0.68628, saving model to feature_best.hdf5
682/682 [==============================] - 2s 3ms/step - loss: 0.6884 - accuracy: 0.5335 - val_loss: 0.6863 - val_accuracy: 0.5531
Epoch 2/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.6873 - accuracy: 0.5458
Epoch 2: val_loss improved from 0.68628 to 0.68528, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6873 - accuracy: 0.5462 - val_loss: 0.6853 - val_accuracy: 0.5531
Epoch 3/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.6870 - accuracy: 0.5445
Epoch 3: val_loss improved from 0.68528 to 0.68468, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6868 - accuracy: 0.5462 - val_loss: 0.6847 - val_accuracy: 0.5531
Epoch 4/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.6865 - accuracy: 0.5456
Epoch 4: val_loss improved from 0.68468 to 0.68431, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6865 - accuracy: 0.5462 - val_loss: 0.6843 - val_accuracy: 0.5531
Epoch 5/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.6865 - accuracy: 0.5464
Epoch 5: val_loss improved from 0.68431 to 0.68410, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6864 - accuracy: 0.5462 - val_loss: 0.6841 - val_accuracy: 0.5531
Epoch 6/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.6865 - accuracy: 0.5451
Epoch 6: val_loss improved from 0.68410 to 0.68398, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6863 - accuracy: 0.5462 - val_loss: 0.6840 - val_accuracy: 0.5531
Epoch 7/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.6858 - accuracy: 0.5488
Epoch 7: val_loss improved from 0.68398 to 0.68388, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6863 - accuracy: 0.5462 - val_loss: 0.6839 - val_accuracy: 0.5531
Epoch 8/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.6863 - accuracy: 0.5458
Epoch 8: val_loss improved from 0.68388 to 0.68382, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6863 - accuracy: 0.5462 - val_loss: 0.6838 - val_accuracy: 0.5531
Epoch 9/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.6860 - accuracy: 0.5460
Epoch 9: val_loss improved from 0.68382 to 0.68379, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6863 - accuracy: 0.5462 - val_loss: 0.6838 - val_accuracy: 0.5531
Epoch 10/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.6861 - accuracy: 0.5468
Epoch 10: val_loss improved from 0.68379 to 0.68373, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6862 - accuracy: 0.5462 - val_loss: 0.6837 - val_accuracy: 0.5531
Epoch 11/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.6861 - accuracy: 0.5468
Epoch 11: val_loss improved from 0.68373 to 0.68373, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6863 - accuracy: 0.5462 - val_loss: 0.6837 - val_accuracy: 0.5531
Epoch 12/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.6863 - accuracy: 0.5470
Epoch 12: val_loss did not improve from 0.68373
682/682 [==============================] - 2s 2ms/step - loss: 0.6862 - accuracy: 0.5462 - val_loss: 0.6838 - val_accuracy: 0.5531
Epoch 13/60
682/682 [==============================] - ETA: 0s - loss: 0.6862 - accuracy: 0.5462
Epoch 13: val_loss did not improve from 0.68373
682/682 [==============================] - 2s 2ms/step - loss: 0.6862 - accuracy: 0.5462 - val_loss: 0.6838 - val_accuracy: 0.5531
Epoch 14/60
650/682 [===========================&gt;..] - ETA: 0s - loss: 0.6860 - accuracy: 0.5480
Epoch 14: val_loss improved from 0.68373 to 0.68372, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6862 - accuracy: 0.5462 - val_loss: 0.6837 - val_accuracy: 0.5531
Epoch 15/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.6861 - accuracy: 0.5469
Epoch 15: val_loss did not improve from 0.68372
682/682 [==============================] - 2s 2ms/step - loss: 0.6862 - accuracy: 0.5462 - val_loss: 0.6837 - val_accuracy: 0.5531
Epoch 16/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.6864 - accuracy: 0.5458
Epoch 16: val_loss improved from 0.68372 to 0.68371, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6862 - accuracy: 0.5462 - val_loss: 0.6837 - val_accuracy: 0.5531
Epoch 17/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.6861 - accuracy: 0.5476
Epoch 17: val_loss improved from 0.68371 to 0.68368, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6863 - accuracy: 0.5462 - val_loss: 0.6837 - val_accuracy: 0.5531
Epoch 18/60
649/682 [===========================&gt;..] - ETA: 0s - loss: 0.6862 - accuracy: 0.5464
Epoch 18: val_loss improved from 0.68368 to 0.68366, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6862 - accuracy: 0.5462 - val_loss: 0.6837 - val_accuracy: 0.5531
Epoch 19/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.6861 - accuracy: 0.5473
Epoch 19: val_loss did not improve from 0.68366
682/682 [==============================] - 2s 2ms/step - loss: 0.6862 - accuracy: 0.5462 - val_loss: 0.6837 - val_accuracy: 0.5531
Epoch 20/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.6865 - accuracy: 0.5456
Epoch 20: val_loss did not improve from 0.68366
682/682 [==============================] - 2s 2ms/step - loss: 0.6862 - accuracy: 0.5462 - val_loss: 0.6837 - val_accuracy: 0.5531
Epoch 21/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.6866 - accuracy: 0.5440
Epoch 21: val_loss did not improve from 0.68366
682/682 [==============================] - 2s 2ms/step - loss: 0.6863 - accuracy: 0.5462 - val_loss: 0.6837 - val_accuracy: 0.5531
Epoch 22/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.6862 - accuracy: 0.5465
Epoch 22: val_loss did not improve from 0.68366
682/682 [==============================] - 2s 2ms/step - loss: 0.6862 - accuracy: 0.5462 - val_loss: 0.6837 - val_accuracy: 0.5531
Epoch 23/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.6862 - accuracy: 0.5473
Epoch 23: val_loss did not improve from 0.68366
682/682 [==============================] - 2s 2ms/step - loss: 0.6863 - accuracy: 0.5462 - val_loss: 0.6837 - val_accuracy: 0.5531
Epoch 24/60
648/682 [===========================&gt;..] - ETA: 0s - loss: 0.6861 - accuracy: 0.5463
Epoch 24: val_loss did not improve from 0.68366
682/682 [==============================] - 1s 2ms/step - loss: 0.6863 - accuracy: 0.5462 - val_loss: 0.6837 - val_accuracy: 0.5531
Epoch 25/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.6863 - accuracy: 0.5461
Epoch 25: val_loss did not improve from 0.68366
682/682 [==============================] - 2s 2ms/step - loss: 0.6862 - accuracy: 0.5462 - val_loss: 0.6838 - val_accuracy: 0.5531
Epoch 26/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.6866 - accuracy: 0.5453
Epoch 26: val_loss did not improve from 0.68366
682/682 [==============================] - 2s 2ms/step - loss: 0.6863 - accuracy: 0.5462 - val_loss: 0.6837 - val_accuracy: 0.5531
Epoch 27/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.6861 - accuracy: 0.5460
Epoch 27: val_loss did not improve from 0.68366
682/682 [==============================] - 2s 2ms/step - loss: 0.6862 - accuracy: 0.5462 - val_loss: 0.6837 - val_accuracy: 0.5531
Epoch 28/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.6863 - accuracy: 0.5461
Epoch 28: val_loss did not improve from 0.68366
682/682 [==============================] - 2s 2ms/step - loss: 0.6862 - accuracy: 0.5462 - val_loss: 0.6837 - val_accuracy: 0.5531
Epoch 29/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.6858 - accuracy: 0.5467
Epoch 29: val_loss did not improve from 0.68366
682/682 [==============================] - 1s 2ms/step - loss: 0.6863 - accuracy: 0.5462 - val_loss: 0.6838 - val_accuracy: 0.5531
Epoch 30/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.6863 - accuracy: 0.5459
Epoch 30: val_loss did not improve from 0.68366
682/682 [==============================] - 1s 2ms/step - loss: 0.6863 - accuracy: 0.5462 - val_loss: 0.6837 - val_accuracy: 0.5531
Epoch 31/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.6862 - accuracy: 0.5458
Epoch 31: val_loss did not improve from 0.68366
682/682 [==============================] - 2s 2ms/step - loss: 0.6862 - accuracy: 0.5462 - val_loss: 0.6837 - val_accuracy: 0.5531
Epoch 32/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.6864 - accuracy: 0.5455
Epoch 32: val_loss did not improve from 0.68366
682/682 [==============================] - 2s 2ms/step - loss: 0.6862 - accuracy: 0.5462 - val_loss: 0.6837 - val_accuracy: 0.5531
Epoch 33/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.6859 - accuracy: 0.5472
Epoch 33: val_loss did not improve from 0.68366
682/682 [==============================] - 2s 2ms/step - loss: 0.6862 - accuracy: 0.5462 - val_loss: 0.6837 - val_accuracy: 0.5531
Epoch 34/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.6863 - accuracy: 0.5466
Epoch 34: val_loss did not improve from 0.68366
682/682 [==============================] - 2s 2ms/step - loss: 0.6862 - accuracy: 0.5462 - val_loss: 0.6837 - val_accuracy: 0.5531
Epoch 35/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.6863 - accuracy: 0.5464
Epoch 35: val_loss did not improve from 0.68366
682/682 [==============================] - 2s 2ms/step - loss: 0.6862 - accuracy: 0.5462 - val_loss: 0.6837 - val_accuracy: 0.5531
Epoch 36/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.6860 - accuracy: 0.5467
Epoch 36: val_loss did not improve from 0.68366
682/682 [==============================] - 2s 2ms/step - loss: 0.6863 - accuracy: 0.5462 - val_loss: 0.6837 - val_accuracy: 0.5531
Epoch 37/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.6862 - accuracy: 0.5462
Epoch 37: val_loss did not improve from 0.68366
682/682 [==============================] - 2s 2ms/step - loss: 0.6862 - accuracy: 0.5462 - val_loss: 0.6837 - val_accuracy: 0.5531
Epoch 38/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.6860 - accuracy: 0.5469
Epoch 38: val_loss did not improve from 0.68366
682/682 [==============================] - 2s 2ms/step - loss: 0.6863 - accuracy: 0.5462 - val_loss: 0.6837 - val_accuracy: 0.5531
Epoch 38: early stopping
54/54 [==============================] - 0s 2ms/step - loss: 0.6837 - accuracy: 0.5531
Processing feature: entrepreneur
data frame x has shape: (8516, 1)
Epoch 1/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.6986 - accuracy: 0.4942
Epoch 1: val_loss improved from inf to 0.69769, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6985 - accuracy: 0.4943 - val_loss: 0.6977 - val_accuracy: 0.4962
Epoch 2/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.6971 - accuracy: 0.4963
Epoch 2: val_loss improved from 0.69769 to 0.69649, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6971 - accuracy: 0.4957 - val_loss: 0.6965 - val_accuracy: 0.4962
Epoch 3/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.6961 - accuracy: 0.4955
Epoch 3: val_loss improved from 0.69649 to 0.69556, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6960 - accuracy: 0.4957 - val_loss: 0.6956 - val_accuracy: 0.4962
Epoch 4/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.6953 - accuracy: 0.4950
Epoch 4: val_loss improved from 0.69556 to 0.69486, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6952 - accuracy: 0.4957 - val_loss: 0.6949 - val_accuracy: 0.4962
Epoch 5/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.6946 - accuracy: 0.4895
Epoch 5: val_loss improved from 0.69486 to 0.69430, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6946 - accuracy: 0.4904 - val_loss: 0.6943 - val_accuracy: 0.4962
Epoch 6/60
682/682 [==============================] - ETA: 0s - loss: 0.6941 - accuracy: 0.4957
Epoch 6: val_loss improved from 0.69430 to 0.69390, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6941 - accuracy: 0.4957 - val_loss: 0.6939 - val_accuracy: 0.4962
Epoch 7/60
653/682 [===========================&gt;..] - ETA: 0s - loss: 0.6938 - accuracy: 0.4946
Epoch 7: val_loss improved from 0.69390 to 0.69359, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6938 - accuracy: 0.4939 - val_loss: 0.6936 - val_accuracy: 0.4962
Epoch 8/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.6935 - accuracy: 0.4928
Epoch 8: val_loss improved from 0.69359 to 0.69339, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6935 - accuracy: 0.4921 - val_loss: 0.6934 - val_accuracy: 0.4991
Epoch 9/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.6934 - accuracy: 0.4944
Epoch 9: val_loss improved from 0.69339 to 0.69322, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6934 - accuracy: 0.4939 - val_loss: 0.6932 - val_accuracy: 0.4991
Epoch 10/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.6933 - accuracy: 0.4876
Epoch 10: val_loss improved from 0.69322 to 0.69309, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6933 - accuracy: 0.4876 - val_loss: 0.6931 - val_accuracy: 0.5009
Epoch 11/60
652/682 [===========================&gt;..] - ETA: 0s - loss: 0.6932 - accuracy: 0.4943
Epoch 11: val_loss improved from 0.69309 to 0.69301, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4933 - val_loss: 0.6930 - val_accuracy: 0.5009
Epoch 12/60
650/682 [===========================&gt;..] - ETA: 0s - loss: 0.6931 - accuracy: 0.4980
Epoch 12: val_loss improved from 0.69301 to 0.69296, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4985 - val_loss: 0.6930 - val_accuracy: 0.5038
Epoch 13/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.4999
Epoch 13: val_loss improved from 0.69296 to 0.69292, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4999 - val_loss: 0.6929 - val_accuracy: 0.5038
Epoch 14/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5066
Epoch 14: val_loss improved from 0.69292 to 0.69289, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.5043 - val_loss: 0.6929 - val_accuracy: 0.5038
Epoch 15/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5025
Epoch 15: val_loss improved from 0.69289 to 0.69285, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.5026 - val_loss: 0.6929 - val_accuracy: 0.5009
Epoch 16/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.6930 - accuracy: 0.4968
Epoch 16: val_loss improved from 0.69285 to 0.69284, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.4976 - val_loss: 0.6928 - val_accuracy: 0.5038
Epoch 17/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5018
Epoch 17: val_loss improved from 0.69284 to 0.69283, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.5021 - val_loss: 0.6928 - val_accuracy: 0.5038
Epoch 18/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.6930 - accuracy: 0.4962
Epoch 18: val_loss did not improve from 0.69283
682/682 [==============================] - 2s 2ms/step - loss: 0.6929 - accuracy: 0.4982 - val_loss: 0.6928 - val_accuracy: 0.5038
Epoch 19/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5027
Epoch 19: val_loss improved from 0.69283 to 0.69282, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.5043 - val_loss: 0.6928 - val_accuracy: 0.5038
Epoch 20/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.6929 - accuracy: 0.5058
Epoch 20: val_loss improved from 0.69282 to 0.69281, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.5043 - val_loss: 0.6928 - val_accuracy: 0.5038
Epoch 21/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.6929 - accuracy: 0.5046
Epoch 21: val_loss improved from 0.69281 to 0.69279, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.5043 - val_loss: 0.6928 - val_accuracy: 0.5038
Epoch 22/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.6930 - accuracy: 0.4992
Epoch 22: val_loss did not improve from 0.69279
682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.4989 - val_loss: 0.6928 - val_accuracy: 0.5038
Epoch 23/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5023
Epoch 23: val_loss did not improve from 0.69279
682/682 [==============================] - 2s 2ms/step - loss: 0.6929 - accuracy: 0.5021 - val_loss: 0.6928 - val_accuracy: 0.5038
Epoch 24/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5004
Epoch 24: val_loss did not improve from 0.69279
682/682 [==============================] - 1s 2ms/step - loss: 0.6930 - accuracy: 0.4993 - val_loss: 0.6928 - val_accuracy: 0.5038
Epoch 25/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.6930 - accuracy: 0.5000
Epoch 25: val_loss improved from 0.69279 to 0.69279, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.5011 - val_loss: 0.6928 - val_accuracy: 0.5038
Epoch 26/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5015
Epoch 26: val_loss improved from 0.69279 to 0.69279, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6929 - accuracy: 0.5024 - val_loss: 0.6928 - val_accuracy: 0.5038
Epoch 27/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.6929 - accuracy: 0.5069
Epoch 27: val_loss improved from 0.69279 to 0.69279, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6929 - accuracy: 0.5043 - val_loss: 0.6928 - val_accuracy: 0.5038
Epoch 28/60
682/682 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.4964
Epoch 28: val_loss did not improve from 0.69279
682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.4964 - val_loss: 0.6928 - val_accuracy: 0.5038
Epoch 29/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.6930 - accuracy: 0.4986
Epoch 29: val_loss improved from 0.69279 to 0.69278, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.4996 - val_loss: 0.6928 - val_accuracy: 0.5038
Epoch 30/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.6929 - accuracy: 0.5009
Epoch 30: val_loss improved from 0.69278 to 0.69278, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6929 - accuracy: 0.5008 - val_loss: 0.6928 - val_accuracy: 0.5038
Epoch 31/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.6929 - accuracy: 0.4933
Epoch 31: val_loss improved from 0.69278 to 0.69278, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.4954 - val_loss: 0.6928 - val_accuracy: 0.5038
Epoch 32/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.6929 - accuracy: 0.4940
Epoch 32: val_loss did not improve from 0.69278
682/682 [==============================] - 2s 2ms/step - loss: 0.6929 - accuracy: 0.4936 - val_loss: 0.6928 - val_accuracy: 0.5038
Epoch 33/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.6929 - accuracy: 0.5034
Epoch 33: val_loss did not improve from 0.69278
682/682 [==============================] - 1s 2ms/step - loss: 0.6929 - accuracy: 0.5043 - val_loss: 0.6928 - val_accuracy: 0.5038
Epoch 34/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.6929 - accuracy: 0.4988
Epoch 34: val_loss did not improve from 0.69278
682/682 [==============================] - 1s 2ms/step - loss: 0.6930 - accuracy: 0.5004 - val_loss: 0.6928 - val_accuracy: 0.5038
Epoch 35/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.6929 - accuracy: 0.4980
Epoch 35: val_loss did not improve from 0.69278
682/682 [==============================] - 1s 2ms/step - loss: 0.6930 - accuracy: 0.4980 - val_loss: 0.6928 - val_accuracy: 0.5038
Epoch 36/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.6929 - accuracy: 0.5027
Epoch 36: val_loss improved from 0.69278 to 0.69278, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6929 - accuracy: 0.5023 - val_loss: 0.6928 - val_accuracy: 0.5038
Epoch 37/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5007
Epoch 37: val_loss did not improve from 0.69278
682/682 [==============================] - 2s 2ms/step - loss: 0.6929 - accuracy: 0.5012 - val_loss: 0.6928 - val_accuracy: 0.5038
Epoch 38/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5025
Epoch 38: val_loss did not improve from 0.69278
682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.5027 - val_loss: 0.6928 - val_accuracy: 0.5038
Epoch 39/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.6930 - accuracy: 0.4994
Epoch 39: val_loss did not improve from 0.69278
682/682 [==============================] - 2s 2ms/step - loss: 0.6929 - accuracy: 0.5012 - val_loss: 0.6928 - val_accuracy: 0.5038
Epoch 40/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5018
Epoch 40: val_loss did not improve from 0.69278
682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.5018 - val_loss: 0.6928 - val_accuracy: 0.5038
Epoch 41/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.6929 - accuracy: 0.5047
Epoch 41: val_loss improved from 0.69278 to 0.69278, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6929 - accuracy: 0.5020 - val_loss: 0.6928 - val_accuracy: 0.5009
Epoch 42/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.6929 - accuracy: 0.4991
Epoch 42: val_loss improved from 0.69278 to 0.69278, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.4985 - val_loss: 0.6928 - val_accuracy: 0.5038
Epoch 43/60
653/682 [===========================&gt;..] - ETA: 0s - loss: 0.6931 - accuracy: 0.4968
Epoch 43: val_loss did not improve from 0.69278
682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.4982 - val_loss: 0.6928 - val_accuracy: 0.5038
Epoch 44/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.6929 - accuracy: 0.5036
Epoch 44: val_loss did not improve from 0.69278
682/682 [==============================] - 2s 2ms/step - loss: 0.6929 - accuracy: 0.5043 - val_loss: 0.6928 - val_accuracy: 0.5038
Epoch 45/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.6929 - accuracy: 0.5034
Epoch 45: val_loss did not improve from 0.69278
682/682 [==============================] - 2s 2ms/step - loss: 0.6929 - accuracy: 0.5027 - val_loss: 0.6928 - val_accuracy: 0.5038
Epoch 46/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.6930 - accuracy: 0.4968
Epoch 46: val_loss did not improve from 0.69278
682/682 [==============================] - 1s 2ms/step - loss: 0.6929 - accuracy: 0.4970 - val_loss: 0.6928 - val_accuracy: 0.5038
Epoch 47/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.6929 - accuracy: 0.5024
Epoch 47: val_loss did not improve from 0.69278
682/682 [==============================] - 2s 2ms/step - loss: 0.6929 - accuracy: 0.5024 - val_loss: 0.6928 - val_accuracy: 0.5038
Epoch 48/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.6930 - accuracy: 0.4973
Epoch 48: val_loss did not improve from 0.69278
682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.4985 - val_loss: 0.6928 - val_accuracy: 0.5038
Epoch 49/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.6929 - accuracy: 0.5044
Epoch 49: val_loss did not improve from 0.69278
682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.5043 - val_loss: 0.6928 - val_accuracy: 0.5038
Epoch 50/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.6929 - accuracy: 0.5050
Epoch 50: val_loss did not improve from 0.69278
682/682 [==============================] - 2s 2ms/step - loss: 0.6929 - accuracy: 0.5043 - val_loss: 0.6928 - val_accuracy: 0.5038
Epoch 51/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5005
Epoch 51: val_loss did not improve from 0.69278
682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.5011 - val_loss: 0.6928 - val_accuracy: 0.5038
Epoch 52/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.6930 - accuracy: 0.4908
Epoch 52: val_loss did not improve from 0.69278
682/682 [==============================] - 1s 2ms/step - loss: 0.6929 - accuracy: 0.4921 - val_loss: 0.6928 - val_accuracy: 0.5038
Epoch 53/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5043
Epoch 53: val_loss did not improve from 0.69278
682/682 [==============================] - 2s 2ms/step - loss: 0.6929 - accuracy: 0.5043 - val_loss: 0.6928 - val_accuracy: 0.5038
Epoch 54/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.6930 - accuracy: 0.4993
Epoch 54: val_loss did not improve from 0.69278
682/682 [==============================] - 2s 2ms/step - loss: 0.6929 - accuracy: 0.4986 - val_loss: 0.6928 - val_accuracy: 0.5038
Epoch 55/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.6929 - accuracy: 0.5043
Epoch 55: val_loss did not improve from 0.69278
682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.5043 - val_loss: 0.6928 - val_accuracy: 0.5038
Epoch 56/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.6930 - accuracy: 0.4985
Epoch 56: val_loss did not improve from 0.69278
682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.4995 - val_loss: 0.6928 - val_accuracy: 0.5038
Epoch 57/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.6929 - accuracy: 0.4916
Epoch 57: val_loss did not improve from 0.69278
682/682 [==============================] - 2s 2ms/step - loss: 0.6929 - accuracy: 0.4917 - val_loss: 0.6928 - val_accuracy: 0.5038
Epoch 58/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.6930 - accuracy: 0.5055
Epoch 58: val_loss did not improve from 0.69278
682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.5043 - val_loss: 0.6928 - val_accuracy: 0.5038
Epoch 59/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.6929 - accuracy: 0.5036
Epoch 59: val_loss did not improve from 0.69278
682/682 [==============================] - 2s 2ms/step - loss: 0.6929 - accuracy: 0.5033 - val_loss: 0.6928 - val_accuracy: 0.5038
Epoch 60/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5044
Epoch 60: val_loss did not improve from 0.69278
682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.5043 - val_loss: 0.6928 - val_accuracy: 0.5038
54/54 [==============================] - 0s 2ms/step - loss: 0.6928 - accuracy: 0.5038
Processing feature: housemaid
data frame x has shape: (8516, 1)
Epoch 1/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.6967 - accuracy: 0.5042
Epoch 1: val_loss improved from inf to 0.70006, saving model to feature_best.hdf5
682/682 [==============================] - 3s 3ms/step - loss: 0.6967 - accuracy: 0.5037 - val_loss: 0.7001 - val_accuracy: 0.4997
Epoch 2/60
682/682 [==============================] - ETA: 0s - loss: 0.6960 - accuracy: 0.5027
Epoch 2: val_loss improved from 0.70006 to 0.69882, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6960 - accuracy: 0.5027 - val_loss: 0.6988 - val_accuracy: 0.4997
Epoch 3/60
653/682 [===========================&gt;..] - ETA: 0s - loss: 0.6953 - accuracy: 0.5032
Epoch 3: val_loss improved from 0.69882 to 0.69786, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6954 - accuracy: 0.5027 - val_loss: 0.6979 - val_accuracy: 0.4997
Epoch 4/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.6949 - accuracy: 0.5039
Epoch 4: val_loss improved from 0.69786 to 0.69698, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6949 - accuracy: 0.5027 - val_loss: 0.6970 - val_accuracy: 0.4997
Epoch 5/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.6945 - accuracy: 0.5031
Epoch 5: val_loss improved from 0.69698 to 0.69626, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6945 - accuracy: 0.5027 - val_loss: 0.6963 - val_accuracy: 0.4997
Epoch 6/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.6941 - accuracy: 0.5034
Epoch 6: val_loss improved from 0.69626 to 0.69570, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6942 - accuracy: 0.5027 - val_loss: 0.6957 - val_accuracy: 0.4997
Epoch 7/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.6939 - accuracy: 0.5017
Epoch 7: val_loss improved from 0.69570 to 0.69522, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6939 - accuracy: 0.5027 - val_loss: 0.6952 - val_accuracy: 0.4997
Epoch 8/60
654/682 [===========================&gt;..] - ETA: 0s - loss: 0.6939 - accuracy: 0.5012
Epoch 8: val_loss improved from 0.69522 to 0.69484, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6937 - accuracy: 0.5027 - val_loss: 0.6948 - val_accuracy: 0.4997
Epoch 9/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.6936 - accuracy: 0.5026
Epoch 9: val_loss improved from 0.69484 to 0.69451, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6936 - accuracy: 0.5027 - val_loss: 0.6945 - val_accuracy: 0.4997
Epoch 10/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.6936 - accuracy: 0.4967
Epoch 10: val_loss improved from 0.69451 to 0.69426, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6934 - accuracy: 0.4992 - val_loss: 0.6943 - val_accuracy: 0.4997
Epoch 11/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.6935 - accuracy: 0.5021
Epoch 11: val_loss improved from 0.69426 to 0.69409, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6934 - accuracy: 0.5027 - val_loss: 0.6941 - val_accuracy: 0.4997
Epoch 12/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.6933 - accuracy: 0.5017
Epoch 12: val_loss improved from 0.69409 to 0.69394, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6933 - accuracy: 0.5010 - val_loss: 0.6939 - val_accuracy: 0.4997
Epoch 13/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.6932 - accuracy: 0.4968
Epoch 13: val_loss improved from 0.69394 to 0.69382, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4966 - val_loss: 0.6938 - val_accuracy: 0.4997
Epoch 14/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4999
Epoch 14: val_loss improved from 0.69382 to 0.69372, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4996 - val_loss: 0.6937 - val_accuracy: 0.4997
Epoch 15/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4957
Epoch 15: val_loss improved from 0.69372 to 0.69365, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4955 - val_loss: 0.6936 - val_accuracy: 0.4997
Epoch 16/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5028
Epoch 16: val_loss improved from 0.69365 to 0.69358, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5027 - val_loss: 0.6936 - val_accuracy: 0.4997
Epoch 17/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.4994
Epoch 17: val_loss improved from 0.69358 to 0.69355, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4973 - val_loss: 0.6935 - val_accuracy: 0.4997
Epoch 18/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.5000
Epoch 18: val_loss improved from 0.69355 to 0.69350, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4998 - val_loss: 0.6935 - val_accuracy: 0.4997
Epoch 19/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.4978
Epoch 19: val_loss improved from 0.69350 to 0.69348, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4973 - val_loss: 0.6935 - val_accuracy: 0.5009
Epoch 20/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.4990
Epoch 20: val_loss improved from 0.69348 to 0.69344, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4980 - val_loss: 0.6934 - val_accuracy: 0.4997
Epoch 21/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5036
Epoch 21: val_loss improved from 0.69344 to 0.69342, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5021 - val_loss: 0.6934 - val_accuracy: 0.4997
Epoch 22/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.4961
Epoch 22: val_loss improved from 0.69342 to 0.69340, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4966 - val_loss: 0.6934 - val_accuracy: 0.4997
Epoch 23/60
682/682 [==============================] - ETA: 0s - loss: 0.6931 - accuracy: 0.5012
Epoch 23: val_loss improved from 0.69340 to 0.69339, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5012 - val_loss: 0.6934 - val_accuracy: 0.4997
Epoch 24/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.6930 - accuracy: 0.4924
Epoch 24: val_loss improved from 0.69339 to 0.69337, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4917 - val_loss: 0.6934 - val_accuracy: 0.4997
Epoch 25/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.4978
Epoch 25: val_loss improved from 0.69337 to 0.69336, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4974 - val_loss: 0.6934 - val_accuracy: 0.4997
Epoch 26/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.4967
Epoch 26: val_loss improved from 0.69336 to 0.69335, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4977 - val_loss: 0.6933 - val_accuracy: 0.4997
Epoch 27/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.4971
Epoch 27: val_loss improved from 0.69335 to 0.69335, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4971 - val_loss: 0.6933 - val_accuracy: 0.4997
Epoch 28/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.6931 - accuracy: 0.5006
Epoch 28: val_loss improved from 0.69335 to 0.69333, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5002 - val_loss: 0.6933 - val_accuracy: 0.4997
Epoch 29/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5024
Epoch 29: val_loss did not improve from 0.69333
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5017 - val_loss: 0.6933 - val_accuracy: 0.4997
Epoch 30/60
653/682 [===========================&gt;..] - ETA: 0s - loss: 0.6931 - accuracy: 0.4975
Epoch 30: val_loss improved from 0.69333 to 0.69333, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4990 - val_loss: 0.6933 - val_accuracy: 0.4997
Epoch 31/60
682/682 [==============================] - ETA: 0s - loss: 0.6931 - accuracy: 0.5015
Epoch 31: val_loss did not improve from 0.69333
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5015 - val_loss: 0.6933 - val_accuracy: 0.4997
Epoch 32/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5035
Epoch 32: val_loss did not improve from 0.69333
682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.5054 - val_loss: 0.6933 - val_accuracy: 0.4997
Epoch 33/60
653/682 [===========================&gt;..] - ETA: 0s - loss: 0.6931 - accuracy: 0.5028
Epoch 33: val_loss improved from 0.69333 to 0.69332, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5027 - val_loss: 0.6933 - val_accuracy: 0.4997
Epoch 34/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.6930 - accuracy: 0.4946
Epoch 34: val_loss did not improve from 0.69332
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4935 - val_loss: 0.6933 - val_accuracy: 0.4997
Epoch 35/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.4956
Epoch 35: val_loss improved from 0.69332 to 0.69331, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4955 - val_loss: 0.6933 - val_accuracy: 0.4997
Epoch 36/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.6931 - accuracy: 0.4935
Epoch 36: val_loss improved from 0.69331 to 0.69331, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4930 - val_loss: 0.6933 - val_accuracy: 0.4997
Epoch 37/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.6932 - accuracy: 0.5030
Epoch 37: val_loss improved from 0.69331 to 0.69331, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5027 - val_loss: 0.6933 - val_accuracy: 0.4997
Epoch 38/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.4990
Epoch 38: val_loss did not improve from 0.69331
682/682 [==============================] - 1s 2ms/step - loss: 0.6931 - accuracy: 0.4988 - val_loss: 0.6933 - val_accuracy: 0.4997
Epoch 39/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.4921
Epoch 39: val_loss improved from 0.69331 to 0.69330, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4932 - val_loss: 0.6933 - val_accuracy: 0.4997
Epoch 40/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.6931 - accuracy: 0.4907
Epoch 40: val_loss did not improve from 0.69330
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4920 - val_loss: 0.6933 - val_accuracy: 0.4997
Epoch 41/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.6931 - accuracy: 0.4953
Epoch 41: val_loss did not improve from 0.69330
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4963 - val_loss: 0.6933 - val_accuracy: 0.4997
Epoch 42/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5032
Epoch 42: val_loss did not improve from 0.69330
682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.5024 - val_loss: 0.6933 - val_accuracy: 0.5009
Epoch 43/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.4981
Epoch 43: val_loss did not improve from 0.69330
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4993 - val_loss: 0.6933 - val_accuracy: 0.5009
Epoch 44/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.4972
Epoch 44: val_loss did not improve from 0.69330
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4964 - val_loss: 0.6933 - val_accuracy: 0.5009
Epoch 45/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.4946
Epoch 45: val_loss improved from 0.69330 to 0.69330, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4945 - val_loss: 0.6933 - val_accuracy: 0.4997
Epoch 46/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.4898
Epoch 46: val_loss improved from 0.69330 to 0.69330, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4907 - val_loss: 0.6933 - val_accuracy: 0.4997
Epoch 47/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.4998
Epoch 47: val_loss did not improve from 0.69330
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5005 - val_loss: 0.6933 - val_accuracy: 0.4997
Epoch 48/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.6931 - accuracy: 0.4992
Epoch 48: val_loss did not improve from 0.69330
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5005 - val_loss: 0.6933 - val_accuracy: 0.4997
Epoch 49/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.4968
Epoch 49: val_loss did not improve from 0.69330
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4974 - val_loss: 0.6933 - val_accuracy: 0.4997
Epoch 50/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.6930 - accuracy: 0.4998
Epoch 50: val_loss did not improve from 0.69330
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4977 - val_loss: 0.6933 - val_accuracy: 0.4997
Epoch 51/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.6930 - accuracy: 0.4935
Epoch 51: val_loss did not improve from 0.69330
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4933 - val_loss: 0.6933 - val_accuracy: 0.4997
Epoch 52/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.6930 - accuracy: 0.4986
Epoch 52: val_loss did not improve from 0.69330
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4993 - val_loss: 0.6933 - val_accuracy: 0.4997
Epoch 53/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5018
Epoch 53: val_loss did not improve from 0.69330
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5027 - val_loss: 0.6933 - val_accuracy: 0.4997
Epoch 54/60
653/682 [===========================&gt;..] - ETA: 0s - loss: 0.6930 - accuracy: 0.4959
Epoch 54: val_loss did not improve from 0.69330
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4943 - val_loss: 0.6933 - val_accuracy: 0.4997
Epoch 55/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.4993
Epoch 55: val_loss did not improve from 0.69330
682/682 [==============================] - 1s 2ms/step - loss: 0.6931 - accuracy: 0.4989 - val_loss: 0.6933 - val_accuracy: 0.4997
Epoch 56/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.6930 - accuracy: 0.4949
Epoch 56: val_loss did not improve from 0.69330
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4955 - val_loss: 0.6933 - val_accuracy: 0.4997
Epoch 57/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5015
Epoch 57: val_loss did not improve from 0.69330
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5024 - val_loss: 0.6933 - val_accuracy: 0.5009
Epoch 58/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.4923
Epoch 58: val_loss did not improve from 0.69330
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4924 - val_loss: 0.6933 - val_accuracy: 0.5009
Epoch 59/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.6931 - accuracy: 0.5002
Epoch 59: val_loss did not improve from 0.69330
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5004 - val_loss: 0.6933 - val_accuracy: 0.4997
Epoch 60/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.4978
Epoch 60: val_loss did not improve from 0.69330
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4983 - val_loss: 0.6933 - val_accuracy: 0.4997
54/54 [==============================] - 0s 2ms/step - loss: 0.6933 - accuracy: 0.4997
Processing feature: management
data frame x has shape: (8516, 1)
Epoch 1/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.6944 - accuracy: 0.4980
Epoch 1: val_loss improved from inf to 0.69357, saving model to feature_best.hdf5
682/682 [==============================] - 2s 3ms/step - loss: 0.6944 - accuracy: 0.4974 - val_loss: 0.6936 - val_accuracy: 0.5009
Epoch 2/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.6939 - accuracy: 0.4985
Epoch 2: val_loss improved from 0.69357 to 0.69334, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6939 - accuracy: 0.4988 - val_loss: 0.6933 - val_accuracy: 0.5009
Epoch 3/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.6936 - accuracy: 0.4973
Epoch 3: val_loss improved from 0.69334 to 0.69321, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6936 - accuracy: 0.4976 - val_loss: 0.6932 - val_accuracy: 0.5009
Epoch 4/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.6935 - accuracy: 0.4949
Epoch 4: val_loss improved from 0.69321 to 0.69315, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6935 - accuracy: 0.4952 - val_loss: 0.6932 - val_accuracy: 0.5009
Epoch 5/60
653/682 [===========================&gt;..] - ETA: 0s - loss: 0.6934 - accuracy: 0.4928
Epoch 5: val_loss improved from 0.69315 to 0.69313, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6934 - accuracy: 0.4930 - val_loss: 0.6931 - val_accuracy: 0.5009
Epoch 6/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.6933 - accuracy: 0.5033
Epoch 6: val_loss did not improve from 0.69313
682/682 [==============================] - 2s 2ms/step - loss: 0.6933 - accuracy: 0.5043 - val_loss: 0.6932 - val_accuracy: 0.5009
Epoch 7/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.6933 - accuracy: 0.4966
Epoch 7: val_loss did not improve from 0.69313
682/682 [==============================] - 2s 2ms/step - loss: 0.6933 - accuracy: 0.4966 - val_loss: 0.6931 - val_accuracy: 0.5009
Epoch 8/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.6933 - accuracy: 0.4928
Epoch 8: val_loss did not improve from 0.69313
682/682 [==============================] - 1s 2ms/step - loss: 0.6933 - accuracy: 0.4920 - val_loss: 0.6931 - val_accuracy: 0.5009
Epoch 9/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4923
Epoch 9: val_loss did not improve from 0.69313
682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4919 - val_loss: 0.6932 - val_accuracy: 0.4991
Epoch 10/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4937
Epoch 10: val_loss did not improve from 0.69313
682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4938 - val_loss: 0.6932 - val_accuracy: 0.4991
Epoch 11/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4961
Epoch 11: val_loss did not improve from 0.69313
682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4952 - val_loss: 0.6932 - val_accuracy: 0.4991
Epoch 12/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4967
Epoch 12: val_loss did not improve from 0.69313
682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4960 - val_loss: 0.6932 - val_accuracy: 0.4991
Epoch 13/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4872
Epoch 13: val_loss did not improve from 0.69313
682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4875 - val_loss: 0.6932 - val_accuracy: 0.4991
Epoch 14/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4899
Epoch 14: val_loss did not improve from 0.69313
682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4879 - val_loss: 0.6932 - val_accuracy: 0.4991
Epoch 15/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.5011
Epoch 15: val_loss did not improve from 0.69313
682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.5014 - val_loss: 0.6932 - val_accuracy: 0.4991
Epoch 16/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4892
Epoch 16: val_loss did not improve from 0.69313
682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4892 - val_loss: 0.6932 - val_accuracy: 0.4991
Epoch 17/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4982
Epoch 17: val_loss did not improve from 0.69313
682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4976 - val_loss: 0.6932 - val_accuracy: 0.4991
Epoch 18/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4946
Epoch 18: val_loss did not improve from 0.69313
682/682 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.4942 - val_loss: 0.6932 - val_accuracy: 0.4991
Epoch 19/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4962
Epoch 19: val_loss did not improve from 0.69313
682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4958 - val_loss: 0.6932 - val_accuracy: 0.4991
Epoch 20/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4949
Epoch 20: val_loss did not improve from 0.69313
682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4952 - val_loss: 0.6932 - val_accuracy: 0.4991
Epoch 21/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4940
Epoch 21: val_loss did not improve from 0.69313
682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4932 - val_loss: 0.6932 - val_accuracy: 0.4991
Epoch 22/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4896
Epoch 22: val_loss did not improve from 0.69313
682/682 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.4910 - val_loss: 0.6932 - val_accuracy: 0.4991
Epoch 23/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.6932 - accuracy: 0.5008
Epoch 23: val_loss did not improve from 0.69313
682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4990 - val_loss: 0.6932 - val_accuracy: 0.4991
Epoch 24/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.6932 - accuracy: 0.4934
Epoch 24: val_loss did not improve from 0.69313
682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4933 - val_loss: 0.6932 - val_accuracy: 0.4991
Epoch 25/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.6932 - accuracy: 0.4898
Epoch 25: val_loss did not improve from 0.69313
682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4891 - val_loss: 0.6932 - val_accuracy: 0.4991
Epoch 25: early stopping
54/54 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5009
Processing feature: retired
data frame x has shape: (8516, 1)
Epoch 1/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.7307 - accuracy: 0.4688
Epoch 1: val_loss improved from inf to 0.71489, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.7311 - accuracy: 0.4692 - val_loss: 0.7149 - val_accuracy: 0.4821
Epoch 2/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.7193 - accuracy: 0.4706
Epoch 2: val_loss improved from 0.71489 to 0.70738, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.7196 - accuracy: 0.4691 - val_loss: 0.7074 - val_accuracy: 0.4821
Epoch 3/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.7103 - accuracy: 0.4697
Epoch 3: val_loss improved from 0.70738 to 0.70150, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.7104 - accuracy: 0.4691 - val_loss: 0.7015 - val_accuracy: 0.4821
Epoch 4/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.7031 - accuracy: 0.4690
Epoch 4: val_loss improved from 0.70150 to 0.69725, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.7030 - accuracy: 0.4691 - val_loss: 0.6973 - val_accuracy: 0.4821
Epoch 5/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.6974 - accuracy: 0.4699
Epoch 5: val_loss improved from 0.69725 to 0.69429, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6974 - accuracy: 0.4691 - val_loss: 0.6943 - val_accuracy: 0.4821
Epoch 6/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.6935 - accuracy: 0.4840
Epoch 6: val_loss improved from 0.69429 to 0.69239, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6934 - accuracy: 0.4839 - val_loss: 0.6924 - val_accuracy: 0.5179
Epoch 7/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.6905 - accuracy: 0.5269
Epoch 7: val_loss improved from 0.69239 to 0.69128, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6905 - accuracy: 0.5266 - val_loss: 0.6913 - val_accuracy: 0.5179
Epoch 8/60
682/682 [==============================] - ETA: 0s - loss: 0.6886 - accuracy: 0.5203
Epoch 8: val_loss improved from 0.69128 to 0.69070, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6886 - accuracy: 0.5203 - val_loss: 0.6907 - val_accuracy: 0.5179
Epoch 9/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.6872 - accuracy: 0.5303
Epoch 9: val_loss improved from 0.69070 to 0.69046, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6872 - accuracy: 0.5309 - val_loss: 0.6905 - val_accuracy: 0.5179
Epoch 10/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.6862 - accuracy: 0.5295
Epoch 10: val_loss improved from 0.69046 to 0.69044, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6864 - accuracy: 0.5309 - val_loss: 0.6904 - val_accuracy: 0.5179
Epoch 11/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.6858 - accuracy: 0.5310
Epoch 11: val_loss did not improve from 0.69044
682/682 [==============================] - 2s 2ms/step - loss: 0.6858 - accuracy: 0.5309 - val_loss: 0.6905 - val_accuracy: 0.5179
Epoch 12/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.6855 - accuracy: 0.5311
Epoch 12: val_loss did not improve from 0.69044
682/682 [==============================] - 2s 2ms/step - loss: 0.6855 - accuracy: 0.5309 - val_loss: 0.6907 - val_accuracy: 0.5179
Epoch 13/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.6850 - accuracy: 0.5310
Epoch 13: val_loss did not improve from 0.69044
682/682 [==============================] - 2s 2ms/step - loss: 0.6853 - accuracy: 0.5309 - val_loss: 0.6908 - val_accuracy: 0.5179
Epoch 14/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.6851 - accuracy: 0.5305
Epoch 14: val_loss did not improve from 0.69044
682/682 [==============================] - 2s 2ms/step - loss: 0.6851 - accuracy: 0.5309 - val_loss: 0.6910 - val_accuracy: 0.5179
Epoch 15/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.6851 - accuracy: 0.5309
Epoch 15: val_loss did not improve from 0.69044
682/682 [==============================] - 2s 2ms/step - loss: 0.6851 - accuracy: 0.5309 - val_loss: 0.6911 - val_accuracy: 0.5179
Epoch 16/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.6851 - accuracy: 0.5310
Epoch 16: val_loss did not improve from 0.69044
682/682 [==============================] - 2s 2ms/step - loss: 0.6850 - accuracy: 0.5309 - val_loss: 0.6912 - val_accuracy: 0.5179
Epoch 17/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.6850 - accuracy: 0.5309
Epoch 17: val_loss did not improve from 0.69044
682/682 [==============================] - 1s 2ms/step - loss: 0.6849 - accuracy: 0.5309 - val_loss: 0.6913 - val_accuracy: 0.5179
Epoch 18/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.6851 - accuracy: 0.5309
Epoch 18: val_loss did not improve from 0.69044
682/682 [==============================] - 1s 2ms/step - loss: 0.6849 - accuracy: 0.5309 - val_loss: 0.6914 - val_accuracy: 0.5179
Epoch 19/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.6848 - accuracy: 0.5312
Epoch 19: val_loss did not improve from 0.69044
682/682 [==============================] - 2s 2ms/step - loss: 0.6849 - accuracy: 0.5309 - val_loss: 0.6915 - val_accuracy: 0.5179
Epoch 20/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.6850 - accuracy: 0.5309
Epoch 20: val_loss did not improve from 0.69044
682/682 [==============================] - 2s 2ms/step - loss: 0.6849 - accuracy: 0.5309 - val_loss: 0.6915 - val_accuracy: 0.5179
Epoch 21/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.6848 - accuracy: 0.5304
Epoch 21: val_loss did not improve from 0.69044
682/682 [==============================] - 2s 2ms/step - loss: 0.6849 - accuracy: 0.5309 - val_loss: 0.6916 - val_accuracy: 0.5179
Epoch 22/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.6848 - accuracy: 0.5300
Epoch 22: val_loss did not improve from 0.69044
682/682 [==============================] - 2s 2ms/step - loss: 0.6849 - accuracy: 0.5309 - val_loss: 0.6916 - val_accuracy: 0.5179
Epoch 23/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.6851 - accuracy: 0.5316
Epoch 23: val_loss did not improve from 0.69044
682/682 [==============================] - 2s 2ms/step - loss: 0.6850 - accuracy: 0.5309 - val_loss: 0.6917 - val_accuracy: 0.5179
Epoch 24/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.6849 - accuracy: 0.5315
Epoch 24: val_loss did not improve from 0.69044
682/682 [==============================] - 2s 2ms/step - loss: 0.6849 - accuracy: 0.5309 - val_loss: 0.6917 - val_accuracy: 0.5179
Epoch 25/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.6848 - accuracy: 0.5319
Epoch 25: val_loss did not improve from 0.69044
682/682 [==============================] - 2s 2ms/step - loss: 0.6849 - accuracy: 0.5309 - val_loss: 0.6917 - val_accuracy: 0.5179
Epoch 26/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.6849 - accuracy: 0.5318
Epoch 26: val_loss did not improve from 0.69044
682/682 [==============================] - 2s 2ms/step - loss: 0.6849 - accuracy: 0.5309 - val_loss: 0.6917 - val_accuracy: 0.5179
Epoch 27/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.6851 - accuracy: 0.5315
Epoch 27: val_loss did not improve from 0.69044
682/682 [==============================] - 2s 2ms/step - loss: 0.6849 - accuracy: 0.5309 - val_loss: 0.6917 - val_accuracy: 0.5179
Epoch 28/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.6850 - accuracy: 0.5300
Epoch 28: val_loss did not improve from 0.69044
682/682 [==============================] - 2s 2ms/step - loss: 0.6849 - accuracy: 0.5309 - val_loss: 0.6918 - val_accuracy: 0.5179
Epoch 29/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.6850 - accuracy: 0.5307
Epoch 29: val_loss did not improve from 0.69044
682/682 [==============================] - 2s 2ms/step - loss: 0.6849 - accuracy: 0.5309 - val_loss: 0.6918 - val_accuracy: 0.5179
Epoch 30/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.6848 - accuracy: 0.5314
Epoch 30: val_loss did not improve from 0.69044
682/682 [==============================] - 1s 2ms/step - loss: 0.6849 - accuracy: 0.5309 - val_loss: 0.6918 - val_accuracy: 0.5179
Epoch 30: early stopping
54/54 [==============================] - 0s 2ms/step - loss: 0.6904 - accuracy: 0.5179
Processing feature: self-employed
data frame x has shape: (8516, 1)
Epoch 1/60
653/682 [===========================&gt;..] - ETA: 0s - loss: 0.6997 - accuracy: 0.4983
Epoch 1: val_loss improved from inf to 0.69682, saving model to feature_best.hdf5
682/682 [==============================] - 2s 3ms/step - loss: 0.6997 - accuracy: 0.4966 - val_loss: 0.6968 - val_accuracy: 0.5044
Epoch 2/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.6981 - accuracy: 0.4990
Epoch 2: val_loss improved from 0.69682 to 0.69564, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6981 - accuracy: 0.4992 - val_loss: 0.6956 - val_accuracy: 0.5044
Epoch 3/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.6969 - accuracy: 0.5017
Epoch 3: val_loss improved from 0.69564 to 0.69472, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6969 - accuracy: 0.4992 - val_loss: 0.6947 - val_accuracy: 0.5044
Epoch 4/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.6959 - accuracy: 0.4989
Epoch 4: val_loss improved from 0.69472 to 0.69409, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6959 - accuracy: 0.4992 - val_loss: 0.6941 - val_accuracy: 0.5044
Epoch 5/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.6952 - accuracy: 0.4993
Epoch 5: val_loss improved from 0.69409 to 0.69363, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6952 - accuracy: 0.4992 - val_loss: 0.6936 - val_accuracy: 0.5044
Epoch 6/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.6947 - accuracy: 0.4980
Epoch 6: val_loss improved from 0.69363 to 0.69335, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6946 - accuracy: 0.4992 - val_loss: 0.6933 - val_accuracy: 0.5044
Epoch 7/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.6942 - accuracy: 0.4996
Epoch 7: val_loss improved from 0.69335 to 0.69314, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6942 - accuracy: 0.4992 - val_loss: 0.6931 - val_accuracy: 0.5044
Epoch 8/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.6939 - accuracy: 0.4927
Epoch 8: val_loss improved from 0.69314 to 0.69304, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6939 - accuracy: 0.4927 - val_loss: 0.6930 - val_accuracy: 0.5044
Epoch 9/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.6937 - accuracy: 0.5000
Epoch 9: val_loss improved from 0.69304 to 0.69298, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6937 - accuracy: 0.4992 - val_loss: 0.6930 - val_accuracy: 0.5044
Epoch 10/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.6935 - accuracy: 0.4925
Epoch 10: val_loss improved from 0.69298 to 0.69296, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6935 - accuracy: 0.4927 - val_loss: 0.6930 - val_accuracy: 0.5044
Epoch 11/60
653/682 [===========================&gt;..] - ETA: 0s - loss: 0.6934 - accuracy: 0.4968
Epoch 11: val_loss did not improve from 0.69296
682/682 [==============================] - 2s 2ms/step - loss: 0.6934 - accuracy: 0.4968 - val_loss: 0.6930 - val_accuracy: 0.5044
Epoch 12/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.6933 - accuracy: 0.5005
Epoch 12: val_loss did not improve from 0.69296
682/682 [==============================] - 2s 2ms/step - loss: 0.6934 - accuracy: 0.4992 - val_loss: 0.6930 - val_accuracy: 0.5044
Epoch 13/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.6933 - accuracy: 0.4897
Epoch 13: val_loss did not improve from 0.69296
682/682 [==============================] - 2s 2ms/step - loss: 0.6933 - accuracy: 0.4892 - val_loss: 0.6930 - val_accuracy: 0.5044
Epoch 14/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.6933 - accuracy: 0.4961
Epoch 14: val_loss did not improve from 0.69296
682/682 [==============================] - 2s 2ms/step - loss: 0.6933 - accuracy: 0.4948 - val_loss: 0.6930 - val_accuracy: 0.4991
Epoch 15/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.6933 - accuracy: 0.4947
Epoch 15: val_loss did not improve from 0.69296
682/682 [==============================] - 2s 2ms/step - loss: 0.6933 - accuracy: 0.4948 - val_loss: 0.6931 - val_accuracy: 0.4991
Epoch 16/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.6933 - accuracy: 0.4987
Epoch 16: val_loss did not improve from 0.69296
682/682 [==============================] - 2s 2ms/step - loss: 0.6933 - accuracy: 0.4985 - val_loss: 0.6931 - val_accuracy: 0.4991
Epoch 17/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.6933 - accuracy: 0.4890
Epoch 17: val_loss did not improve from 0.69296
682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4897 - val_loss: 0.6931 - val_accuracy: 0.4991
Epoch 18/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.6932 - accuracy: 0.4995
Epoch 18: val_loss did not improve from 0.69296
682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4977 - val_loss: 0.6931 - val_accuracy: 0.5044
Epoch 19/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.5006
Epoch 19: val_loss did not improve from 0.69296
682/682 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.5001 - val_loss: 0.6931 - val_accuracy: 0.4991
Epoch 20/60
653/682 [===========================&gt;..] - ETA: 0s - loss: 0.6932 - accuracy: 0.4876
Epoch 20: val_loss did not improve from 0.69296
682/682 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.4876 - val_loss: 0.6932 - val_accuracy: 0.4956
Epoch 21/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4900
Epoch 21: val_loss did not improve from 0.69296
682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4895 - val_loss: 0.6932 - val_accuracy: 0.4956
Epoch 22/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4835
Epoch 22: val_loss did not improve from 0.69296
682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4833 - val_loss: 0.6932 - val_accuracy: 0.4956
Epoch 23/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4940
Epoch 23: val_loss did not improve from 0.69296
682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4933 - val_loss: 0.6932 - val_accuracy: 0.4956
Epoch 24/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.5011
Epoch 24: val_loss did not improve from 0.69296
682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4988 - val_loss: 0.6932 - val_accuracy: 0.4956
Epoch 25/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4941
Epoch 25: val_loss did not improve from 0.69296
682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4927 - val_loss: 0.6932 - val_accuracy: 0.4956
Epoch 26/60
654/682 [===========================&gt;..] - ETA: 0s - loss: 0.6932 - accuracy: 0.4873
Epoch 26: val_loss did not improve from 0.69296
682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4875 - val_loss: 0.6932 - val_accuracy: 0.5009
Epoch 27/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4991
Epoch 27: val_loss did not improve from 0.69296
682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4999 - val_loss: 0.6932 - val_accuracy: 0.4956
Epoch 28/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.5018
Epoch 28: val_loss did not improve from 0.69296
682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.5020 - val_loss: 0.6932 - val_accuracy: 0.4956
Epoch 29/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4932
Epoch 29: val_loss did not improve from 0.69296
682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4939 - val_loss: 0.6932 - val_accuracy: 0.4956
Epoch 30/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4973
Epoch 30: val_loss did not improve from 0.69296
682/682 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.4973 - val_loss: 0.6932 - val_accuracy: 0.4956
Epoch 30: early stopping
54/54 [==============================] - 0s 3ms/step - loss: 0.6930 - accuracy: 0.5044
Processing feature: services
data frame x has shape: (8516, 1)
Epoch 1/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.6911 - accuracy: 0.5176
Epoch 1: val_loss improved from inf to 0.69114, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6913 - accuracy: 0.5181 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 2/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.6911 - accuracy: 0.5202
Epoch 2: val_loss improved from 0.69114 to 0.69112, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 3/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.6913 - accuracy: 0.5188
Epoch 3: val_loss improved from 0.69112 to 0.69110, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 4/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.6911 - accuracy: 0.5173
Epoch 4: val_loss improved from 0.69110 to 0.69110, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 5/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.6911 - accuracy: 0.5206
Epoch 5: val_loss improved from 0.69110 to 0.69110, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 6/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5183
Epoch 6: val_loss improved from 0.69110 to 0.69109, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 7/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.6911 - accuracy: 0.5187
Epoch 7: val_loss improved from 0.69109 to 0.69109, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 8/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.6913 - accuracy: 0.5162
Epoch 8: val_loss did not improve from 0.69109
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 9/60
682/682 [==============================] - ETA: 0s - loss: 0.6912 - accuracy: 0.5184
Epoch 9: val_loss did not improve from 0.69109
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 10/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5184
Epoch 10: val_loss did not improve from 0.69109
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 11/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5182
Epoch 11: val_loss did not improve from 0.69109
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 12/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.6910 - accuracy: 0.5206
Epoch 12: val_loss did not improve from 0.69109
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 13/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.6911 - accuracy: 0.5177
Epoch 13: val_loss improved from 0.69109 to 0.69109, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 14/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5177
Epoch 14: val_loss did not improve from 0.69109
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 15/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5186
Epoch 15: val_loss did not improve from 0.69109
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 16/60
682/682 [==============================] - ETA: 0s - loss: 0.6912 - accuracy: 0.5184
Epoch 16: val_loss improved from 0.69109 to 0.69109, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 17/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5191
Epoch 17: val_loss did not improve from 0.69109
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 18/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5186
Epoch 18: val_loss did not improve from 0.69109
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 19/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.6909 - accuracy: 0.5192
Epoch 19: val_loss did not improve from 0.69109
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 20/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5181
Epoch 20: val_loss did not improve from 0.69109
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 21/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.6911 - accuracy: 0.5193
Epoch 21: val_loss did not improve from 0.69109
682/682 [==============================] - 1s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 22/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.6913 - accuracy: 0.5175
Epoch 22: val_loss did not improve from 0.69109
682/682 [==============================] - 1s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 23/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5182
Epoch 23: val_loss did not improve from 0.69109
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 24/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.6911 - accuracy: 0.5174
Epoch 24: val_loss did not improve from 0.69109
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 25/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.6911 - accuracy: 0.5188
Epoch 25: val_loss did not improve from 0.69109
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 26/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5176
Epoch 26: val_loss did not improve from 0.69109
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 27/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5196
Epoch 27: val_loss did not improve from 0.69109
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 28/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.6911 - accuracy: 0.5190
Epoch 28: val_loss did not improve from 0.69109
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 29/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.6911 - accuracy: 0.5185
Epoch 29: val_loss did not improve from 0.69109
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 30/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.6910 - accuracy: 0.5186
Epoch 30: val_loss improved from 0.69109 to 0.69109, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 31/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.6911 - accuracy: 0.5186
Epoch 31: val_loss did not improve from 0.69109
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 32/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.6913 - accuracy: 0.5179
Epoch 32: val_loss did not improve from 0.69109
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 33/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5180
Epoch 33: val_loss did not improve from 0.69109
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 34/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5180
Epoch 34: val_loss did not improve from 0.69109
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 35/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.6911 - accuracy: 0.5183
Epoch 35: val_loss did not improve from 0.69109
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 36/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5185
Epoch 36: val_loss did not improve from 0.69109
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 37/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5182
Epoch 37: val_loss did not improve from 0.69109
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 38/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.6912 - accuracy: 0.5190
Epoch 38: val_loss did not improve from 0.69109
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 39/60
653/682 [===========================&gt;..] - ETA: 0s - loss: 0.6910 - accuracy: 0.5196
Epoch 39: val_loss did not improve from 0.69109
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 40/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.6913 - accuracy: 0.5180
Epoch 40: val_loss did not improve from 0.69109
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 41/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5182
Epoch 41: val_loss did not improve from 0.69109
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 42/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.6913 - accuracy: 0.5171
Epoch 42: val_loss did not improve from 0.69109
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 43/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.6914 - accuracy: 0.5166
Epoch 43: val_loss improved from 0.69109 to 0.69109, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 44/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.6911 - accuracy: 0.5182
Epoch 44: val_loss did not improve from 0.69109
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 45/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5177
Epoch 45: val_loss did not improve from 0.69109
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 46/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5189
Epoch 46: val_loss did not improve from 0.69109
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 47/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5186
Epoch 47: val_loss did not improve from 0.69109
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 48/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5182
Epoch 48: val_loss did not improve from 0.69109
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 49/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.6911 - accuracy: 0.5198
Epoch 49: val_loss did not improve from 0.69109
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 50/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.6911 - accuracy: 0.5192
Epoch 50: val_loss did not improve from 0.69109
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 51/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.6915 - accuracy: 0.5183
Epoch 51: val_loss did not improve from 0.69109
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 52/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.6914 - accuracy: 0.5153
Epoch 52: val_loss did not improve from 0.69109
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 53/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5179
Epoch 53: val_loss did not improve from 0.69109
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 54/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5188
Epoch 54: val_loss improved from 0.69109 to 0.69108, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 55/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5182
Epoch 55: val_loss did not improve from 0.69108
682/682 [==============================] - 1s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 56/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.6910 - accuracy: 0.5202
Epoch 56: val_loss did not improve from 0.69108
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 57/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.6913 - accuracy: 0.5171
Epoch 57: val_loss did not improve from 0.69108
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 58/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5182
Epoch 58: val_loss did not improve from 0.69108
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 59/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.6913 - accuracy: 0.5179
Epoch 59: val_loss did not improve from 0.69108
682/682 [==============================] - 1s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
Epoch 60/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.6913 - accuracy: 0.5174
Epoch 60: val_loss did not improve from 0.69108
682/682 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6911 - val_accuracy: 0.5167
54/54 [==============================] - 0s 2ms/step - loss: 0.6911 - accuracy: 0.5167
Processing feature: student
data frame x has shape: (8516, 1)
Epoch 1/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.6893 - accuracy: 0.5096
Epoch 1: val_loss improved from inf to 0.68697, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6893 - accuracy: 0.5098 - val_loss: 0.6870 - val_accuracy: 0.5238
Epoch 2/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.6889 - accuracy: 0.5158
Epoch 2: val_loss improved from 0.68697 to 0.68647, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6889 - accuracy: 0.5165 - val_loss: 0.6865 - val_accuracy: 0.5238
Epoch 3/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.6887 - accuracy: 0.5130
Epoch 3: val_loss improved from 0.68647 to 0.68610, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6887 - accuracy: 0.5128 - val_loss: 0.6861 - val_accuracy: 0.5238
Epoch 4/60
650/682 [===========================&gt;..] - ETA: 0s - loss: 0.6888 - accuracy: 0.5158
Epoch 4: val_loss improved from 0.68610 to 0.68582, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6886 - accuracy: 0.5165 - val_loss: 0.6858 - val_accuracy: 0.5238
Epoch 5/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.6885 - accuracy: 0.5170
Epoch 5: val_loss improved from 0.68582 to 0.68562, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6885 - accuracy: 0.5165 - val_loss: 0.6856 - val_accuracy: 0.5238
Epoch 6/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.6885 - accuracy: 0.5161
Epoch 6: val_loss improved from 0.68562 to 0.68544, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6884 - accuracy: 0.5165 - val_loss: 0.6854 - val_accuracy: 0.5238
Epoch 7/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.6883 - accuracy: 0.5180
Epoch 7: val_loss improved from 0.68544 to 0.68530, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6883 - accuracy: 0.5165 - val_loss: 0.6853 - val_accuracy: 0.5238
Epoch 8/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.6883 - accuracy: 0.5153
Epoch 8: val_loss improved from 0.68530 to 0.68520, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6883 - accuracy: 0.5165 - val_loss: 0.6852 - val_accuracy: 0.5238
Epoch 9/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.6883 - accuracy: 0.5175
Epoch 9: val_loss improved from 0.68520 to 0.68511, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6883 - accuracy: 0.5165 - val_loss: 0.6851 - val_accuracy: 0.5238
Epoch 10/60
654/682 [===========================&gt;..] - ETA: 0s - loss: 0.6883 - accuracy: 0.5128
Epoch 10: val_loss improved from 0.68511 to 0.68504, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6850 - val_accuracy: 0.5238
Epoch 11/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.6882 - accuracy: 0.5164
Epoch 11: val_loss improved from 0.68504 to 0.68499, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6850 - val_accuracy: 0.5238
Epoch 12/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.6879 - accuracy: 0.5157
Epoch 12: val_loss improved from 0.68499 to 0.68495, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6850 - val_accuracy: 0.5238
Epoch 13/60
682/682 [==============================] - ETA: 0s - loss: 0.6882 - accuracy: 0.5165
Epoch 13: val_loss improved from 0.68495 to 0.68490, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6849 - val_accuracy: 0.5238
Epoch 14/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.6882 - accuracy: 0.5170
Epoch 14: val_loss improved from 0.68490 to 0.68486, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6849 - val_accuracy: 0.5238
Epoch 15/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.6883 - accuracy: 0.5162
Epoch 15: val_loss improved from 0.68486 to 0.68486, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6849 - val_accuracy: 0.5238
Epoch 16/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.6883 - accuracy: 0.5170
Epoch 16: val_loss improved from 0.68486 to 0.68483, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6848 - val_accuracy: 0.5238
Epoch 17/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.6883 - accuracy: 0.5161
Epoch 17: val_loss improved from 0.68483 to 0.68480, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6848 - val_accuracy: 0.5238
Epoch 18/60
654/682 [===========================&gt;..] - ETA: 0s - loss: 0.6886 - accuracy: 0.5156
Epoch 18: val_loss improved from 0.68480 to 0.68480, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6848 - val_accuracy: 0.5238
Epoch 19/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.6882 - accuracy: 0.5161
Epoch 19: val_loss improved from 0.68480 to 0.68480, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6848 - val_accuracy: 0.5238
Epoch 20/60
653/682 [===========================&gt;..] - ETA: 0s - loss: 0.6881 - accuracy: 0.5173
Epoch 20: val_loss improved from 0.68480 to 0.68477, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6848 - val_accuracy: 0.5238
Epoch 21/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.6883 - accuracy: 0.5176
Epoch 21: val_loss improved from 0.68477 to 0.68476, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6848 - val_accuracy: 0.5238
Epoch 22/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.6882 - accuracy: 0.5176
Epoch 22: val_loss improved from 0.68476 to 0.68474, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238
Epoch 23/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.6882 - accuracy: 0.5167
Epoch 23: val_loss improved from 0.68474 to 0.68474, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238
Epoch 24/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.6884 - accuracy: 0.5148
Epoch 24: val_loss did not improve from 0.68474
682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238
Epoch 25/60
652/682 [===========================&gt;..] - ETA: 0s - loss: 0.6885 - accuracy: 0.5164
Epoch 25: val_loss did not improve from 0.68474
682/682 [==============================] - 1s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238
Epoch 26/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.6880 - accuracy: 0.5175
Epoch 26: val_loss did not improve from 0.68474
682/682 [==============================] - 1s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238
Epoch 27/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.6883 - accuracy: 0.5168
Epoch 27: val_loss improved from 0.68474 to 0.68473, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238
Epoch 28/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.6882 - accuracy: 0.5156
Epoch 28: val_loss did not improve from 0.68473
682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238
Epoch 29/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.6882 - accuracy: 0.5158
Epoch 29: val_loss did not improve from 0.68473
682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238
Epoch 30/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.6882 - accuracy: 0.5160
Epoch 30: val_loss improved from 0.68473 to 0.68472, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238
Epoch 31/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.6879 - accuracy: 0.5177
Epoch 31: val_loss improved from 0.68472 to 0.68471, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238
Epoch 32/60
682/682 [==============================] - ETA: 0s - loss: 0.6881 - accuracy: 0.5114
Epoch 32: val_loss did not improve from 0.68471
682/682 [==============================] - 2s 2ms/step - loss: 0.6881 - accuracy: 0.5114 - val_loss: 0.6847 - val_accuracy: 0.5238
Epoch 33/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.6882 - accuracy: 0.5170
Epoch 33: val_loss did not improve from 0.68471
682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238
Epoch 34/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.6882 - accuracy: 0.5171
Epoch 34: val_loss did not improve from 0.68471
682/682 [==============================] - 1s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238
Epoch 35/60
654/682 [===========================&gt;..] - ETA: 0s - loss: 0.6885 - accuracy: 0.5135
Epoch 35: val_loss did not improve from 0.68471
682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238
Epoch 36/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.6880 - accuracy: 0.5158
Epoch 36: val_loss did not improve from 0.68471
682/682 [==============================] - 1s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238
Epoch 37/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.6882 - accuracy: 0.5160
Epoch 37: val_loss did not improve from 0.68471
682/682 [==============================] - 1s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6848 - val_accuracy: 0.5238
Epoch 38/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.6883 - accuracy: 0.5153
Epoch 38: val_loss did not improve from 0.68471
682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238
Epoch 39/60
653/682 [===========================&gt;..] - ETA: 0s - loss: 0.6885 - accuracy: 0.5167
Epoch 39: val_loss did not improve from 0.68471
682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238
Epoch 40/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.6883 - accuracy: 0.5151
Epoch 40: val_loss did not improve from 0.68471
682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238
Epoch 41/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.6882 - accuracy: 0.5176
Epoch 41: val_loss did not improve from 0.68471
682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238
Epoch 42/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.6883 - accuracy: 0.5162
Epoch 42: val_loss did not improve from 0.68471
682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238
Epoch 43/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.6883 - accuracy: 0.5171
Epoch 43: val_loss did not improve from 0.68471
682/682 [==============================] - 1s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238
Epoch 44/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.6881 - accuracy: 0.5193
Epoch 44: val_loss did not improve from 0.68471
682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238
Epoch 45/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.6882 - accuracy: 0.5156
Epoch 45: val_loss did not improve from 0.68471
682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238
Epoch 46/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.6877 - accuracy: 0.5195
Epoch 46: val_loss improved from 0.68471 to 0.68470, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238
Epoch 47/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.6883 - accuracy: 0.5158
Epoch 47: val_loss did not improve from 0.68470
682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238
Epoch 48/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.6884 - accuracy: 0.5169
Epoch 48: val_loss did not improve from 0.68470
682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238
Epoch 49/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.6881 - accuracy: 0.5169
Epoch 49: val_loss did not improve from 0.68470
682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238
Epoch 50/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.6881 - accuracy: 0.5150
Epoch 50: val_loss did not improve from 0.68470
682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238
Epoch 51/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.6882 - accuracy: 0.5163
Epoch 51: val_loss did not improve from 0.68470
682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238
Epoch 52/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.6880 - accuracy: 0.5160
Epoch 52: val_loss did not improve from 0.68470
682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238
Epoch 53/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.6885 - accuracy: 0.5154
Epoch 53: val_loss did not improve from 0.68470
682/682 [==============================] - 1s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238
Epoch 54/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.6879 - accuracy: 0.5168
Epoch 54: val_loss did not improve from 0.68470
682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238
Epoch 55/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.6878 - accuracy: 0.5173
Epoch 55: val_loss did not improve from 0.68470
682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238
Epoch 56/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.6880 - accuracy: 0.5158
Epoch 56: val_loss did not improve from 0.68470
682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238
Epoch 57/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.6886 - accuracy: 0.5144
Epoch 57: val_loss did not improve from 0.68470
682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238
Epoch 58/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.6881 - accuracy: 0.5161
Epoch 58: val_loss improved from 0.68470 to 0.68470, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238
Epoch 59/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.6882 - accuracy: 0.5168
Epoch 59: val_loss improved from 0.68470 to 0.68470, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238
Epoch 60/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.6885 - accuracy: 0.5145
Epoch 60: val_loss did not improve from 0.68470
682/682 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.5165 - val_loss: 0.6847 - val_accuracy: 0.5238
54/54 [==============================] - 0s 2ms/step - loss: 0.6847 - accuracy: 0.5238
Processing feature: technician
data frame x has shape: (8516, 1)
Epoch 1/60
682/682 [==============================] - ETA: 0s - loss: 0.6991 - accuracy: 0.4949
Epoch 1: val_loss improved from inf to 0.69570, saving model to feature_best.hdf5
682/682 [==============================] - 2s 3ms/step - loss: 0.6991 - accuracy: 0.4949 - val_loss: 0.6957 - val_accuracy: 0.5015
Epoch 2/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.6960 - accuracy: 0.4939
Epoch 2: val_loss improved from 0.69570 to 0.69416, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6960 - accuracy: 0.4942 - val_loss: 0.6942 - val_accuracy: 0.5015
Epoch 3/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.6945 - accuracy: 0.4941
Epoch 3: val_loss improved from 0.69416 to 0.69351, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6946 - accuracy: 0.4942 - val_loss: 0.6935 - val_accuracy: 0.5015
Epoch 4/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.6938 - accuracy: 0.4962
Epoch 4: val_loss improved from 0.69351 to 0.69326, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6939 - accuracy: 0.4942 - val_loss: 0.6933 - val_accuracy: 0.5015
Epoch 5/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.6935 - accuracy: 0.4942
Epoch 5: val_loss improved from 0.69326 to 0.69316, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6935 - accuracy: 0.4942 - val_loss: 0.6932 - val_accuracy: 0.5015
Epoch 6/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.6933 - accuracy: 0.4955
Epoch 6: val_loss improved from 0.69316 to 0.69315, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6933 - accuracy: 0.4960 - val_loss: 0.6931 - val_accuracy: 0.5015
Epoch 7/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4888
Epoch 7: val_loss did not improve from 0.69315
682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4889 - val_loss: 0.6932 - val_accuracy: 0.5009
Epoch 8/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4981
Epoch 8: val_loss did not improve from 0.69315
682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4970 - val_loss: 0.6932 - val_accuracy: 0.4985
Epoch 9/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.5046
Epoch 9: val_loss did not improve from 0.69315
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6932 - val_accuracy: 0.4985
Epoch 10/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5073
Epoch 10: val_loss did not improve from 0.69315
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5065 - val_loss: 0.6933 - val_accuracy: 0.5009
Epoch 11/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.4939
Epoch 11: val_loss did not improve from 0.69315
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4954 - val_loss: 0.6933 - val_accuracy: 0.4985
Epoch 12/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5056
Epoch 12: val_loss did not improve from 0.69315
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5058 - val_loss: 0.6933 - val_accuracy: 0.4985
Epoch 13/60
682/682 [==============================] - ETA: 0s - loss: 0.6931 - accuracy: 0.5059
Epoch 13: val_loss did not improve from 0.69315
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5059 - val_loss: 0.6933 - val_accuracy: 0.4985
Epoch 14/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5063
Epoch 14: val_loss did not improve from 0.69315
682/682 [==============================] - 1s 2ms/step - loss: 0.6931 - accuracy: 0.5058 - val_loss: 0.6933 - val_accuracy: 0.4985
Epoch 15/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4987
Epoch 15: val_loss did not improve from 0.69315
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4985 - val_loss: 0.6933 - val_accuracy: 0.4985
Epoch 16/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5028
Epoch 16: val_loss did not improve from 0.69315
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5036 - val_loss: 0.6933 - val_accuracy: 0.4985
Epoch 17/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5057
Epoch 17: val_loss did not improve from 0.69315
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5058 - val_loss: 0.6933 - val_accuracy: 0.4985
Epoch 18/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5058
Epoch 18: val_loss did not improve from 0.69315
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5058 - val_loss: 0.6933 - val_accuracy: 0.4985
Epoch 19/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5057
Epoch 19: val_loss did not improve from 0.69315
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5058 - val_loss: 0.6933 - val_accuracy: 0.4985
Epoch 20/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5061
Epoch 20: val_loss did not improve from 0.69315
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5058 - val_loss: 0.6933 - val_accuracy: 0.4985
Epoch 21/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5059
Epoch 21: val_loss did not improve from 0.69315
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5058 - val_loss: 0.6933 - val_accuracy: 0.4985
Epoch 22/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5064
Epoch 22: val_loss did not improve from 0.69315
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5058 - val_loss: 0.6933 - val_accuracy: 0.4985
Epoch 23/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.6932 - accuracy: 0.5059
Epoch 23: val_loss did not improve from 0.69315
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5058 - val_loss: 0.6933 - val_accuracy: 0.4985
Epoch 24/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5061
Epoch 24: val_loss did not improve from 0.69315
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5058 - val_loss: 0.6933 - val_accuracy: 0.4985
Epoch 25/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5000
Epoch 25: val_loss did not improve from 0.69315
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.4996 - val_loss: 0.6933 - val_accuracy: 0.4985
Epoch 26/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5015
Epoch 26: val_loss did not improve from 0.69315
682/682 [==============================] - 2s 2ms/step - loss: 0.6931 - accuracy: 0.5012 - val_loss: 0.6933 - val_accuracy: 0.4985
Epoch 26: early stopping
54/54 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5015
Processing feature: unemployed
data frame x has shape: (8516, 1)
Epoch 1/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.6954 - accuracy: 0.4960
Epoch 1: val_loss improved from inf to 0.69384, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6954 - accuracy: 0.4961 - val_loss: 0.6938 - val_accuracy: 0.4979
Epoch 2/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.6946 - accuracy: 0.4964
Epoch 2: val_loss improved from 0.69384 to 0.69356, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6946 - accuracy: 0.4955 - val_loss: 0.6936 - val_accuracy: 0.4979
Epoch 3/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.6940 - accuracy: 0.4880
Epoch 3: val_loss improved from 0.69356 to 0.69337, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6940 - accuracy: 0.4885 - val_loss: 0.6934 - val_accuracy: 0.4979
Epoch 4/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.6937 - accuracy: 0.4938
Epoch 4: val_loss improved from 0.69337 to 0.69322, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6937 - accuracy: 0.4946 - val_loss: 0.6932 - val_accuracy: 0.4979
Epoch 5/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.6934 - accuracy: 0.4921
Epoch 5: val_loss improved from 0.69322 to 0.69315, saving model to feature_best.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6934 - accuracy: 0.4921 - val_loss: 0.6932 - val_accuracy: 0.5009
Epoch 6/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.6932 - accuracy: 0.4968
Epoch 6: val_loss improved from 0.69315 to 0.69312, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4963 - val_loss: 0.6931 - val_accuracy: 0.5021
Epoch 7/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.6930 - accuracy: 0.4854
Epoch 7: val_loss improved from 0.69312 to 0.69312, saving model to feature_best.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.4838 - val_loss: 0.6931 - val_accuracy: 0.4991
Epoch 8/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.6929 - accuracy: 0.4945
Epoch 8: val_loss did not improve from 0.69312
682/682 [==============================] - 2s 2ms/step - loss: 0.6929 - accuracy: 0.4957 - val_loss: 0.6931 - val_accuracy: 0.5021
Epoch 9/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.6929 - accuracy: 0.5014
Epoch 9: val_loss did not improve from 0.69312
682/682 [==============================] - 2s 2ms/step - loss: 0.6929 - accuracy: 0.5012 - val_loss: 0.6931 - val_accuracy: 0.5021
Epoch 10/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.6928 - accuracy: 0.5033
Epoch 10: val_loss did not improve from 0.69312
682/682 [==============================] - 1s 2ms/step - loss: 0.6928 - accuracy: 0.5029 - val_loss: 0.6932 - val_accuracy: 0.5021
Epoch 11/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.6928 - accuracy: 0.4976
Epoch 11: val_loss did not improve from 0.69312
682/682 [==============================] - 1s 2ms/step - loss: 0.6928 - accuracy: 0.4971 - val_loss: 0.6932 - val_accuracy: 0.4991
Epoch 12/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.6928 - accuracy: 0.5002
Epoch 12: val_loss did not improve from 0.69312
682/682 [==============================] - 2s 2ms/step - loss: 0.6927 - accuracy: 0.5001 - val_loss: 0.6932 - val_accuracy: 0.5021
Epoch 13/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.6927 - accuracy: 0.5031
Epoch 13: val_loss did not improve from 0.69312
682/682 [==============================] - 2s 2ms/step - loss: 0.6927 - accuracy: 0.5024 - val_loss: 0.6932 - val_accuracy: 0.4991
Epoch 14/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.6927 - accuracy: 0.4999
Epoch 14: val_loss did not improve from 0.69312
682/682 [==============================] - 2s 2ms/step - loss: 0.6927 - accuracy: 0.4998 - val_loss: 0.6932 - val_accuracy: 0.5021
Epoch 15/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.6926 - accuracy: 0.4986
Epoch 15: val_loss did not improve from 0.69312
682/682 [==============================] - 1s 2ms/step - loss: 0.6927 - accuracy: 0.5001 - val_loss: 0.6932 - val_accuracy: 0.5021
Epoch 16/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.6927 - accuracy: 0.5053
Epoch 16: val_loss did not improve from 0.69312
682/682 [==============================] - 2s 2ms/step - loss: 0.6927 - accuracy: 0.5054 - val_loss: 0.6933 - val_accuracy: 0.5021
Epoch 17/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.6927 - accuracy: 0.5058
Epoch 17: val_loss did not improve from 0.69312
682/682 [==============================] - 2s 2ms/step - loss: 0.6927 - accuracy: 0.5054 - val_loss: 0.6933 - val_accuracy: 0.5021
Epoch 18/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.6927 - accuracy: 0.4972
Epoch 18: val_loss did not improve from 0.69312
682/682 [==============================] - 2s 2ms/step - loss: 0.6927 - accuracy: 0.4963 - val_loss: 0.6933 - val_accuracy: 0.5021
Epoch 19/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.6927 - accuracy: 0.5008
Epoch 19: val_loss did not improve from 0.69312
682/682 [==============================] - 2s 2ms/step - loss: 0.6927 - accuracy: 0.4999 - val_loss: 0.6933 - val_accuracy: 0.5021
Epoch 20/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.6927 - accuracy: 0.5021
Epoch 20: val_loss did not improve from 0.69312
682/682 [==============================] - 1s 2ms/step - loss: 0.6926 - accuracy: 0.5032 - val_loss: 0.6933 - val_accuracy: 0.5021
Epoch 21/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.6927 - accuracy: 0.5015
Epoch 21: val_loss did not improve from 0.69312
682/682 [==============================] - 2s 2ms/step - loss: 0.6927 - accuracy: 0.5018 - val_loss: 0.6933 - val_accuracy: 0.5021
Epoch 22/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.6927 - accuracy: 0.5051
Epoch 22: val_loss did not improve from 0.69312
682/682 [==============================] - 1s 2ms/step - loss: 0.6927 - accuracy: 0.5058 - val_loss: 0.6933 - val_accuracy: 0.5021
Epoch 23/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.6927 - accuracy: 0.5052
Epoch 23: val_loss did not improve from 0.69312
682/682 [==============================] - 2s 2ms/step - loss: 0.6927 - accuracy: 0.5054 - val_loss: 0.6933 - val_accuracy: 0.5021
Epoch 24/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.6926 - accuracy: 0.5027
Epoch 24: val_loss did not improve from 0.69312
682/682 [==============================] - 2s 2ms/step - loss: 0.6927 - accuracy: 0.5027 - val_loss: 0.6933 - val_accuracy: 0.5021
Epoch 25/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.6927 - accuracy: 0.5053
Epoch 25: val_loss did not improve from 0.69312
682/682 [==============================] - 2s 2ms/step - loss: 0.6927 - accuracy: 0.5054 - val_loss: 0.6933 - val_accuracy: 0.5021
Epoch 26/60
649/682 [===========================&gt;..] - ETA: 0s - loss: 0.6927 - accuracy: 0.4985
Epoch 26: val_loss did not improve from 0.69312
682/682 [==============================] - 2s 2ms/step - loss: 0.6927 - accuracy: 0.4992 - val_loss: 0.6933 - val_accuracy: 0.5021
Epoch 27/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.6926 - accuracy: 0.4979
Epoch 27: val_loss did not improve from 0.69312
682/682 [==============================] - 2s 2ms/step - loss: 0.6927 - accuracy: 0.4970 - val_loss: 0.6933 - val_accuracy: 0.5021
Epoch 27: early stopping
54/54 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.4991
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=26d9207c-db22-4345-9415-028986492da1">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [9]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span> <span class="n">results</span><span class="o">.</span><span class="n">head</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[9]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>&lt;bound method NDFrame.head of            Feature  Validation_Accuracy
0              age            44.274810
1        education            55.901349
2          housing            50.734001
3             loan            50.205517
4         campaign            53.082794
5            pdays            59.072226
6         previous            59.718144
7         poutcome            58.719903
8     emp.var.rate            71.579564
9   cons.price.idx            57.839108
10   cons.conf.idx            54.198474
11       euribor3m            71.579564
12        divorced            50.675279
13         married            53.200233
14          single            53.963596
15          admin.            53.258955
16     blue-collar            55.314153
17    entrepreneur            50.381678
18       housemaid            49.970639
19      management            50.088078
20         retired            51.790959
21   self-employed            50.440401
22        services            51.673520
23         student            52.378154
24      technician            50.146800
25      unemployed            49.911919&gt;</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=1546b498-4592-40ee-ae66-ea81778c4298">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="4.3-Graph-the-Accuracy-of-the-Model-for-each-Feature">4.3 Graph the Accuracy of the Model for each Feature<a class="anchor-link" href="#4.3-Graph-the-Accuracy-of-the-Model-for-each-Feature">¶</a></h4>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=92d6e224-d9a4-4057-a33a-3c5abf9a5a7c">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [10]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Plotting the accuracy by feature bar graph</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">results</span><span class="p">[</span><span class="s1">'Feature'</span><span class="p">],</span> <span class="n">results</span><span class="p">[</span><span class="s1">'Validation_Accuracy'</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Feature'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Validation Accuracy (%)'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Validation Accuracy for Each Type'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="s1">'vertical'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,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"/>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=753b8c67-d576-425c-9b8b-a78f503e1058">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="4.4-Sort-the-Features-by-Accuracy">4.4 Sort the Features by Accuracy<a class="anchor-link" href="#4.4-Sort-the-Features-by-Accuracy">¶</a></h4>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=211f4f9b-be7a-492b-8fa7-dce347109fd0">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [11]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">results</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s1">'Validation_Accuracy'</span><span class="p">)</span>

<span class="c1"># plot the sorted result dataframe. </span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">results</span><span class="p">[</span><span class="s1">'Feature'</span><span class="p">],</span> <span class="n">results</span><span class="p">[</span><span class="s1">'Validation_Accuracy'</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Feature'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Validation Accuracy (%)'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Validation Accuracy for Each Type'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="s1">'vertical'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,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"/>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=332bb54d-05c6-4943-a7f8-97c60cd80e57">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="4.5-Train-New-Models-and-Dropping-Features-to-Determine-which-Features-are-Best">4.5 Train New Models and Dropping Features to Determine which Features are Best<a class="anchor-link" href="#4.5-Train-New-Models-and-Dropping-Features-to-Determine-which-Features-are-Best">¶</a></h4><p>To determine if our model would have performed better without specific features, I will build another 26 models. In each model I will remove a feature that I found earlier had low validation accuracy and led to poor predictions to see if I had removed specific features if the model would have performed better.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=755b0166-a787-4995-a080-ddf09045a04b">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">drop_X</span> <span class="o">=</span> <span class="n">banking_data</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">'output'</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="s1">'columns'</span><span class="p">)</span>
<span class="n">drop_result</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">'Features_Removed'</span><span class="p">,</span> <span class="s1">'Validation_Accuracy'</span><span class="p">])</span>


<span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">results</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>

    <span class="n">feature_value</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Feature'</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Drop Feature </span><span class="si">{</span><span class="n">feature_value</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="n">drop_X</span> <span class="o">=</span> <span class="n">drop_X</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="n">feature_value</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="s1">'columns'</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"data frame x has shape: </span><span class="si">{</span><span class="n">drop_X</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">drop_X</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
    <span class="c1"># Split x into validation (20%) and training (80%)</span>
    <span class="n">X_train</span> <span class="o">=</span> <span class="n">drop_X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">index_20</span><span class="p">:,</span> <span class="p">:]</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="n">drop_X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:</span><span class="n">index_20</span><span class="p">,</span> <span class="p">:]</span>
    <span class="nb">min</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> 
    <span class="nb">max</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> 
    <span class="n">X_train</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_train</span> <span class="o">-</span> <span class="nb">min</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="nb">max</span> <span class="o">-</span> <span class="nb">min</span><span class="p">)</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_test</span> <span class="o">-</span> <span class="nb">min</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="nb">max</span> <span class="o">-</span> <span class="nb">min</span><span class="p">)</span>

    <span class="n">drop_regression_model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
    <span class="n">drop_regression_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">columns</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'sigmoid'</span><span class="p">))</span>

    <span class="n">drop_regression_model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">'binary_crossentropy'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s1">'adam'</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'accuracy'</span><span class="p">])</span>
    <span class="n">callback_c</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">filepath</span> <span class="o">=</span> <span class="s1">'best_drop.hdf5'</span><span class="p">,</span> <span class="n">monitor</span><span class="o">=</span><span class="s1">'val_loss'</span><span class="p">,</span> <span class="n">save_best_only</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">save_weights_only</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">verbose</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">callback_d</span> <span class="o">=</span> <span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s1">'val_loss'</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">'min'</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">history_regression_drop</span> <span class="o">=</span> <span class="n">drop_regression_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">callback_c</span><span class="p">,</span> <span class="n">callback_d</span><span class="p">])</span>
    <span class="n">drop_regression_model</span><span class="o">.</span><span class="n">load_weights</span><span class="p">(</span><span class="s1">'best_drop.hdf5'</span><span class="p">)</span>
    <span class="n">drop_regression_model_scores</span> <span class="o">=</span>  <span class="n">drop_regression_model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="p">(</span><span class="n">drop_regression_model_scores</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">100</span><span class="p">)</span>

    <span class="n">drop_result</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">drop_result</span><span class="o">.</span><span class="n">index</span><span class="p">)]</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">"Dropped </span><span class="si">{</span><span class="n">feature_value</span><span class="si">}</span><span class="s2">"</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">]</span> 
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Drop Feature age
data frame x has shape: (8516, 25)
['education', 'housing', 'loan', 'campaign', 'pdays', 'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'divorced', 'married', 'single', 'admin.', 'blue-collar', 'entrepreneur', 'housemaid', 'management', 'retired', 'self-employed', 'services', 'student', 'technician', 'unemployed']
Epoch 1/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.6683 - accuracy: 0.5983
Epoch 1: val_loss improved from inf to 0.63109, saving model to best_drop.hdf5
682/682 [==============================] - 2s 3ms/step - loss: 0.6672 - accuracy: 0.6002 - val_loss: 0.6311 - val_accuracy: 0.6788
Epoch 2/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.6162 - accuracy: 0.6894
Epoch 2: val_loss improved from 0.63109 to 0.59908, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6164 - accuracy: 0.6890 - val_loss: 0.5991 - val_accuracy: 0.7170
Epoch 3/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.5948 - accuracy: 0.7057
Epoch 3: val_loss improved from 0.59908 to 0.58444, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5947 - accuracy: 0.7059 - val_loss: 0.5844 - val_accuracy: 0.7246
Epoch 4/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.5848 - accuracy: 0.7078
Epoch 4: val_loss improved from 0.58444 to 0.57717, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5842 - accuracy: 0.7085 - val_loss: 0.5772 - val_accuracy: 0.7264
Epoch 5/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.5788 - accuracy: 0.7103
Epoch 5: val_loss improved from 0.57717 to 0.57330, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5784 - accuracy: 0.7106 - val_loss: 0.5733 - val_accuracy: 0.7258
Epoch 6/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.5743 - accuracy: 0.7133
Epoch 6: val_loss improved from 0.57330 to 0.57123, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5751 - accuracy: 0.7119 - val_loss: 0.5712 - val_accuracy: 0.7187
Epoch 7/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.5720 - accuracy: 0.7121
Epoch 7: val_loss improved from 0.57123 to 0.57002, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5730 - accuracy: 0.7116 - val_loss: 0.5700 - val_accuracy: 0.7252
Epoch 8/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.5720 - accuracy: 0.7126
Epoch 8: val_loss improved from 0.57002 to 0.56907, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5717 - accuracy: 0.7125 - val_loss: 0.5691 - val_accuracy: 0.7176
Epoch 9/60
682/682 [==============================] - ETA: 0s - loss: 0.5706 - accuracy: 0.7120
Epoch 9: val_loss improved from 0.56907 to 0.56867, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5706 - accuracy: 0.7120 - val_loss: 0.5687 - val_accuracy: 0.7158
Epoch 10/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.5705 - accuracy: 0.7098
Epoch 10: val_loss improved from 0.56867 to 0.56761, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5698 - accuracy: 0.7104 - val_loss: 0.5676 - val_accuracy: 0.7234
Epoch 11/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.5679 - accuracy: 0.7131
Epoch 11: val_loss improved from 0.56761 to 0.56746, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5691 - accuracy: 0.7117 - val_loss: 0.5675 - val_accuracy: 0.7217
Epoch 12/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.5685 - accuracy: 0.7128
Epoch 12: val_loss improved from 0.56746 to 0.56700, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5685 - accuracy: 0.7130 - val_loss: 0.5670 - val_accuracy: 0.7205
Epoch 13/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.5681 - accuracy: 0.7117
Epoch 13: val_loss improved from 0.56700 to 0.56664, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5680 - accuracy: 0.7119 - val_loss: 0.5666 - val_accuracy: 0.7223
Epoch 14/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.5674 - accuracy: 0.7126
Epoch 14: val_loss improved from 0.56664 to 0.56619, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5676 - accuracy: 0.7123 - val_loss: 0.5662 - val_accuracy: 0.7234
Epoch 15/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.5665 - accuracy: 0.7121
Epoch 15: val_loss improved from 0.56619 to 0.56604, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5672 - accuracy: 0.7123 - val_loss: 0.5660 - val_accuracy: 0.7217
Epoch 16/60
682/682 [==============================] - ETA: 0s - loss: 0.5669 - accuracy: 0.7141
Epoch 16: val_loss improved from 0.56604 to 0.56579, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5669 - accuracy: 0.7141 - val_loss: 0.5658 - val_accuracy: 0.7217
Epoch 17/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.5666 - accuracy: 0.7138
Epoch 17: val_loss improved from 0.56579 to 0.56539, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5664 - accuracy: 0.7141 - val_loss: 0.5654 - val_accuracy: 0.7211
Epoch 18/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.5651 - accuracy: 0.7150
Epoch 18: val_loss improved from 0.56539 to 0.56504, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5662 - accuracy: 0.7141 - val_loss: 0.5650 - val_accuracy: 0.7211
Epoch 19/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.5664 - accuracy: 0.7132
Epoch 19: val_loss improved from 0.56504 to 0.56497, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5658 - accuracy: 0.7138 - val_loss: 0.5650 - val_accuracy: 0.7205
Epoch 20/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.5649 - accuracy: 0.7170
Epoch 20: val_loss did not improve from 0.56497
682/682 [==============================] - 2s 2ms/step - loss: 0.5652 - accuracy: 0.7172 - val_loss: 0.5650 - val_accuracy: 0.7199
Epoch 21/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.5654 - accuracy: 0.7127
Epoch 21: val_loss improved from 0.56497 to 0.56478, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5653 - accuracy: 0.7129 - val_loss: 0.5648 - val_accuracy: 0.7234
Epoch 22/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.5650 - accuracy: 0.7160
Epoch 22: val_loss improved from 0.56478 to 0.56418, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5650 - accuracy: 0.7160 - val_loss: 0.5642 - val_accuracy: 0.7217
Epoch 23/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.5644 - accuracy: 0.7165
Epoch 23: val_loss did not improve from 0.56418
682/682 [==============================] - 2s 2ms/step - loss: 0.5648 - accuracy: 0.7163 - val_loss: 0.5643 - val_accuracy: 0.7217
Epoch 24/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.5646 - accuracy: 0.7175
Epoch 24: val_loss improved from 0.56418 to 0.56414, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5645 - accuracy: 0.7175 - val_loss: 0.5641 - val_accuracy: 0.7217
Epoch 25/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.5643 - accuracy: 0.7168
Epoch 25: val_loss improved from 0.56414 to 0.56378, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5644 - accuracy: 0.7166 - val_loss: 0.5638 - val_accuracy: 0.7228
Epoch 26/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.5642 - accuracy: 0.7166
Epoch 26: val_loss improved from 0.56378 to 0.56359, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5643 - accuracy: 0.7167 - val_loss: 0.5636 - val_accuracy: 0.7228
Epoch 27/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.5639 - accuracy: 0.7175
Epoch 27: val_loss improved from 0.56359 to 0.56340, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5641 - accuracy: 0.7177 - val_loss: 0.5634 - val_accuracy: 0.7205
Epoch 28/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.5634 - accuracy: 0.7185
Epoch 28: val_loss improved from 0.56340 to 0.56332, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5637 - accuracy: 0.7186 - val_loss: 0.5633 - val_accuracy: 0.7228
Epoch 29/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.5638 - accuracy: 0.7175
Epoch 29: val_loss improved from 0.56332 to 0.56319, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5637 - accuracy: 0.7179 - val_loss: 0.5632 - val_accuracy: 0.7228
Epoch 30/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.5645 - accuracy: 0.7185
Epoch 30: val_loss improved from 0.56319 to 0.56311, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5633 - accuracy: 0.7194 - val_loss: 0.5631 - val_accuracy: 0.7228
Epoch 31/60
652/682 [===========================&gt;..] - ETA: 0s - loss: 0.5627 - accuracy: 0.7179
Epoch 31: val_loss did not improve from 0.56311
682/682 [==============================] - 1s 2ms/step - loss: 0.5635 - accuracy: 0.7172 - val_loss: 0.5633 - val_accuracy: 0.7211
Epoch 32/60
682/682 [==============================] - ETA: 0s - loss: 0.5632 - accuracy: 0.7202
Epoch 32: val_loss improved from 0.56311 to 0.56307, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5632 - accuracy: 0.7202 - val_loss: 0.5631 - val_accuracy: 0.7228
Epoch 33/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.5630 - accuracy: 0.7185
Epoch 33: val_loss improved from 0.56307 to 0.56302, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5630 - accuracy: 0.7191 - val_loss: 0.5630 - val_accuracy: 0.7228
Epoch 34/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.5628 - accuracy: 0.7194
Epoch 34: val_loss did not improve from 0.56302
682/682 [==============================] - 2s 2ms/step - loss: 0.5630 - accuracy: 0.7191 - val_loss: 0.5631 - val_accuracy: 0.7228
Epoch 35/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.5628 - accuracy: 0.7195
Epoch 35: val_loss improved from 0.56302 to 0.56286, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5629 - accuracy: 0.7191 - val_loss: 0.5629 - val_accuracy: 0.7234
Epoch 36/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.5630 - accuracy: 0.7184
Epoch 36: val_loss improved from 0.56286 to 0.56254, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5628 - accuracy: 0.7185 - val_loss: 0.5625 - val_accuracy: 0.7199
Epoch 37/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.5621 - accuracy: 0.7216
Epoch 37: val_loss improved from 0.56254 to 0.56251, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5629 - accuracy: 0.7208 - val_loss: 0.5625 - val_accuracy: 0.7217
Epoch 38/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.5631 - accuracy: 0.7194
Epoch 38: val_loss improved from 0.56251 to 0.56238, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5628 - accuracy: 0.7197 - val_loss: 0.5624 - val_accuracy: 0.7211
Epoch 39/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.5620 - accuracy: 0.7180
Epoch 39: val_loss did not improve from 0.56238
682/682 [==============================] - 2s 2ms/step - loss: 0.5623 - accuracy: 0.7179 - val_loss: 0.5628 - val_accuracy: 0.7187
Epoch 40/60
651/682 [===========================&gt;..] - ETA: 0s - loss: 0.5635 - accuracy: 0.7194
Epoch 40: val_loss did not improve from 0.56238
682/682 [==============================] - 1s 2ms/step - loss: 0.5624 - accuracy: 0.7201 - val_loss: 0.5628 - val_accuracy: 0.7211
Epoch 41/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.5631 - accuracy: 0.7191
Epoch 41: val_loss improved from 0.56238 to 0.56223, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5626 - accuracy: 0.7191 - val_loss: 0.5622 - val_accuracy: 0.7217
Epoch 42/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.5622 - accuracy: 0.7201
Epoch 42: val_loss improved from 0.56223 to 0.56215, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5624 - accuracy: 0.7199 - val_loss: 0.5622 - val_accuracy: 0.7211
Epoch 43/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.5624 - accuracy: 0.7206
Epoch 43: val_loss improved from 0.56215 to 0.56197, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5624 - accuracy: 0.7204 - val_loss: 0.5620 - val_accuracy: 0.7211
Epoch 44/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.5631 - accuracy: 0.7207
Epoch 44: val_loss did not improve from 0.56197
682/682 [==============================] - 2s 2ms/step - loss: 0.5623 - accuracy: 0.7210 - val_loss: 0.5621 - val_accuracy: 0.7199
Epoch 45/60
653/682 [===========================&gt;..] - ETA: 0s - loss: 0.5631 - accuracy: 0.7194
Epoch 45: val_loss did not improve from 0.56197
682/682 [==============================] - 2s 2ms/step - loss: 0.5620 - accuracy: 0.7210 - val_loss: 0.5631 - val_accuracy: 0.7205
Epoch 46/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.5623 - accuracy: 0.7215
Epoch 46: val_loss did not improve from 0.56197
682/682 [==============================] - 2s 2ms/step - loss: 0.5623 - accuracy: 0.7216 - val_loss: 0.5621 - val_accuracy: 0.7193
Epoch 47/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.5615 - accuracy: 0.7200
Epoch 47: val_loss did not improve from 0.56197
682/682 [==============================] - 2s 2ms/step - loss: 0.5619 - accuracy: 0.7198 - val_loss: 0.5621 - val_accuracy: 0.7205
Epoch 48/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.5630 - accuracy: 0.7196
Epoch 48: val_loss improved from 0.56197 to 0.56195, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5623 - accuracy: 0.7205 - val_loss: 0.5620 - val_accuracy: 0.7211
Epoch 49/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.5609 - accuracy: 0.7220
Epoch 49: val_loss improved from 0.56195 to 0.56180, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5621 - accuracy: 0.7207 - val_loss: 0.5618 - val_accuracy: 0.7211
Epoch 50/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.5621 - accuracy: 0.7215
Epoch 50: val_loss did not improve from 0.56180
682/682 [==============================] - 2s 2ms/step - loss: 0.5621 - accuracy: 0.7217 - val_loss: 0.5618 - val_accuracy: 0.7193
Epoch 51/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.5631 - accuracy: 0.7210
Epoch 51: val_loss improved from 0.56180 to 0.56172, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5621 - accuracy: 0.7211 - val_loss: 0.5617 - val_accuracy: 0.7217
Epoch 52/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.5619 - accuracy: 0.7206
Epoch 52: val_loss did not improve from 0.56172
682/682 [==============================] - 2s 2ms/step - loss: 0.5618 - accuracy: 0.7210 - val_loss: 0.5618 - val_accuracy: 0.7181
Epoch 53/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.5620 - accuracy: 0.7216
Epoch 53: val_loss did not improve from 0.56172
682/682 [==============================] - 2s 2ms/step - loss: 0.5619 - accuracy: 0.7214 - val_loss: 0.5619 - val_accuracy: 0.7181
Epoch 54/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.5620 - accuracy: 0.7224
Epoch 54: val_loss improved from 0.56172 to 0.56161, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5620 - accuracy: 0.7221 - val_loss: 0.5616 - val_accuracy: 0.7223
Epoch 55/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.5616 - accuracy: 0.7215
Epoch 55: val_loss did not improve from 0.56161
682/682 [==============================] - 2s 2ms/step - loss: 0.5619 - accuracy: 0.7213 - val_loss: 0.5618 - val_accuracy: 0.7181
Epoch 56/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.5612 - accuracy: 0.7223
Epoch 56: val_loss improved from 0.56161 to 0.56154, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5619 - accuracy: 0.7217 - val_loss: 0.5615 - val_accuracy: 0.7217
Epoch 57/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.5621 - accuracy: 0.7213
Epoch 57: val_loss did not improve from 0.56154
682/682 [==============================] - 2s 2ms/step - loss: 0.5618 - accuracy: 0.7210 - val_loss: 0.5618 - val_accuracy: 0.7181
Epoch 58/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.5618 - accuracy: 0.7231
Epoch 58: val_loss did not improve from 0.56154
682/682 [==============================] - 2s 2ms/step - loss: 0.5618 - accuracy: 0.7230 - val_loss: 0.5616 - val_accuracy: 0.7199
Epoch 59/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.5611 - accuracy: 0.7215
Epoch 59: val_loss improved from 0.56154 to 0.56150, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5616 - accuracy: 0.7211 - val_loss: 0.5615 - val_accuracy: 0.7211
Epoch 60/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.5618 - accuracy: 0.7213
Epoch 60: val_loss did not improve from 0.56150
682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7213 - val_loss: 0.5618 - val_accuracy: 0.7193
54/54 [==============================] - 0s 2ms/step - loss: 0.5615 - accuracy: 0.7211
Drop Feature unemployed
data frame x has shape: (8516, 24)
['education', 'housing', 'loan', 'campaign', 'pdays', 'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'divorced', 'married', 'single', 'admin.', 'blue-collar', 'entrepreneur', 'housemaid', 'management', 'retired', 'self-employed', 'services', 'student', 'technician']
Epoch 1/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.6516 - accuracy: 0.6304
Epoch 1: val_loss improved from inf to 0.62058, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6511 - accuracy: 0.6313 - val_loss: 0.6206 - val_accuracy: 0.6635
Epoch 2/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.6053 - accuracy: 0.6971
Epoch 2: val_loss improved from 0.62058 to 0.59078, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6056 - accuracy: 0.6972 - val_loss: 0.5908 - val_accuracy: 0.7146
Epoch 3/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.5864 - accuracy: 0.7159
Epoch 3: val_loss improved from 0.59078 to 0.57843, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5863 - accuracy: 0.7150 - val_loss: 0.5784 - val_accuracy: 0.7211
Epoch 4/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.5785 - accuracy: 0.7171
Epoch 4: val_loss improved from 0.57843 to 0.57286, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5775 - accuracy: 0.7180 - val_loss: 0.5729 - val_accuracy: 0.7252
Epoch 5/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.5724 - accuracy: 0.7174
Epoch 5: val_loss improved from 0.57286 to 0.57008, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5730 - accuracy: 0.7163 - val_loss: 0.5701 - val_accuracy: 0.7170
Epoch 6/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.5709 - accuracy: 0.7167
Epoch 6: val_loss improved from 0.57008 to 0.56827, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5708 - accuracy: 0.7170 - val_loss: 0.5683 - val_accuracy: 0.7205
Epoch 7/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.5687 - accuracy: 0.7186
Epoch 7: val_loss improved from 0.56827 to 0.56724, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5692 - accuracy: 0.7182 - val_loss: 0.5672 - val_accuracy: 0.7176
Epoch 8/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.5687 - accuracy: 0.7158
Epoch 8: val_loss improved from 0.56724 to 0.56664, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5682 - accuracy: 0.7160 - val_loss: 0.5666 - val_accuracy: 0.7187
Epoch 9/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.5672 - accuracy: 0.7169
Epoch 9: val_loss did not improve from 0.56664
682/682 [==============================] - 2s 2ms/step - loss: 0.5674 - accuracy: 0.7161 - val_loss: 0.5668 - val_accuracy: 0.7234
Epoch 10/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.5663 - accuracy: 0.7175
Epoch 10: val_loss improved from 0.56664 to 0.56609, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5670 - accuracy: 0.7169 - val_loss: 0.5661 - val_accuracy: 0.7146
Epoch 11/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.5668 - accuracy: 0.7139
Epoch 11: val_loss improved from 0.56609 to 0.56553, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5665 - accuracy: 0.7153 - val_loss: 0.5655 - val_accuracy: 0.7181
Epoch 12/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.5662 - accuracy: 0.7151
Epoch 12: val_loss did not improve from 0.56553
682/682 [==============================] - 1s 2ms/step - loss: 0.5659 - accuracy: 0.7155 - val_loss: 0.5655 - val_accuracy: 0.7152
Epoch 13/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.5658 - accuracy: 0.7159
Epoch 13: val_loss improved from 0.56553 to 0.56482, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5658 - accuracy: 0.7155 - val_loss: 0.5648 - val_accuracy: 0.7217
Epoch 14/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.5651 - accuracy: 0.7149
Epoch 14: val_loss did not improve from 0.56482
682/682 [==============================] - 2s 2ms/step - loss: 0.5651 - accuracy: 0.7148 - val_loss: 0.5660 - val_accuracy: 0.7199
Epoch 15/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.5661 - accuracy: 0.7164
Epoch 15: val_loss improved from 0.56482 to 0.56452, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5652 - accuracy: 0.7173 - val_loss: 0.5645 - val_accuracy: 0.7199
Epoch 16/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.5641 - accuracy: 0.7162
Epoch 16: val_loss improved from 0.56452 to 0.56417, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5650 - accuracy: 0.7157 - val_loss: 0.5642 - val_accuracy: 0.7223
Epoch 17/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.5654 - accuracy: 0.7169
Epoch 17: val_loss improved from 0.56417 to 0.56396, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5647 - accuracy: 0.7176 - val_loss: 0.5640 - val_accuracy: 0.7228
Epoch 18/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.5649 - accuracy: 0.7170
Epoch 18: val_loss did not improve from 0.56396
682/682 [==============================] - 2s 2ms/step - loss: 0.5644 - accuracy: 0.7167 - val_loss: 0.5640 - val_accuracy: 0.7205
Epoch 19/60
654/682 [===========================&gt;..] - ETA: 0s - loss: 0.5651 - accuracy: 0.7171
Epoch 19: val_loss improved from 0.56396 to 0.56366, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5642 - accuracy: 0.7169 - val_loss: 0.5637 - val_accuracy: 0.7223
Epoch 20/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.5644 - accuracy: 0.7175
Epoch 20: val_loss improved from 0.56366 to 0.56353, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5641 - accuracy: 0.7176 - val_loss: 0.5635 - val_accuracy: 0.7205
Epoch 21/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.5633 - accuracy: 0.7177
Epoch 21: val_loss did not improve from 0.56353
682/682 [==============================] - 2s 2ms/step - loss: 0.5636 - accuracy: 0.7175 - val_loss: 0.5637 - val_accuracy: 0.7205
Epoch 22/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.5639 - accuracy: 0.7185
Epoch 22: val_loss improved from 0.56353 to 0.56329, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5638 - accuracy: 0.7186 - val_loss: 0.5633 - val_accuracy: 0.7211
Epoch 23/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.5644 - accuracy: 0.7173
Epoch 23: val_loss improved from 0.56329 to 0.56324, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5636 - accuracy: 0.7180 - val_loss: 0.5632 - val_accuracy: 0.7205
Epoch 24/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.5633 - accuracy: 0.7201
Epoch 24: val_loss improved from 0.56324 to 0.56310, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5633 - accuracy: 0.7197 - val_loss: 0.5631 - val_accuracy: 0.7193
Epoch 25/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.5632 - accuracy: 0.7190
Epoch 25: val_loss improved from 0.56310 to 0.56291, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5633 - accuracy: 0.7191 - val_loss: 0.5629 - val_accuracy: 0.7205
Epoch 26/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.5640 - accuracy: 0.7196
Epoch 26: val_loss improved from 0.56291 to 0.56273, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5632 - accuracy: 0.7207 - val_loss: 0.5627 - val_accuracy: 0.7217
Epoch 27/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.5627 - accuracy: 0.7208
Epoch 27: val_loss did not improve from 0.56273
682/682 [==============================] - 2s 2ms/step - loss: 0.5629 - accuracy: 0.7205 - val_loss: 0.5629 - val_accuracy: 0.7228
Epoch 28/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.5622 - accuracy: 0.7196
Epoch 28: val_loss improved from 0.56273 to 0.56263, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5631 - accuracy: 0.7191 - val_loss: 0.5626 - val_accuracy: 0.7217
Epoch 29/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.5629 - accuracy: 0.7200
Epoch 29: val_loss did not improve from 0.56263
682/682 [==============================] - 2s 2ms/step - loss: 0.5628 - accuracy: 0.7201 - val_loss: 0.5629 - val_accuracy: 0.7217
Epoch 30/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.5618 - accuracy: 0.7212
Epoch 30: val_loss improved from 0.56263 to 0.56241, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5627 - accuracy: 0.7204 - val_loss: 0.5624 - val_accuracy: 0.7199
Epoch 31/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.5625 - accuracy: 0.7182
Epoch 31: val_loss did not improve from 0.56241
682/682 [==============================] - 2s 2ms/step - loss: 0.5625 - accuracy: 0.7182 - val_loss: 0.5630 - val_accuracy: 0.7211
Epoch 32/60
649/682 [===========================&gt;..] - ETA: 0s - loss: 0.5639 - accuracy: 0.7168
Epoch 32: val_loss did not improve from 0.56241
682/682 [==============================] - 2s 2ms/step - loss: 0.5624 - accuracy: 0.7191 - val_loss: 0.5628 - val_accuracy: 0.7199
Epoch 33/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.5623 - accuracy: 0.7205
Epoch 33: val_loss improved from 0.56241 to 0.56214, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5626 - accuracy: 0.7201 - val_loss: 0.5621 - val_accuracy: 0.7223
Epoch 34/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.5622 - accuracy: 0.7196
Epoch 34: val_loss improved from 0.56214 to 0.56193, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5625 - accuracy: 0.7199 - val_loss: 0.5619 - val_accuracy: 0.7211
Epoch 35/60
682/682 [==============================] - ETA: 0s - loss: 0.5623 - accuracy: 0.7195
Epoch 35: val_loss did not improve from 0.56193
682/682 [==============================] - 2s 2ms/step - loss: 0.5623 - accuracy: 0.7195 - val_loss: 0.5626 - val_accuracy: 0.7181
Epoch 36/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.5639 - accuracy: 0.7185
Epoch 36: val_loss improved from 0.56193 to 0.56179, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5625 - accuracy: 0.7197 - val_loss: 0.5618 - val_accuracy: 0.7187
Epoch 37/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.5635 - accuracy: 0.7195
Epoch 37: val_loss did not improve from 0.56179
682/682 [==============================] - 2s 2ms/step - loss: 0.5623 - accuracy: 0.7199 - val_loss: 0.5619 - val_accuracy: 0.7193
Epoch 38/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.5621 - accuracy: 0.7201
Epoch 38: val_loss did not improve from 0.56179
682/682 [==============================] - 2s 2ms/step - loss: 0.5621 - accuracy: 0.7204 - val_loss: 0.5618 - val_accuracy: 0.7211
Epoch 39/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.5608 - accuracy: 0.7222
Epoch 39: val_loss improved from 0.56179 to 0.56177, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5622 - accuracy: 0.7210 - val_loss: 0.5618 - val_accuracy: 0.7199
Epoch 40/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.5612 - accuracy: 0.7222
Epoch 40: val_loss did not improve from 0.56177
682/682 [==============================] - 2s 2ms/step - loss: 0.5622 - accuracy: 0.7213 - val_loss: 0.5618 - val_accuracy: 0.7211
Epoch 41/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.5629 - accuracy: 0.7202
Epoch 41: val_loss did not improve from 0.56177
682/682 [==============================] - 2s 2ms/step - loss: 0.5620 - accuracy: 0.7202 - val_loss: 0.5619 - val_accuracy: 0.7199
Epoch 42/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.5622 - accuracy: 0.7216
Epoch 42: val_loss did not improve from 0.56177
682/682 [==============================] - 2s 2ms/step - loss: 0.5620 - accuracy: 0.7217 - val_loss: 0.5620 - val_accuracy: 0.7199
Epoch 43/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.5617 - accuracy: 0.7207
Epoch 43: val_loss improved from 0.56177 to 0.56155, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5620 - accuracy: 0.7204 - val_loss: 0.5615 - val_accuracy: 0.7205
Epoch 44/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.5613 - accuracy: 0.7216
Epoch 44: val_loss did not improve from 0.56155
682/682 [==============================] - 1s 2ms/step - loss: 0.5619 - accuracy: 0.7213 - val_loss: 0.5616 - val_accuracy: 0.7211
Epoch 45/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.5622 - accuracy: 0.7205
Epoch 45: val_loss did not improve from 0.56155
682/682 [==============================] - 1s 2ms/step - loss: 0.5618 - accuracy: 0.7205 - val_loss: 0.5616 - val_accuracy: 0.7193
Epoch 46/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.5605 - accuracy: 0.7208
Epoch 46: val_loss did not improve from 0.56155
682/682 [==============================] - 2s 2ms/step - loss: 0.5619 - accuracy: 0.7192 - val_loss: 0.5617 - val_accuracy: 0.7199
Epoch 47/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.5615 - accuracy: 0.7198
Epoch 47: val_loss improved from 0.56155 to 0.56150, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5618 - accuracy: 0.7197 - val_loss: 0.5615 - val_accuracy: 0.7223
Epoch 48/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.5619 - accuracy: 0.7225
Epoch 48: val_loss improved from 0.56150 to 0.56143, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5618 - accuracy: 0.7221 - val_loss: 0.5614 - val_accuracy: 0.7217
Epoch 49/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.5627 - accuracy: 0.7184
Epoch 49: val_loss did not improve from 0.56143
682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7194 - val_loss: 0.5615 - val_accuracy: 0.7205
Epoch 50/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.5601 - accuracy: 0.7231
Epoch 50: val_loss improved from 0.56143 to 0.56142, saving model to best_drop.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.5617 - accuracy: 0.7207 - val_loss: 0.5614 - val_accuracy: 0.7199
Epoch 51/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.5618 - accuracy: 0.7207
Epoch 51: val_loss did not improve from 0.56142
682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7205 - val_loss: 0.5615 - val_accuracy: 0.7199
Epoch 52/60
682/682 [==============================] - ETA: 0s - loss: 0.5617 - accuracy: 0.7220
Epoch 52: val_loss improved from 0.56142 to 0.56135, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7220 - val_loss: 0.5613 - val_accuracy: 0.7217
Epoch 53/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.5612 - accuracy: 0.7226
Epoch 53: val_loss did not improve from 0.56135
682/682 [==============================] - 2s 2ms/step - loss: 0.5616 - accuracy: 0.7220 - val_loss: 0.5616 - val_accuracy: 0.7211
Epoch 54/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.5609 - accuracy: 0.7225
Epoch 54: val_loss did not improve from 0.56135
682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7214 - val_loss: 0.5615 - val_accuracy: 0.7187
Epoch 55/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.5614 - accuracy: 0.7215
Epoch 55: val_loss did not improve from 0.56135
682/682 [==============================] - 2s 2ms/step - loss: 0.5616 - accuracy: 0.7210 - val_loss: 0.5615 - val_accuracy: 0.7217
Epoch 56/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.5625 - accuracy: 0.7189
Epoch 56: val_loss did not improve from 0.56135
682/682 [==============================] - 1s 2ms/step - loss: 0.5617 - accuracy: 0.7197 - val_loss: 0.5614 - val_accuracy: 0.7211
Epoch 57/60
653/682 [===========================&gt;..] - ETA: 0s - loss: 0.5583 - accuracy: 0.7243
Epoch 57: val_loss did not improve from 0.56135
682/682 [==============================] - 2s 2ms/step - loss: 0.5616 - accuracy: 0.7214 - val_loss: 0.5614 - val_accuracy: 0.7217
Epoch 58/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.5619 - accuracy: 0.7213
Epoch 58: val_loss did not improve from 0.56135
682/682 [==============================] - 2s 2ms/step - loss: 0.5615 - accuracy: 0.7211 - val_loss: 0.5614 - val_accuracy: 0.7205
Epoch 59/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.5624 - accuracy: 0.7201
Epoch 59: val_loss improved from 0.56135 to 0.56125, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5615 - accuracy: 0.7213 - val_loss: 0.5613 - val_accuracy: 0.7217
Epoch 60/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.5604 - accuracy: 0.7228
Epoch 60: val_loss did not improve from 0.56125
682/682 [==============================] - 2s 2ms/step - loss: 0.5614 - accuracy: 0.7217 - val_loss: 0.5616 - val_accuracy: 0.7193
54/54 [==============================] - 0s 2ms/step - loss: 0.5613 - accuracy: 0.7217
Drop Feature housemaid
data frame x has shape: (8516, 23)
['education', 'housing', 'loan', 'campaign', 'pdays', 'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'divorced', 'married', 'single', 'admin.', 'blue-collar', 'entrepreneur', 'management', 'retired', 'self-employed', 'services', 'student', 'technician']
Epoch 1/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.6547 - accuracy: 0.6417
Epoch 1: val_loss improved from inf to 0.62072, saving model to best_drop.hdf5
682/682 [==============================] - 2s 3ms/step - loss: 0.6546 - accuracy: 0.6423 - val_loss: 0.6207 - val_accuracy: 0.7041
Epoch 2/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.6063 - accuracy: 0.7132
Epoch 2: val_loss improved from 0.62072 to 0.59093, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6062 - accuracy: 0.7130 - val_loss: 0.5909 - val_accuracy: 0.7187
Epoch 3/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.5873 - accuracy: 0.7176
Epoch 3: val_loss improved from 0.59093 to 0.57872, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5871 - accuracy: 0.7176 - val_loss: 0.5787 - val_accuracy: 0.7199
Epoch 4/60
648/682 [===========================&gt;..] - ETA: 0s - loss: 0.5784 - accuracy: 0.7173
Epoch 4: val_loss improved from 0.57872 to 0.57289, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5786 - accuracy: 0.7179 - val_loss: 0.5729 - val_accuracy: 0.7234
Epoch 5/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.5725 - accuracy: 0.7156
Epoch 5: val_loss improved from 0.57289 to 0.57008, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5744 - accuracy: 0.7145 - val_loss: 0.5701 - val_accuracy: 0.7234
Epoch 6/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.5721 - accuracy: 0.7139
Epoch 6: val_loss improved from 0.57008 to 0.56897, saving model to best_drop.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.5718 - accuracy: 0.7144 - val_loss: 0.5690 - val_accuracy: 0.7240
Epoch 7/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.5696 - accuracy: 0.7164
Epoch 7: val_loss improved from 0.56897 to 0.56762, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5703 - accuracy: 0.7160 - val_loss: 0.5676 - val_accuracy: 0.7211
Epoch 8/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.5684 - accuracy: 0.7152
Epoch 8: val_loss improved from 0.56762 to 0.56695, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5693 - accuracy: 0.7147 - val_loss: 0.5669 - val_accuracy: 0.7223
Epoch 9/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.5681 - accuracy: 0.7147
Epoch 9: val_loss improved from 0.56695 to 0.56652, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5685 - accuracy: 0.7139 - val_loss: 0.5665 - val_accuracy: 0.7205
Epoch 10/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.5676 - accuracy: 0.7131
Epoch 10: val_loss improved from 0.56652 to 0.56618, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5679 - accuracy: 0.7128 - val_loss: 0.5662 - val_accuracy: 0.7181
Epoch 11/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.5667 - accuracy: 0.7140
Epoch 11: val_loss improved from 0.56618 to 0.56604, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5673 - accuracy: 0.7132 - val_loss: 0.5660 - val_accuracy: 0.7181
Epoch 12/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.5666 - accuracy: 0.7128
Epoch 12: val_loss improved from 0.56604 to 0.56554, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5668 - accuracy: 0.7122 - val_loss: 0.5655 - val_accuracy: 0.7217
Epoch 13/60
654/682 [===========================&gt;..] - ETA: 0s - loss: 0.5677 - accuracy: 0.7102
Epoch 13: val_loss improved from 0.56554 to 0.56521, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5664 - accuracy: 0.7116 - val_loss: 0.5652 - val_accuracy: 0.7181
Epoch 14/60
654/682 [===========================&gt;..] - ETA: 0s - loss: 0.5655 - accuracy: 0.7145
Epoch 14: val_loss improved from 0.56521 to 0.56494, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5660 - accuracy: 0.7141 - val_loss: 0.5649 - val_accuracy: 0.7205
Epoch 15/60
653/682 [===========================&gt;..] - ETA: 0s - loss: 0.5641 - accuracy: 0.7141
Epoch 15: val_loss improved from 0.56494 to 0.56471, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5658 - accuracy: 0.7119 - val_loss: 0.5647 - val_accuracy: 0.7199
Epoch 16/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.5650 - accuracy: 0.7142
Epoch 16: val_loss improved from 0.56471 to 0.56451, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5654 - accuracy: 0.7139 - val_loss: 0.5645 - val_accuracy: 0.7199
Epoch 17/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.5651 - accuracy: 0.7120
Epoch 17: val_loss did not improve from 0.56451
682/682 [==============================] - 2s 2ms/step - loss: 0.5650 - accuracy: 0.7120 - val_loss: 0.5647 - val_accuracy: 0.7240
Epoch 18/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.5645 - accuracy: 0.7149
Epoch 18: val_loss improved from 0.56451 to 0.56409, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5649 - accuracy: 0.7150 - val_loss: 0.5641 - val_accuracy: 0.7228
Epoch 19/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.5648 - accuracy: 0.7157
Epoch 19: val_loss improved from 0.56409 to 0.56397, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5645 - accuracy: 0.7163 - val_loss: 0.5640 - val_accuracy: 0.7211
Epoch 20/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.5645 - accuracy: 0.7164
Epoch 20: val_loss improved from 0.56397 to 0.56368, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5644 - accuracy: 0.7166 - val_loss: 0.5637 - val_accuracy: 0.7217
Epoch 21/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.5638 - accuracy: 0.7175
Epoch 21: val_loss improved from 0.56368 to 0.56355, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5642 - accuracy: 0.7175 - val_loss: 0.5636 - val_accuracy: 0.7211
Epoch 22/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.5645 - accuracy: 0.7140
Epoch 22: val_loss improved from 0.56355 to 0.56339, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5640 - accuracy: 0.7157 - val_loss: 0.5634 - val_accuracy: 0.7223
Epoch 23/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.5634 - accuracy: 0.7177
Epoch 23: val_loss improved from 0.56339 to 0.56326, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5637 - accuracy: 0.7175 - val_loss: 0.5633 - val_accuracy: 0.7223
Epoch 24/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.5637 - accuracy: 0.7184
Epoch 24: val_loss improved from 0.56326 to 0.56308, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5637 - accuracy: 0.7185 - val_loss: 0.5631 - val_accuracy: 0.7217
Epoch 25/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.5649 - accuracy: 0.7156
Epoch 25: val_loss improved from 0.56308 to 0.56301, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5635 - accuracy: 0.7170 - val_loss: 0.5630 - val_accuracy: 0.7217
Epoch 26/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.5632 - accuracy: 0.7181
Epoch 26: val_loss improved from 0.56301 to 0.56294, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5634 - accuracy: 0.7179 - val_loss: 0.5629 - val_accuracy: 0.7217
Epoch 27/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.5635 - accuracy: 0.7176
Epoch 27: val_loss did not improve from 0.56294
682/682 [==============================] - 2s 2ms/step - loss: 0.5632 - accuracy: 0.7176 - val_loss: 0.5631 - val_accuracy: 0.7205
Epoch 28/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.5627 - accuracy: 0.7204
Epoch 28: val_loss improved from 0.56294 to 0.56273, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5630 - accuracy: 0.7195 - val_loss: 0.5627 - val_accuracy: 0.7211
Epoch 29/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.5642 - accuracy: 0.7170
Epoch 29: val_loss improved from 0.56273 to 0.56269, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5629 - accuracy: 0.7183 - val_loss: 0.5627 - val_accuracy: 0.7217
Epoch 30/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.5631 - accuracy: 0.7193
Epoch 30: val_loss did not improve from 0.56269
682/682 [==============================] - 2s 2ms/step - loss: 0.5628 - accuracy: 0.7199 - val_loss: 0.5627 - val_accuracy: 0.7223
Epoch 31/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.5642 - accuracy: 0.7172
Epoch 31: val_loss improved from 0.56269 to 0.56252, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5627 - accuracy: 0.7183 - val_loss: 0.5625 - val_accuracy: 0.7205
Epoch 32/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.5624 - accuracy: 0.7186
Epoch 32: val_loss did not improve from 0.56252
682/682 [==============================] - 1s 2ms/step - loss: 0.5625 - accuracy: 0.7185 - val_loss: 0.5631 - val_accuracy: 0.7228
Epoch 33/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.5613 - accuracy: 0.7212
Epoch 33: val_loss improved from 0.56252 to 0.56232, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5626 - accuracy: 0.7198 - val_loss: 0.5623 - val_accuracy: 0.7217
Epoch 34/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.5619 - accuracy: 0.7188
Epoch 34: val_loss did not improve from 0.56232
682/682 [==============================] - 2s 2ms/step - loss: 0.5625 - accuracy: 0.7182 - val_loss: 0.5623 - val_accuracy: 0.7193
Epoch 35/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.5615 - accuracy: 0.7206
Epoch 35: val_loss improved from 0.56232 to 0.56204, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5623 - accuracy: 0.7201 - val_loss: 0.5620 - val_accuracy: 0.7228
Epoch 36/60
651/682 [===========================&gt;..] - ETA: 0s - loss: 0.5622 - accuracy: 0.7203
Epoch 36: val_loss did not improve from 0.56204
682/682 [==============================] - 2s 2ms/step - loss: 0.5622 - accuracy: 0.7211 - val_loss: 0.5622 - val_accuracy: 0.7205
Epoch 37/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.5619 - accuracy: 0.7197
Epoch 37: val_loss did not improve from 0.56204
682/682 [==============================] - 2s 2ms/step - loss: 0.5622 - accuracy: 0.7195 - val_loss: 0.5627 - val_accuracy: 0.7187
Epoch 38/60
682/682 [==============================] - ETA: 0s - loss: 0.5620 - accuracy: 0.7201
Epoch 38: val_loss improved from 0.56204 to 0.56193, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5620 - accuracy: 0.7201 - val_loss: 0.5619 - val_accuracy: 0.7234
Epoch 39/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.5630 - accuracy: 0.7194
Epoch 39: val_loss improved from 0.56193 to 0.56193, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5621 - accuracy: 0.7201 - val_loss: 0.5619 - val_accuracy: 0.7211
Epoch 40/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.5598 - accuracy: 0.7218
Epoch 40: val_loss improved from 0.56193 to 0.56185, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5619 - accuracy: 0.7208 - val_loss: 0.5619 - val_accuracy: 0.7211
Epoch 41/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.5599 - accuracy: 0.7231
Epoch 41: val_loss improved from 0.56185 to 0.56184, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5620 - accuracy: 0.7208 - val_loss: 0.5618 - val_accuracy: 0.7223
Epoch 42/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.5625 - accuracy: 0.7205
Epoch 42: val_loss did not improve from 0.56184
682/682 [==============================] - 2s 2ms/step - loss: 0.5619 - accuracy: 0.7213 - val_loss: 0.5619 - val_accuracy: 0.7211
Epoch 43/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.5605 - accuracy: 0.7215
Epoch 43: val_loss did not improve from 0.56184
682/682 [==============================] - 2s 2ms/step - loss: 0.5618 - accuracy: 0.7208 - val_loss: 0.5620 - val_accuracy: 0.7205
Epoch 44/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.5619 - accuracy: 0.7188
Epoch 44: val_loss did not improve from 0.56184
682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7191 - val_loss: 0.5619 - val_accuracy: 0.7199
Epoch 45/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.5623 - accuracy: 0.7195
Epoch 45: val_loss improved from 0.56184 to 0.56151, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7199 - val_loss: 0.5615 - val_accuracy: 0.7234
Epoch 46/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.5611 - accuracy: 0.7192
Epoch 46: val_loss did not improve from 0.56151
682/682 [==============================] - 2s 2ms/step - loss: 0.5616 - accuracy: 0.7189 - val_loss: 0.5626 - val_accuracy: 0.7205
Epoch 47/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.5620 - accuracy: 0.7201
Epoch 47: val_loss did not improve from 0.56151
682/682 [==============================] - 2s 2ms/step - loss: 0.5616 - accuracy: 0.7208 - val_loss: 0.5626 - val_accuracy: 0.7228
Epoch 48/60
652/682 [===========================&gt;..] - ETA: 0s - loss: 0.5607 - accuracy: 0.7210
Epoch 48: val_loss improved from 0.56151 to 0.56134, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5615 - accuracy: 0.7205 - val_loss: 0.5613 - val_accuracy: 0.7223
Epoch 49/60
654/682 [===========================&gt;..] - ETA: 0s - loss: 0.5625 - accuracy: 0.7206
Epoch 49: val_loss did not improve from 0.56134
682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7211 - val_loss: 0.5614 - val_accuracy: 0.7246
Epoch 50/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.5611 - accuracy: 0.7206
Epoch 50: val_loss did not improve from 0.56134
682/682 [==============================] - 1s 2ms/step - loss: 0.5614 - accuracy: 0.7208 - val_loss: 0.5616 - val_accuracy: 0.7199
Epoch 51/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.5619 - accuracy: 0.7201
Epoch 51: val_loss did not improve from 0.56134
682/682 [==============================] - 2s 2ms/step - loss: 0.5616 - accuracy: 0.7195 - val_loss: 0.5615 - val_accuracy: 0.7234
Epoch 52/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.5629 - accuracy: 0.7207
Epoch 52: val_loss did not improve from 0.56134
682/682 [==============================] - 2s 2ms/step - loss: 0.5614 - accuracy: 0.7219 - val_loss: 0.5617 - val_accuracy: 0.7205
Epoch 53/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.5608 - accuracy: 0.7222
Epoch 53: val_loss did not improve from 0.56134
682/682 [==============================] - 2s 2ms/step - loss: 0.5615 - accuracy: 0.7220 - val_loss: 0.5615 - val_accuracy: 0.7211
Epoch 54/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.5606 - accuracy: 0.7211
Epoch 54: val_loss improved from 0.56134 to 0.56130, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5615 - accuracy: 0.7210 - val_loss: 0.5613 - val_accuracy: 0.7217
Epoch 55/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.5611 - accuracy: 0.7220
Epoch 55: val_loss did not improve from 0.56130
682/682 [==============================] - 1s 2ms/step - loss: 0.5615 - accuracy: 0.7211 - val_loss: 0.5616 - val_accuracy: 0.7187
Epoch 56/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.5606 - accuracy: 0.7225
Epoch 56: val_loss did not improve from 0.56130
682/682 [==============================] - 1s 2ms/step - loss: 0.5615 - accuracy: 0.7226 - val_loss: 0.5613 - val_accuracy: 0.7217
Epoch 57/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.5614 - accuracy: 0.7217
Epoch 57: val_loss improved from 0.56130 to 0.56129, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5613 - accuracy: 0.7217 - val_loss: 0.5613 - val_accuracy: 0.7217
Epoch 58/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.5612 - accuracy: 0.7217
Epoch 58: val_loss improved from 0.56129 to 0.56122, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5614 - accuracy: 0.7216 - val_loss: 0.5612 - val_accuracy: 0.7211
Epoch 59/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.5619 - accuracy: 0.7223
Epoch 59: val_loss did not improve from 0.56122
682/682 [==============================] - 2s 2ms/step - loss: 0.5613 - accuracy: 0.7232 - val_loss: 0.5613 - val_accuracy: 0.7234
Epoch 60/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.5615 - accuracy: 0.7218
Epoch 60: val_loss did not improve from 0.56122
682/682 [==============================] - 2s 2ms/step - loss: 0.5614 - accuracy: 0.7220 - val_loss: 0.5613 - val_accuracy: 0.7211
54/54 [==============================] - 0s 2ms/step - loss: 0.5612 - accuracy: 0.7211
Drop Feature management
data frame x has shape: (8516, 22)
['education', 'housing', 'loan', 'campaign', 'pdays', 'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'divorced', 'married', 'single', 'admin.', 'blue-collar', 'entrepreneur', 'retired', 'self-employed', 'services', 'student', 'technician']
Epoch 1/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.6848 - accuracy: 0.5650
Epoch 1: val_loss improved from inf to 0.64007, saving model to best_drop.hdf5
682/682 [==============================] - 2s 3ms/step - loss: 0.6847 - accuracy: 0.5652 - val_loss: 0.6401 - val_accuracy: 0.6371
Epoch 2/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.6248 - accuracy: 0.6762
Epoch 2: val_loss improved from 0.64007 to 0.60333, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6248 - accuracy: 0.6759 - val_loss: 0.6033 - val_accuracy: 0.7011
Epoch 3/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.5986 - accuracy: 0.7037
Epoch 3: val_loss improved from 0.60333 to 0.58700, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5989 - accuracy: 0.7034 - val_loss: 0.5870 - val_accuracy: 0.7199
Epoch 4/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.5870 - accuracy: 0.7104
Epoch 4: val_loss improved from 0.58700 to 0.57950, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5868 - accuracy: 0.7101 - val_loss: 0.5795 - val_accuracy: 0.7211
Epoch 5/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.5802 - accuracy: 0.7107
Epoch 5: val_loss improved from 0.57950 to 0.57519, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5804 - accuracy: 0.7101 - val_loss: 0.5752 - val_accuracy: 0.7211
Epoch 6/60
654/682 [===========================&gt;..] - ETA: 0s - loss: 0.5775 - accuracy: 0.7099
Epoch 6: val_loss improved from 0.57519 to 0.57299, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5771 - accuracy: 0.7101 - val_loss: 0.5730 - val_accuracy: 0.7228
Epoch 7/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.5752 - accuracy: 0.7110
Epoch 7: val_loss improved from 0.57299 to 0.57147, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5750 - accuracy: 0.7108 - val_loss: 0.5715 - val_accuracy: 0.7234
Epoch 8/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.5723 - accuracy: 0.7126
Epoch 8: val_loss improved from 0.57147 to 0.57083, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5733 - accuracy: 0.7113 - val_loss: 0.5708 - val_accuracy: 0.7211
Epoch 9/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.5719 - accuracy: 0.7110
Epoch 9: val_loss improved from 0.57083 to 0.56974, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5722 - accuracy: 0.7108 - val_loss: 0.5697 - val_accuracy: 0.7228
Epoch 10/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.5715 - accuracy: 0.7123
Epoch 10: val_loss improved from 0.56974 to 0.56909, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5712 - accuracy: 0.7125 - val_loss: 0.5691 - val_accuracy: 0.7228
Epoch 11/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.5699 - accuracy: 0.7132
Epoch 11: val_loss improved from 0.56909 to 0.56856, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5705 - accuracy: 0.7123 - val_loss: 0.5686 - val_accuracy: 0.7217
Epoch 12/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.5690 - accuracy: 0.7134
Epoch 12: val_loss improved from 0.56856 to 0.56813, saving model to best_drop.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.5697 - accuracy: 0.7125 - val_loss: 0.5681 - val_accuracy: 0.7193
Epoch 13/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.5702 - accuracy: 0.7122
Epoch 13: val_loss did not improve from 0.56813
682/682 [==============================] - 2s 2ms/step - loss: 0.5689 - accuracy: 0.7128 - val_loss: 0.5681 - val_accuracy: 0.7228
Epoch 14/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.5688 - accuracy: 0.7134
Epoch 14: val_loss improved from 0.56813 to 0.56722, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5687 - accuracy: 0.7138 - val_loss: 0.5672 - val_accuracy: 0.7223
Epoch 15/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.5692 - accuracy: 0.7123
Epoch 15: val_loss improved from 0.56722 to 0.56688, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5681 - accuracy: 0.7135 - val_loss: 0.5669 - val_accuracy: 0.7211
Epoch 16/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.5679 - accuracy: 0.7112
Epoch 16: val_loss improved from 0.56688 to 0.56662, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5676 - accuracy: 0.7119 - val_loss: 0.5666 - val_accuracy: 0.7199
Epoch 17/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.5652 - accuracy: 0.7150
Epoch 17: val_loss improved from 0.56662 to 0.56606, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5672 - accuracy: 0.7130 - val_loss: 0.5661 - val_accuracy: 0.7228
Epoch 18/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.5670 - accuracy: 0.7146
Epoch 18: val_loss did not improve from 0.56606
682/682 [==============================] - 2s 2ms/step - loss: 0.5666 - accuracy: 0.7142 - val_loss: 0.5662 - val_accuracy: 0.7170
Epoch 19/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.5668 - accuracy: 0.7142
Epoch 19: val_loss improved from 0.56606 to 0.56541, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5666 - accuracy: 0.7151 - val_loss: 0.5654 - val_accuracy: 0.7234
Epoch 20/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.5661 - accuracy: 0.7137
Epoch 20: val_loss improved from 0.56541 to 0.56509, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5661 - accuracy: 0.7136 - val_loss: 0.5651 - val_accuracy: 0.7246
Epoch 21/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.5669 - accuracy: 0.7138
Epoch 21: val_loss did not improve from 0.56509
682/682 [==============================] - 1s 2ms/step - loss: 0.5656 - accuracy: 0.7153 - val_loss: 0.5657 - val_accuracy: 0.7234
Epoch 22/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.5640 - accuracy: 0.7180
Epoch 22: val_loss improved from 0.56509 to 0.56481, saving model to best_drop.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.5656 - accuracy: 0.7166 - val_loss: 0.5648 - val_accuracy: 0.7211
Epoch 23/60
654/682 [===========================&gt;..] - ETA: 0s - loss: 0.5676 - accuracy: 0.7115
Epoch 23: val_loss improved from 0.56481 to 0.56463, saving model to best_drop.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.5652 - accuracy: 0.7139 - val_loss: 0.5646 - val_accuracy: 0.7234
Epoch 24/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.5643 - accuracy: 0.7156
Epoch 24: val_loss improved from 0.56463 to 0.56428, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5650 - accuracy: 0.7151 - val_loss: 0.5643 - val_accuracy: 0.7217
Epoch 25/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.5647 - accuracy: 0.7182
Epoch 25: val_loss improved from 0.56428 to 0.56415, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5647 - accuracy: 0.7180 - val_loss: 0.5641 - val_accuracy: 0.7217
Epoch 26/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.5629 - accuracy: 0.7183
Epoch 26: val_loss improved from 0.56415 to 0.56410, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5644 - accuracy: 0.7167 - val_loss: 0.5641 - val_accuracy: 0.7228
Epoch 27/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.5645 - accuracy: 0.7192
Epoch 27: val_loss improved from 0.56410 to 0.56385, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5641 - accuracy: 0.7189 - val_loss: 0.5638 - val_accuracy: 0.7223
Epoch 28/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.5640 - accuracy: 0.7173
Epoch 28: val_loss improved from 0.56385 to 0.56354, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5642 - accuracy: 0.7172 - val_loss: 0.5635 - val_accuracy: 0.7234
Epoch 29/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.5644 - accuracy: 0.7173
Epoch 29: val_loss did not improve from 0.56354
682/682 [==============================] - 2s 2ms/step - loss: 0.5638 - accuracy: 0.7179 - val_loss: 0.5636 - val_accuracy: 0.7234
Epoch 30/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.5634 - accuracy: 0.7166
Epoch 30: val_loss improved from 0.56354 to 0.56330, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5635 - accuracy: 0.7163 - val_loss: 0.5633 - val_accuracy: 0.7240
Epoch 31/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.5632 - accuracy: 0.7167
Epoch 31: val_loss improved from 0.56330 to 0.56311, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5635 - accuracy: 0.7167 - val_loss: 0.5631 - val_accuracy: 0.7217
Epoch 32/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.5638 - accuracy: 0.7181
Epoch 32: val_loss improved from 0.56311 to 0.56293, saving model to best_drop.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.5634 - accuracy: 0.7182 - val_loss: 0.5629 - val_accuracy: 0.7223
Epoch 33/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.5637 - accuracy: 0.7175
Epoch 33: val_loss improved from 0.56293 to 0.56283, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5634 - accuracy: 0.7185 - val_loss: 0.5628 - val_accuracy: 0.7228
Epoch 34/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.5627 - accuracy: 0.7204
Epoch 34: val_loss did not improve from 0.56283
682/682 [==============================] - 2s 2ms/step - loss: 0.5630 - accuracy: 0.7192 - val_loss: 0.5629 - val_accuracy: 0.7228
Epoch 35/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.5617 - accuracy: 0.7202
Epoch 35: val_loss improved from 0.56283 to 0.56274, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5630 - accuracy: 0.7186 - val_loss: 0.5627 - val_accuracy: 0.7228
Epoch 36/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.5613 - accuracy: 0.7187
Epoch 36: val_loss improved from 0.56274 to 0.56272, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5629 - accuracy: 0.7177 - val_loss: 0.5627 - val_accuracy: 0.7234
Epoch 37/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.5627 - accuracy: 0.7183
Epoch 37: val_loss improved from 0.56272 to 0.56254, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5629 - accuracy: 0.7180 - val_loss: 0.5625 - val_accuracy: 0.7217
Epoch 38/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.5634 - accuracy: 0.7187
Epoch 38: val_loss improved from 0.56254 to 0.56246, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5628 - accuracy: 0.7189 - val_loss: 0.5625 - val_accuracy: 0.7217
Epoch 39/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.5632 - accuracy: 0.7192
Epoch 39: val_loss did not improve from 0.56246
682/682 [==============================] - 2s 2ms/step - loss: 0.5626 - accuracy: 0.7194 - val_loss: 0.5625 - val_accuracy: 0.7199
Epoch 40/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.5627 - accuracy: 0.7191
Epoch 40: val_loss improved from 0.56246 to 0.56227, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5625 - accuracy: 0.7199 - val_loss: 0.5623 - val_accuracy: 0.7211
Epoch 41/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.5619 - accuracy: 0.7197
Epoch 41: val_loss did not improve from 0.56227
682/682 [==============================] - 2s 2ms/step - loss: 0.5624 - accuracy: 0.7186 - val_loss: 0.5626 - val_accuracy: 0.7205
Epoch 42/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.5632 - accuracy: 0.7190
Epoch 42: val_loss improved from 0.56227 to 0.56209, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5624 - accuracy: 0.7201 - val_loss: 0.5621 - val_accuracy: 0.7211
Epoch 43/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.5621 - accuracy: 0.7203
Epoch 43: val_loss improved from 0.56209 to 0.56200, saving model to best_drop.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.5623 - accuracy: 0.7201 - val_loss: 0.5620 - val_accuracy: 0.7211
Epoch 44/60
649/682 [===========================&gt;..] - ETA: 0s - loss: 0.5641 - accuracy: 0.7191
Epoch 44: val_loss did not improve from 0.56200
682/682 [==============================] - 2s 2ms/step - loss: 0.5622 - accuracy: 0.7207 - val_loss: 0.5620 - val_accuracy: 0.7205
Epoch 45/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.5631 - accuracy: 0.7183
Epoch 45: val_loss did not improve from 0.56200
682/682 [==============================] - 1s 2ms/step - loss: 0.5623 - accuracy: 0.7195 - val_loss: 0.5621 - val_accuracy: 0.7217
Epoch 46/60
652/682 [===========================&gt;..] - ETA: 0s - loss: 0.5609 - accuracy: 0.7196
Epoch 46: val_loss improved from 0.56200 to 0.56174, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5621 - accuracy: 0.7192 - val_loss: 0.5617 - val_accuracy: 0.7228
Epoch 47/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.5621 - accuracy: 0.7191
Epoch 47: val_loss did not improve from 0.56174
682/682 [==============================] - 2s 2ms/step - loss: 0.5621 - accuracy: 0.7194 - val_loss: 0.5619 - val_accuracy: 0.7223
Epoch 48/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.5623 - accuracy: 0.7202
Epoch 48: val_loss did not improve from 0.56174
682/682 [==============================] - 2s 2ms/step - loss: 0.5618 - accuracy: 0.7205 - val_loss: 0.5622 - val_accuracy: 0.7217
Epoch 49/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.5615 - accuracy: 0.7211
Epoch 49: val_loss improved from 0.56174 to 0.56157, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5618 - accuracy: 0.7202 - val_loss: 0.5616 - val_accuracy: 0.7211
Epoch 50/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.5620 - accuracy: 0.7201
Epoch 50: val_loss did not improve from 0.56157
682/682 [==============================] - 2s 2ms/step - loss: 0.5620 - accuracy: 0.7201 - val_loss: 0.5617 - val_accuracy: 0.7205
Epoch 51/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.5628 - accuracy: 0.7189
Epoch 51: val_loss did not improve from 0.56157
682/682 [==============================] - 2s 2ms/step - loss: 0.5618 - accuracy: 0.7201 - val_loss: 0.5620 - val_accuracy: 0.7193
Epoch 52/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.5606 - accuracy: 0.7227
Epoch 52: val_loss did not improve from 0.56157
682/682 [==============================] - 2s 2ms/step - loss: 0.5618 - accuracy: 0.7219 - val_loss: 0.5618 - val_accuracy: 0.7205
Epoch 53/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.5626 - accuracy: 0.7199
Epoch 53: val_loss did not improve from 0.56157
682/682 [==============================] - 1s 2ms/step - loss: 0.5618 - accuracy: 0.7207 - val_loss: 0.5617 - val_accuracy: 0.7211
Epoch 54/60
654/682 [===========================&gt;..] - ETA: 0s - loss: 0.5621 - accuracy: 0.7206
Epoch 54: val_loss did not improve from 0.56157
682/682 [==============================] - 1s 2ms/step - loss: 0.5617 - accuracy: 0.7210 - val_loss: 0.5617 - val_accuracy: 0.7211
Epoch 55/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.5615 - accuracy: 0.7200
Epoch 55: val_loss did not improve from 0.56157
682/682 [==============================] - 1s 2ms/step - loss: 0.5617 - accuracy: 0.7199 - val_loss: 0.5617 - val_accuracy: 0.7199
Epoch 56/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.5604 - accuracy: 0.7217
Epoch 56: val_loss did not improve from 0.56157
682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7211 - val_loss: 0.5616 - val_accuracy: 0.7234
Epoch 57/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.5611 - accuracy: 0.7202
Epoch 57: val_loss did not improve from 0.56157
682/682 [==============================] - 2s 2ms/step - loss: 0.5616 - accuracy: 0.7199 - val_loss: 0.5617 - val_accuracy: 0.7199
Epoch 58/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.5616 - accuracy: 0.7218
Epoch 58: val_loss improved from 0.56157 to 0.56145, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5616 - accuracy: 0.7217 - val_loss: 0.5615 - val_accuracy: 0.7211
Epoch 59/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.5619 - accuracy: 0.7209
Epoch 59: val_loss did not improve from 0.56145
682/682 [==============================] - 2s 2ms/step - loss: 0.5616 - accuracy: 0.7211 - val_loss: 0.5617 - val_accuracy: 0.7199
Epoch 60/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.5621 - accuracy: 0.7213
Epoch 60: val_loss did not improve from 0.56145
682/682 [==============================] - 2s 2ms/step - loss: 0.5615 - accuracy: 0.7224 - val_loss: 0.5615 - val_accuracy: 0.7211
54/54 [==============================] - 0s 2ms/step - loss: 0.5615 - accuracy: 0.7211
Drop Feature technician
data frame x has shape: (8516, 21)
['education', 'housing', 'loan', 'campaign', 'pdays', 'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'divorced', 'married', 'single', 'admin.', 'blue-collar', 'entrepreneur', 'retired', 'self-employed', 'services', 'student']
Epoch 1/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.6753 - accuracy: 0.5550
Epoch 1: val_loss improved from inf to 0.63657, saving model to best_drop.hdf5
682/682 [==============================] - 2s 3ms/step - loss: 0.6750 - accuracy: 0.5566 - val_loss: 0.6366 - val_accuracy: 0.6494
Epoch 2/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.6239 - accuracy: 0.6598
Epoch 2: val_loss improved from 0.63657 to 0.60533, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6227 - accuracy: 0.6624 - val_loss: 0.6053 - val_accuracy: 0.6782
Epoch 3/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.6003 - accuracy: 0.6914
Epoch 3: val_loss improved from 0.60533 to 0.58878, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6000 - accuracy: 0.6916 - val_loss: 0.5888 - val_accuracy: 0.7070
Epoch 4/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.5886 - accuracy: 0.7053
Epoch 4: val_loss improved from 0.58878 to 0.58063, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5881 - accuracy: 0.7063 - val_loss: 0.5806 - val_accuracy: 0.7146
Epoch 5/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.5815 - accuracy: 0.7129
Epoch 5: val_loss improved from 0.58063 to 0.57582, saving model to best_drop.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.5814 - accuracy: 0.7128 - val_loss: 0.5758 - val_accuracy: 0.7181
Epoch 6/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.5778 - accuracy: 0.7156
Epoch 6: val_loss improved from 0.57582 to 0.57310, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5774 - accuracy: 0.7151 - val_loss: 0.5731 - val_accuracy: 0.7170
Epoch 7/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.5748 - accuracy: 0.7145
Epoch 7: val_loss improved from 0.57310 to 0.57160, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5748 - accuracy: 0.7147 - val_loss: 0.5716 - val_accuracy: 0.7193
Epoch 8/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.5730 - accuracy: 0.7154
Epoch 8: val_loss improved from 0.57160 to 0.56993, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5730 - accuracy: 0.7151 - val_loss: 0.5699 - val_accuracy: 0.7217
Epoch 9/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.5732 - accuracy: 0.7159
Epoch 9: val_loss improved from 0.56993 to 0.56900, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5718 - accuracy: 0.7170 - val_loss: 0.5690 - val_accuracy: 0.7205
Epoch 10/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.5681 - accuracy: 0.7169
Epoch 10: val_loss improved from 0.56900 to 0.56840, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5707 - accuracy: 0.7145 - val_loss: 0.5684 - val_accuracy: 0.7240
Epoch 11/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.5693 - accuracy: 0.7152
Epoch 11: val_loss improved from 0.56840 to 0.56809, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5697 - accuracy: 0.7144 - val_loss: 0.5681 - val_accuracy: 0.7176
Epoch 12/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.5692 - accuracy: 0.7150
Epoch 12: val_loss improved from 0.56809 to 0.56752, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5692 - accuracy: 0.7150 - val_loss: 0.5675 - val_accuracy: 0.7187
Epoch 13/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.5692 - accuracy: 0.7151
Epoch 13: val_loss improved from 0.56752 to 0.56676, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5685 - accuracy: 0.7157 - val_loss: 0.5668 - val_accuracy: 0.7211
Epoch 14/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.5668 - accuracy: 0.7153
Epoch 14: val_loss improved from 0.56676 to 0.56637, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5680 - accuracy: 0.7130 - val_loss: 0.5664 - val_accuracy: 0.7228
Epoch 15/60
654/682 [===========================&gt;..] - ETA: 0s - loss: 0.5691 - accuracy: 0.7131
Epoch 15: val_loss improved from 0.56637 to 0.56605, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5676 - accuracy: 0.7147 - val_loss: 0.5661 - val_accuracy: 0.7205
Epoch 16/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.5682 - accuracy: 0.7145
Epoch 16: val_loss improved from 0.56605 to 0.56594, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5670 - accuracy: 0.7153 - val_loss: 0.5659 - val_accuracy: 0.7240
Epoch 17/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.5668 - accuracy: 0.7183
Epoch 17: val_loss improved from 0.56594 to 0.56544, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5667 - accuracy: 0.7172 - val_loss: 0.5654 - val_accuracy: 0.7223
Epoch 18/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.5663 - accuracy: 0.7177
Epoch 18: val_loss improved from 0.56544 to 0.56502, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5664 - accuracy: 0.7175 - val_loss: 0.5650 - val_accuracy: 0.7217
Epoch 19/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.5659 - accuracy: 0.7149
Epoch 19: val_loss did not improve from 0.56502
682/682 [==============================] - 2s 2ms/step - loss: 0.5660 - accuracy: 0.7147 - val_loss: 0.5653 - val_accuracy: 0.7246
Epoch 20/60
653/682 [===========================&gt;..] - ETA: 0s - loss: 0.5647 - accuracy: 0.7168
Epoch 20: val_loss improved from 0.56502 to 0.56456, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5658 - accuracy: 0.7161 - val_loss: 0.5646 - val_accuracy: 0.7228
Epoch 21/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.5650 - accuracy: 0.7184
Epoch 21: val_loss did not improve from 0.56456
682/682 [==============================] - 2s 2ms/step - loss: 0.5654 - accuracy: 0.7176 - val_loss: 0.5647 - val_accuracy: 0.7193
Epoch 22/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.5654 - accuracy: 0.7153
Epoch 22: val_loss did not improve from 0.56456
682/682 [==============================] - 2s 2ms/step - loss: 0.5650 - accuracy: 0.7155 - val_loss: 0.5649 - val_accuracy: 0.7228
Epoch 23/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.5651 - accuracy: 0.7175
Epoch 23: val_loss improved from 0.56456 to 0.56397, saving model to best_drop.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.5648 - accuracy: 0.7180 - val_loss: 0.5640 - val_accuracy: 0.7234
Epoch 24/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.5642 - accuracy: 0.7150
Epoch 24: val_loss improved from 0.56397 to 0.56382, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5646 - accuracy: 0.7151 - val_loss: 0.5638 - val_accuracy: 0.7217
Epoch 25/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.5634 - accuracy: 0.7171
Epoch 25: val_loss improved from 0.56382 to 0.56359, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5645 - accuracy: 0.7167 - val_loss: 0.5636 - val_accuracy: 0.7228
Epoch 26/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.5630 - accuracy: 0.7195
Epoch 26: val_loss improved from 0.56359 to 0.56341, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5642 - accuracy: 0.7186 - val_loss: 0.5634 - val_accuracy: 0.7246
Epoch 27/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.5638 - accuracy: 0.7169
Epoch 27: val_loss improved from 0.56341 to 0.56330, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5640 - accuracy: 0.7163 - val_loss: 0.5633 - val_accuracy: 0.7217
Epoch 28/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.5638 - accuracy: 0.7190
Epoch 28: val_loss did not improve from 0.56330
682/682 [==============================] - 2s 2ms/step - loss: 0.5638 - accuracy: 0.7186 - val_loss: 0.5634 - val_accuracy: 0.7234
Epoch 29/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.5638 - accuracy: 0.7187
Epoch 29: val_loss improved from 0.56330 to 0.56305, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5637 - accuracy: 0.7188 - val_loss: 0.5631 - val_accuracy: 0.7217
Epoch 30/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.5634 - accuracy: 0.7185
Epoch 30: val_loss improved from 0.56305 to 0.56290, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5635 - accuracy: 0.7176 - val_loss: 0.5629 - val_accuracy: 0.7223
Epoch 31/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.5636 - accuracy: 0.7195
Epoch 31: val_loss did not improve from 0.56290
682/682 [==============================] - 2s 2ms/step - loss: 0.5633 - accuracy: 0.7198 - val_loss: 0.5631 - val_accuracy: 0.7234
Epoch 32/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.5636 - accuracy: 0.7183
Epoch 32: val_loss improved from 0.56290 to 0.56278, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5634 - accuracy: 0.7185 - val_loss: 0.5628 - val_accuracy: 0.7228
Epoch 33/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.5630 - accuracy: 0.7195
Epoch 33: val_loss improved from 0.56278 to 0.56266, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5631 - accuracy: 0.7191 - val_loss: 0.5627 - val_accuracy: 0.7217
Epoch 34/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.5629 - accuracy: 0.7192
Epoch 34: val_loss did not improve from 0.56266
682/682 [==============================] - 2s 2ms/step - loss: 0.5629 - accuracy: 0.7192 - val_loss: 0.5627 - val_accuracy: 0.7217
Epoch 35/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.5639 - accuracy: 0.7206
Epoch 35: val_loss did not improve from 0.56266
682/682 [==============================] - 2s 2ms/step - loss: 0.5628 - accuracy: 0.7214 - val_loss: 0.5627 - val_accuracy: 0.7240
Epoch 36/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.5631 - accuracy: 0.7183
Epoch 36: val_loss improved from 0.56266 to 0.56265, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5627 - accuracy: 0.7186 - val_loss: 0.5627 - val_accuracy: 0.7240
Epoch 37/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.5620 - accuracy: 0.7187
Epoch 37: val_loss improved from 0.56265 to 0.56236, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5626 - accuracy: 0.7183 - val_loss: 0.5624 - val_accuracy: 0.7228
Epoch 38/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.5623 - accuracy: 0.7196
Epoch 38: val_loss improved from 0.56236 to 0.56216, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5627 - accuracy: 0.7191 - val_loss: 0.5622 - val_accuracy: 0.7199
Epoch 39/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.5621 - accuracy: 0.7199
Epoch 39: val_loss did not improve from 0.56216
682/682 [==============================] - 2s 2ms/step - loss: 0.5623 - accuracy: 0.7199 - val_loss: 0.5625 - val_accuracy: 0.7217
Epoch 40/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.5621 - accuracy: 0.7217
Epoch 40: val_loss did not improve from 0.56216
682/682 [==============================] - 2s 2ms/step - loss: 0.5622 - accuracy: 0.7217 - val_loss: 0.5624 - val_accuracy: 0.7193
Epoch 41/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.5610 - accuracy: 0.7209
Epoch 41: val_loss improved from 0.56216 to 0.56215, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5624 - accuracy: 0.7194 - val_loss: 0.5622 - val_accuracy: 0.7211
Epoch 42/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.5626 - accuracy: 0.7197
Epoch 42: val_loss did not improve from 0.56215
682/682 [==============================] - 2s 2ms/step - loss: 0.5623 - accuracy: 0.7199 - val_loss: 0.5624 - val_accuracy: 0.7187
Epoch 43/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.5616 - accuracy: 0.7206
Epoch 43: val_loss improved from 0.56215 to 0.56177, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5621 - accuracy: 0.7202 - val_loss: 0.5618 - val_accuracy: 0.7205
Epoch 44/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.5618 - accuracy: 0.7204
Epoch 44: val_loss improved from 0.56177 to 0.56172, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5621 - accuracy: 0.7204 - val_loss: 0.5617 - val_accuracy: 0.7205
Epoch 45/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.5617 - accuracy: 0.7204
Epoch 45: val_loss did not improve from 0.56172
682/682 [==============================] - 2s 2ms/step - loss: 0.5619 - accuracy: 0.7202 - val_loss: 0.5623 - val_accuracy: 0.7211
Epoch 46/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.5627 - accuracy: 0.7192
Epoch 46: val_loss improved from 0.56172 to 0.56168, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5621 - accuracy: 0.7204 - val_loss: 0.5617 - val_accuracy: 0.7205
Epoch 47/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.5619 - accuracy: 0.7208
Epoch 47: val_loss did not improve from 0.56168
682/682 [==============================] - 2s 2ms/step - loss: 0.5619 - accuracy: 0.7207 - val_loss: 0.5620 - val_accuracy: 0.7217
Epoch 48/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.5628 - accuracy: 0.7200
Epoch 48: val_loss did not improve from 0.56168
682/682 [==============================] - 2s 2ms/step - loss: 0.5619 - accuracy: 0.7202 - val_loss: 0.5617 - val_accuracy: 0.7223
Epoch 49/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.5619 - accuracy: 0.7209
Epoch 49: val_loss improved from 0.56168 to 0.56165, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5619 - accuracy: 0.7205 - val_loss: 0.5616 - val_accuracy: 0.7211
Epoch 50/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.5603 - accuracy: 0.7216
Epoch 50: val_loss improved from 0.56165 to 0.56150, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5618 - accuracy: 0.7205 - val_loss: 0.5615 - val_accuracy: 0.7217
Epoch 51/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.5622 - accuracy: 0.7223
Epoch 51: val_loss did not improve from 0.56150
682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7220 - val_loss: 0.5616 - val_accuracy: 0.7211
Epoch 52/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.5623 - accuracy: 0.7224
Epoch 52: val_loss did not improve from 0.56150
682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7229 - val_loss: 0.5616 - val_accuracy: 0.7211
Epoch 53/60
651/682 [===========================&gt;..] - ETA: 0s - loss: 0.5611 - accuracy: 0.7203
Epoch 53: val_loss improved from 0.56150 to 0.56140, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7208 - val_loss: 0.5614 - val_accuracy: 0.7211
Epoch 54/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.5620 - accuracy: 0.7216
Epoch 54: val_loss did not improve from 0.56140
682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7211 - val_loss: 0.5616 - val_accuracy: 0.7223
Epoch 55/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.5619 - accuracy: 0.7199
Epoch 55: val_loss did not improve from 0.56140
682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7204 - val_loss: 0.5615 - val_accuracy: 0.7211
Epoch 56/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.5616 - accuracy: 0.7198
Epoch 56: val_loss did not improve from 0.56140
682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7208 - val_loss: 0.5620 - val_accuracy: 0.7187
Epoch 57/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.5623 - accuracy: 0.7206
Epoch 57: val_loss improved from 0.56140 to 0.56135, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7214 - val_loss: 0.5614 - val_accuracy: 0.7211
Epoch 58/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.5629 - accuracy: 0.7206
Epoch 58: val_loss did not improve from 0.56135
682/682 [==============================] - 2s 2ms/step - loss: 0.5616 - accuracy: 0.7217 - val_loss: 0.5615 - val_accuracy: 0.7205
Epoch 59/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.5617 - accuracy: 0.7212
Epoch 59: val_loss did not improve from 0.56135
682/682 [==============================] - 2s 2ms/step - loss: 0.5615 - accuracy: 0.7210 - val_loss: 0.5615 - val_accuracy: 0.7223
Epoch 60/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.5609 - accuracy: 0.7234
Epoch 60: val_loss did not improve from 0.56135
682/682 [==============================] - 2s 2ms/step - loss: 0.5615 - accuracy: 0.7226 - val_loss: 0.5614 - val_accuracy: 0.7217
54/54 [==============================] - 0s 2ms/step - loss: 0.5614 - accuracy: 0.7211
Drop Feature loan
data frame x has shape: (8516, 20)
['education', 'housing', 'campaign', 'pdays', 'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'divorced', 'married', 'single', 'admin.', 'blue-collar', 'entrepreneur', 'retired', 'self-employed', 'services', 'student']
Epoch 1/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.7084 - accuracy: 0.5214
Epoch 1: val_loss improved from inf to 0.65460, saving model to best_drop.hdf5
682/682 [==============================] - 2s 3ms/step - loss: 0.7081 - accuracy: 0.5219 - val_loss: 0.6546 - val_accuracy: 0.6207
Epoch 2/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.6408 - accuracy: 0.6427
Epoch 2: val_loss improved from 0.65460 to 0.61211, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6407 - accuracy: 0.6429 - val_loss: 0.6121 - val_accuracy: 0.6970
Epoch 3/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.6092 - accuracy: 0.6894
Epoch 3: val_loss improved from 0.61211 to 0.59192, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6091 - accuracy: 0.6896 - val_loss: 0.5919 - val_accuracy: 0.7111
Epoch 4/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.5933 - accuracy: 0.7029
Epoch 4: val_loss improved from 0.59192 to 0.58218, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5933 - accuracy: 0.7029 - val_loss: 0.5822 - val_accuracy: 0.7199
Epoch 5/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.5842 - accuracy: 0.7090
Epoch 5: val_loss improved from 0.58218 to 0.57728, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5850 - accuracy: 0.7081 - val_loss: 0.5773 - val_accuracy: 0.7181
Epoch 6/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.5801 - accuracy: 0.7098
Epoch 6: val_loss improved from 0.57728 to 0.57494, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5802 - accuracy: 0.7098 - val_loss: 0.5749 - val_accuracy: 0.7146
Epoch 7/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.5779 - accuracy: 0.7107
Epoch 7: val_loss improved from 0.57494 to 0.57261, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5774 - accuracy: 0.7107 - val_loss: 0.5726 - val_accuracy: 0.7193
Epoch 8/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.5751 - accuracy: 0.7097
Epoch 8: val_loss improved from 0.57261 to 0.57142, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5753 - accuracy: 0.7097 - val_loss: 0.5714 - val_accuracy: 0.7199
Epoch 9/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.5745 - accuracy: 0.7128
Epoch 9: val_loss improved from 0.57142 to 0.57054, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5740 - accuracy: 0.7123 - val_loss: 0.5705 - val_accuracy: 0.7223
Epoch 10/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.5729 - accuracy: 0.7112
Epoch 10: val_loss improved from 0.57054 to 0.56970, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5728 - accuracy: 0.7104 - val_loss: 0.5697 - val_accuracy: 0.7205
Epoch 11/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.5713 - accuracy: 0.7122
Epoch 11: val_loss improved from 0.56970 to 0.56915, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5717 - accuracy: 0.7119 - val_loss: 0.5692 - val_accuracy: 0.7228
Epoch 12/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.5703 - accuracy: 0.7123
Epoch 12: val_loss improved from 0.56915 to 0.56880, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5710 - accuracy: 0.7114 - val_loss: 0.5688 - val_accuracy: 0.7152
Epoch 13/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.5711 - accuracy: 0.7113
Epoch 13: val_loss improved from 0.56880 to 0.56806, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5704 - accuracy: 0.7114 - val_loss: 0.5681 - val_accuracy: 0.7193
Epoch 14/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.5699 - accuracy: 0.7120
Epoch 14: val_loss improved from 0.56806 to 0.56760, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5697 - accuracy: 0.7122 - val_loss: 0.5676 - val_accuracy: 0.7158
Epoch 15/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.5683 - accuracy: 0.7123
Epoch 15: val_loss improved from 0.56760 to 0.56727, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5691 - accuracy: 0.7116 - val_loss: 0.5673 - val_accuracy: 0.7158
Epoch 16/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.5678 - accuracy: 0.7126
Epoch 16: val_loss improved from 0.56727 to 0.56678, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5684 - accuracy: 0.7117 - val_loss: 0.5668 - val_accuracy: 0.7176
Epoch 17/60
652/682 [===========================&gt;..] - ETA: 0s - loss: 0.5683 - accuracy: 0.7144
Epoch 17: val_loss did not improve from 0.56678
682/682 [==============================] - 2s 2ms/step - loss: 0.5678 - accuracy: 0.7136 - val_loss: 0.5669 - val_accuracy: 0.7152
Epoch 18/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.5665 - accuracy: 0.7138
Epoch 18: val_loss improved from 0.56678 to 0.56604, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5677 - accuracy: 0.7120 - val_loss: 0.5660 - val_accuracy: 0.7211
Epoch 19/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.5680 - accuracy: 0.7117
Epoch 19: val_loss improved from 0.56604 to 0.56568, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5672 - accuracy: 0.7123 - val_loss: 0.5657 - val_accuracy: 0.7199
Epoch 20/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.5662 - accuracy: 0.7127
Epoch 20: val_loss improved from 0.56568 to 0.56548, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5668 - accuracy: 0.7128 - val_loss: 0.5655 - val_accuracy: 0.7228
Epoch 21/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.5655 - accuracy: 0.7148
Epoch 21: val_loss improved from 0.56548 to 0.56533, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5663 - accuracy: 0.7138 - val_loss: 0.5653 - val_accuracy: 0.7170
Epoch 22/60
653/682 [===========================&gt;..] - ETA: 0s - loss: 0.5683 - accuracy: 0.7106
Epoch 22: val_loss improved from 0.56533 to 0.56490, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5660 - accuracy: 0.7126 - val_loss: 0.5649 - val_accuracy: 0.7217
Epoch 23/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.5663 - accuracy: 0.7137
Epoch 23: val_loss did not improve from 0.56490
682/682 [==============================] - 2s 2ms/step - loss: 0.5656 - accuracy: 0.7144 - val_loss: 0.5652 - val_accuracy: 0.7211
Epoch 24/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.5654 - accuracy: 0.7158
Epoch 24: val_loss improved from 0.56490 to 0.56446, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5656 - accuracy: 0.7155 - val_loss: 0.5645 - val_accuracy: 0.7199
Epoch 25/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.5652 - accuracy: 0.7134
Epoch 25: val_loss improved from 0.56446 to 0.56418, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5652 - accuracy: 0.7133 - val_loss: 0.5642 - val_accuracy: 0.7199
Epoch 26/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.5651 - accuracy: 0.7160
Epoch 26: val_loss improved from 0.56418 to 0.56412, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5648 - accuracy: 0.7154 - val_loss: 0.5641 - val_accuracy: 0.7228
Epoch 27/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.5652 - accuracy: 0.7164
Epoch 27: val_loss improved from 0.56412 to 0.56377, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5648 - accuracy: 0.7164 - val_loss: 0.5638 - val_accuracy: 0.7193
Epoch 28/60
654/682 [===========================&gt;..] - ETA: 0s - loss: 0.5663 - accuracy: 0.7138
Epoch 28: val_loss improved from 0.56377 to 0.56370, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5644 - accuracy: 0.7153 - val_loss: 0.5637 - val_accuracy: 0.7234
Epoch 29/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.5640 - accuracy: 0.7186
Epoch 29: val_loss improved from 0.56370 to 0.56343, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5641 - accuracy: 0.7182 - val_loss: 0.5634 - val_accuracy: 0.7181
Epoch 30/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.5643 - accuracy: 0.7160
Epoch 30: val_loss improved from 0.56343 to 0.56325, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5641 - accuracy: 0.7161 - val_loss: 0.5632 - val_accuracy: 0.7199
Epoch 31/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.5636 - accuracy: 0.7179
Epoch 31: val_loss did not improve from 0.56325
682/682 [==============================] - 2s 2ms/step - loss: 0.5637 - accuracy: 0.7180 - val_loss: 0.5636 - val_accuracy: 0.7205
Epoch 32/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.5642 - accuracy: 0.7168
Epoch 32: val_loss improved from 0.56325 to 0.56308, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5639 - accuracy: 0.7170 - val_loss: 0.5631 - val_accuracy: 0.7199
Epoch 33/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.5621 - accuracy: 0.7191
Epoch 33: val_loss did not improve from 0.56308
682/682 [==============================] - 2s 2ms/step - loss: 0.5635 - accuracy: 0.7179 - val_loss: 0.5633 - val_accuracy: 0.7217
Epoch 34/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.5632 - accuracy: 0.7182
Epoch 34: val_loss improved from 0.56308 to 0.56288, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5635 - accuracy: 0.7175 - val_loss: 0.5629 - val_accuracy: 0.7205
Epoch 35/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.5632 - accuracy: 0.7191
Epoch 35: val_loss improved from 0.56288 to 0.56274, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5633 - accuracy: 0.7186 - val_loss: 0.5627 - val_accuracy: 0.7205
Epoch 36/60
653/682 [===========================&gt;..] - ETA: 0s - loss: 0.5645 - accuracy: 0.7159
Epoch 36: val_loss improved from 0.56274 to 0.56267, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5632 - accuracy: 0.7175 - val_loss: 0.5627 - val_accuracy: 0.7199
Epoch 37/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.5633 - accuracy: 0.7175
Epoch 37: val_loss improved from 0.56267 to 0.56259, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5630 - accuracy: 0.7179 - val_loss: 0.5626 - val_accuracy: 0.7223
Epoch 38/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.5620 - accuracy: 0.7210
Epoch 38: val_loss improved from 0.56259 to 0.56244, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5629 - accuracy: 0.7204 - val_loss: 0.5624 - val_accuracy: 0.7223
Epoch 39/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.5623 - accuracy: 0.7196
Epoch 39: val_loss improved from 0.56244 to 0.56208, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5627 - accuracy: 0.7189 - val_loss: 0.5621 - val_accuracy: 0.7217
Epoch 40/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.5619 - accuracy: 0.7186
Epoch 40: val_loss did not improve from 0.56208
682/682 [==============================] - 2s 2ms/step - loss: 0.5628 - accuracy: 0.7185 - val_loss: 0.5624 - val_accuracy: 0.7187
Epoch 41/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.5625 - accuracy: 0.7191
Epoch 41: val_loss did not improve from 0.56208
682/682 [==============================] - 2s 2ms/step - loss: 0.5626 - accuracy: 0.7191 - val_loss: 0.5621 - val_accuracy: 0.7217
Epoch 42/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.5624 - accuracy: 0.7190
Epoch 42: val_loss did not improve from 0.56208
682/682 [==============================] - 2s 2ms/step - loss: 0.5624 - accuracy: 0.7186 - val_loss: 0.5621 - val_accuracy: 0.7205
Epoch 43/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.5632 - accuracy: 0.7185
Epoch 43: val_loss did not improve from 0.56208
682/682 [==============================] - 2s 2ms/step - loss: 0.5625 - accuracy: 0.7189 - val_loss: 0.5622 - val_accuracy: 0.7205
Epoch 44/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.5627 - accuracy: 0.7192
Epoch 44: val_loss improved from 0.56208 to 0.56189, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5623 - accuracy: 0.7199 - val_loss: 0.5619 - val_accuracy: 0.7211
Epoch 45/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.5619 - accuracy: 0.7213
Epoch 45: val_loss did not improve from 0.56189
682/682 [==============================] - 2s 2ms/step - loss: 0.5621 - accuracy: 0.7205 - val_loss: 0.5619 - val_accuracy: 0.7199
Epoch 46/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.5628 - accuracy: 0.7186
Epoch 46: val_loss improved from 0.56189 to 0.56178, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5623 - accuracy: 0.7189 - val_loss: 0.5618 - val_accuracy: 0.7211
Epoch 47/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.5622 - accuracy: 0.7199
Epoch 47: val_loss improved from 0.56178 to 0.56169, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5621 - accuracy: 0.7197 - val_loss: 0.5617 - val_accuracy: 0.7228
Epoch 48/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.5617 - accuracy: 0.7193
Epoch 48: val_loss did not improve from 0.56169
682/682 [==============================] - 2s 2ms/step - loss: 0.5622 - accuracy: 0.7192 - val_loss: 0.5618 - val_accuracy: 0.7193
Epoch 49/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.5625 - accuracy: 0.7195
Epoch 49: val_loss did not improve from 0.56169
682/682 [==============================] - 2s 2ms/step - loss: 0.5620 - accuracy: 0.7191 - val_loss: 0.5618 - val_accuracy: 0.7205
Epoch 50/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.5617 - accuracy: 0.7202
Epoch 50: val_loss did not improve from 0.56169
682/682 [==============================] - 2s 2ms/step - loss: 0.5619 - accuracy: 0.7205 - val_loss: 0.5620 - val_accuracy: 0.7223
Epoch 51/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.5627 - accuracy: 0.7200
Epoch 51: val_loss improved from 0.56169 to 0.56156, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5621 - accuracy: 0.7199 - val_loss: 0.5616 - val_accuracy: 0.7205
Epoch 52/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.5620 - accuracy: 0.7189
Epoch 52: val_loss did not improve from 0.56156
682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7197 - val_loss: 0.5623 - val_accuracy: 0.7211
Epoch 53/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.5611 - accuracy: 0.7222
Epoch 53: val_loss improved from 0.56156 to 0.56155, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5619 - accuracy: 0.7214 - val_loss: 0.5615 - val_accuracy: 0.7205
Epoch 54/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.5609 - accuracy: 0.7204
Epoch 54: val_loss did not improve from 0.56155
682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7197 - val_loss: 0.5617 - val_accuracy: 0.7217
Epoch 55/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.5619 - accuracy: 0.7193
Epoch 55: val_loss did not improve from 0.56155
682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7199 - val_loss: 0.5616 - val_accuracy: 0.7205
Epoch 56/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.5619 - accuracy: 0.7198
Epoch 56: val_loss improved from 0.56155 to 0.56150, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5618 - accuracy: 0.7199 - val_loss: 0.5615 - val_accuracy: 0.7205
Epoch 57/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.5612 - accuracy: 0.7214
Epoch 57: val_loss improved from 0.56150 to 0.56141, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5616 - accuracy: 0.7208 - val_loss: 0.5614 - val_accuracy: 0.7193
Epoch 58/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.5618 - accuracy: 0.7201
Epoch 58: val_loss did not improve from 0.56141
682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7202 - val_loss: 0.5615 - val_accuracy: 0.7187
Epoch 59/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.5616 - accuracy: 0.7217
Epoch 59: val_loss did not improve from 0.56141
682/682 [==============================] - 2s 2ms/step - loss: 0.5616 - accuracy: 0.7217 - val_loss: 0.5617 - val_accuracy: 0.7193
Epoch 60/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.5611 - accuracy: 0.7210
Epoch 60: val_loss did not improve from 0.56141
682/682 [==============================] - 2s 2ms/step - loss: 0.5616 - accuracy: 0.7207 - val_loss: 0.5617 - val_accuracy: 0.7187
54/54 [==============================] - 0s 2ms/step - loss: 0.5614 - accuracy: 0.7193
Drop Feature entrepreneur
data frame x has shape: (8516, 19)
['education', 'housing', 'campaign', 'pdays', 'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'divorced', 'married', 'single', 'admin.', 'blue-collar', 'retired', 'self-employed', 'services', 'student']
Epoch 1/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.7075 - accuracy: 0.5178
Epoch 1: val_loss improved from inf to 0.65347, saving model to best_drop.hdf5
682/682 [==============================] - 2s 3ms/step - loss: 0.7056 - accuracy: 0.5214 - val_loss: 0.6535 - val_accuracy: 0.6301
Epoch 2/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.6346 - accuracy: 0.6581
Epoch 2: val_loss improved from 0.65347 to 0.61037, saving model to best_drop.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6346 - accuracy: 0.6582 - val_loss: 0.6104 - val_accuracy: 0.7129
Epoch 3/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.6054 - accuracy: 0.6961
Epoch 3: val_loss improved from 0.61037 to 0.59064, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6046 - accuracy: 0.6968 - val_loss: 0.5906 - val_accuracy: 0.7223
Epoch 4/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.5904 - accuracy: 0.7072
Epoch 4: val_loss improved from 0.59064 to 0.58080, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5902 - accuracy: 0.7079 - val_loss: 0.5808 - val_accuracy: 0.7240
Epoch 5/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.5820 - accuracy: 0.7105
Epoch 5: val_loss improved from 0.58080 to 0.57592, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5825 - accuracy: 0.7104 - val_loss: 0.5759 - val_accuracy: 0.7252
Epoch 6/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.5791 - accuracy: 0.7080
Epoch 6: val_loss improved from 0.57592 to 0.57298, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5782 - accuracy: 0.7100 - val_loss: 0.5730 - val_accuracy: 0.7264
Epoch 7/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.5756 - accuracy: 0.7122
Epoch 7: val_loss improved from 0.57298 to 0.57192, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5752 - accuracy: 0.7126 - val_loss: 0.5719 - val_accuracy: 0.7205
Epoch 8/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.5737 - accuracy: 0.7118
Epoch 8: val_loss improved from 0.57192 to 0.57011, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5737 - accuracy: 0.7119 - val_loss: 0.5701 - val_accuracy: 0.7228
Epoch 9/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.5740 - accuracy: 0.7107
Epoch 9: val_loss improved from 0.57011 to 0.56920, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5723 - accuracy: 0.7123 - val_loss: 0.5692 - val_accuracy: 0.7223
Epoch 10/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.5715 - accuracy: 0.7129
Epoch 10: val_loss improved from 0.56920 to 0.56861, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5712 - accuracy: 0.7129 - val_loss: 0.5686 - val_accuracy: 0.7217
Epoch 11/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.5695 - accuracy: 0.7140
Epoch 11: val_loss improved from 0.56861 to 0.56844, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5703 - accuracy: 0.7129 - val_loss: 0.5684 - val_accuracy: 0.7252
Epoch 12/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.5691 - accuracy: 0.7145
Epoch 12: val_loss improved from 0.56844 to 0.56762, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5696 - accuracy: 0.7138 - val_loss: 0.5676 - val_accuracy: 0.7181
Epoch 13/60
653/682 [===========================&gt;..] - ETA: 0s - loss: 0.5677 - accuracy: 0.7156
Epoch 13: val_loss improved from 0.56762 to 0.56710, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5691 - accuracy: 0.7145 - val_loss: 0.5671 - val_accuracy: 0.7211
Epoch 14/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.5692 - accuracy: 0.7123
Epoch 14: val_loss improved from 0.56710 to 0.56683, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5684 - accuracy: 0.7128 - val_loss: 0.5668 - val_accuracy: 0.7234
Epoch 15/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.5677 - accuracy: 0.7145
Epoch 15: val_loss improved from 0.56683 to 0.56638, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5679 - accuracy: 0.7141 - val_loss: 0.5664 - val_accuracy: 0.7187
Epoch 16/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.5672 - accuracy: 0.7129
Epoch 16: val_loss improved from 0.56638 to 0.56611, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5674 - accuracy: 0.7132 - val_loss: 0.5661 - val_accuracy: 0.7187
Epoch 17/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.5675 - accuracy: 0.7132
Epoch 17: val_loss improved from 0.56611 to 0.56592, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5670 - accuracy: 0.7135 - val_loss: 0.5659 - val_accuracy: 0.7176
Epoch 18/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.5656 - accuracy: 0.7133
Epoch 18: val_loss improved from 0.56592 to 0.56542, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5666 - accuracy: 0.7126 - val_loss: 0.5654 - val_accuracy: 0.7181
Epoch 19/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.5665 - accuracy: 0.7127
Epoch 19: val_loss improved from 0.56542 to 0.56505, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5663 - accuracy: 0.7129 - val_loss: 0.5650 - val_accuracy: 0.7211
Epoch 20/60
653/682 [===========================&gt;..] - ETA: 0s - loss: 0.5651 - accuracy: 0.7139
Epoch 20: val_loss improved from 0.56505 to 0.56497, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5659 - accuracy: 0.7133 - val_loss: 0.5650 - val_accuracy: 0.7205
Epoch 21/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.5653 - accuracy: 0.7151
Epoch 21: val_loss did not improve from 0.56497
682/682 [==============================] - 2s 2ms/step - loss: 0.5653 - accuracy: 0.7154 - val_loss: 0.5650 - val_accuracy: 0.7240
Epoch 22/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.5652 - accuracy: 0.7124
Epoch 22: val_loss improved from 0.56497 to 0.56466, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5653 - accuracy: 0.7125 - val_loss: 0.5647 - val_accuracy: 0.7228
Epoch 23/60
682/682 [==============================] - ETA: 0s - loss: 0.5650 - accuracy: 0.7147
Epoch 23: val_loss improved from 0.56466 to 0.56426, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5650 - accuracy: 0.7147 - val_loss: 0.5643 - val_accuracy: 0.7193
Epoch 24/60
654/682 [===========================&gt;..] - ETA: 0s - loss: 0.5638 - accuracy: 0.7148
Epoch 24: val_loss improved from 0.56426 to 0.56422, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5648 - accuracy: 0.7136 - val_loss: 0.5642 - val_accuracy: 0.7211
Epoch 25/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.5647 - accuracy: 0.7155
Epoch 25: val_loss improved from 0.56422 to 0.56378, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5646 - accuracy: 0.7154 - val_loss: 0.5638 - val_accuracy: 0.7193
Epoch 26/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.5634 - accuracy: 0.7167
Epoch 26: val_loss improved from 0.56378 to 0.56360, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5645 - accuracy: 0.7161 - val_loss: 0.5636 - val_accuracy: 0.7217
Epoch 27/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.5639 - accuracy: 0.7188
Epoch 27: val_loss did not improve from 0.56360
682/682 [==============================] - 2s 2ms/step - loss: 0.5640 - accuracy: 0.7186 - val_loss: 0.5640 - val_accuracy: 0.7187
Epoch 28/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.5648 - accuracy: 0.7156
Epoch 28: val_loss improved from 0.56360 to 0.56332, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5643 - accuracy: 0.7163 - val_loss: 0.5633 - val_accuracy: 0.7205
Epoch 29/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.5638 - accuracy: 0.7165
Epoch 29: val_loss improved from 0.56332 to 0.56304, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5639 - accuracy: 0.7163 - val_loss: 0.5630 - val_accuracy: 0.7205
Epoch 30/60
652/682 [===========================&gt;..] - ETA: 0s - loss: 0.5625 - accuracy: 0.7170
Epoch 30: val_loss did not improve from 0.56304
682/682 [==============================] - 2s 2ms/step - loss: 0.5631 - accuracy: 0.7175 - val_loss: 0.5642 - val_accuracy: 0.7258
Epoch 31/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.5646 - accuracy: 0.7190
Epoch 31: val_loss improved from 0.56304 to 0.56275, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5639 - accuracy: 0.7197 - val_loss: 0.5627 - val_accuracy: 0.7211
Epoch 32/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.5637 - accuracy: 0.7184
Epoch 32: val_loss improved from 0.56275 to 0.56265, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5634 - accuracy: 0.7191 - val_loss: 0.5626 - val_accuracy: 0.7217
Epoch 33/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.5633 - accuracy: 0.7181
Epoch 33: val_loss improved from 0.56265 to 0.56263, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5633 - accuracy: 0.7180 - val_loss: 0.5626 - val_accuracy: 0.7211
Epoch 34/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.5630 - accuracy: 0.7195
Epoch 34: val_loss did not improve from 0.56263
682/682 [==============================] - 2s 2ms/step - loss: 0.5630 - accuracy: 0.7194 - val_loss: 0.5628 - val_accuracy: 0.7223
Epoch 35/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.5645 - accuracy: 0.7182
Epoch 35: val_loss improved from 0.56263 to 0.56232, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5631 - accuracy: 0.7189 - val_loss: 0.5623 - val_accuracy: 0.7217
Epoch 36/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.5630 - accuracy: 0.7192
Epoch 36: val_loss improved from 0.56232 to 0.56225, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5630 - accuracy: 0.7197 - val_loss: 0.5623 - val_accuracy: 0.7223
Epoch 37/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.5633 - accuracy: 0.7190
Epoch 37: val_loss did not improve from 0.56225
682/682 [==============================] - 2s 2ms/step - loss: 0.5627 - accuracy: 0.7198 - val_loss: 0.5630 - val_accuracy: 0.7193
Epoch 38/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.5626 - accuracy: 0.7165
Epoch 38: val_loss did not improve from 0.56225
682/682 [==============================] - 2s 2ms/step - loss: 0.5626 - accuracy: 0.7172 - val_loss: 0.5626 - val_accuracy: 0.7223
Epoch 39/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.5615 - accuracy: 0.7200
Epoch 39: val_loss did not improve from 0.56225
682/682 [==============================] - 2s 2ms/step - loss: 0.5625 - accuracy: 0.7185 - val_loss: 0.5627 - val_accuracy: 0.7234
Epoch 40/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.5615 - accuracy: 0.7205
Epoch 40: val_loss improved from 0.56225 to 0.56215, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5626 - accuracy: 0.7198 - val_loss: 0.5621 - val_accuracy: 0.7223
Epoch 41/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.5620 - accuracy: 0.7204
Epoch 41: val_loss did not improve from 0.56215
682/682 [==============================] - 2s 2ms/step - loss: 0.5625 - accuracy: 0.7195 - val_loss: 0.5622 - val_accuracy: 0.7199
Epoch 42/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.5631 - accuracy: 0.7174
Epoch 42: val_loss improved from 0.56215 to 0.56194, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5624 - accuracy: 0.7182 - val_loss: 0.5619 - val_accuracy: 0.7223
Epoch 43/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.5618 - accuracy: 0.7197
Epoch 43: val_loss improved from 0.56194 to 0.56190, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5623 - accuracy: 0.7195 - val_loss: 0.5619 - val_accuracy: 0.7205
Epoch 44/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.5616 - accuracy: 0.7196
Epoch 44: val_loss improved from 0.56190 to 0.56188, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5620 - accuracy: 0.7191 - val_loss: 0.5619 - val_accuracy: 0.7211
Epoch 45/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.5617 - accuracy: 0.7190
Epoch 45: val_loss did not improve from 0.56188
682/682 [==============================] - 2s 2ms/step - loss: 0.5621 - accuracy: 0.7186 - val_loss: 0.5620 - val_accuracy: 0.7187
Epoch 46/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.5635 - accuracy: 0.7194
Epoch 46: val_loss improved from 0.56188 to 0.56179, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5620 - accuracy: 0.7205 - val_loss: 0.5618 - val_accuracy: 0.7199
Epoch 47/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.5622 - accuracy: 0.7191
Epoch 47: val_loss improved from 0.56179 to 0.56173, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5621 - accuracy: 0.7195 - val_loss: 0.5617 - val_accuracy: 0.7217
Epoch 48/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.5623 - accuracy: 0.7187
Epoch 48: val_loss improved from 0.56173 to 0.56161, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5620 - accuracy: 0.7192 - val_loss: 0.5616 - val_accuracy: 0.7223
Epoch 49/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.5614 - accuracy: 0.7209
Epoch 49: val_loss improved from 0.56161 to 0.56159, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5619 - accuracy: 0.7201 - val_loss: 0.5616 - val_accuracy: 0.7211
Epoch 50/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.5642 - accuracy: 0.7179
Epoch 50: val_loss improved from 0.56159 to 0.56155, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5619 - accuracy: 0.7199 - val_loss: 0.5615 - val_accuracy: 0.7234
Epoch 51/60
648/682 [===========================&gt;..] - ETA: 0s - loss: 0.5609 - accuracy: 0.7196
Epoch 51: val_loss did not improve from 0.56155
682/682 [==============================] - 2s 2ms/step - loss: 0.5619 - accuracy: 0.7198 - val_loss: 0.5616 - val_accuracy: 0.7211
Epoch 52/60
652/682 [===========================&gt;..] - ETA: 0s - loss: 0.5627 - accuracy: 0.7198
Epoch 52: val_loss did not improve from 0.56155
682/682 [==============================] - 2s 2ms/step - loss: 0.5618 - accuracy: 0.7204 - val_loss: 0.5617 - val_accuracy: 0.7187
Epoch 53/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.5623 - accuracy: 0.7210
Epoch 53: val_loss improved from 0.56155 to 0.56146, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5616 - accuracy: 0.7216 - val_loss: 0.5615 - val_accuracy: 0.7205
Epoch 54/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.5625 - accuracy: 0.7192
Epoch 54: val_loss improved from 0.56146 to 0.56139, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7199 - val_loss: 0.5614 - val_accuracy: 0.7223
Epoch 55/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.5619 - accuracy: 0.7221
Epoch 55: val_loss did not improve from 0.56139
682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7217 - val_loss: 0.5618 - val_accuracy: 0.7205
Epoch 56/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.5631 - accuracy: 0.7194
Epoch 56: val_loss improved from 0.56139 to 0.56138, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5616 - accuracy: 0.7207 - val_loss: 0.5614 - val_accuracy: 0.7199
Epoch 57/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.5619 - accuracy: 0.7199
Epoch 57: val_loss improved from 0.56138 to 0.56126, saving model to best_drop.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.5616 - accuracy: 0.7202 - val_loss: 0.5613 - val_accuracy: 0.7187
Epoch 58/60
682/682 [==============================] - ETA: 0s - loss: 0.5617 - accuracy: 0.7224
Epoch 58: val_loss did not improve from 0.56126
682/682 [==============================] - 1s 2ms/step - loss: 0.5617 - accuracy: 0.7224 - val_loss: 0.5613 - val_accuracy: 0.7240
Epoch 59/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.5627 - accuracy: 0.7195
Epoch 59: val_loss improved from 0.56126 to 0.56122, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5616 - accuracy: 0.7207 - val_loss: 0.5612 - val_accuracy: 0.7205
Epoch 60/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.5607 - accuracy: 0.7215
Epoch 60: val_loss did not improve from 0.56122
682/682 [==============================] - 2s 2ms/step - loss: 0.5615 - accuracy: 0.7205 - val_loss: 0.5614 - val_accuracy: 0.7193
54/54 [==============================] - 0s 2ms/step - loss: 0.5612 - accuracy: 0.7205
Drop Feature self-employed
data frame x has shape: (8516, 18)
['education', 'housing', 'campaign', 'pdays', 'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'divorced', 'married', 'single', 'admin.', 'blue-collar', 'retired', 'services', 'student']
Epoch 1/60
652/682 [===========================&gt;..] - ETA: 0s - loss: 0.6512 - accuracy: 0.6426
Epoch 1: val_loss improved from inf to 0.61886, saving model to best_drop.hdf5
682/682 [==============================] - 2s 3ms/step - loss: 0.6497 - accuracy: 0.6454 - val_loss: 0.6189 - val_accuracy: 0.7129
Epoch 2/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.6063 - accuracy: 0.7068
Epoch 2: val_loss improved from 0.61886 to 0.59236, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6068 - accuracy: 0.7060 - val_loss: 0.5924 - val_accuracy: 0.7234
Epoch 3/60
682/682 [==============================] - ETA: 0s - loss: 0.5895 - accuracy: 0.7085
Epoch 3: val_loss improved from 0.59236 to 0.58149, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5895 - accuracy: 0.7085 - val_loss: 0.5815 - val_accuracy: 0.7211
Epoch 4/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.5815 - accuracy: 0.7108
Epoch 4: val_loss improved from 0.58149 to 0.57590, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5812 - accuracy: 0.7106 - val_loss: 0.5759 - val_accuracy: 0.7187
Epoch 5/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.5772 - accuracy: 0.7104
Epoch 5: val_loss improved from 0.57590 to 0.57290, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5769 - accuracy: 0.7106 - val_loss: 0.5729 - val_accuracy: 0.7176
Epoch 6/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.5735 - accuracy: 0.7120
Epoch 6: val_loss improved from 0.57290 to 0.57114, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5743 - accuracy: 0.7116 - val_loss: 0.5711 - val_accuracy: 0.7176
Epoch 7/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.5726 - accuracy: 0.7118
Epoch 7: val_loss improved from 0.57114 to 0.57002, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5726 - accuracy: 0.7114 - val_loss: 0.5700 - val_accuracy: 0.7176
Epoch 8/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.5710 - accuracy: 0.7121
Epoch 8: val_loss improved from 0.57002 to 0.56904, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5714 - accuracy: 0.7113 - val_loss: 0.5690 - val_accuracy: 0.7176
Epoch 9/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.5722 - accuracy: 0.7092
Epoch 9: val_loss improved from 0.56904 to 0.56854, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5705 - accuracy: 0.7111 - val_loss: 0.5685 - val_accuracy: 0.7170
Epoch 10/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.5698 - accuracy: 0.7115
Epoch 10: val_loss improved from 0.56854 to 0.56807, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5696 - accuracy: 0.7116 - val_loss: 0.5681 - val_accuracy: 0.7146
Epoch 11/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.5684 - accuracy: 0.7116
Epoch 11: val_loss improved from 0.56807 to 0.56784, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5689 - accuracy: 0.7111 - val_loss: 0.5678 - val_accuracy: 0.7152
Epoch 12/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.5691 - accuracy: 0.7113
Epoch 12: val_loss improved from 0.56784 to 0.56718, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5685 - accuracy: 0.7120 - val_loss: 0.5672 - val_accuracy: 0.7164
Epoch 13/60
682/682 [==============================] - ETA: 0s - loss: 0.5680 - accuracy: 0.7111
Epoch 13: val_loss improved from 0.56718 to 0.56705, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5680 - accuracy: 0.7111 - val_loss: 0.5671 - val_accuracy: 0.7170
Epoch 14/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.5663 - accuracy: 0.7137
Epoch 14: val_loss improved from 0.56705 to 0.56679, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5675 - accuracy: 0.7128 - val_loss: 0.5668 - val_accuracy: 0.7146
Epoch 15/60
652/682 [===========================&gt;..] - ETA: 0s - loss: 0.5663 - accuracy: 0.7124
Epoch 15: val_loss improved from 0.56679 to 0.56605, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5671 - accuracy: 0.7116 - val_loss: 0.5660 - val_accuracy: 0.7164
Epoch 16/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.5676 - accuracy: 0.7110
Epoch 16: val_loss did not improve from 0.56605
682/682 [==============================] - 2s 2ms/step - loss: 0.5667 - accuracy: 0.7122 - val_loss: 0.5665 - val_accuracy: 0.7146
Epoch 17/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.5663 - accuracy: 0.7117
Epoch 17: val_loss did not improve from 0.56605
682/682 [==============================] - 2s 2ms/step - loss: 0.5662 - accuracy: 0.7119 - val_loss: 0.5661 - val_accuracy: 0.7205
Epoch 18/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.5675 - accuracy: 0.7129
Epoch 18: val_loss improved from 0.56605 to 0.56516, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5662 - accuracy: 0.7138 - val_loss: 0.5652 - val_accuracy: 0.7170
Epoch 19/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.5651 - accuracy: 0.7137
Epoch 19: val_loss improved from 0.56516 to 0.56510, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5655 - accuracy: 0.7135 - val_loss: 0.5651 - val_accuracy: 0.7199
Epoch 20/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.5654 - accuracy: 0.7131
Epoch 20: val_loss improved from 0.56510 to 0.56474, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5655 - accuracy: 0.7128 - val_loss: 0.5647 - val_accuracy: 0.7199
Epoch 21/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.5644 - accuracy: 0.7151
Epoch 21: val_loss improved from 0.56474 to 0.56447, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5653 - accuracy: 0.7136 - val_loss: 0.5645 - val_accuracy: 0.7193
Epoch 22/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.5640 - accuracy: 0.7155
Epoch 22: val_loss improved from 0.56447 to 0.56423, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5650 - accuracy: 0.7141 - val_loss: 0.5642 - val_accuracy: 0.7181
Epoch 23/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.5650 - accuracy: 0.7154
Epoch 23: val_loss improved from 0.56423 to 0.56401, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5647 - accuracy: 0.7155 - val_loss: 0.5640 - val_accuracy: 0.7187
Epoch 24/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.5636 - accuracy: 0.7159
Epoch 24: val_loss improved from 0.56401 to 0.56374, saving model to best_drop.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.5645 - accuracy: 0.7148 - val_loss: 0.5637 - val_accuracy: 0.7187
Epoch 25/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.5636 - accuracy: 0.7148
Epoch 25: val_loss did not improve from 0.56374
682/682 [==============================] - 2s 2ms/step - loss: 0.5642 - accuracy: 0.7147 - val_loss: 0.5641 - val_accuracy: 0.7217
Epoch 26/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.5634 - accuracy: 0.7164
Epoch 26: val_loss improved from 0.56374 to 0.56362, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5643 - accuracy: 0.7155 - val_loss: 0.5636 - val_accuracy: 0.7205
Epoch 27/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.5628 - accuracy: 0.7174
Epoch 27: val_loss improved from 0.56362 to 0.56333, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5639 - accuracy: 0.7175 - val_loss: 0.5633 - val_accuracy: 0.7181
Epoch 28/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.5635 - accuracy: 0.7169
Epoch 28: val_loss improved from 0.56333 to 0.56326, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5639 - accuracy: 0.7166 - val_loss: 0.5633 - val_accuracy: 0.7211
Epoch 29/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.5634 - accuracy: 0.7170
Epoch 29: val_loss improved from 0.56326 to 0.56306, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5636 - accuracy: 0.7169 - val_loss: 0.5631 - val_accuracy: 0.7187
Epoch 30/60
682/682 [==============================] - ETA: 0s - loss: 0.5635 - accuracy: 0.7179
Epoch 30: val_loss improved from 0.56306 to 0.56296, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5635 - accuracy: 0.7179 - val_loss: 0.5630 - val_accuracy: 0.7199
Epoch 31/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.5617 - accuracy: 0.7177
Epoch 31: val_loss did not improve from 0.56296
682/682 [==============================] - 2s 2ms/step - loss: 0.5634 - accuracy: 0.7166 - val_loss: 0.5630 - val_accuracy: 0.7187
Epoch 32/60
682/682 [==============================] - ETA: 0s - loss: 0.5633 - accuracy: 0.7192
Epoch 32: val_loss improved from 0.56296 to 0.56278, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5633 - accuracy: 0.7192 - val_loss: 0.5628 - val_accuracy: 0.7187
Epoch 33/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.5626 - accuracy: 0.7188
Epoch 33: val_loss improved from 0.56278 to 0.56270, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5632 - accuracy: 0.7180 - val_loss: 0.5627 - val_accuracy: 0.7193
Epoch 34/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.5634 - accuracy: 0.7175
Epoch 34: val_loss improved from 0.56270 to 0.56246, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5630 - accuracy: 0.7173 - val_loss: 0.5625 - val_accuracy: 0.7205
Epoch 35/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.5629 - accuracy: 0.7190
Epoch 35: val_loss did not improve from 0.56246
682/682 [==============================] - 2s 2ms/step - loss: 0.5628 - accuracy: 0.7191 - val_loss: 0.5625 - val_accuracy: 0.7199
Epoch 36/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.5638 - accuracy: 0.7179
Epoch 36: val_loss did not improve from 0.56246
682/682 [==============================] - 2s 2ms/step - loss: 0.5627 - accuracy: 0.7191 - val_loss: 0.5625 - val_accuracy: 0.7228
Epoch 37/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.5612 - accuracy: 0.7213
Epoch 37: val_loss improved from 0.56246 to 0.56235, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5623 - accuracy: 0.7204 - val_loss: 0.5624 - val_accuracy: 0.7193
Epoch 38/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.5624 - accuracy: 0.7184
Epoch 38: val_loss did not improve from 0.56235
682/682 [==============================] - 2s 2ms/step - loss: 0.5624 - accuracy: 0.7180 - val_loss: 0.5629 - val_accuracy: 0.7223
Epoch 39/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.5620 - accuracy: 0.7187
Epoch 39: val_loss improved from 0.56235 to 0.56226, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5626 - accuracy: 0.7182 - val_loss: 0.5623 - val_accuracy: 0.7193
Epoch 40/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.5632 - accuracy: 0.7171
Epoch 40: val_loss improved from 0.56226 to 0.56203, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5625 - accuracy: 0.7183 - val_loss: 0.5620 - val_accuracy: 0.7199
Epoch 41/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.5620 - accuracy: 0.7185
Epoch 41: val_loss did not improve from 0.56203
682/682 [==============================] - 2s 2ms/step - loss: 0.5623 - accuracy: 0.7183 - val_loss: 0.5620 - val_accuracy: 0.7193
Epoch 42/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.5626 - accuracy: 0.7189
Epoch 42: val_loss improved from 0.56203 to 0.56186, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5623 - accuracy: 0.7191 - val_loss: 0.5619 - val_accuracy: 0.7240
Epoch 43/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.5624 - accuracy: 0.7172
Epoch 43: val_loss did not improve from 0.56186
682/682 [==============================] - 2s 2ms/step - loss: 0.5621 - accuracy: 0.7182 - val_loss: 0.5623 - val_accuracy: 0.7205
Epoch 44/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.5620 - accuracy: 0.7208
Epoch 44: val_loss improved from 0.56186 to 0.56178, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5622 - accuracy: 0.7197 - val_loss: 0.5618 - val_accuracy: 0.7217
Epoch 45/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.5621 - accuracy: 0.7195
Epoch 45: val_loss did not improve from 0.56178
682/682 [==============================] - 1s 2ms/step - loss: 0.5619 - accuracy: 0.7199 - val_loss: 0.5619 - val_accuracy: 0.7193
Epoch 46/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.5621 - accuracy: 0.7189
Epoch 46: val_loss improved from 0.56178 to 0.56174, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5620 - accuracy: 0.7192 - val_loss: 0.5617 - val_accuracy: 0.7211
Epoch 47/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.5614 - accuracy: 0.7198
Epoch 47: val_loss improved from 0.56174 to 0.56167, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5620 - accuracy: 0.7188 - val_loss: 0.5617 - val_accuracy: 0.7228
Epoch 48/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.5627 - accuracy: 0.7193
Epoch 48: val_loss did not improve from 0.56167
682/682 [==============================] - 2s 2ms/step - loss: 0.5619 - accuracy: 0.7207 - val_loss: 0.5617 - val_accuracy: 0.7205
Epoch 49/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.5624 - accuracy: 0.7202
Epoch 49: val_loss did not improve from 0.56167
682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7202 - val_loss: 0.5622 - val_accuracy: 0.7240
Epoch 50/60
649/682 [===========================&gt;..] - ETA: 0s - loss: 0.5618 - accuracy: 0.7216
Epoch 50: val_loss improved from 0.56167 to 0.56156, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5619 - accuracy: 0.7210 - val_loss: 0.5616 - val_accuracy: 0.7234
Epoch 51/60
650/682 [===========================&gt;..] - ETA: 0s - loss: 0.5624 - accuracy: 0.7200
Epoch 51: val_loss did not improve from 0.56156
682/682 [==============================] - 1s 2ms/step - loss: 0.5618 - accuracy: 0.7198 - val_loss: 0.5617 - val_accuracy: 0.7199
Epoch 52/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.5615 - accuracy: 0.7205
Epoch 52: val_loss improved from 0.56156 to 0.56150, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5618 - accuracy: 0.7207 - val_loss: 0.5615 - val_accuracy: 0.7205
Epoch 53/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.5618 - accuracy: 0.7218
Epoch 53: val_loss did not improve from 0.56150
682/682 [==============================] - 2s 2ms/step - loss: 0.5618 - accuracy: 0.7219 - val_loss: 0.5616 - val_accuracy: 0.7205
Epoch 54/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.5618 - accuracy: 0.7201
Epoch 54: val_loss did not improve from 0.56150
682/682 [==============================] - 2s 2ms/step - loss: 0.5616 - accuracy: 0.7198 - val_loss: 0.5617 - val_accuracy: 0.7199
Epoch 55/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.5621 - accuracy: 0.7202
Epoch 55: val_loss improved from 0.56150 to 0.56146, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5616 - accuracy: 0.7214 - val_loss: 0.5615 - val_accuracy: 0.7205
Epoch 56/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.5617 - accuracy: 0.7206
Epoch 56: val_loss did not improve from 0.56146
682/682 [==============================] - 2s 2ms/step - loss: 0.5616 - accuracy: 0.7207 - val_loss: 0.5615 - val_accuracy: 0.7211
Epoch 57/60
654/682 [===========================&gt;..] - ETA: 0s - loss: 0.5628 - accuracy: 0.7176
Epoch 57: val_loss improved from 0.56146 to 0.56136, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5615 - accuracy: 0.7189 - val_loss: 0.5614 - val_accuracy: 0.7228
Epoch 58/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.5600 - accuracy: 0.7207
Epoch 58: val_loss did not improve from 0.56136
682/682 [==============================] - 2s 2ms/step - loss: 0.5615 - accuracy: 0.7194 - val_loss: 0.5615 - val_accuracy: 0.7205
Epoch 59/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.5618 - accuracy: 0.7202
Epoch 59: val_loss did not improve from 0.56136
682/682 [==============================] - 2s 2ms/step - loss: 0.5615 - accuracy: 0.7213 - val_loss: 0.5614 - val_accuracy: 0.7205
Epoch 60/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.5605 - accuracy: 0.7217
Epoch 60: val_loss improved from 0.56136 to 0.56133, saving model to best_drop.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.5614 - accuracy: 0.7210 - val_loss: 0.5613 - val_accuracy: 0.7211
54/54 [==============================] - 0s 3ms/step - loss: 0.5613 - accuracy: 0.7211
Drop Feature divorced
data frame x has shape: (8516, 17)
['education', 'housing', 'campaign', 'pdays', 'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'married', 'single', 'admin.', 'blue-collar', 'retired', 'services', 'student']
Epoch 1/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.7100 - accuracy: 0.5559
Epoch 1: val_loss improved from inf to 0.64562, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.7076 - accuracy: 0.5591 - val_loss: 0.6456 - val_accuracy: 0.6424
Epoch 2/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.6220 - accuracy: 0.6774
Epoch 2: val_loss improved from 0.64562 to 0.60156, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6211 - accuracy: 0.6786 - val_loss: 0.6016 - val_accuracy: 0.6958
Epoch 3/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.5932 - accuracy: 0.7036
Epoch 3: val_loss improved from 0.60156 to 0.58359, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5929 - accuracy: 0.7044 - val_loss: 0.5836 - val_accuracy: 0.7082
Epoch 4/60
653/682 [===========================&gt;..] - ETA: 0s - loss: 0.5807 - accuracy: 0.7086
Epoch 4: val_loss improved from 0.58359 to 0.57586, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5813 - accuracy: 0.7082 - val_loss: 0.5759 - val_accuracy: 0.7111
Epoch 5/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.5759 - accuracy: 0.7109
Epoch 5: val_loss improved from 0.57586 to 0.57193, saving model to best_drop.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.5759 - accuracy: 0.7110 - val_loss: 0.5719 - val_accuracy: 0.7164
Epoch 6/60
653/682 [===========================&gt;..] - ETA: 0s - loss: 0.5729 - accuracy: 0.7119
Epoch 6: val_loss improved from 0.57193 to 0.56999, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5731 - accuracy: 0.7111 - val_loss: 0.5700 - val_accuracy: 0.7164
Epoch 7/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.5715 - accuracy: 0.7116
Epoch 7: val_loss improved from 0.56999 to 0.56936, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5711 - accuracy: 0.7120 - val_loss: 0.5694 - val_accuracy: 0.7140
Epoch 8/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.5700 - accuracy: 0.7121
Epoch 8: val_loss improved from 0.56936 to 0.56803, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5701 - accuracy: 0.7123 - val_loss: 0.5680 - val_accuracy: 0.7205
Epoch 9/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.5692 - accuracy: 0.7139
Epoch 9: val_loss improved from 0.56803 to 0.56725, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5691 - accuracy: 0.7135 - val_loss: 0.5672 - val_accuracy: 0.7205
Epoch 10/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.5693 - accuracy: 0.7119
Epoch 10: val_loss improved from 0.56725 to 0.56690, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5686 - accuracy: 0.7129 - val_loss: 0.5669 - val_accuracy: 0.7211
Epoch 11/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.5681 - accuracy: 0.7136
Epoch 11: val_loss improved from 0.56690 to 0.56658, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5679 - accuracy: 0.7139 - val_loss: 0.5666 - val_accuracy: 0.7211
Epoch 12/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.5673 - accuracy: 0.7126
Epoch 12: val_loss improved from 0.56658 to 0.56641, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5674 - accuracy: 0.7136 - val_loss: 0.5664 - val_accuracy: 0.7234
Epoch 13/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.5670 - accuracy: 0.7145
Epoch 13: val_loss improved from 0.56641 to 0.56569, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5671 - accuracy: 0.7145 - val_loss: 0.5657 - val_accuracy: 0.7199
Epoch 14/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.5665 - accuracy: 0.7126
Epoch 14: val_loss improved from 0.56569 to 0.56540, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5665 - accuracy: 0.7119 - val_loss: 0.5654 - val_accuracy: 0.7187
Epoch 15/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.5662 - accuracy: 0.7149
Epoch 15: val_loss did not improve from 0.56540
682/682 [==============================] - 2s 2ms/step - loss: 0.5660 - accuracy: 0.7150 - val_loss: 0.5657 - val_accuracy: 0.7234
Epoch 16/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.5650 - accuracy: 0.7180
Epoch 16: val_loss improved from 0.56540 to 0.56481, saving model to best_drop.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.5658 - accuracy: 0.7172 - val_loss: 0.5648 - val_accuracy: 0.7217
Epoch 17/60
682/682 [==============================] - ETA: 0s - loss: 0.5652 - accuracy: 0.7151
Epoch 17: val_loss improved from 0.56481 to 0.56454, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5652 - accuracy: 0.7151 - val_loss: 0.5645 - val_accuracy: 0.7211
Epoch 18/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.5635 - accuracy: 0.7182
Epoch 18: val_loss improved from 0.56454 to 0.56432, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5651 - accuracy: 0.7164 - val_loss: 0.5643 - val_accuracy: 0.7211
Epoch 19/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.5655 - accuracy: 0.7156
Epoch 19: val_loss improved from 0.56432 to 0.56399, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5648 - accuracy: 0.7161 - val_loss: 0.5640 - val_accuracy: 0.7223
Epoch 20/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.5658 - accuracy: 0.7142
Epoch 20: val_loss improved from 0.56399 to 0.56379, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5646 - accuracy: 0.7158 - val_loss: 0.5638 - val_accuracy: 0.7205
Epoch 21/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.5641 - accuracy: 0.7161
Epoch 21: val_loss improved from 0.56379 to 0.56367, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5645 - accuracy: 0.7164 - val_loss: 0.5637 - val_accuracy: 0.7211
Epoch 22/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.5632 - accuracy: 0.7178
Epoch 22: val_loss improved from 0.56367 to 0.56352, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5641 - accuracy: 0.7173 - val_loss: 0.5635 - val_accuracy: 0.7205
Epoch 23/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.5634 - accuracy: 0.7192
Epoch 23: val_loss improved from 0.56352 to 0.56339, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5639 - accuracy: 0.7182 - val_loss: 0.5634 - val_accuracy: 0.7217
Epoch 24/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.5637 - accuracy: 0.7170
Epoch 24: val_loss did not improve from 0.56339
682/682 [==============================] - 2s 2ms/step - loss: 0.5636 - accuracy: 0.7176 - val_loss: 0.5637 - val_accuracy: 0.7199
Epoch 25/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.5640 - accuracy: 0.7176
Epoch 25: val_loss improved from 0.56339 to 0.56313, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5638 - accuracy: 0.7175 - val_loss: 0.5631 - val_accuracy: 0.7217
Epoch 26/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.5634 - accuracy: 0.7177
Epoch 26: val_loss improved from 0.56313 to 0.56286, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5635 - accuracy: 0.7176 - val_loss: 0.5629 - val_accuracy: 0.7223
Epoch 27/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.5629 - accuracy: 0.7179
Epoch 27: val_loss did not improve from 0.56286
682/682 [==============================] - 2s 2ms/step - loss: 0.5632 - accuracy: 0.7186 - val_loss: 0.5629 - val_accuracy: 0.7211
Epoch 28/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.5635 - accuracy: 0.7195
Epoch 28: val_loss improved from 0.56286 to 0.56268, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5630 - accuracy: 0.7189 - val_loss: 0.5627 - val_accuracy: 0.7228
Epoch 29/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.5623 - accuracy: 0.7215
Epoch 29: val_loss did not improve from 0.56268
682/682 [==============================] - 2s 2ms/step - loss: 0.5626 - accuracy: 0.7213 - val_loss: 0.5628 - val_accuracy: 0.7211
Epoch 30/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.5627 - accuracy: 0.7175
Epoch 30: val_loss improved from 0.56268 to 0.56245, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5630 - accuracy: 0.7176 - val_loss: 0.5625 - val_accuracy: 0.7205
Epoch 31/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.5638 - accuracy: 0.7173
Epoch 31: val_loss improved from 0.56245 to 0.56235, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5628 - accuracy: 0.7182 - val_loss: 0.5624 - val_accuracy: 0.7211
Epoch 32/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.5629 - accuracy: 0.7188
Epoch 32: val_loss did not improve from 0.56235
682/682 [==============================] - 2s 2ms/step - loss: 0.5625 - accuracy: 0.7191 - val_loss: 0.5626 - val_accuracy: 0.7205
Epoch 33/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.5637 - accuracy: 0.7177
Epoch 33: val_loss improved from 0.56235 to 0.56223, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5625 - accuracy: 0.7188 - val_loss: 0.5622 - val_accuracy: 0.7217
Epoch 34/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.5632 - accuracy: 0.7189
Epoch 34: val_loss improved from 0.56223 to 0.56208, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5624 - accuracy: 0.7191 - val_loss: 0.5621 - val_accuracy: 0.7217
Epoch 35/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.5619 - accuracy: 0.7201
Epoch 35: val_loss improved from 0.56208 to 0.56203, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5624 - accuracy: 0.7197 - val_loss: 0.5620 - val_accuracy: 0.7187
Epoch 36/60
653/682 [===========================&gt;..] - ETA: 0s - loss: 0.5623 - accuracy: 0.7187
Epoch 36: val_loss did not improve from 0.56203
682/682 [==============================] - 2s 2ms/step - loss: 0.5624 - accuracy: 0.7189 - val_loss: 0.5620 - val_accuracy: 0.7211
Epoch 37/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.5630 - accuracy: 0.7198
Epoch 37: val_loss improved from 0.56203 to 0.56177, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5622 - accuracy: 0.7207 - val_loss: 0.5618 - val_accuracy: 0.7199
Epoch 38/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.5624 - accuracy: 0.7201
Epoch 38: val_loss did not improve from 0.56177
682/682 [==============================] - 2s 2ms/step - loss: 0.5622 - accuracy: 0.7204 - val_loss: 0.5619 - val_accuracy: 0.7199
Epoch 39/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.5621 - accuracy: 0.7195
Epoch 39: val_loss did not improve from 0.56177
682/682 [==============================] - 2s 2ms/step - loss: 0.5622 - accuracy: 0.7195 - val_loss: 0.5618 - val_accuracy: 0.7199
Epoch 40/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.5614 - accuracy: 0.7210
Epoch 40: val_loss did not improve from 0.56177
682/682 [==============================] - 1s 2ms/step - loss: 0.5619 - accuracy: 0.7204 - val_loss: 0.5623 - val_accuracy: 0.7217
Epoch 41/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.5622 - accuracy: 0.7200
Epoch 41: val_loss did not improve from 0.56177
682/682 [==============================] - 2s 2ms/step - loss: 0.5619 - accuracy: 0.7204 - val_loss: 0.5619 - val_accuracy: 0.7217
Epoch 42/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.5614 - accuracy: 0.7194
Epoch 42: val_loss improved from 0.56177 to 0.56165, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5619 - accuracy: 0.7188 - val_loss: 0.5617 - val_accuracy: 0.7199
Epoch 43/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.5609 - accuracy: 0.7220
Epoch 43: val_loss did not improve from 0.56165
682/682 [==============================] - 2s 2ms/step - loss: 0.5618 - accuracy: 0.7210 - val_loss: 0.5617 - val_accuracy: 0.7205
Epoch 44/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.5626 - accuracy: 0.7192
Epoch 44: val_loss did not improve from 0.56165
682/682 [==============================] - 2s 2ms/step - loss: 0.5618 - accuracy: 0.7195 - val_loss: 0.5617 - val_accuracy: 0.7217
Epoch 45/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.5613 - accuracy: 0.7187
Epoch 45: val_loss improved from 0.56165 to 0.56161, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7191 - val_loss: 0.5616 - val_accuracy: 0.7187
Epoch 46/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.5599 - accuracy: 0.7204
Epoch 46: val_loss did not improve from 0.56161
682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7199 - val_loss: 0.5617 - val_accuracy: 0.7199
Epoch 47/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.5628 - accuracy: 0.7190
Epoch 47: val_loss improved from 0.56161 to 0.56148, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7199 - val_loss: 0.5615 - val_accuracy: 0.7211
Epoch 48/60
653/682 [===========================&gt;..] - ETA: 0s - loss: 0.5617 - accuracy: 0.7198
Epoch 48: val_loss did not improve from 0.56148
682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7195 - val_loss: 0.5617 - val_accuracy: 0.7223
Epoch 49/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.5612 - accuracy: 0.7216
Epoch 49: val_loss improved from 0.56148 to 0.56135, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5616 - accuracy: 0.7213 - val_loss: 0.5613 - val_accuracy: 0.7193
Epoch 50/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.5631 - accuracy: 0.7162
Epoch 50: val_loss did not improve from 0.56135
682/682 [==============================] - 2s 2ms/step - loss: 0.5614 - accuracy: 0.7189 - val_loss: 0.5618 - val_accuracy: 0.7211
Epoch 51/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.5613 - accuracy: 0.7215
Epoch 51: val_loss did not improve from 0.56135
682/682 [==============================] - 1s 2ms/step - loss: 0.5616 - accuracy: 0.7207 - val_loss: 0.5614 - val_accuracy: 0.7211
Epoch 52/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.5618 - accuracy: 0.7202
Epoch 52: val_loss did not improve from 0.56135
682/682 [==============================] - 2s 2ms/step - loss: 0.5615 - accuracy: 0.7202 - val_loss: 0.5614 - val_accuracy: 0.7217
Epoch 53/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.5618 - accuracy: 0.7209
Epoch 53: val_loss did not improve from 0.56135
682/682 [==============================] - 2s 2ms/step - loss: 0.5613 - accuracy: 0.7213 - val_loss: 0.5616 - val_accuracy: 0.7187
Epoch 54/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.5615 - accuracy: 0.7209
Epoch 54: val_loss did not improve from 0.56135
682/682 [==============================] - 2s 2ms/step - loss: 0.5616 - accuracy: 0.7208 - val_loss: 0.5614 - val_accuracy: 0.7217
Epoch 55/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.5611 - accuracy: 0.7218
Epoch 55: val_loss did not improve from 0.56135
682/682 [==============================] - 2s 2ms/step - loss: 0.5615 - accuracy: 0.7211 - val_loss: 0.5614 - val_accuracy: 0.7205
Epoch 56/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.5598 - accuracy: 0.7226
Epoch 56: val_loss did not improve from 0.56135
682/682 [==============================] - 2s 2ms/step - loss: 0.5613 - accuracy: 0.7216 - val_loss: 0.5615 - val_accuracy: 0.7181
Epoch 57/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.5636 - accuracy: 0.7177
Epoch 57: val_loss improved from 0.56135 to 0.56117, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5616 - accuracy: 0.7198 - val_loss: 0.5612 - val_accuracy: 0.7199
Epoch 58/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.5618 - accuracy: 0.7204
Epoch 58: val_loss improved from 0.56117 to 0.56113, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5613 - accuracy: 0.7210 - val_loss: 0.5611 - val_accuracy: 0.7199
Epoch 59/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.5615 - accuracy: 0.7206
Epoch 59: val_loss did not improve from 0.56113
682/682 [==============================] - 2s 2ms/step - loss: 0.5612 - accuracy: 0.7210 - val_loss: 0.5612 - val_accuracy: 0.7223
Epoch 60/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.5607 - accuracy: 0.7215
Epoch 60: val_loss did not improve from 0.56113
682/682 [==============================] - 2s 2ms/step - loss: 0.5614 - accuracy: 0.7213 - val_loss: 0.5613 - val_accuracy: 0.7205
54/54 [==============================] - 0s 2ms/step - loss: 0.5611 - accuracy: 0.7199
Drop Feature housing
data frame x has shape: (8516, 16)
['education', 'campaign', 'pdays', 'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'married', 'single', 'admin.', 'blue-collar', 'retired', 'services', 'student']
Epoch 1/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.6776 - accuracy: 0.5875
Epoch 1: val_loss improved from inf to 0.63762, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6769 - accuracy: 0.5889 - val_loss: 0.6376 - val_accuracy: 0.6835
Epoch 2/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.6214 - accuracy: 0.6852
Epoch 2: val_loss improved from 0.63762 to 0.60222, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6206 - accuracy: 0.6866 - val_loss: 0.6022 - val_accuracy: 0.6999
Epoch 3/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.5970 - accuracy: 0.7014
Epoch 3: val_loss improved from 0.60222 to 0.58648, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5971 - accuracy: 0.7015 - val_loss: 0.5865 - val_accuracy: 0.7140
Epoch 4/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.5859 - accuracy: 0.7066
Epoch 4: val_loss improved from 0.58648 to 0.57935, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5862 - accuracy: 0.7067 - val_loss: 0.5794 - val_accuracy: 0.7158
Epoch 5/60
653/682 [===========================&gt;..] - ETA: 0s - loss: 0.5800 - accuracy: 0.7069
Epoch 5: val_loss improved from 0.57935 to 0.57561, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5806 - accuracy: 0.7059 - val_loss: 0.5756 - val_accuracy: 0.7152
Epoch 6/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.5752 - accuracy: 0.7100
Epoch 6: val_loss improved from 0.57561 to 0.57329, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5773 - accuracy: 0.7081 - val_loss: 0.5733 - val_accuracy: 0.7193
Epoch 7/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.5750 - accuracy: 0.7094
Epoch 7: val_loss improved from 0.57329 to 0.57195, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5750 - accuracy: 0.7095 - val_loss: 0.5719 - val_accuracy: 0.7217
Epoch 8/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.5735 - accuracy: 0.7104
Epoch 8: val_loss improved from 0.57195 to 0.57096, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5735 - accuracy: 0.7100 - val_loss: 0.5710 - val_accuracy: 0.7152
Epoch 9/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.5732 - accuracy: 0.7090
Epoch 9: val_loss improved from 0.57096 to 0.56993, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5723 - accuracy: 0.7104 - val_loss: 0.5699 - val_accuracy: 0.7181
Epoch 10/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.5699 - accuracy: 0.7104
Epoch 10: val_loss improved from 0.56993 to 0.56914, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5714 - accuracy: 0.7103 - val_loss: 0.5691 - val_accuracy: 0.7176
Epoch 11/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.5703 - accuracy: 0.7113
Epoch 11: val_loss improved from 0.56914 to 0.56849, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5705 - accuracy: 0.7111 - val_loss: 0.5685 - val_accuracy: 0.7193
Epoch 12/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.5698 - accuracy: 0.7111
Epoch 12: val_loss improved from 0.56849 to 0.56843, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5697 - accuracy: 0.7104 - val_loss: 0.5684 - val_accuracy: 0.7164
Epoch 13/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.5696 - accuracy: 0.7102
Epoch 13: val_loss improved from 0.56843 to 0.56740, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5690 - accuracy: 0.7108 - val_loss: 0.5674 - val_accuracy: 0.7199
Epoch 14/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.5689 - accuracy: 0.7119
Epoch 14: val_loss improved from 0.56740 to 0.56698, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5684 - accuracy: 0.7120 - val_loss: 0.5670 - val_accuracy: 0.7199
Epoch 15/60
650/682 [===========================&gt;..] - ETA: 0s - loss: 0.5680 - accuracy: 0.7131
Epoch 15: val_loss improved from 0.56698 to 0.56675, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5676 - accuracy: 0.7132 - val_loss: 0.5667 - val_accuracy: 0.7187
Epoch 16/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.5674 - accuracy: 0.7121
Epoch 16: val_loss improved from 0.56675 to 0.56616, saving model to best_drop.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.5673 - accuracy: 0.7117 - val_loss: 0.5662 - val_accuracy: 0.7205
Epoch 17/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.5673 - accuracy: 0.7123
Epoch 17: val_loss improved from 0.56616 to 0.56578, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5668 - accuracy: 0.7126 - val_loss: 0.5658 - val_accuracy: 0.7205
Epoch 18/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.5661 - accuracy: 0.7134
Epoch 18: val_loss improved from 0.56578 to 0.56566, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5663 - accuracy: 0.7129 - val_loss: 0.5657 - val_accuracy: 0.7258
Epoch 19/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.5655 - accuracy: 0.7144
Epoch 19: val_loss improved from 0.56566 to 0.56555, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5659 - accuracy: 0.7142 - val_loss: 0.5655 - val_accuracy: 0.7264
Epoch 20/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.5664 - accuracy: 0.7151
Epoch 20: val_loss improved from 0.56555 to 0.56475, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5658 - accuracy: 0.7145 - val_loss: 0.5647 - val_accuracy: 0.7228
Epoch 21/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.5655 - accuracy: 0.7153
Epoch 21: val_loss improved from 0.56475 to 0.56452, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5655 - accuracy: 0.7150 - val_loss: 0.5645 - val_accuracy: 0.7252
Epoch 22/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.5636 - accuracy: 0.7170
Epoch 22: val_loss improved from 0.56452 to 0.56432, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5650 - accuracy: 0.7153 - val_loss: 0.5643 - val_accuracy: 0.7234
Epoch 23/60
652/682 [===========================&gt;..] - ETA: 0s - loss: 0.5639 - accuracy: 0.7166
Epoch 23: val_loss improved from 0.56432 to 0.56398, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5648 - accuracy: 0.7158 - val_loss: 0.5640 - val_accuracy: 0.7240
Epoch 24/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.5646 - accuracy: 0.7173
Epoch 24: val_loss did not improve from 0.56398
682/682 [==============================] - 2s 2ms/step - loss: 0.5645 - accuracy: 0.7170 - val_loss: 0.5640 - val_accuracy: 0.7246
Epoch 25/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.5651 - accuracy: 0.7185
Epoch 25: val_loss improved from 0.56398 to 0.56391, saving model to best_drop.hdf5
682/682 [==============================] - 2s 3ms/step - loss: 0.5642 - accuracy: 0.7191 - val_loss: 0.5639 - val_accuracy: 0.7217
Epoch 26/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.5628 - accuracy: 0.7179
Epoch 26: val_loss improved from 0.56391 to 0.56336, saving model to best_drop.hdf5
682/682 [==============================] - 2s 3ms/step - loss: 0.5639 - accuracy: 0.7170 - val_loss: 0.5634 - val_accuracy: 0.7246
Epoch 27/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.5639 - accuracy: 0.7194
Epoch 27: val_loss improved from 0.56336 to 0.56321, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5639 - accuracy: 0.7189 - val_loss: 0.5632 - val_accuracy: 0.7246
Epoch 28/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.5629 - accuracy: 0.7190
Epoch 28: val_loss improved from 0.56321 to 0.56313, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5636 - accuracy: 0.7185 - val_loss: 0.5631 - val_accuracy: 0.7223
Epoch 29/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.5626 - accuracy: 0.7183
Epoch 29: val_loss improved from 0.56313 to 0.56296, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5633 - accuracy: 0.7182 - val_loss: 0.5630 - val_accuracy: 0.7223
Epoch 30/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.5633 - accuracy: 0.7182
Epoch 30: val_loss did not improve from 0.56296
682/682 [==============================] - 2s 2ms/step - loss: 0.5633 - accuracy: 0.7177 - val_loss: 0.5630 - val_accuracy: 0.7252
Epoch 31/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.5643 - accuracy: 0.7167
Epoch 31: val_loss improved from 0.56296 to 0.56290, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5631 - accuracy: 0.7183 - val_loss: 0.5629 - val_accuracy: 0.7217
Epoch 32/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.5626 - accuracy: 0.7181
Epoch 32: val_loss improved from 0.56290 to 0.56263, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5630 - accuracy: 0.7177 - val_loss: 0.5626 - val_accuracy: 0.7246
Epoch 33/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.5628 - accuracy: 0.7180
Epoch 33: val_loss did not improve from 0.56263
682/682 [==============================] - 1s 2ms/step - loss: 0.5626 - accuracy: 0.7185 - val_loss: 0.5629 - val_accuracy: 0.7193
Epoch 34/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.5616 - accuracy: 0.7175
Epoch 34: val_loss improved from 0.56263 to 0.56248, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5627 - accuracy: 0.7164 - val_loss: 0.5625 - val_accuracy: 0.7234
Epoch 35/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.5633 - accuracy: 0.7174
Epoch 35: val_loss improved from 0.56248 to 0.56238, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5626 - accuracy: 0.7176 - val_loss: 0.5624 - val_accuracy: 0.7211
Epoch 36/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.5628 - accuracy: 0.7185
Epoch 36: val_loss improved from 0.56238 to 0.56233, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5624 - accuracy: 0.7183 - val_loss: 0.5623 - val_accuracy: 0.7252
Epoch 37/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.5640 - accuracy: 0.7186
Epoch 37: val_loss improved from 0.56233 to 0.56208, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5624 - accuracy: 0.7199 - val_loss: 0.5621 - val_accuracy: 0.7211
Epoch 38/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.5633 - accuracy: 0.7188
Epoch 38: val_loss did not improve from 0.56208
682/682 [==============================] - 1s 2ms/step - loss: 0.5623 - accuracy: 0.7191 - val_loss: 0.5621 - val_accuracy: 0.7217
Epoch 39/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.5625 - accuracy: 0.7202
Epoch 39: val_loss improved from 0.56208 to 0.56198, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5623 - accuracy: 0.7204 - val_loss: 0.5620 - val_accuracy: 0.7246
Epoch 40/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.5613 - accuracy: 0.7208
Epoch 40: val_loss did not improve from 0.56198
682/682 [==============================] - 2s 2ms/step - loss: 0.5621 - accuracy: 0.7199 - val_loss: 0.5621 - val_accuracy: 0.7205
Epoch 41/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.5615 - accuracy: 0.7191
Epoch 41: val_loss did not improve from 0.56198
682/682 [==============================] - 1s 2ms/step - loss: 0.5621 - accuracy: 0.7189 - val_loss: 0.5620 - val_accuracy: 0.7246
Epoch 42/60
650/682 [===========================&gt;..] - ETA: 0s - loss: 0.5608 - accuracy: 0.7212
Epoch 42: val_loss improved from 0.56198 to 0.56196, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5619 - accuracy: 0.7194 - val_loss: 0.5620 - val_accuracy: 0.7199
Epoch 43/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.5616 - accuracy: 0.7212
Epoch 43: val_loss did not improve from 0.56196
682/682 [==============================] - 2s 2ms/step - loss: 0.5621 - accuracy: 0.7208 - val_loss: 0.5620 - val_accuracy: 0.7199
Epoch 44/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.5614 - accuracy: 0.7210
Epoch 44: val_loss improved from 0.56196 to 0.56165, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5618 - accuracy: 0.7208 - val_loss: 0.5616 - val_accuracy: 0.7240
Epoch 45/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.5617 - accuracy: 0.7189
Epoch 45: val_loss did not improve from 0.56165
682/682 [==============================] - 2s 2ms/step - loss: 0.5618 - accuracy: 0.7183 - val_loss: 0.5617 - val_accuracy: 0.7240
Epoch 46/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.5610 - accuracy: 0.7192
Epoch 46: val_loss did not improve from 0.56165
682/682 [==============================] - 2s 2ms/step - loss: 0.5616 - accuracy: 0.7188 - val_loss: 0.5619 - val_accuracy: 0.7193
Epoch 47/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.5631 - accuracy: 0.7196
Epoch 47: val_loss did not improve from 0.56165
682/682 [==============================] - 2s 2ms/step - loss: 0.5618 - accuracy: 0.7208 - val_loss: 0.5617 - val_accuracy: 0.7199
Epoch 48/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.5622 - accuracy: 0.7193
Epoch 48: val_loss improved from 0.56165 to 0.56150, saving model to best_drop.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.5617 - accuracy: 0.7198 - val_loss: 0.5615 - val_accuracy: 0.7199
Epoch 49/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.5623 - accuracy: 0.7182
Epoch 49: val_loss improved from 0.56150 to 0.56142, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5615 - accuracy: 0.7194 - val_loss: 0.5614 - val_accuracy: 0.7217
Epoch 50/60
653/682 [===========================&gt;..] - ETA: 0s - loss: 0.5612 - accuracy: 0.7182
Epoch 50: val_loss did not improve from 0.56142
682/682 [==============================] - 2s 2ms/step - loss: 0.5614 - accuracy: 0.7182 - val_loss: 0.5621 - val_accuracy: 0.7205
Epoch 51/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.5622 - accuracy: 0.7188
Epoch 51: val_loss did not improve from 0.56142
682/682 [==============================] - 2s 2ms/step - loss: 0.5615 - accuracy: 0.7201 - val_loss: 0.5616 - val_accuracy: 0.7217
Epoch 52/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.5613 - accuracy: 0.7204
Epoch 52: val_loss did not improve from 0.56142
682/682 [==============================] - 2s 2ms/step - loss: 0.5614 - accuracy: 0.7204 - val_loss: 0.5622 - val_accuracy: 0.7211
Epoch 53/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.5628 - accuracy: 0.7187
Epoch 53: val_loss improved from 0.56142 to 0.56138, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5615 - accuracy: 0.7211 - val_loss: 0.5614 - val_accuracy: 0.7223
Epoch 54/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.5610 - accuracy: 0.7209
Epoch 54: val_loss improved from 0.56138 to 0.56134, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5614 - accuracy: 0.7205 - val_loss: 0.5613 - val_accuracy: 0.7187
Epoch 55/60
650/682 [===========================&gt;..] - ETA: 0s - loss: 0.5635 - accuracy: 0.7188
Epoch 55: val_loss improved from 0.56134 to 0.56128, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5613 - accuracy: 0.7213 - val_loss: 0.5613 - val_accuracy: 0.7211
Epoch 56/60
653/682 [===========================&gt;..] - ETA: 0s - loss: 0.5593 - accuracy: 0.7221
Epoch 56: val_loss did not improve from 0.56128
682/682 [==============================] - 2s 2ms/step - loss: 0.5614 - accuracy: 0.7205 - val_loss: 0.5613 - val_accuracy: 0.7223
Epoch 57/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.5618 - accuracy: 0.7209
Epoch 57: val_loss improved from 0.56128 to 0.56121, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5613 - accuracy: 0.7216 - val_loss: 0.5612 - val_accuracy: 0.7205
Epoch 58/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.5613 - accuracy: 0.7212
Epoch 58: val_loss did not improve from 0.56121
682/682 [==============================] - 2s 2ms/step - loss: 0.5614 - accuracy: 0.7207 - val_loss: 0.5614 - val_accuracy: 0.7199
Epoch 59/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.5612 - accuracy: 0.7213
Epoch 59: val_loss did not improve from 0.56121
682/682 [==============================] - 2s 2ms/step - loss: 0.5612 - accuracy: 0.7213 - val_loss: 0.5617 - val_accuracy: 0.7211
Epoch 60/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.5618 - accuracy: 0.7202
Epoch 60: val_loss did not improve from 0.56121
682/682 [==============================] - 1s 2ms/step - loss: 0.5612 - accuracy: 0.7210 - val_loss: 0.5612 - val_accuracy: 0.7193
54/54 [==============================] - 0s 2ms/step - loss: 0.5612 - accuracy: 0.7205
Drop Feature services
data frame x has shape: (8516, 15)
['education', 'campaign', 'pdays', 'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'married', 'single', 'admin.', 'blue-collar', 'retired', 'student']
Epoch 1/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.6248 - accuracy: 0.6687
Epoch 1: val_loss improved from inf to 0.60470, saving model to best_drop.hdf5
682/682 [==============================] - 2s 3ms/step - loss: 0.6249 - accuracy: 0.6687 - val_loss: 0.6047 - val_accuracy: 0.6882
Epoch 2/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.5983 - accuracy: 0.7018
Epoch 2: val_loss improved from 0.60470 to 0.58773, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5981 - accuracy: 0.7022 - val_loss: 0.5877 - val_accuracy: 0.7158
Epoch 3/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.5853 - accuracy: 0.7081
Epoch 3: val_loss improved from 0.58773 to 0.57973, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5857 - accuracy: 0.7078 - val_loss: 0.5797 - val_accuracy: 0.7228
Epoch 4/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.5790 - accuracy: 0.7098
Epoch 4: val_loss improved from 0.57973 to 0.57570, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5798 - accuracy: 0.7091 - val_loss: 0.5757 - val_accuracy: 0.7223
Epoch 5/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.5760 - accuracy: 0.7100
Epoch 5: val_loss improved from 0.57570 to 0.57321, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5763 - accuracy: 0.7103 - val_loss: 0.5732 - val_accuracy: 0.7252
Epoch 6/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.5742 - accuracy: 0.7092
Epoch 6: val_loss improved from 0.57321 to 0.57174, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5743 - accuracy: 0.7097 - val_loss: 0.5717 - val_accuracy: 0.7234
Epoch 7/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.5748 - accuracy: 0.7075
Epoch 7: val_loss improved from 0.57174 to 0.57083, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5728 - accuracy: 0.7092 - val_loss: 0.5708 - val_accuracy: 0.7211
Epoch 8/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.5714 - accuracy: 0.7113
Epoch 8: val_loss improved from 0.57083 to 0.56980, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5717 - accuracy: 0.7103 - val_loss: 0.5698 - val_accuracy: 0.7223
Epoch 9/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.5708 - accuracy: 0.7116
Epoch 9: val_loss improved from 0.56980 to 0.56920, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5709 - accuracy: 0.7119 - val_loss: 0.5692 - val_accuracy: 0.7176
Epoch 10/60
682/682 [==============================] - ETA: 0s - loss: 0.5701 - accuracy: 0.7098
Epoch 10: val_loss improved from 0.56920 to 0.56877, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5701 - accuracy: 0.7098 - val_loss: 0.5688 - val_accuracy: 0.7223
Epoch 11/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.5699 - accuracy: 0.7114
Epoch 11: val_loss improved from 0.56877 to 0.56834, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5695 - accuracy: 0.7114 - val_loss: 0.5683 - val_accuracy: 0.7158
Epoch 12/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.5686 - accuracy: 0.7102
Epoch 12: val_loss improved from 0.56834 to 0.56769, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5689 - accuracy: 0.7104 - val_loss: 0.5677 - val_accuracy: 0.7223
Epoch 13/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.5678 - accuracy: 0.7123
Epoch 13: val_loss improved from 0.56769 to 0.56707, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5684 - accuracy: 0.7116 - val_loss: 0.5671 - val_accuracy: 0.7193
Epoch 14/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.5687 - accuracy: 0.7108
Epoch 14: val_loss improved from 0.56707 to 0.56675, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5678 - accuracy: 0.7114 - val_loss: 0.5667 - val_accuracy: 0.7181
Epoch 15/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.5691 - accuracy: 0.7088
Epoch 15: val_loss improved from 0.56675 to 0.56643, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5674 - accuracy: 0.7108 - val_loss: 0.5664 - val_accuracy: 0.7164
Epoch 16/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.5663 - accuracy: 0.7126
Epoch 16: val_loss improved from 0.56643 to 0.56609, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5670 - accuracy: 0.7120 - val_loss: 0.5661 - val_accuracy: 0.7228
Epoch 17/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.5665 - accuracy: 0.7118
Epoch 17: val_loss improved from 0.56609 to 0.56565, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5667 - accuracy: 0.7117 - val_loss: 0.5656 - val_accuracy: 0.7199
Epoch 18/60
654/682 [===========================&gt;..] - ETA: 0s - loss: 0.5667 - accuracy: 0.7119
Epoch 18: val_loss improved from 0.56565 to 0.56542, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5663 - accuracy: 0.7116 - val_loss: 0.5654 - val_accuracy: 0.7193
Epoch 19/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.5658 - accuracy: 0.7135
Epoch 19: val_loss improved from 0.56542 to 0.56504, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5660 - accuracy: 0.7132 - val_loss: 0.5650 - val_accuracy: 0.7199
Epoch 20/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.5652 - accuracy: 0.7154
Epoch 20: val_loss did not improve from 0.56504
682/682 [==============================] - 2s 2ms/step - loss: 0.5656 - accuracy: 0.7151 - val_loss: 0.5651 - val_accuracy: 0.7223
Epoch 21/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.5660 - accuracy: 0.7137
Epoch 21: val_loss improved from 0.56504 to 0.56471, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5654 - accuracy: 0.7141 - val_loss: 0.5647 - val_accuracy: 0.7181
Epoch 22/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.5628 - accuracy: 0.7171
Epoch 22: val_loss improved from 0.56471 to 0.56442, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5651 - accuracy: 0.7145 - val_loss: 0.5644 - val_accuracy: 0.7181
Epoch 23/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.5650 - accuracy: 0.7146
Epoch 23: val_loss improved from 0.56442 to 0.56422, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5649 - accuracy: 0.7145 - val_loss: 0.5642 - val_accuracy: 0.7176
Epoch 24/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.5654 - accuracy: 0.7147
Epoch 24: val_loss improved from 0.56422 to 0.56400, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5646 - accuracy: 0.7151 - val_loss: 0.5640 - val_accuracy: 0.7199
Epoch 25/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.5643 - accuracy: 0.7150
Epoch 25: val_loss improved from 0.56400 to 0.56378, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5644 - accuracy: 0.7150 - val_loss: 0.5638 - val_accuracy: 0.7187
Epoch 26/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.5640 - accuracy: 0.7145
Epoch 26: val_loss did not improve from 0.56378
682/682 [==============================] - 2s 2ms/step - loss: 0.5640 - accuracy: 0.7142 - val_loss: 0.5642 - val_accuracy: 0.7234
Epoch 27/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.5637 - accuracy: 0.7151
Epoch 27: val_loss improved from 0.56378 to 0.56351, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5641 - accuracy: 0.7148 - val_loss: 0.5635 - val_accuracy: 0.7217
Epoch 28/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.5637 - accuracy: 0.7150
Epoch 28: val_loss did not improve from 0.56351
682/682 [==============================] - 2s 2ms/step - loss: 0.5638 - accuracy: 0.7151 - val_loss: 0.5635 - val_accuracy: 0.7223
Epoch 29/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.5636 - accuracy: 0.7171
Epoch 29: val_loss improved from 0.56351 to 0.56314, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5637 - accuracy: 0.7172 - val_loss: 0.5631 - val_accuracy: 0.7228
Epoch 30/60
682/682 [==============================] - ETA: 0s - loss: 0.5636 - accuracy: 0.7164
Epoch 30: val_loss improved from 0.56314 to 0.56302, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5636 - accuracy: 0.7164 - val_loss: 0.5630 - val_accuracy: 0.7240
Epoch 31/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.5639 - accuracy: 0.7167
Epoch 31: val_loss did not improve from 0.56302
682/682 [==============================] - 2s 2ms/step - loss: 0.5635 - accuracy: 0.7163 - val_loss: 0.5632 - val_accuracy: 0.7258
Epoch 32/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.5635 - accuracy: 0.7157
Epoch 32: val_loss improved from 0.56302 to 0.56286, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5633 - accuracy: 0.7166 - val_loss: 0.5629 - val_accuracy: 0.7234
Epoch 33/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.5632 - accuracy: 0.7160
Epoch 33: val_loss improved from 0.56286 to 0.56273, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5631 - accuracy: 0.7161 - val_loss: 0.5627 - val_accuracy: 0.7223
Epoch 34/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.5635 - accuracy: 0.7169
Epoch 34: val_loss improved from 0.56273 to 0.56264, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5631 - accuracy: 0.7163 - val_loss: 0.5626 - val_accuracy: 0.7217
Epoch 35/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.5620 - accuracy: 0.7177
Epoch 35: val_loss did not improve from 0.56264
682/682 [==============================] - 2s 2ms/step - loss: 0.5630 - accuracy: 0.7172 - val_loss: 0.5627 - val_accuracy: 0.7211
Epoch 36/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.5627 - accuracy: 0.7184
Epoch 36: val_loss improved from 0.56264 to 0.56245, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5629 - accuracy: 0.7182 - val_loss: 0.5625 - val_accuracy: 0.7234
Epoch 37/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.5630 - accuracy: 0.7163
Epoch 37: val_loss improved from 0.56245 to 0.56238, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5628 - accuracy: 0.7164 - val_loss: 0.5624 - val_accuracy: 0.7223
Epoch 38/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.5624 - accuracy: 0.7181
Epoch 38: val_loss did not improve from 0.56238
682/682 [==============================] - 2s 2ms/step - loss: 0.5627 - accuracy: 0.7180 - val_loss: 0.5624 - val_accuracy: 0.7217
Epoch 39/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.5624 - accuracy: 0.7177
Epoch 39: val_loss improved from 0.56238 to 0.56223, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5626 - accuracy: 0.7173 - val_loss: 0.5622 - val_accuracy: 0.7223
Epoch 40/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.5619 - accuracy: 0.7180
Epoch 40: val_loss did not improve from 0.56223
682/682 [==============================] - 2s 2ms/step - loss: 0.5625 - accuracy: 0.7170 - val_loss: 0.5622 - val_accuracy: 0.7228
Epoch 41/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.5634 - accuracy: 0.7170
Epoch 41: val_loss did not improve from 0.56223
682/682 [==============================] - 1s 2ms/step - loss: 0.5624 - accuracy: 0.7182 - val_loss: 0.5624 - val_accuracy: 0.7205
Epoch 42/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.5617 - accuracy: 0.7198
Epoch 42: val_loss did not improve from 0.56223
682/682 [==============================] - 2s 2ms/step - loss: 0.5625 - accuracy: 0.7192 - val_loss: 0.5624 - val_accuracy: 0.7211
Epoch 43/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.5628 - accuracy: 0.7185
Epoch 43: val_loss improved from 0.56223 to 0.56210, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5624 - accuracy: 0.7188 - val_loss: 0.5621 - val_accuracy: 0.7205
Epoch 44/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.5621 - accuracy: 0.7186
Epoch 44: val_loss improved from 0.56210 to 0.56203, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5622 - accuracy: 0.7185 - val_loss: 0.5620 - val_accuracy: 0.7205
Epoch 45/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.5628 - accuracy: 0.7200
Epoch 45: val_loss did not improve from 0.56203
682/682 [==============================] - 2s 2ms/step - loss: 0.5622 - accuracy: 0.7202 - val_loss: 0.5622 - val_accuracy: 0.7205
Epoch 46/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.5620 - accuracy: 0.7185
Epoch 46: val_loss improved from 0.56203 to 0.56200, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5621 - accuracy: 0.7185 - val_loss: 0.5620 - val_accuracy: 0.7223
Epoch 47/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.5626 - accuracy: 0.7181
Epoch 47: val_loss improved from 0.56200 to 0.56194, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5621 - accuracy: 0.7183 - val_loss: 0.5619 - val_accuracy: 0.7217
Epoch 48/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.5621 - accuracy: 0.7181
Epoch 48: val_loss improved from 0.56194 to 0.56190, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5620 - accuracy: 0.7182 - val_loss: 0.5619 - val_accuracy: 0.7228
Epoch 49/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.5635 - accuracy: 0.7191
Epoch 49: val_loss did not improve from 0.56190
682/682 [==============================] - 2s 2ms/step - loss: 0.5621 - accuracy: 0.7205 - val_loss: 0.5620 - val_accuracy: 0.7211
Epoch 50/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.5620 - accuracy: 0.7202
Epoch 50: val_loss did not improve from 0.56190
682/682 [==============================] - 2s 2ms/step - loss: 0.5619 - accuracy: 0.7201 - val_loss: 0.5620 - val_accuracy: 0.7205
Epoch 51/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.5618 - accuracy: 0.7204
Epoch 51: val_loss improved from 0.56190 to 0.56181, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5619 - accuracy: 0.7198 - val_loss: 0.5618 - val_accuracy: 0.7223
Epoch 52/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.5618 - accuracy: 0.7185
Epoch 52: val_loss did not improve from 0.56181
682/682 [==============================] - 2s 2ms/step - loss: 0.5616 - accuracy: 0.7189 - val_loss: 0.5618 - val_accuracy: 0.7217
Epoch 53/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.5631 - accuracy: 0.7188
Epoch 53: val_loss did not improve from 0.56181
682/682 [==============================] - 1s 2ms/step - loss: 0.5618 - accuracy: 0.7202 - val_loss: 0.5618 - val_accuracy: 0.7223
Epoch 54/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.5615 - accuracy: 0.7189
Epoch 54: val_loss did not improve from 0.56181
682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7195 - val_loss: 0.5620 - val_accuracy: 0.7217
Epoch 55/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.5616 - accuracy: 0.7206
Epoch 55: val_loss did not improve from 0.56181
682/682 [==============================] - 2s 2ms/step - loss: 0.5618 - accuracy: 0.7207 - val_loss: 0.5620 - val_accuracy: 0.7205
Epoch 56/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.5605 - accuracy: 0.7222
Epoch 56: val_loss improved from 0.56181 to 0.56173, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5618 - accuracy: 0.7208 - val_loss: 0.5617 - val_accuracy: 0.7199
Epoch 57/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.5630 - accuracy: 0.7186
Epoch 57: val_loss improved from 0.56173 to 0.56161, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5616 - accuracy: 0.7202 - val_loss: 0.5616 - val_accuracy: 0.7228
Epoch 58/60
654/682 [===========================&gt;..] - ETA: 0s - loss: 0.5595 - accuracy: 0.7214
Epoch 58: val_loss did not improve from 0.56161
682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7201 - val_loss: 0.5617 - val_accuracy: 0.7211
Epoch 59/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.5624 - accuracy: 0.7205
Epoch 59: val_loss did not improve from 0.56161
682/682 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.7205 - val_loss: 0.5617 - val_accuracy: 0.7211
Epoch 60/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.5612 - accuracy: 0.7218
Epoch 60: val_loss did not improve from 0.56161
682/682 [==============================] - 2s 2ms/step - loss: 0.5614 - accuracy: 0.7210 - val_loss: 0.5619 - val_accuracy: 0.7211
54/54 [==============================] - 0s 2ms/step - loss: 0.5616 - accuracy: 0.7228
Drop Feature retired
data frame x has shape: (8516, 14)
['education', 'campaign', 'pdays', 'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'married', 'single', 'admin.', 'blue-collar', 'student']
Epoch 1/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.6641 - accuracy: 0.6229
Epoch 1: val_loss improved from inf to 0.61953, saving model to best_drop.hdf5
682/682 [==============================] - 2s 3ms/step - loss: 0.6641 - accuracy: 0.6228 - val_loss: 0.6195 - val_accuracy: 0.6735
Epoch 2/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.6036 - accuracy: 0.6987
Epoch 2: val_loss improved from 0.61953 to 0.59141, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6036 - accuracy: 0.6987 - val_loss: 0.5914 - val_accuracy: 0.7134
Epoch 3/60
654/682 [===========================&gt;..] - ETA: 0s - loss: 0.5866 - accuracy: 0.7038
Epoch 3: val_loss improved from 0.59141 to 0.57951, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5859 - accuracy: 0.7050 - val_loss: 0.5795 - val_accuracy: 0.7240
Epoch 4/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.5789 - accuracy: 0.7085
Epoch 4: val_loss improved from 0.57951 to 0.57366, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5785 - accuracy: 0.7094 - val_loss: 0.5737 - val_accuracy: 0.7187
Epoch 5/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.5741 - accuracy: 0.7110
Epoch 5: val_loss improved from 0.57366 to 0.57073, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5746 - accuracy: 0.7108 - val_loss: 0.5707 - val_accuracy: 0.7164
Epoch 6/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.5733 - accuracy: 0.7093
Epoch 6: val_loss improved from 0.57073 to 0.56923, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5725 - accuracy: 0.7106 - val_loss: 0.5692 - val_accuracy: 0.7223
Epoch 7/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.5708 - accuracy: 0.7110
Epoch 7: val_loss improved from 0.56923 to 0.56790, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5714 - accuracy: 0.7103 - val_loss: 0.5679 - val_accuracy: 0.7164
Epoch 8/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.5711 - accuracy: 0.7100
Epoch 8: val_loss improved from 0.56790 to 0.56744, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5705 - accuracy: 0.7107 - val_loss: 0.5674 - val_accuracy: 0.7199
Epoch 9/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.5699 - accuracy: 0.7097
Epoch 9: val_loss improved from 0.56744 to 0.56671, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5699 - accuracy: 0.7100 - val_loss: 0.5667 - val_accuracy: 0.7158
Epoch 10/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.5691 - accuracy: 0.7101
Epoch 10: val_loss improved from 0.56671 to 0.56622, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5694 - accuracy: 0.7100 - val_loss: 0.5662 - val_accuracy: 0.7152
Epoch 11/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.5694 - accuracy: 0.7110
Epoch 11: val_loss improved from 0.56622 to 0.56584, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5688 - accuracy: 0.7113 - val_loss: 0.5658 - val_accuracy: 0.7170
Epoch 12/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.5681 - accuracy: 0.7118
Epoch 12: val_loss improved from 0.56584 to 0.56536, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5684 - accuracy: 0.7110 - val_loss: 0.5654 - val_accuracy: 0.7152
Epoch 13/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.5697 - accuracy: 0.7077
Epoch 13: val_loss improved from 0.56536 to 0.56503, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5680 - accuracy: 0.7100 - val_loss: 0.5650 - val_accuracy: 0.7158
Epoch 14/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.5686 - accuracy: 0.7105
Epoch 14: val_loss improved from 0.56503 to 0.56469, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5678 - accuracy: 0.7110 - val_loss: 0.5647 - val_accuracy: 0.7152
Epoch 15/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.5678 - accuracy: 0.7111
Epoch 15: val_loss improved from 0.56469 to 0.56463, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5673 - accuracy: 0.7114 - val_loss: 0.5646 - val_accuracy: 0.7187
Epoch 16/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.5671 - accuracy: 0.7101
Epoch 16: val_loss improved from 0.56463 to 0.56436, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5670 - accuracy: 0.7110 - val_loss: 0.5644 - val_accuracy: 0.7170
Epoch 17/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.5661 - accuracy: 0.7133
Epoch 17: val_loss did not improve from 0.56436
682/682 [==============================] - 2s 2ms/step - loss: 0.5667 - accuracy: 0.7128 - val_loss: 0.5646 - val_accuracy: 0.7187
Epoch 18/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.5660 - accuracy: 0.7129
Epoch 18: val_loss improved from 0.56436 to 0.56413, saving model to best_drop.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.5665 - accuracy: 0.7125 - val_loss: 0.5641 - val_accuracy: 0.7193
Epoch 19/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.5667 - accuracy: 0.7127
Epoch 19: val_loss improved from 0.56413 to 0.56369, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5663 - accuracy: 0.7132 - val_loss: 0.5637 - val_accuracy: 0.7164
Epoch 20/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.5669 - accuracy: 0.7101
Epoch 20: val_loss improved from 0.56369 to 0.56341, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5660 - accuracy: 0.7119 - val_loss: 0.5634 - val_accuracy: 0.7170
Epoch 21/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.5646 - accuracy: 0.7145
Epoch 21: val_loss did not improve from 0.56341
682/682 [==============================] - 2s 2ms/step - loss: 0.5659 - accuracy: 0.7139 - val_loss: 0.5635 - val_accuracy: 0.7170
Epoch 22/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.5657 - accuracy: 0.7125
Epoch 22: val_loss improved from 0.56341 to 0.56333, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5656 - accuracy: 0.7126 - val_loss: 0.5633 - val_accuracy: 0.7199
Epoch 23/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.5656 - accuracy: 0.7131
Epoch 23: val_loss improved from 0.56333 to 0.56306, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5655 - accuracy: 0.7129 - val_loss: 0.5631 - val_accuracy: 0.7187
Epoch 24/60
682/682 [==============================] - ETA: 0s - loss: 0.5653 - accuracy: 0.7135
Epoch 24: val_loss improved from 0.56306 to 0.56278, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5653 - accuracy: 0.7135 - val_loss: 0.5628 - val_accuracy: 0.7164
Epoch 25/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.5640 - accuracy: 0.7157
Epoch 25: val_loss improved from 0.56278 to 0.56266, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5652 - accuracy: 0.7145 - val_loss: 0.5627 - val_accuracy: 0.7158
Epoch 26/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.5643 - accuracy: 0.7149
Epoch 26: val_loss improved from 0.56266 to 0.56259, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5650 - accuracy: 0.7141 - val_loss: 0.5626 - val_accuracy: 0.7164
Epoch 27/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.5646 - accuracy: 0.7145
Epoch 27: val_loss improved from 0.56259 to 0.56248, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5649 - accuracy: 0.7142 - val_loss: 0.5625 - val_accuracy: 0.7170
Epoch 28/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.5659 - accuracy: 0.7128
Epoch 28: val_loss did not improve from 0.56248
682/682 [==============================] - 2s 2ms/step - loss: 0.5648 - accuracy: 0.7138 - val_loss: 0.5626 - val_accuracy: 0.7199
Epoch 29/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.5648 - accuracy: 0.7138
Epoch 29: val_loss improved from 0.56248 to 0.56236, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5647 - accuracy: 0.7139 - val_loss: 0.5624 - val_accuracy: 0.7170
Epoch 30/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.5648 - accuracy: 0.7140
Epoch 30: val_loss did not improve from 0.56236
682/682 [==============================] - 1s 2ms/step - loss: 0.5645 - accuracy: 0.7142 - val_loss: 0.5625 - val_accuracy: 0.7193
Epoch 31/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.5649 - accuracy: 0.7151
Epoch 31: val_loss improved from 0.56236 to 0.56213, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5645 - accuracy: 0.7151 - val_loss: 0.5621 - val_accuracy: 0.7193
Epoch 32/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.5653 - accuracy: 0.7140
Epoch 32: val_loss did not improve from 0.56213
682/682 [==============================] - 2s 2ms/step - loss: 0.5643 - accuracy: 0.7151 - val_loss: 0.5623 - val_accuracy: 0.7158
Epoch 33/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.5667 - accuracy: 0.7122
Epoch 33: val_loss did not improve from 0.56213
682/682 [==============================] - 1s 2ms/step - loss: 0.5643 - accuracy: 0.7145 - val_loss: 0.5621 - val_accuracy: 0.7193
Epoch 34/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.5645 - accuracy: 0.7151
Epoch 34: val_loss improved from 0.56213 to 0.56204, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5642 - accuracy: 0.7150 - val_loss: 0.5620 - val_accuracy: 0.7176
Epoch 35/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.5644 - accuracy: 0.7129
Epoch 35: val_loss did not improve from 0.56204
682/682 [==============================] - 2s 2ms/step - loss: 0.5641 - accuracy: 0.7135 - val_loss: 0.5622 - val_accuracy: 0.7193
Epoch 36/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.5636 - accuracy: 0.7161
Epoch 36: val_loss improved from 0.56204 to 0.56182, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5640 - accuracy: 0.7158 - val_loss: 0.5618 - val_accuracy: 0.7176
Epoch 37/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.5648 - accuracy: 0.7138
Epoch 37: val_loss did not improve from 0.56182
682/682 [==============================] - 2s 2ms/step - loss: 0.5641 - accuracy: 0.7144 - val_loss: 0.5618 - val_accuracy: 0.7176
Epoch 38/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.5636 - accuracy: 0.7154
Epoch 38: val_loss did not improve from 0.56182
682/682 [==============================] - 2s 2ms/step - loss: 0.5640 - accuracy: 0.7150 - val_loss: 0.5619 - val_accuracy: 0.7181
Epoch 39/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.5628 - accuracy: 0.7160
Epoch 39: val_loss improved from 0.56182 to 0.56164, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5639 - accuracy: 0.7147 - val_loss: 0.5616 - val_accuracy: 0.7176
Epoch 40/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.5639 - accuracy: 0.7137
Epoch 40: val_loss did not improve from 0.56164
682/682 [==============================] - 2s 2ms/step - loss: 0.5639 - accuracy: 0.7136 - val_loss: 0.5617 - val_accuracy: 0.7164
Epoch 41/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.5620 - accuracy: 0.7166
Epoch 41: val_loss improved from 0.56164 to 0.56146, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5638 - accuracy: 0.7151 - val_loss: 0.5615 - val_accuracy: 0.7181
Epoch 42/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.5627 - accuracy: 0.7160
Epoch 42: val_loss did not improve from 0.56146
682/682 [==============================] - 1s 2ms/step - loss: 0.5635 - accuracy: 0.7154 - val_loss: 0.5620 - val_accuracy: 0.7158
Epoch 43/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.5656 - accuracy: 0.7124
Epoch 43: val_loss improved from 0.56146 to 0.56141, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5638 - accuracy: 0.7142 - val_loss: 0.5614 - val_accuracy: 0.7199
Epoch 44/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.5631 - accuracy: 0.7169
Epoch 44: val_loss did not improve from 0.56141
682/682 [==============================] - 2s 2ms/step - loss: 0.5637 - accuracy: 0.7163 - val_loss: 0.5615 - val_accuracy: 0.7187
Epoch 45/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.5613 - accuracy: 0.7179
Epoch 45: val_loss did not improve from 0.56141
682/682 [==============================] - 2s 2ms/step - loss: 0.5636 - accuracy: 0.7160 - val_loss: 0.5615 - val_accuracy: 0.7181
Epoch 46/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.5627 - accuracy: 0.7176
Epoch 46: val_loss did not improve from 0.56141
682/682 [==============================] - 2s 2ms/step - loss: 0.5636 - accuracy: 0.7164 - val_loss: 0.5614 - val_accuracy: 0.7181
Epoch 47/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.5637 - accuracy: 0.7172
Epoch 47: val_loss did not improve from 0.56141
682/682 [==============================] - 2s 2ms/step - loss: 0.5636 - accuracy: 0.7176 - val_loss: 0.5616 - val_accuracy: 0.7176
Epoch 48/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.5642 - accuracy: 0.7151
Epoch 48: val_loss did not improve from 0.56141
682/682 [==============================] - 2s 2ms/step - loss: 0.5635 - accuracy: 0.7157 - val_loss: 0.5615 - val_accuracy: 0.7176
Epoch 49/60
682/682 [==============================] - ETA: 0s - loss: 0.5634 - accuracy: 0.7169
Epoch 49: val_loss did not improve from 0.56141
682/682 [==============================] - 2s 2ms/step - loss: 0.5634 - accuracy: 0.7169 - val_loss: 0.5614 - val_accuracy: 0.7187
Epoch 50/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.5641 - accuracy: 0.7157
Epoch 50: val_loss improved from 0.56141 to 0.56128, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5635 - accuracy: 0.7163 - val_loss: 0.5613 - val_accuracy: 0.7187
Epoch 51/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.5637 - accuracy: 0.7166
Epoch 51: val_loss did not improve from 0.56128
682/682 [==============================] - 2s 2ms/step - loss: 0.5634 - accuracy: 0.7169 - val_loss: 0.5613 - val_accuracy: 0.7187
Epoch 52/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.5635 - accuracy: 0.7167
Epoch 52: val_loss improved from 0.56128 to 0.56127, saving model to best_drop.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.5634 - accuracy: 0.7167 - val_loss: 0.5613 - val_accuracy: 0.7176
Epoch 53/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.5629 - accuracy: 0.7158
Epoch 53: val_loss improved from 0.56127 to 0.56121, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5635 - accuracy: 0.7155 - val_loss: 0.5612 - val_accuracy: 0.7181
Epoch 54/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.5623 - accuracy: 0.7191
Epoch 54: val_loss did not improve from 0.56121
682/682 [==============================] - 2s 2ms/step - loss: 0.5632 - accuracy: 0.7180 - val_loss: 0.5613 - val_accuracy: 0.7170
Epoch 55/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.5639 - accuracy: 0.7163
Epoch 55: val_loss did not improve from 0.56121
682/682 [==============================] - 2s 2ms/step - loss: 0.5633 - accuracy: 0.7169 - val_loss: 0.5614 - val_accuracy: 0.7170
Epoch 56/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.5618 - accuracy: 0.7177
Epoch 56: val_loss did not improve from 0.56121
682/682 [==============================] - 1s 2ms/step - loss: 0.5634 - accuracy: 0.7161 - val_loss: 0.5614 - val_accuracy: 0.7181
Epoch 57/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.5634 - accuracy: 0.7178
Epoch 57: val_loss improved from 0.56121 to 0.56117, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5634 - accuracy: 0.7177 - val_loss: 0.5612 - val_accuracy: 0.7158
Epoch 58/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.5627 - accuracy: 0.7166
Epoch 58: val_loss did not improve from 0.56117
682/682 [==============================] - 2s 2ms/step - loss: 0.5633 - accuracy: 0.7164 - val_loss: 0.5613 - val_accuracy: 0.7193
Epoch 59/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.5645 - accuracy: 0.7170
Epoch 59: val_loss did not improve from 0.56117
682/682 [==============================] - 2s 2ms/step - loss: 0.5633 - accuracy: 0.7176 - val_loss: 0.5613 - val_accuracy: 0.7181
Epoch 60/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.5644 - accuracy: 0.7184
Epoch 60: val_loss did not improve from 0.56117
682/682 [==============================] - 2s 2ms/step - loss: 0.5632 - accuracy: 0.7186 - val_loss: 0.5613 - val_accuracy: 0.7170
54/54 [==============================] - 0s 2ms/step - loss: 0.5612 - accuracy: 0.7158
Drop Feature student
data frame x has shape: (8516, 13)
['education', 'campaign', 'pdays', 'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'married', 'single', 'admin.', 'blue-collar']
Epoch 1/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.6691 - accuracy: 0.6054
Epoch 1: val_loss improved from inf to 0.62385, saving model to best_drop.hdf5
682/682 [==============================] - 2s 3ms/step - loss: 0.6674 - accuracy: 0.6080 - val_loss: 0.6239 - val_accuracy: 0.6782
Epoch 2/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.6153 - accuracy: 0.6902
Epoch 2: val_loss improved from 0.62385 to 0.59742, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6150 - accuracy: 0.6903 - val_loss: 0.5974 - val_accuracy: 0.7123
Epoch 3/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.5956 - accuracy: 0.7071
Epoch 3: val_loss improved from 0.59742 to 0.58435, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5954 - accuracy: 0.7073 - val_loss: 0.5844 - val_accuracy: 0.7264
Epoch 4/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.5854 - accuracy: 0.7142
Epoch 4: val_loss improved from 0.58435 to 0.57770, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5850 - accuracy: 0.7145 - val_loss: 0.5777 - val_accuracy: 0.7287
Epoch 5/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.5804 - accuracy: 0.7141
Epoch 5: val_loss improved from 0.57770 to 0.57385, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5799 - accuracy: 0.7142 - val_loss: 0.5738 - val_accuracy: 0.7311
Epoch 6/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.5766 - accuracy: 0.7110
Epoch 6: val_loss improved from 0.57385 to 0.57173, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5768 - accuracy: 0.7107 - val_loss: 0.5717 - val_accuracy: 0.7264
Epoch 7/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.5740 - accuracy: 0.7119
Epoch 7: val_loss improved from 0.57173 to 0.57048, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5748 - accuracy: 0.7113 - val_loss: 0.5705 - val_accuracy: 0.7287
Epoch 8/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.5734 - accuracy: 0.7105
Epoch 8: val_loss improved from 0.57048 to 0.56945, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5734 - accuracy: 0.7103 - val_loss: 0.5694 - val_accuracy: 0.7181
Epoch 9/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.5727 - accuracy: 0.7089
Epoch 9: val_loss improved from 0.56945 to 0.56878, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5723 - accuracy: 0.7095 - val_loss: 0.5688 - val_accuracy: 0.7170
Epoch 10/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.5709 - accuracy: 0.7111
Epoch 10: val_loss improved from 0.56878 to 0.56813, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5715 - accuracy: 0.7108 - val_loss: 0.5681 - val_accuracy: 0.7158
Epoch 11/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.5706 - accuracy: 0.7109
Epoch 11: val_loss improved from 0.56813 to 0.56777, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5709 - accuracy: 0.7107 - val_loss: 0.5678 - val_accuracy: 0.7211
Epoch 12/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.5706 - accuracy: 0.7104
Epoch 12: val_loss improved from 0.56777 to 0.56735, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5702 - accuracy: 0.7110 - val_loss: 0.5674 - val_accuracy: 0.7205
Epoch 13/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.5696 - accuracy: 0.7108
Epoch 13: val_loss improved from 0.56735 to 0.56664, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5698 - accuracy: 0.7108 - val_loss: 0.5666 - val_accuracy: 0.7176
Epoch 14/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.5692 - accuracy: 0.7105
Epoch 14: val_loss improved from 0.56664 to 0.56627, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5694 - accuracy: 0.7100 - val_loss: 0.5663 - val_accuracy: 0.7170
Epoch 15/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.5686 - accuracy: 0.7108
Epoch 15: val_loss improved from 0.56627 to 0.56600, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5689 - accuracy: 0.7103 - val_loss: 0.5660 - val_accuracy: 0.7176
Epoch 16/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.5689 - accuracy: 0.7096
Epoch 16: val_loss did not improve from 0.56600
682/682 [==============================] - 2s 2ms/step - loss: 0.5685 - accuracy: 0.7100 - val_loss: 0.5660 - val_accuracy: 0.7193
Epoch 17/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.5684 - accuracy: 0.7111
Epoch 17: val_loss improved from 0.56600 to 0.56544, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5683 - accuracy: 0.7104 - val_loss: 0.5654 - val_accuracy: 0.7176
Epoch 18/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.5691 - accuracy: 0.7094
Epoch 18: val_loss did not improve from 0.56544
682/682 [==============================] - 2s 2ms/step - loss: 0.5675 - accuracy: 0.7104 - val_loss: 0.5663 - val_accuracy: 0.7217
Epoch 19/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.5669 - accuracy: 0.7141
Epoch 19: val_loss did not improve from 0.56544
682/682 [==============================] - 2s 2ms/step - loss: 0.5676 - accuracy: 0.7133 - val_loss: 0.5657 - val_accuracy: 0.7164
Epoch 20/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.5682 - accuracy: 0.7117
Epoch 20: val_loss improved from 0.56544 to 0.56491, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5673 - accuracy: 0.7126 - val_loss: 0.5649 - val_accuracy: 0.7176
Epoch 21/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.5691 - accuracy: 0.7111
Epoch 21: val_loss improved from 0.56491 to 0.56473, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5669 - accuracy: 0.7136 - val_loss: 0.5647 - val_accuracy: 0.7181
Epoch 22/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.5682 - accuracy: 0.7127
Epoch 22: val_loss improved from 0.56473 to 0.56455, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5667 - accuracy: 0.7136 - val_loss: 0.5646 - val_accuracy: 0.7170
Epoch 23/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.5671 - accuracy: 0.7114
Epoch 23: val_loss improved from 0.56455 to 0.56410, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5664 - accuracy: 0.7122 - val_loss: 0.5641 - val_accuracy: 0.7176
Epoch 24/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.5662 - accuracy: 0.7130
Epoch 24: val_loss did not improve from 0.56410
682/682 [==============================] - 2s 2ms/step - loss: 0.5662 - accuracy: 0.7136 - val_loss: 0.5644 - val_accuracy: 0.7187
Epoch 25/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.5662 - accuracy: 0.7141
Epoch 25: val_loss improved from 0.56410 to 0.56383, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5663 - accuracy: 0.7141 - val_loss: 0.5638 - val_accuracy: 0.7152
Epoch 26/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.5653 - accuracy: 0.7159
Epoch 26: val_loss improved from 0.56383 to 0.56369, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5661 - accuracy: 0.7153 - val_loss: 0.5637 - val_accuracy: 0.7152
Epoch 27/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.5659 - accuracy: 0.7143
Epoch 27: val_loss improved from 0.56369 to 0.56364, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5658 - accuracy: 0.7144 - val_loss: 0.5636 - val_accuracy: 0.7152
Epoch 28/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.5657 - accuracy: 0.7154
Epoch 28: val_loss improved from 0.56364 to 0.56357, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5658 - accuracy: 0.7154 - val_loss: 0.5636 - val_accuracy: 0.7176
Epoch 29/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.5642 - accuracy: 0.7156
Epoch 29: val_loss did not improve from 0.56357
682/682 [==============================] - 1s 2ms/step - loss: 0.5654 - accuracy: 0.7151 - val_loss: 0.5642 - val_accuracy: 0.7164
Epoch 30/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.5665 - accuracy: 0.7137
Epoch 30: val_loss improved from 0.56357 to 0.56343, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5655 - accuracy: 0.7148 - val_loss: 0.5634 - val_accuracy: 0.7193
Epoch 31/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.5653 - accuracy: 0.7149
Epoch 31: val_loss did not improve from 0.56343
682/682 [==============================] - 2s 2ms/step - loss: 0.5653 - accuracy: 0.7145 - val_loss: 0.5635 - val_accuracy: 0.7176
Epoch 32/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.5651 - accuracy: 0.7150
Epoch 32: val_loss did not improve from 0.56343
682/682 [==============================] - 2s 2ms/step - loss: 0.5651 - accuracy: 0.7151 - val_loss: 0.5637 - val_accuracy: 0.7164
Epoch 33/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.5650 - accuracy: 0.7143
Epoch 33: val_loss improved from 0.56343 to 0.56308, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5651 - accuracy: 0.7145 - val_loss: 0.5631 - val_accuracy: 0.7164
Epoch 34/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.5654 - accuracy: 0.7142
Epoch 34: val_loss did not improve from 0.56308
682/682 [==============================] - 2s 2ms/step - loss: 0.5651 - accuracy: 0.7145 - val_loss: 0.5634 - val_accuracy: 0.7164
Epoch 35/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.5650 - accuracy: 0.7160
Epoch 35: val_loss improved from 0.56308 to 0.56303, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5651 - accuracy: 0.7158 - val_loss: 0.5630 - val_accuracy: 0.7158
Epoch 36/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.5653 - accuracy: 0.7132
Epoch 36: val_loss improved from 0.56303 to 0.56284, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5649 - accuracy: 0.7145 - val_loss: 0.5628 - val_accuracy: 0.7164
Epoch 37/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.5639 - accuracy: 0.7152
Epoch 37: val_loss did not improve from 0.56284
682/682 [==============================] - 2s 2ms/step - loss: 0.5649 - accuracy: 0.7150 - val_loss: 0.5629 - val_accuracy: 0.7176
Epoch 38/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.5646 - accuracy: 0.7151
Epoch 38: val_loss did not improve from 0.56284
682/682 [==============================] - 2s 2ms/step - loss: 0.5645 - accuracy: 0.7154 - val_loss: 0.5630 - val_accuracy: 0.7158
Epoch 39/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.5653 - accuracy: 0.7150
Epoch 39: val_loss did not improve from 0.56284
682/682 [==============================] - 2s 2ms/step - loss: 0.5647 - accuracy: 0.7154 - val_loss: 0.5630 - val_accuracy: 0.7158
Epoch 40/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.5650 - accuracy: 0.7163
Epoch 40: val_loss improved from 0.56284 to 0.56274, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5647 - accuracy: 0.7157 - val_loss: 0.5627 - val_accuracy: 0.7170
Epoch 41/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.5651 - accuracy: 0.7141
Epoch 41: val_loss did not improve from 0.56274
682/682 [==============================] - 2s 2ms/step - loss: 0.5645 - accuracy: 0.7142 - val_loss: 0.5628 - val_accuracy: 0.7170
Epoch 42/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.5643 - accuracy: 0.7149
Epoch 42: val_loss did not improve from 0.56274
682/682 [==============================] - 2s 2ms/step - loss: 0.5644 - accuracy: 0.7151 - val_loss: 0.5629 - val_accuracy: 0.7158
Epoch 43/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.5665 - accuracy: 0.7135
Epoch 43: val_loss did not improve from 0.56274
682/682 [==============================] - 2s 2ms/step - loss: 0.5643 - accuracy: 0.7160 - val_loss: 0.5628 - val_accuracy: 0.7164
Epoch 44/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.5640 - accuracy: 0.7166
Epoch 44: val_loss improved from 0.56274 to 0.56250, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5645 - accuracy: 0.7155 - val_loss: 0.5625 - val_accuracy: 0.7170
Epoch 45/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.5642 - accuracy: 0.7157
Epoch 45: val_loss improved from 0.56250 to 0.56244, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5644 - accuracy: 0.7155 - val_loss: 0.5624 - val_accuracy: 0.7176
Epoch 46/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.5635 - accuracy: 0.7168
Epoch 46: val_loss did not improve from 0.56244
682/682 [==============================] - 2s 2ms/step - loss: 0.5643 - accuracy: 0.7157 - val_loss: 0.5625 - val_accuracy: 0.7176
Epoch 47/60
654/682 [===========================&gt;..] - ETA: 0s - loss: 0.5652 - accuracy: 0.7150
Epoch 47: val_loss improved from 0.56244 to 0.56244, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5642 - accuracy: 0.7157 - val_loss: 0.5624 - val_accuracy: 0.7158
Epoch 48/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.5647 - accuracy: 0.7158
Epoch 48: val_loss improved from 0.56244 to 0.56232, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5643 - accuracy: 0.7164 - val_loss: 0.5623 - val_accuracy: 0.7158
Epoch 49/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.5646 - accuracy: 0.7156
Epoch 49: val_loss did not improve from 0.56232
682/682 [==============================] - 2s 2ms/step - loss: 0.5643 - accuracy: 0.7157 - val_loss: 0.5623 - val_accuracy: 0.7176
Epoch 50/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.5642 - accuracy: 0.7164
Epoch 50: val_loss did not improve from 0.56232
682/682 [==============================] - 2s 2ms/step - loss: 0.5642 - accuracy: 0.7164 - val_loss: 0.5624 - val_accuracy: 0.7164
Epoch 51/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.5617 - accuracy: 0.7194
Epoch 51: val_loss did not improve from 0.56232
682/682 [==============================] - 2s 2ms/step - loss: 0.5641 - accuracy: 0.7166 - val_loss: 0.5625 - val_accuracy: 0.7164
Epoch 52/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.5643 - accuracy: 0.7161
Epoch 52: val_loss improved from 0.56232 to 0.56225, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5642 - accuracy: 0.7164 - val_loss: 0.5623 - val_accuracy: 0.7164
Epoch 53/60
682/682 [==============================] - ETA: 0s - loss: 0.5642 - accuracy: 0.7158
Epoch 53: val_loss improved from 0.56225 to 0.56224, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5642 - accuracy: 0.7158 - val_loss: 0.5622 - val_accuracy: 0.7158
Epoch 54/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.5640 - accuracy: 0.7168
Epoch 54: val_loss did not improve from 0.56224
682/682 [==============================] - 2s 2ms/step - loss: 0.5641 - accuracy: 0.7167 - val_loss: 0.5623 - val_accuracy: 0.7170
Epoch 55/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.5642 - accuracy: 0.7167
Epoch 55: val_loss improved from 0.56224 to 0.56223, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5641 - accuracy: 0.7166 - val_loss: 0.5622 - val_accuracy: 0.7158
Epoch 56/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.5652 - accuracy: 0.7151
Epoch 56: val_loss improved from 0.56223 to 0.56220, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5640 - accuracy: 0.7161 - val_loss: 0.5622 - val_accuracy: 0.7164
Epoch 57/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.5642 - accuracy: 0.7174
Epoch 57: val_loss did not improve from 0.56220
682/682 [==============================] - 2s 2ms/step - loss: 0.5639 - accuracy: 0.7176 - val_loss: 0.5622 - val_accuracy: 0.7170
Epoch 58/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.5642 - accuracy: 0.7168
Epoch 58: val_loss improved from 0.56220 to 0.56214, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5641 - accuracy: 0.7166 - val_loss: 0.5621 - val_accuracy: 0.7164
Epoch 59/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.5627 - accuracy: 0.7179
Epoch 59: val_loss improved from 0.56214 to 0.56210, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5640 - accuracy: 0.7163 - val_loss: 0.5621 - val_accuracy: 0.7176
Epoch 60/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.5648 - accuracy: 0.7159
Epoch 60: val_loss did not improve from 0.56210
682/682 [==============================] - 2s 2ms/step - loss: 0.5639 - accuracy: 0.7173 - val_loss: 0.5623 - val_accuracy: 0.7170
54/54 [==============================] - 0s 2ms/step - loss: 0.5621 - accuracy: 0.7176
Drop Feature campaign
data frame x has shape: (8516, 12)
['education', 'pdays', 'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'married', 'single', 'admin.', 'blue-collar']
Epoch 1/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.6710 - accuracy: 0.6000
Epoch 1: val_loss improved from inf to 0.62838, saving model to best_drop.hdf5
682/682 [==============================] - 2s 3ms/step - loss: 0.6709 - accuracy: 0.6000 - val_loss: 0.6284 - val_accuracy: 0.6788
Epoch 2/60
650/682 [===========================&gt;..] - ETA: 0s - loss: 0.6161 - accuracy: 0.6932
Epoch 2: val_loss improved from 0.62838 to 0.59784, saving model to best_drop.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6152 - accuracy: 0.6934 - val_loss: 0.5978 - val_accuracy: 0.7041
Epoch 3/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.5948 - accuracy: 0.7064
Epoch 3: val_loss improved from 0.59784 to 0.58425, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5951 - accuracy: 0.7060 - val_loss: 0.5842 - val_accuracy: 0.7076
Epoch 4/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.5855 - accuracy: 0.7084
Epoch 4: val_loss improved from 0.58425 to 0.57776, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5857 - accuracy: 0.7081 - val_loss: 0.5778 - val_accuracy: 0.7105
Epoch 5/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.5813 - accuracy: 0.7084
Epoch 5: val_loss improved from 0.57776 to 0.57463, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5811 - accuracy: 0.7085 - val_loss: 0.5746 - val_accuracy: 0.7087
Epoch 6/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.5783 - accuracy: 0.7100
Epoch 6: val_loss improved from 0.57463 to 0.57247, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5783 - accuracy: 0.7095 - val_loss: 0.5725 - val_accuracy: 0.7111
Epoch 7/60
682/682 [==============================] - ETA: 0s - loss: 0.5766 - accuracy: 0.7107
Epoch 7: val_loss improved from 0.57247 to 0.57119, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5766 - accuracy: 0.7107 - val_loss: 0.5712 - val_accuracy: 0.7123
Epoch 8/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.5756 - accuracy: 0.7112
Epoch 8: val_loss improved from 0.57119 to 0.57017, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5752 - accuracy: 0.7116 - val_loss: 0.5702 - val_accuracy: 0.7164
Epoch 9/60
682/682 [==============================] - ETA: 0s - loss: 0.5741 - accuracy: 0.7125
Epoch 9: val_loss improved from 0.57017 to 0.56943, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5741 - accuracy: 0.7125 - val_loss: 0.5694 - val_accuracy: 0.7140
Epoch 10/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.5730 - accuracy: 0.7117
Epoch 10: val_loss improved from 0.56943 to 0.56879, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5734 - accuracy: 0.7111 - val_loss: 0.5688 - val_accuracy: 0.7176
Epoch 11/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.5726 - accuracy: 0.7134
Epoch 11: val_loss improved from 0.56879 to 0.56834, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5725 - accuracy: 0.7133 - val_loss: 0.5683 - val_accuracy: 0.7134
Epoch 12/60
653/682 [===========================&gt;..] - ETA: 0s - loss: 0.5726 - accuracy: 0.7095
Epoch 12: val_loss improved from 0.56834 to 0.56793, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5719 - accuracy: 0.7108 - val_loss: 0.5679 - val_accuracy: 0.7134
Epoch 13/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.5709 - accuracy: 0.7119
Epoch 13: val_loss improved from 0.56793 to 0.56746, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5712 - accuracy: 0.7111 - val_loss: 0.5675 - val_accuracy: 0.7181
Epoch 14/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.5703 - accuracy: 0.7134
Epoch 14: val_loss improved from 0.56746 to 0.56736, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5707 - accuracy: 0.7129 - val_loss: 0.5674 - val_accuracy: 0.7176
Epoch 15/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.5703 - accuracy: 0.7130
Epoch 15: val_loss improved from 0.56736 to 0.56701, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5701 - accuracy: 0.7133 - val_loss: 0.5670 - val_accuracy: 0.7129
Epoch 16/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.5693 - accuracy: 0.7128
Epoch 16: val_loss improved from 0.56701 to 0.56638, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5698 - accuracy: 0.7116 - val_loss: 0.5664 - val_accuracy: 0.7176
Epoch 17/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.5691 - accuracy: 0.7125
Epoch 17: val_loss improved from 0.56638 to 0.56634, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5690 - accuracy: 0.7126 - val_loss: 0.5663 - val_accuracy: 0.7134
Epoch 18/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.5692 - accuracy: 0.7127
Epoch 18: val_loss improved from 0.56634 to 0.56586, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5690 - accuracy: 0.7123 - val_loss: 0.5659 - val_accuracy: 0.7134
Epoch 19/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.5692 - accuracy: 0.7128
Epoch 19: val_loss improved from 0.56586 to 0.56548, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5687 - accuracy: 0.7128 - val_loss: 0.5655 - val_accuracy: 0.7140
Epoch 20/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.5686 - accuracy: 0.7129
Epoch 20: val_loss improved from 0.56548 to 0.56535, saving model to best_drop.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.5684 - accuracy: 0.7129 - val_loss: 0.5654 - val_accuracy: 0.7187
Epoch 21/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.5687 - accuracy: 0.7115
Epoch 21: val_loss improved from 0.56535 to 0.56525, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5680 - accuracy: 0.7123 - val_loss: 0.5652 - val_accuracy: 0.7176
Epoch 22/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.5675 - accuracy: 0.7122
Epoch 22: val_loss improved from 0.56525 to 0.56501, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5678 - accuracy: 0.7122 - val_loss: 0.5650 - val_accuracy: 0.7181
Epoch 23/60
682/682 [==============================] - ETA: 0s - loss: 0.5674 - accuracy: 0.7129
Epoch 23: val_loss improved from 0.56501 to 0.56489, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5674 - accuracy: 0.7129 - val_loss: 0.5649 - val_accuracy: 0.7134
Epoch 24/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.5663 - accuracy: 0.7139
Epoch 24: val_loss improved from 0.56489 to 0.56460, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5672 - accuracy: 0.7138 - val_loss: 0.5646 - val_accuracy: 0.7146
Epoch 25/60
654/682 [===========================&gt;..] - ETA: 0s - loss: 0.5679 - accuracy: 0.7124
Epoch 25: val_loss did not improve from 0.56460
682/682 [==============================] - 2s 2ms/step - loss: 0.5671 - accuracy: 0.7130 - val_loss: 0.5646 - val_accuracy: 0.7181
Epoch 26/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.5670 - accuracy: 0.7133
Epoch 26: val_loss improved from 0.56460 to 0.56432, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5669 - accuracy: 0.7128 - val_loss: 0.5643 - val_accuracy: 0.7140
Epoch 27/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.5663 - accuracy: 0.7135
Epoch 27: val_loss did not improve from 0.56432
682/682 [==============================] - 2s 2ms/step - loss: 0.5667 - accuracy: 0.7129 - val_loss: 0.5644 - val_accuracy: 0.7181
Epoch 28/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.5665 - accuracy: 0.7135
Epoch 28: val_loss improved from 0.56432 to 0.56410, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5665 - accuracy: 0.7135 - val_loss: 0.5641 - val_accuracy: 0.7146
Epoch 29/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.5661 - accuracy: 0.7144
Epoch 29: val_loss improved from 0.56410 to 0.56406, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5664 - accuracy: 0.7139 - val_loss: 0.5641 - val_accuracy: 0.7146
Epoch 30/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.5656 - accuracy: 0.7133
Epoch 30: val_loss did not improve from 0.56406
682/682 [==============================] - 1s 2ms/step - loss: 0.5662 - accuracy: 0.7130 - val_loss: 0.5641 - val_accuracy: 0.7140
Epoch 31/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.5663 - accuracy: 0.7146
Epoch 31: val_loss did not improve from 0.56406
682/682 [==============================] - 2s 2ms/step - loss: 0.5660 - accuracy: 0.7147 - val_loss: 0.5641 - val_accuracy: 0.7140
Epoch 32/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.5661 - accuracy: 0.7137
Epoch 32: val_loss improved from 0.56406 to 0.56391, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5660 - accuracy: 0.7139 - val_loss: 0.5639 - val_accuracy: 0.7140
Epoch 33/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.5660 - accuracy: 0.7138
Epoch 33: val_loss improved from 0.56391 to 0.56373, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5658 - accuracy: 0.7136 - val_loss: 0.5637 - val_accuracy: 0.7146
Epoch 34/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.5644 - accuracy: 0.7153
Epoch 34: val_loss improved from 0.56373 to 0.56370, saving model to best_drop.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.5657 - accuracy: 0.7141 - val_loss: 0.5637 - val_accuracy: 0.7140
Epoch 35/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.5655 - accuracy: 0.7147
Epoch 35: val_loss improved from 0.56370 to 0.56359, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5657 - accuracy: 0.7142 - val_loss: 0.5636 - val_accuracy: 0.7152
Epoch 36/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.5656 - accuracy: 0.7139
Epoch 36: val_loss improved from 0.56359 to 0.56347, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5656 - accuracy: 0.7139 - val_loss: 0.5635 - val_accuracy: 0.7140
Epoch 37/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.5657 - accuracy: 0.7139
Epoch 37: val_loss improved from 0.56347 to 0.56334, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5655 - accuracy: 0.7135 - val_loss: 0.5633 - val_accuracy: 0.7152
Epoch 38/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.5656 - accuracy: 0.7135
Epoch 38: val_loss improved from 0.56334 to 0.56325, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5654 - accuracy: 0.7138 - val_loss: 0.5633 - val_accuracy: 0.7158
Epoch 39/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.5644 - accuracy: 0.7150
Epoch 39: val_loss did not improve from 0.56325
682/682 [==============================] - 2s 2ms/step - loss: 0.5653 - accuracy: 0.7139 - val_loss: 0.5633 - val_accuracy: 0.7146
Epoch 40/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.5663 - accuracy: 0.7127
Epoch 40: val_loss did not improve from 0.56325
682/682 [==============================] - 2s 2ms/step - loss: 0.5652 - accuracy: 0.7139 - val_loss: 0.5635 - val_accuracy: 0.7152
Epoch 41/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.5658 - accuracy: 0.7141
Epoch 41: val_loss did not improve from 0.56325
682/682 [==============================] - 1s 2ms/step - loss: 0.5651 - accuracy: 0.7147 - val_loss: 0.5634 - val_accuracy: 0.7140
Epoch 42/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.5651 - accuracy: 0.7154
Epoch 42: val_loss did not improve from 0.56325
682/682 [==============================] - 1s 2ms/step - loss: 0.5651 - accuracy: 0.7150 - val_loss: 0.5634 - val_accuracy: 0.7158
Epoch 43/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.5643 - accuracy: 0.7146
Epoch 43: val_loss improved from 0.56325 to 0.56325, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5651 - accuracy: 0.7139 - val_loss: 0.5632 - val_accuracy: 0.7181
Epoch 44/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.5654 - accuracy: 0.7143
Epoch 44: val_loss did not improve from 0.56325
682/682 [==============================] - 2s 2ms/step - loss: 0.5648 - accuracy: 0.7150 - val_loss: 0.5634 - val_accuracy: 0.7164
Epoch 45/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.5648 - accuracy: 0.7147
Epoch 45: val_loss did not improve from 0.56325
682/682 [==============================] - 2s 2ms/step - loss: 0.5648 - accuracy: 0.7147 - val_loss: 0.5639 - val_accuracy: 0.7158
Epoch 46/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.5655 - accuracy: 0.7144
Epoch 46: val_loss improved from 0.56325 to 0.56318, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5650 - accuracy: 0.7142 - val_loss: 0.5632 - val_accuracy: 0.7158
Epoch 47/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.5643 - accuracy: 0.7153
Epoch 47: val_loss did not improve from 0.56318
682/682 [==============================] - 2s 2ms/step - loss: 0.5647 - accuracy: 0.7153 - val_loss: 0.5634 - val_accuracy: 0.7158
Epoch 48/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.5651 - accuracy: 0.7151
Epoch 48: val_loss improved from 0.56318 to 0.56309, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5649 - accuracy: 0.7151 - val_loss: 0.5631 - val_accuracy: 0.7140
Epoch 49/60
682/682 [==============================] - ETA: 0s - loss: 0.5648 - accuracy: 0.7148
Epoch 49: val_loss did not improve from 0.56309
682/682 [==============================] - 2s 2ms/step - loss: 0.5648 - accuracy: 0.7148 - val_loss: 0.5632 - val_accuracy: 0.7152
Epoch 50/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.5653 - accuracy: 0.7142
Epoch 50: val_loss did not improve from 0.56309
682/682 [==============================] - 2s 2ms/step - loss: 0.5647 - accuracy: 0.7150 - val_loss: 0.5631 - val_accuracy: 0.7146
Epoch 51/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.5647 - accuracy: 0.7149
Epoch 51: val_loss did not improve from 0.56309
682/682 [==============================] - 2s 2ms/step - loss: 0.5647 - accuracy: 0.7150 - val_loss: 0.5631 - val_accuracy: 0.7134
Epoch 52/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.5640 - accuracy: 0.7162
Epoch 52: val_loss did not improve from 0.56309
682/682 [==============================] - 2s 2ms/step - loss: 0.5643 - accuracy: 0.7158 - val_loss: 0.5633 - val_accuracy: 0.7146
Epoch 53/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.5657 - accuracy: 0.7142
Epoch 53: val_loss did not improve from 0.56309
682/682 [==============================] - 2s 2ms/step - loss: 0.5646 - accuracy: 0.7148 - val_loss: 0.5631 - val_accuracy: 0.7140
Epoch 54/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.5641 - accuracy: 0.7161
Epoch 54: val_loss did not improve from 0.56309
682/682 [==============================] - 2s 2ms/step - loss: 0.5646 - accuracy: 0.7160 - val_loss: 0.5631 - val_accuracy: 0.7129
Epoch 55/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.5649 - accuracy: 0.7162
Epoch 55: val_loss did not improve from 0.56309
682/682 [==============================] - 1s 2ms/step - loss: 0.5646 - accuracy: 0.7160 - val_loss: 0.5631 - val_accuracy: 0.7140
Epoch 56/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.5645 - accuracy: 0.7135
Epoch 56: val_loss did not improve from 0.56309
682/682 [==============================] - 1s 2ms/step - loss: 0.5646 - accuracy: 0.7138 - val_loss: 0.5631 - val_accuracy: 0.7123
Epoch 57/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.5637 - accuracy: 0.7159
Epoch 57: val_loss did not improve from 0.56309
682/682 [==============================] - 2s 2ms/step - loss: 0.5646 - accuracy: 0.7153 - val_loss: 0.5631 - val_accuracy: 0.7123
Epoch 58/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.5659 - accuracy: 0.7140
Epoch 58: val_loss did not improve from 0.56309
682/682 [==============================] - 2s 2ms/step - loss: 0.5643 - accuracy: 0.7151 - val_loss: 0.5635 - val_accuracy: 0.7170
Epoch 59/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.5649 - accuracy: 0.7153
Epoch 59: val_loss did not improve from 0.56309
682/682 [==============================] - 2s 2ms/step - loss: 0.5643 - accuracy: 0.7154 - val_loss: 0.5634 - val_accuracy: 0.7176
Epoch 60/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.5640 - accuracy: 0.7165
Epoch 60: val_loss improved from 0.56309 to 0.56306, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5646 - accuracy: 0.7153 - val_loss: 0.5631 - val_accuracy: 0.7146
54/54 [==============================] - 0s 2ms/step - loss: 0.5631 - accuracy: 0.7146
Drop Feature married
data frame x has shape: (8516, 11)
['education', 'pdays', 'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'single', 'admin.', 'blue-collar']
Epoch 1/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.6562 - accuracy: 0.6310
Epoch 1: val_loss improved from inf to 0.62407, saving model to best_drop.hdf5
682/682 [==============================] - 2s 3ms/step - loss: 0.6563 - accuracy: 0.6307 - val_loss: 0.6241 - val_accuracy: 0.6894
Epoch 2/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.6048 - accuracy: 0.7040
Epoch 2: val_loss improved from 0.62407 to 0.59213, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6050 - accuracy: 0.7032 - val_loss: 0.5921 - val_accuracy: 0.7123
Epoch 3/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.5860 - accuracy: 0.7075
Epoch 3: val_loss improved from 0.59213 to 0.57890, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5863 - accuracy: 0.7066 - val_loss: 0.5789 - val_accuracy: 0.7158
Epoch 4/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.5776 - accuracy: 0.7130
Epoch 4: val_loss improved from 0.57890 to 0.57390, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5784 - accuracy: 0.7116 - val_loss: 0.5739 - val_accuracy: 0.7193
Epoch 5/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.5736 - accuracy: 0.7133
Epoch 5: val_loss improved from 0.57390 to 0.57079, saving model to best_drop.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.5750 - accuracy: 0.7116 - val_loss: 0.5708 - val_accuracy: 0.7170
Epoch 6/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.5741 - accuracy: 0.7101
Epoch 6: val_loss improved from 0.57079 to 0.56890, saving model to best_drop.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.5729 - accuracy: 0.7107 - val_loss: 0.5689 - val_accuracy: 0.7181
Epoch 7/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.5702 - accuracy: 0.7123
Epoch 7: val_loss improved from 0.56890 to 0.56783, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5717 - accuracy: 0.7107 - val_loss: 0.5678 - val_accuracy: 0.7134
Epoch 8/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.5703 - accuracy: 0.7092
Epoch 8: val_loss improved from 0.56783 to 0.56728, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5709 - accuracy: 0.7089 - val_loss: 0.5673 - val_accuracy: 0.7140
Epoch 9/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.5697 - accuracy: 0.7104
Epoch 9: val_loss improved from 0.56728 to 0.56678, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5702 - accuracy: 0.7098 - val_loss: 0.5668 - val_accuracy: 0.7146
Epoch 10/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.5704 - accuracy: 0.7084
Epoch 10: val_loss improved from 0.56678 to 0.56674, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5697 - accuracy: 0.7098 - val_loss: 0.5667 - val_accuracy: 0.7134
Epoch 11/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.5687 - accuracy: 0.7106
Epoch 11: val_loss improved from 0.56674 to 0.56631, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5693 - accuracy: 0.7092 - val_loss: 0.5663 - val_accuracy: 0.7140
Epoch 12/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.5699 - accuracy: 0.7103
Epoch 12: val_loss improved from 0.56631 to 0.56594, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5688 - accuracy: 0.7110 - val_loss: 0.5659 - val_accuracy: 0.7129
Epoch 13/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.5707 - accuracy: 0.7092
Epoch 13: val_loss improved from 0.56594 to 0.56573, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5686 - accuracy: 0.7117 - val_loss: 0.5657 - val_accuracy: 0.7123
Epoch 14/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.5673 - accuracy: 0.7115
Epoch 14: val_loss improved from 0.56573 to 0.56555, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5682 - accuracy: 0.7108 - val_loss: 0.5655 - val_accuracy: 0.7152
Epoch 15/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.5684 - accuracy: 0.7112
Epoch 15: val_loss improved from 0.56555 to 0.56524, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5680 - accuracy: 0.7119 - val_loss: 0.5652 - val_accuracy: 0.7129
Epoch 16/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.5673 - accuracy: 0.7109
Epoch 16: val_loss did not improve from 0.56524
682/682 [==============================] - 2s 2ms/step - loss: 0.5676 - accuracy: 0.7113 - val_loss: 0.5653 - val_accuracy: 0.7140
Epoch 17/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.5670 - accuracy: 0.7120
Epoch 17: val_loss improved from 0.56524 to 0.56490, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5674 - accuracy: 0.7110 - val_loss: 0.5649 - val_accuracy: 0.7158
Epoch 18/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.5666 - accuracy: 0.7126
Epoch 18: val_loss improved from 0.56490 to 0.56469, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5674 - accuracy: 0.7120 - val_loss: 0.5647 - val_accuracy: 0.7164
Epoch 19/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.5669 - accuracy: 0.7116
Epoch 19: val_loss improved from 0.56469 to 0.56466, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5671 - accuracy: 0.7119 - val_loss: 0.5647 - val_accuracy: 0.7164
Epoch 20/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.5676 - accuracy: 0.7105
Epoch 20: val_loss improved from 0.56466 to 0.56454, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5668 - accuracy: 0.7113 - val_loss: 0.5645 - val_accuracy: 0.7158
Epoch 21/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.5668 - accuracy: 0.7130
Epoch 21: val_loss improved from 0.56454 to 0.56428, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5667 - accuracy: 0.7132 - val_loss: 0.5643 - val_accuracy: 0.7170
Epoch 22/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.5676 - accuracy: 0.7121
Epoch 22: val_loss did not improve from 0.56428
682/682 [==============================] - 2s 2ms/step - loss: 0.5664 - accuracy: 0.7130 - val_loss: 0.5647 - val_accuracy: 0.7134
Epoch 23/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.5663 - accuracy: 0.7114
Epoch 23: val_loss improved from 0.56428 to 0.56425, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5664 - accuracy: 0.7114 - val_loss: 0.5642 - val_accuracy: 0.7170
Epoch 24/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.5662 - accuracy: 0.7129
Epoch 24: val_loss improved from 0.56425 to 0.56405, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5662 - accuracy: 0.7129 - val_loss: 0.5641 - val_accuracy: 0.7158
Epoch 25/60
653/682 [===========================&gt;..] - ETA: 0s - loss: 0.5650 - accuracy: 0.7142
Epoch 25: val_loss improved from 0.56405 to 0.56401, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5662 - accuracy: 0.7129 - val_loss: 0.5640 - val_accuracy: 0.7152
Epoch 26/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.5663 - accuracy: 0.7126
Epoch 26: val_loss improved from 0.56401 to 0.56394, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5661 - accuracy: 0.7119 - val_loss: 0.5639 - val_accuracy: 0.7140
Epoch 27/60
654/682 [===========================&gt;..] - ETA: 0s - loss: 0.5665 - accuracy: 0.7118
Epoch 27: val_loss improved from 0.56394 to 0.56381, saving model to best_drop.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.5659 - accuracy: 0.7122 - val_loss: 0.5638 - val_accuracy: 0.7146
Epoch 28/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.5650 - accuracy: 0.7149
Epoch 28: val_loss did not improve from 0.56381
682/682 [==============================] - 1s 2ms/step - loss: 0.5657 - accuracy: 0.7130 - val_loss: 0.5643 - val_accuracy: 0.7146
Epoch 29/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.5655 - accuracy: 0.7127
Epoch 29: val_loss did not improve from 0.56381
682/682 [==============================] - 2s 2ms/step - loss: 0.5657 - accuracy: 0.7123 - val_loss: 0.5639 - val_accuracy: 0.7158
Epoch 30/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.5655 - accuracy: 0.7118
Epoch 30: val_loss improved from 0.56381 to 0.56379, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5654 - accuracy: 0.7125 - val_loss: 0.5638 - val_accuracy: 0.7117
Epoch 31/60
653/682 [===========================&gt;..] - ETA: 0s - loss: 0.5666 - accuracy: 0.7133
Epoch 31: val_loss improved from 0.56379 to 0.56356, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5656 - accuracy: 0.7144 - val_loss: 0.5636 - val_accuracy: 0.7129
Epoch 32/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.5656 - accuracy: 0.7130
Epoch 32: val_loss improved from 0.56356 to 0.56347, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5655 - accuracy: 0.7130 - val_loss: 0.5635 - val_accuracy: 0.7146
Epoch 33/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.5650 - accuracy: 0.7131
Epoch 33: val_loss did not improve from 0.56347
682/682 [==============================] - 2s 2ms/step - loss: 0.5652 - accuracy: 0.7132 - val_loss: 0.5641 - val_accuracy: 0.7152
Epoch 34/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.5662 - accuracy: 0.7114
Epoch 34: val_loss improved from 0.56347 to 0.56335, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5654 - accuracy: 0.7119 - val_loss: 0.5633 - val_accuracy: 0.7146
Epoch 35/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.5632 - accuracy: 0.7150
Epoch 35: val_loss did not improve from 0.56335
682/682 [==============================] - 2s 2ms/step - loss: 0.5652 - accuracy: 0.7128 - val_loss: 0.5634 - val_accuracy: 0.7158
Epoch 36/60
682/682 [==============================] - ETA: 0s - loss: 0.5653 - accuracy: 0.7130
Epoch 36: val_loss improved from 0.56335 to 0.56326, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5653 - accuracy: 0.7130 - val_loss: 0.5633 - val_accuracy: 0.7164
Epoch 37/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.5666 - accuracy: 0.7126
Epoch 37: val_loss did not improve from 0.56326
682/682 [==============================] - 2s 2ms/step - loss: 0.5650 - accuracy: 0.7138 - val_loss: 0.5638 - val_accuracy: 0.7164
Epoch 38/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.5641 - accuracy: 0.7148
Epoch 38: val_loss improved from 0.56326 to 0.56318, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5653 - accuracy: 0.7136 - val_loss: 0.5632 - val_accuracy: 0.7158
Epoch 39/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.5648 - accuracy: 0.7130
Epoch 39: val_loss did not improve from 0.56318
682/682 [==============================] - 2s 2ms/step - loss: 0.5650 - accuracy: 0.7128 - val_loss: 0.5633 - val_accuracy: 0.7158
Epoch 40/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.5662 - accuracy: 0.7131
Epoch 40: val_loss improved from 0.56318 to 0.56314, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5650 - accuracy: 0.7139 - val_loss: 0.5631 - val_accuracy: 0.7158
Epoch 41/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.5660 - accuracy: 0.7134
Epoch 41: val_loss did not improve from 0.56314
682/682 [==============================] - 2s 2ms/step - loss: 0.5650 - accuracy: 0.7138 - val_loss: 0.5635 - val_accuracy: 0.7164
Epoch 42/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.5648 - accuracy: 0.7135
Epoch 42: val_loss did not improve from 0.56314
682/682 [==============================] - 2s 2ms/step - loss: 0.5651 - accuracy: 0.7130 - val_loss: 0.5631 - val_accuracy: 0.7152
Epoch 43/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.5638 - accuracy: 0.7139
Epoch 43: val_loss did not improve from 0.56314
682/682 [==============================] - 2s 2ms/step - loss: 0.5648 - accuracy: 0.7129 - val_loss: 0.5631 - val_accuracy: 0.7117
Epoch 44/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.5650 - accuracy: 0.7146
Epoch 44: val_loss did not improve from 0.56314
682/682 [==============================] - 2s 2ms/step - loss: 0.5649 - accuracy: 0.7147 - val_loss: 0.5632 - val_accuracy: 0.7111
Epoch 45/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.5662 - accuracy: 0.7127
Epoch 45: val_loss did not improve from 0.56314
682/682 [==============================] - 2s 2ms/step - loss: 0.5646 - accuracy: 0.7138 - val_loss: 0.5633 - val_accuracy: 0.7123
Epoch 46/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.5653 - accuracy: 0.7138
Epoch 46: val_loss did not improve from 0.56314
682/682 [==============================] - 2s 2ms/step - loss: 0.5648 - accuracy: 0.7142 - val_loss: 0.5633 - val_accuracy: 0.7146
Epoch 47/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.5644 - accuracy: 0.7142
Epoch 47: val_loss did not improve from 0.56314
682/682 [==============================] - 2s 2ms/step - loss: 0.5648 - accuracy: 0.7141 - val_loss: 0.5633 - val_accuracy: 0.7158
Epoch 48/60
652/682 [===========================&gt;..] - ETA: 0s - loss: 0.5646 - accuracy: 0.7143
Epoch 48: val_loss did not improve from 0.56314
682/682 [==============================] - 2s 2ms/step - loss: 0.5647 - accuracy: 0.7142 - val_loss: 0.5632 - val_accuracy: 0.7146
Epoch 49/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.5645 - accuracy: 0.7149
Epoch 49: val_loss improved from 0.56314 to 0.56305, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5647 - accuracy: 0.7147 - val_loss: 0.5630 - val_accuracy: 0.7105
Epoch 50/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.5647 - accuracy: 0.7143
Epoch 50: val_loss did not improve from 0.56305
682/682 [==============================] - 2s 2ms/step - loss: 0.5646 - accuracy: 0.7144 - val_loss: 0.5631 - val_accuracy: 0.7140
Epoch 51/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.5654 - accuracy: 0.7146
Epoch 51: val_loss did not improve from 0.56305
682/682 [==============================] - 2s 2ms/step - loss: 0.5647 - accuracy: 0.7150 - val_loss: 0.5631 - val_accuracy: 0.7164
Epoch 52/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.5634 - accuracy: 0.7157
Epoch 52: val_loss improved from 0.56305 to 0.56295, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5646 - accuracy: 0.7153 - val_loss: 0.5630 - val_accuracy: 0.7158
Epoch 53/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.5645 - accuracy: 0.7139
Epoch 53: val_loss did not improve from 0.56295
682/682 [==============================] - 2s 2ms/step - loss: 0.5645 - accuracy: 0.7141 - val_loss: 0.5636 - val_accuracy: 0.7170
Epoch 54/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.5627 - accuracy: 0.7154
Epoch 54: val_loss did not improve from 0.56295
682/682 [==============================] - 2s 2ms/step - loss: 0.5644 - accuracy: 0.7147 - val_loss: 0.5645 - val_accuracy: 0.7164
Epoch 55/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.5651 - accuracy: 0.7139
Epoch 55: val_loss did not improve from 0.56295
682/682 [==============================] - 2s 2ms/step - loss: 0.5648 - accuracy: 0.7141 - val_loss: 0.5633 - val_accuracy: 0.7164
Epoch 56/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.5629 - accuracy: 0.7147
Epoch 56: val_loss did not improve from 0.56295
682/682 [==============================] - 2s 2ms/step - loss: 0.5646 - accuracy: 0.7136 - val_loss: 0.5630 - val_accuracy: 0.7129
Epoch 57/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.5639 - accuracy: 0.7147
Epoch 57: val_loss did not improve from 0.56295
682/682 [==============================] - 2s 2ms/step - loss: 0.5645 - accuracy: 0.7139 - val_loss: 0.5630 - val_accuracy: 0.7140
Epoch 58/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.5641 - accuracy: 0.7158
Epoch 58: val_loss did not improve from 0.56295
682/682 [==============================] - 2s 2ms/step - loss: 0.5647 - accuracy: 0.7148 - val_loss: 0.5630 - val_accuracy: 0.7164
Epoch 59/60
654/682 [===========================&gt;..] - ETA: 0s - loss: 0.5641 - accuracy: 0.7150
Epoch 59: val_loss did not improve from 0.56295
682/682 [==============================] - 1s 2ms/step - loss: 0.5646 - accuracy: 0.7153 - val_loss: 0.5632 - val_accuracy: 0.7164
Epoch 60/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.5644 - accuracy: 0.7148
Epoch 60: val_loss did not improve from 0.56295
682/682 [==============================] - 2s 2ms/step - loss: 0.5645 - accuracy: 0.7147 - val_loss: 0.5633 - val_accuracy: 0.7164
54/54 [==============================] - 0s 2ms/step - loss: 0.5630 - accuracy: 0.7158
Drop Feature admin.
data frame x has shape: (8516, 10)
['education', 'pdays', 'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'single', 'blue-collar']
Epoch 1/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.7413 - accuracy: 0.4741
Epoch 1: val_loss improved from inf to 0.68365, saving model to best_drop.hdf5
682/682 [==============================] - 3s 4ms/step - loss: 0.7395 - accuracy: 0.4759 - val_loss: 0.6837 - val_accuracy: 0.5608
Epoch 2/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.6589 - accuracy: 0.6201
Epoch 2: val_loss improved from 0.68365 to 0.62863, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6588 - accuracy: 0.6203 - val_loss: 0.6286 - val_accuracy: 0.6741
Epoch 3/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.6197 - accuracy: 0.6782
Epoch 3: val_loss improved from 0.62863 to 0.60156, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6197 - accuracy: 0.6784 - val_loss: 0.6016 - val_accuracy: 0.6964
Epoch 4/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.6007 - accuracy: 0.6957
Epoch 4: val_loss improved from 0.60156 to 0.58834, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6006 - accuracy: 0.6959 - val_loss: 0.5883 - val_accuracy: 0.7087
Epoch 5/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.5904 - accuracy: 0.7045
Epoch 5: val_loss improved from 0.58834 to 0.58092, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5906 - accuracy: 0.7047 - val_loss: 0.5809 - val_accuracy: 0.7111
Epoch 6/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.5851 - accuracy: 0.7094
Epoch 6: val_loss improved from 0.58092 to 0.57713, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5849 - accuracy: 0.7089 - val_loss: 0.5771 - val_accuracy: 0.7140
Epoch 7/60
651/682 [===========================&gt;..] - ETA: 0s - loss: 0.5827 - accuracy: 0.7098
Epoch 7: val_loss improved from 0.57713 to 0.57476, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5816 - accuracy: 0.7113 - val_loss: 0.5748 - val_accuracy: 0.7158
Epoch 8/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.5791 - accuracy: 0.7127
Epoch 8: val_loss improved from 0.57476 to 0.57330, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5793 - accuracy: 0.7123 - val_loss: 0.5733 - val_accuracy: 0.7158
Epoch 9/60
682/682 [==============================] - ETA: 0s - loss: 0.5778 - accuracy: 0.7120
Epoch 9: val_loss improved from 0.57330 to 0.57229, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5778 - accuracy: 0.7120 - val_loss: 0.5723 - val_accuracy: 0.7152
Epoch 10/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.5761 - accuracy: 0.7134
Epoch 10: val_loss improved from 0.57229 to 0.57142, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5765 - accuracy: 0.7126 - val_loss: 0.5714 - val_accuracy: 0.7146
Epoch 11/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.5743 - accuracy: 0.7124
Epoch 11: val_loss improved from 0.57142 to 0.57081, saving model to best_drop.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.5755 - accuracy: 0.7125 - val_loss: 0.5708 - val_accuracy: 0.7146
Epoch 12/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.5744 - accuracy: 0.7131
Epoch 12: val_loss improved from 0.57081 to 0.57030, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5747 - accuracy: 0.7128 - val_loss: 0.5703 - val_accuracy: 0.7146
Epoch 13/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.5730 - accuracy: 0.7142
Epoch 13: val_loss improved from 0.57030 to 0.56980, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5740 - accuracy: 0.7128 - val_loss: 0.5698 - val_accuracy: 0.7146
Epoch 14/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.5739 - accuracy: 0.7116
Epoch 14: val_loss improved from 0.56980 to 0.56971, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5733 - accuracy: 0.7126 - val_loss: 0.5697 - val_accuracy: 0.7152
Epoch 15/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.5719 - accuracy: 0.7137
Epoch 15: val_loss improved from 0.56971 to 0.56898, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5727 - accuracy: 0.7128 - val_loss: 0.5690 - val_accuracy: 0.7146
Epoch 16/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.5712 - accuracy: 0.7133
Epoch 16: val_loss improved from 0.56898 to 0.56877, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5721 - accuracy: 0.7126 - val_loss: 0.5688 - val_accuracy: 0.7146
Epoch 17/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.5714 - accuracy: 0.7130
Epoch 17: val_loss improved from 0.56877 to 0.56829, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5718 - accuracy: 0.7126 - val_loss: 0.5683 - val_accuracy: 0.7146
Epoch 18/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.5692 - accuracy: 0.7136
Epoch 18: val_loss improved from 0.56829 to 0.56789, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5713 - accuracy: 0.7126 - val_loss: 0.5679 - val_accuracy: 0.7146
Epoch 19/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.5718 - accuracy: 0.7115
Epoch 19: val_loss improved from 0.56789 to 0.56761, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5709 - accuracy: 0.7126 - val_loss: 0.5676 - val_accuracy: 0.7146
Epoch 20/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.5714 - accuracy: 0.7126
Epoch 20: val_loss improved from 0.56761 to 0.56729, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5705 - accuracy: 0.7128 - val_loss: 0.5673 - val_accuracy: 0.7146
Epoch 21/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.5704 - accuracy: 0.7124
Epoch 21: val_loss improved from 0.56729 to 0.56702, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5701 - accuracy: 0.7126 - val_loss: 0.5670 - val_accuracy: 0.7146
Epoch 22/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.5697 - accuracy: 0.7126
Epoch 22: val_loss improved from 0.56702 to 0.56674, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5697 - accuracy: 0.7126 - val_loss: 0.5667 - val_accuracy: 0.7146
Epoch 23/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.5693 - accuracy: 0.7135
Epoch 23: val_loss improved from 0.56674 to 0.56655, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5694 - accuracy: 0.7128 - val_loss: 0.5666 - val_accuracy: 0.7146
Epoch 24/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.5692 - accuracy: 0.7126
Epoch 24: val_loss improved from 0.56655 to 0.56641, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5691 - accuracy: 0.7126 - val_loss: 0.5664 - val_accuracy: 0.7146
Epoch 25/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.5703 - accuracy: 0.7115
Epoch 25: val_loss improved from 0.56641 to 0.56608, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5688 - accuracy: 0.7129 - val_loss: 0.5661 - val_accuracy: 0.7146
Epoch 26/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.5680 - accuracy: 0.7144
Epoch 26: val_loss improved from 0.56608 to 0.56590, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5685 - accuracy: 0.7133 - val_loss: 0.5659 - val_accuracy: 0.7146
Epoch 27/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.5688 - accuracy: 0.7126
Epoch 27: val_loss improved from 0.56590 to 0.56572, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5684 - accuracy: 0.7129 - val_loss: 0.5657 - val_accuracy: 0.7152
Epoch 28/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.5685 - accuracy: 0.7123
Epoch 28: val_loss did not improve from 0.56572
682/682 [==============================] - 2s 2ms/step - loss: 0.5681 - accuracy: 0.7125 - val_loss: 0.5658 - val_accuracy: 0.7146
Epoch 29/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.5681 - accuracy: 0.7144
Epoch 29: val_loss improved from 0.56572 to 0.56558, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5679 - accuracy: 0.7147 - val_loss: 0.5656 - val_accuracy: 0.7140
Epoch 30/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.5682 - accuracy: 0.7135
Epoch 30: val_loss improved from 0.56558 to 0.56522, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5677 - accuracy: 0.7147 - val_loss: 0.5652 - val_accuracy: 0.7146
Epoch 31/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.5685 - accuracy: 0.7128
Epoch 31: val_loss did not improve from 0.56522
682/682 [==============================] - 2s 2ms/step - loss: 0.5675 - accuracy: 0.7141 - val_loss: 0.5653 - val_accuracy: 0.7140
Epoch 32/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.5670 - accuracy: 0.7143
Epoch 32: val_loss improved from 0.56522 to 0.56500, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5673 - accuracy: 0.7138 - val_loss: 0.5650 - val_accuracy: 0.7140
Epoch 33/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.5674 - accuracy: 0.7138
Epoch 33: val_loss did not improve from 0.56500
682/682 [==============================] - 2s 2ms/step - loss: 0.5671 - accuracy: 0.7142 - val_loss: 0.5652 - val_accuracy: 0.7123
Epoch 34/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.5669 - accuracy: 0.7152
Epoch 34: val_loss improved from 0.56500 to 0.56497, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5671 - accuracy: 0.7142 - val_loss: 0.5650 - val_accuracy: 0.7123
Epoch 35/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.5664 - accuracy: 0.7148
Epoch 35: val_loss did not improve from 0.56497
682/682 [==============================] - 2s 2ms/step - loss: 0.5668 - accuracy: 0.7141 - val_loss: 0.5651 - val_accuracy: 0.7152
Epoch 36/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.5679 - accuracy: 0.7127
Epoch 36: val_loss improved from 0.56497 to 0.56453, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5668 - accuracy: 0.7136 - val_loss: 0.5645 - val_accuracy: 0.7134
Epoch 37/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.5667 - accuracy: 0.7146
Epoch 37: val_loss improved from 0.56453 to 0.56450, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5666 - accuracy: 0.7148 - val_loss: 0.5645 - val_accuracy: 0.7123
Epoch 38/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.5663 - accuracy: 0.7137
Epoch 38: val_loss did not improve from 0.56450
682/682 [==============================] - 2s 2ms/step - loss: 0.5663 - accuracy: 0.7141 - val_loss: 0.5646 - val_accuracy: 0.7140
Epoch 39/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.5659 - accuracy: 0.7151
Epoch 39: val_loss improved from 0.56450 to 0.56424, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5665 - accuracy: 0.7150 - val_loss: 0.5642 - val_accuracy: 0.7134
Epoch 40/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.5657 - accuracy: 0.7148
Epoch 40: val_loss did not improve from 0.56424
682/682 [==============================] - 2s 2ms/step - loss: 0.5662 - accuracy: 0.7138 - val_loss: 0.5644 - val_accuracy: 0.7152
Epoch 41/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.5659 - accuracy: 0.7147
Epoch 41: val_loss did not improve from 0.56424
682/682 [==============================] - 2s 2ms/step - loss: 0.5662 - accuracy: 0.7147 - val_loss: 0.5643 - val_accuracy: 0.7140
Epoch 42/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.5655 - accuracy: 0.7147
Epoch 42: val_loss improved from 0.56424 to 0.56397, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5661 - accuracy: 0.7138 - val_loss: 0.5640 - val_accuracy: 0.7134
Epoch 43/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.5657 - accuracy: 0.7157
Epoch 43: val_loss improved from 0.56397 to 0.56391, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5661 - accuracy: 0.7150 - val_loss: 0.5639 - val_accuracy: 0.7134
Epoch 44/60
682/682 [==============================] - ETA: 0s - loss: 0.5659 - accuracy: 0.7144
Epoch 44: val_loss did not improve from 0.56391
682/682 [==============================] - 2s 2ms/step - loss: 0.5659 - accuracy: 0.7144 - val_loss: 0.5640 - val_accuracy: 0.7146
Epoch 45/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.5664 - accuracy: 0.7143
Epoch 45: val_loss improved from 0.56391 to 0.56390, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5658 - accuracy: 0.7147 - val_loss: 0.5639 - val_accuracy: 0.7129
Epoch 46/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.5653 - accuracy: 0.7142
Epoch 46: val_loss improved from 0.56390 to 0.56376, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5658 - accuracy: 0.7142 - val_loss: 0.5638 - val_accuracy: 0.7134
Epoch 47/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.5652 - accuracy: 0.7149
Epoch 47: val_loss did not improve from 0.56376
682/682 [==============================] - 2s 2ms/step - loss: 0.5657 - accuracy: 0.7147 - val_loss: 0.5638 - val_accuracy: 0.7140
Epoch 48/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.5648 - accuracy: 0.7152
Epoch 48: val_loss improved from 0.56376 to 0.56371, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5657 - accuracy: 0.7147 - val_loss: 0.5637 - val_accuracy: 0.7134
Epoch 49/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.5657 - accuracy: 0.7143
Epoch 49: val_loss did not improve from 0.56371
682/682 [==============================] - 2s 2ms/step - loss: 0.5656 - accuracy: 0.7142 - val_loss: 0.5637 - val_accuracy: 0.7140
Epoch 50/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.5655 - accuracy: 0.7142
Epoch 50: val_loss improved from 0.56371 to 0.56371, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5655 - accuracy: 0.7142 - val_loss: 0.5637 - val_accuracy: 0.7134
Epoch 51/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.5659 - accuracy: 0.7146
Epoch 51: val_loss improved from 0.56371 to 0.56359, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5654 - accuracy: 0.7150 - val_loss: 0.5636 - val_accuracy: 0.7134
Epoch 52/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.5654 - accuracy: 0.7151
Epoch 52: val_loss improved from 0.56359 to 0.56354, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5654 - accuracy: 0.7157 - val_loss: 0.5635 - val_accuracy: 0.7140
Epoch 53/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.5646 - accuracy: 0.7164
Epoch 53: val_loss did not improve from 0.56354
682/682 [==============================] - 2s 2ms/step - loss: 0.5652 - accuracy: 0.7166 - val_loss: 0.5636 - val_accuracy: 0.7134
Epoch 54/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.5658 - accuracy: 0.7147
Epoch 54: val_loss improved from 0.56354 to 0.56353, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5653 - accuracy: 0.7154 - val_loss: 0.5635 - val_accuracy: 0.7140
Epoch 55/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.5654 - accuracy: 0.7153
Epoch 55: val_loss improved from 0.56353 to 0.56344, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5653 - accuracy: 0.7153 - val_loss: 0.5634 - val_accuracy: 0.7146
Epoch 56/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.5660 - accuracy: 0.7151
Epoch 56: val_loss improved from 0.56344 to 0.56339, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5652 - accuracy: 0.7157 - val_loss: 0.5634 - val_accuracy: 0.7146
Epoch 57/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.5652 - accuracy: 0.7153
Epoch 57: val_loss improved from 0.56339 to 0.56337, saving model to best_drop.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.5652 - accuracy: 0.7154 - val_loss: 0.5634 - val_accuracy: 0.7146
Epoch 58/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.5648 - accuracy: 0.7154
Epoch 58: val_loss improved from 0.56337 to 0.56333, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5650 - accuracy: 0.7157 - val_loss: 0.5633 - val_accuracy: 0.7146
Epoch 59/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.5642 - accuracy: 0.7184
Epoch 59: val_loss did not improve from 0.56333
682/682 [==============================] - 2s 2ms/step - loss: 0.5651 - accuracy: 0.7166 - val_loss: 0.5633 - val_accuracy: 0.7146
Epoch 60/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.5638 - accuracy: 0.7163
Epoch 60: val_loss did not improve from 0.56333
682/682 [==============================] - 2s 2ms/step - loss: 0.5650 - accuracy: 0.7154 - val_loss: 0.5634 - val_accuracy: 0.7140
54/54 [==============================] - 0s 2ms/step - loss: 0.5633 - accuracy: 0.7146
Drop Feature single
data frame x has shape: (8516, 9)
['education', 'pdays', 'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'blue-collar']
Epoch 1/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.7299 - accuracy: 0.4470
Epoch 1: val_loss improved from inf to 0.67669, saving model to best_drop.hdf5
682/682 [==============================] - 2s 3ms/step - loss: 0.7290 - accuracy: 0.4497 - val_loss: 0.6767 - val_accuracy: 0.5966
Epoch 2/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.6535 - accuracy: 0.6400
Epoch 2: val_loss improved from 0.67669 to 0.62523, saving model to best_drop.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6533 - accuracy: 0.6402 - val_loss: 0.6252 - val_accuracy: 0.6882
Epoch 3/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.6170 - accuracy: 0.6891
Epoch 3: val_loss improved from 0.62523 to 0.59960, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6168 - accuracy: 0.6894 - val_loss: 0.5996 - val_accuracy: 0.7035
Epoch 4/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.5985 - accuracy: 0.7006
Epoch 4: val_loss improved from 0.59960 to 0.58719, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5986 - accuracy: 0.7003 - val_loss: 0.5872 - val_accuracy: 0.7099
Epoch 5/60
682/682 [==============================] - ETA: 0s - loss: 0.5895 - accuracy: 0.7038
Epoch 5: val_loss improved from 0.58719 to 0.58071, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5895 - accuracy: 0.7038 - val_loss: 0.5807 - val_accuracy: 0.7123
Epoch 6/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.5841 - accuracy: 0.7067
Epoch 6: val_loss improved from 0.58071 to 0.57671, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5843 - accuracy: 0.7069 - val_loss: 0.5767 - val_accuracy: 0.7140
Epoch 7/60
654/682 [===========================&gt;..] - ETA: 0s - loss: 0.5807 - accuracy: 0.7093
Epoch 7: val_loss improved from 0.57671 to 0.57449, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5811 - accuracy: 0.7089 - val_loss: 0.5745 - val_accuracy: 0.7152
Epoch 8/60
682/682 [==============================] - ETA: 0s - loss: 0.5790 - accuracy: 0.7095
Epoch 8: val_loss improved from 0.57449 to 0.57304, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5790 - accuracy: 0.7095 - val_loss: 0.5730 - val_accuracy: 0.7152
Epoch 9/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.5765 - accuracy: 0.7119
Epoch 9: val_loss improved from 0.57304 to 0.57228, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5774 - accuracy: 0.7106 - val_loss: 0.5723 - val_accuracy: 0.7129
Epoch 10/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.5765 - accuracy: 0.7110
Epoch 10: val_loss improved from 0.57228 to 0.57109, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5764 - accuracy: 0.7113 - val_loss: 0.5711 - val_accuracy: 0.7170
Epoch 11/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.5752 - accuracy: 0.7110
Epoch 11: val_loss improved from 0.57109 to 0.57072, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5752 - accuracy: 0.7110 - val_loss: 0.5707 - val_accuracy: 0.7129
Epoch 12/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.5750 - accuracy: 0.7100
Epoch 12: val_loss improved from 0.57072 to 0.56984, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5745 - accuracy: 0.7104 - val_loss: 0.5698 - val_accuracy: 0.7129
Epoch 13/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.5739 - accuracy: 0.7104
Epoch 13: val_loss improved from 0.56984 to 0.56932, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5736 - accuracy: 0.7104 - val_loss: 0.5693 - val_accuracy: 0.7129
Epoch 14/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.5732 - accuracy: 0.7116
Epoch 14: val_loss improved from 0.56932 to 0.56887, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5730 - accuracy: 0.7114 - val_loss: 0.5689 - val_accuracy: 0.7129
Epoch 15/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.5727 - accuracy: 0.7123
Epoch 15: val_loss improved from 0.56887 to 0.56848, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5724 - accuracy: 0.7128 - val_loss: 0.5685 - val_accuracy: 0.7129
Epoch 16/60
653/682 [===========================&gt;..] - ETA: 0s - loss: 0.5714 - accuracy: 0.7129
Epoch 16: val_loss improved from 0.56848 to 0.56798, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5718 - accuracy: 0.7119 - val_loss: 0.5680 - val_accuracy: 0.7129
Epoch 17/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.5719 - accuracy: 0.7121
Epoch 17: val_loss improved from 0.56798 to 0.56760, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5714 - accuracy: 0.7122 - val_loss: 0.5676 - val_accuracy: 0.7129
Epoch 18/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.5707 - accuracy: 0.7118
Epoch 18: val_loss improved from 0.56760 to 0.56753, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5707 - accuracy: 0.7116 - val_loss: 0.5675 - val_accuracy: 0.7181
Epoch 19/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.5717 - accuracy: 0.7111
Epoch 19: val_loss improved from 0.56753 to 0.56706, saving model to best_drop.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.5705 - accuracy: 0.7116 - val_loss: 0.5671 - val_accuracy: 0.7123
Epoch 20/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.5699 - accuracy: 0.7110
Epoch 20: val_loss improved from 0.56706 to 0.56678, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5698 - accuracy: 0.7111 - val_loss: 0.5668 - val_accuracy: 0.7134
Epoch 21/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.5689 - accuracy: 0.7133
Epoch 21: val_loss improved from 0.56678 to 0.56653, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5696 - accuracy: 0.7126 - val_loss: 0.5665 - val_accuracy: 0.7129
Epoch 22/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.5684 - accuracy: 0.7124
Epoch 22: val_loss improved from 0.56653 to 0.56631, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5692 - accuracy: 0.7116 - val_loss: 0.5663 - val_accuracy: 0.7129
Epoch 23/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.5688 - accuracy: 0.7130
Epoch 23: val_loss improved from 0.56631 to 0.56610, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5689 - accuracy: 0.7130 - val_loss: 0.5661 - val_accuracy: 0.7134
Epoch 24/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.5695 - accuracy: 0.7118
Epoch 24: val_loss improved from 0.56610 to 0.56591, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5687 - accuracy: 0.7125 - val_loss: 0.5659 - val_accuracy: 0.7134
Epoch 25/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.5678 - accuracy: 0.7143
Epoch 25: val_loss improved from 0.56591 to 0.56569, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5684 - accuracy: 0.7132 - val_loss: 0.5657 - val_accuracy: 0.7134
Epoch 26/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.5688 - accuracy: 0.7120
Epoch 26: val_loss improved from 0.56569 to 0.56552, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5681 - accuracy: 0.7126 - val_loss: 0.5655 - val_accuracy: 0.7134
Epoch 27/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.5682 - accuracy: 0.7129
Epoch 27: val_loss improved from 0.56552 to 0.56534, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5679 - accuracy: 0.7129 - val_loss: 0.5653 - val_accuracy: 0.7117
Epoch 28/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.5683 - accuracy: 0.7120
Epoch 28: val_loss did not improve from 0.56534
682/682 [==============================] - 2s 2ms/step - loss: 0.5676 - accuracy: 0.7129 - val_loss: 0.5656 - val_accuracy: 0.7140
Epoch 29/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.5666 - accuracy: 0.7147
Epoch 29: val_loss improved from 0.56534 to 0.56513, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5675 - accuracy: 0.7136 - val_loss: 0.5651 - val_accuracy: 0.7134
Epoch 30/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.5663 - accuracy: 0.7147
Epoch 30: val_loss improved from 0.56513 to 0.56489, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5672 - accuracy: 0.7132 - val_loss: 0.5649 - val_accuracy: 0.7123
Epoch 31/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.5670 - accuracy: 0.7128
Epoch 31: val_loss improved from 0.56489 to 0.56484, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5672 - accuracy: 0.7130 - val_loss: 0.5648 - val_accuracy: 0.7129
Epoch 32/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.5677 - accuracy: 0.7134
Epoch 32: val_loss improved from 0.56484 to 0.56468, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5669 - accuracy: 0.7136 - val_loss: 0.5647 - val_accuracy: 0.7123
Epoch 33/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.5672 - accuracy: 0.7136
Epoch 33: val_loss did not improve from 0.56468
682/682 [==============================] - 1s 2ms/step - loss: 0.5668 - accuracy: 0.7135 - val_loss: 0.5647 - val_accuracy: 0.7123
Epoch 34/60
682/682 [==============================] - ETA: 0s - loss: 0.5666 - accuracy: 0.7133
Epoch 34: val_loss improved from 0.56468 to 0.56465, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5666 - accuracy: 0.7133 - val_loss: 0.5647 - val_accuracy: 0.7123
Epoch 35/60
654/682 [===========================&gt;..] - ETA: 0s - loss: 0.5686 - accuracy: 0.7116
Epoch 35: val_loss improved from 0.56465 to 0.56441, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5666 - accuracy: 0.7138 - val_loss: 0.5644 - val_accuracy: 0.7123
Epoch 36/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.5671 - accuracy: 0.7124
Epoch 36: val_loss improved from 0.56441 to 0.56434, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5663 - accuracy: 0.7132 - val_loss: 0.5643 - val_accuracy: 0.7123
Epoch 37/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.5661 - accuracy: 0.7126
Epoch 37: val_loss improved from 0.56434 to 0.56428, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5664 - accuracy: 0.7123 - val_loss: 0.5643 - val_accuracy: 0.7123
Epoch 38/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.5662 - accuracy: 0.7132
Epoch 38: val_loss improved from 0.56428 to 0.56419, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5662 - accuracy: 0.7128 - val_loss: 0.5642 - val_accuracy: 0.7123
Epoch 39/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.5668 - accuracy: 0.7125
Epoch 39: val_loss improved from 0.56419 to 0.56411, saving model to best_drop.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.5661 - accuracy: 0.7129 - val_loss: 0.5641 - val_accuracy: 0.7123
Epoch 40/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.5660 - accuracy: 0.7131
Epoch 40: val_loss did not improve from 0.56411
682/682 [==============================] - 2s 2ms/step - loss: 0.5660 - accuracy: 0.7128 - val_loss: 0.5642 - val_accuracy: 0.7134
Epoch 41/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.5649 - accuracy: 0.7144
Epoch 41: val_loss did not improve from 0.56411
682/682 [==============================] - 2s 2ms/step - loss: 0.5659 - accuracy: 0.7133 - val_loss: 0.5641 - val_accuracy: 0.7123
Epoch 42/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.5659 - accuracy: 0.7124
Epoch 42: val_loss did not improve from 0.56411
682/682 [==============================] - 2s 2ms/step - loss: 0.5658 - accuracy: 0.7126 - val_loss: 0.5642 - val_accuracy: 0.7140
Epoch 43/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.5649 - accuracy: 0.7141
Epoch 43: val_loss improved from 0.56411 to 0.56409, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5657 - accuracy: 0.7132 - val_loss: 0.5641 - val_accuracy: 0.7129
Epoch 44/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.5665 - accuracy: 0.7127
Epoch 44: val_loss improved from 0.56409 to 0.56391, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5657 - accuracy: 0.7135 - val_loss: 0.5639 - val_accuracy: 0.7123
Epoch 45/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.5656 - accuracy: 0.7132
Epoch 45: val_loss improved from 0.56391 to 0.56382, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5657 - accuracy: 0.7132 - val_loss: 0.5638 - val_accuracy: 0.7123
Epoch 46/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.5654 - accuracy: 0.7135
Epoch 46: val_loss did not improve from 0.56382
682/682 [==============================] - 1s 2ms/step - loss: 0.5655 - accuracy: 0.7135 - val_loss: 0.5642 - val_accuracy: 0.7134
Epoch 47/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.5664 - accuracy: 0.7142
Epoch 47: val_loss improved from 0.56382 to 0.56382, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5655 - accuracy: 0.7147 - val_loss: 0.5638 - val_accuracy: 0.7129
Epoch 48/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.5664 - accuracy: 0.7123
Epoch 48: val_loss improved from 0.56382 to 0.56379, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5654 - accuracy: 0.7136 - val_loss: 0.5638 - val_accuracy: 0.7123
Epoch 49/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.5669 - accuracy: 0.7133
Epoch 49: val_loss improved from 0.56379 to 0.56373, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5654 - accuracy: 0.7141 - val_loss: 0.5637 - val_accuracy: 0.7140
Epoch 50/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.5645 - accuracy: 0.7156
Epoch 50: val_loss did not improve from 0.56373
682/682 [==============================] - 2s 2ms/step - loss: 0.5653 - accuracy: 0.7148 - val_loss: 0.5638 - val_accuracy: 0.7123
Epoch 51/60
682/682 [==============================] - ETA: 0s - loss: 0.5653 - accuracy: 0.7136
Epoch 51: val_loss did not improve from 0.56373
682/682 [==============================] - 2s 2ms/step - loss: 0.5653 - accuracy: 0.7136 - val_loss: 0.5638 - val_accuracy: 0.7123
Epoch 52/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.5657 - accuracy: 0.7148
Epoch 52: val_loss improved from 0.56373 to 0.56370, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5654 - accuracy: 0.7148 - val_loss: 0.5637 - val_accuracy: 0.7140
Epoch 53/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.5661 - accuracy: 0.7141
Epoch 53: val_loss improved from 0.56370 to 0.56369, saving model to best_drop.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.5652 - accuracy: 0.7150 - val_loss: 0.5637 - val_accuracy: 0.7140
Epoch 54/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.5653 - accuracy: 0.7160
Epoch 54: val_loss did not improve from 0.56369
682/682 [==============================] - 2s 2ms/step - loss: 0.5651 - accuracy: 0.7155 - val_loss: 0.5637 - val_accuracy: 0.7123
Epoch 55/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.5632 - accuracy: 0.7162
Epoch 55: val_loss improved from 0.56369 to 0.56364, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5651 - accuracy: 0.7148 - val_loss: 0.5636 - val_accuracy: 0.7123
Epoch 56/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.5641 - accuracy: 0.7158
Epoch 56: val_loss improved from 0.56364 to 0.56360, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5650 - accuracy: 0.7147 - val_loss: 0.5636 - val_accuracy: 0.7129
Epoch 57/60
682/682 [==============================] - ETA: 0s - loss: 0.5651 - accuracy: 0.7144
Epoch 57: val_loss did not improve from 0.56360
682/682 [==============================] - 2s 2ms/step - loss: 0.5651 - accuracy: 0.7144 - val_loss: 0.5636 - val_accuracy: 0.7134
Epoch 58/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.5652 - accuracy: 0.7139
Epoch 58: val_loss did not improve from 0.56360
682/682 [==============================] - 2s 2ms/step - loss: 0.5650 - accuracy: 0.7142 - val_loss: 0.5637 - val_accuracy: 0.7134
Epoch 59/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.5655 - accuracy: 0.7142
Epoch 59: val_loss did not improve from 0.56360
682/682 [==============================] - 1s 2ms/step - loss: 0.5649 - accuracy: 0.7147 - val_loss: 0.5639 - val_accuracy: 0.7134
Epoch 60/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.5640 - accuracy: 0.7171
Epoch 60: val_loss improved from 0.56360 to 0.56360, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5650 - accuracy: 0.7150 - val_loss: 0.5636 - val_accuracy: 0.7117
54/54 [==============================] - 0s 2ms/step - loss: 0.5636 - accuracy: 0.7117
Drop Feature cons.conf.idx
data frame x has shape: (8516, 8)
['education', 'pdays', 'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx', 'euribor3m', 'blue-collar']
Epoch 1/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.6345 - accuracy: 0.6295
Epoch 1: val_loss improved from inf to 0.61603, saving model to best_drop.hdf5
682/682 [==============================] - 2s 3ms/step - loss: 0.6345 - accuracy: 0.6295 - val_loss: 0.6160 - val_accuracy: 0.6671
Epoch 2/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.6039 - accuracy: 0.6919
Epoch 2: val_loss improved from 0.61603 to 0.59520, saving model to best_drop.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.6037 - accuracy: 0.6922 - val_loss: 0.5952 - val_accuracy: 0.7029
Epoch 3/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.5908 - accuracy: 0.7030
Epoch 3: val_loss improved from 0.59520 to 0.58479, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5901 - accuracy: 0.7038 - val_loss: 0.5848 - val_accuracy: 0.7093
Epoch 4/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.5831 - accuracy: 0.7097
Epoch 4: val_loss improved from 0.58479 to 0.57957, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5830 - accuracy: 0.7097 - val_loss: 0.5796 - val_accuracy: 0.7082
Epoch 5/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.5798 - accuracy: 0.7113
Epoch 5: val_loss improved from 0.57957 to 0.57610, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5798 - accuracy: 0.7111 - val_loss: 0.5761 - val_accuracy: 0.7140
Epoch 6/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.5778 - accuracy: 0.7124
Epoch 6: val_loss improved from 0.57610 to 0.57432, saving model to best_drop.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.5777 - accuracy: 0.7126 - val_loss: 0.5743 - val_accuracy: 0.7129
Epoch 7/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.5760 - accuracy: 0.7145
Epoch 7: val_loss improved from 0.57432 to 0.57319, saving model to best_drop.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.5764 - accuracy: 0.7133 - val_loss: 0.5732 - val_accuracy: 0.7123
Epoch 8/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.5742 - accuracy: 0.7128
Epoch 8: val_loss improved from 0.57319 to 0.57226, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5757 - accuracy: 0.7113 - val_loss: 0.5723 - val_accuracy: 0.7134
Epoch 9/60
682/682 [==============================] - ETA: 0s - loss: 0.5748 - accuracy: 0.7119
Epoch 9: val_loss improved from 0.57226 to 0.57210, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5748 - accuracy: 0.7119 - val_loss: 0.5721 - val_accuracy: 0.7176
Epoch 10/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.5735 - accuracy: 0.7130
Epoch 10: val_loss improved from 0.57210 to 0.57118, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5747 - accuracy: 0.7117 - val_loss: 0.5712 - val_accuracy: 0.7129
Epoch 11/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.5739 - accuracy: 0.7124
Epoch 11: val_loss improved from 0.57118 to 0.57079, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5741 - accuracy: 0.7122 - val_loss: 0.5708 - val_accuracy: 0.7146
Epoch 12/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.5729 - accuracy: 0.7140
Epoch 12: val_loss improved from 0.57079 to 0.57051, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5737 - accuracy: 0.7126 - val_loss: 0.5705 - val_accuracy: 0.7146
Epoch 13/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.5735 - accuracy: 0.7119
Epoch 13: val_loss improved from 0.57051 to 0.57028, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5735 - accuracy: 0.7120 - val_loss: 0.5703 - val_accuracy: 0.7146
Epoch 14/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.5737 - accuracy: 0.7120
Epoch 14: val_loss improved from 0.57028 to 0.57000, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5731 - accuracy: 0.7126 - val_loss: 0.5700 - val_accuracy: 0.7146
Epoch 15/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.5727 - accuracy: 0.7128
Epoch 15: val_loss improved from 0.57000 to 0.56983, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5729 - accuracy: 0.7126 - val_loss: 0.5698 - val_accuracy: 0.7146
Epoch 16/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.5714 - accuracy: 0.7144
Epoch 16: val_loss improved from 0.56983 to 0.56978, saving model to best_drop.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.5725 - accuracy: 0.7128 - val_loss: 0.5698 - val_accuracy: 0.7146
Epoch 17/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.5726 - accuracy: 0.7128
Epoch 17: val_loss improved from 0.56978 to 0.56955, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5723 - accuracy: 0.7130 - val_loss: 0.5695 - val_accuracy: 0.7152
Epoch 18/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.5719 - accuracy: 0.7127
Epoch 18: val_loss improved from 0.56955 to 0.56946, saving model to best_drop.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.5721 - accuracy: 0.7125 - val_loss: 0.5695 - val_accuracy: 0.7152
Epoch 19/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.5720 - accuracy: 0.7128
Epoch 19: val_loss improved from 0.56946 to 0.56945, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5719 - accuracy: 0.7129 - val_loss: 0.5694 - val_accuracy: 0.7152
Epoch 20/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.5718 - accuracy: 0.7131
Epoch 20: val_loss improved from 0.56945 to 0.56915, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5718 - accuracy: 0.7130 - val_loss: 0.5691 - val_accuracy: 0.7152
Epoch 21/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.5715 - accuracy: 0.7132
Epoch 21: val_loss did not improve from 0.56915
682/682 [==============================] - 2s 2ms/step - loss: 0.5714 - accuracy: 0.7129 - val_loss: 0.5696 - val_accuracy: 0.7152
Epoch 22/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.5724 - accuracy: 0.7118
Epoch 22: val_loss improved from 0.56915 to 0.56892, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5715 - accuracy: 0.7132 - val_loss: 0.5689 - val_accuracy: 0.7158
Epoch 23/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.5711 - accuracy: 0.7131
Epoch 23: val_loss improved from 0.56892 to 0.56882, saving model to best_drop.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.5714 - accuracy: 0.7129 - val_loss: 0.5688 - val_accuracy: 0.7158
Epoch 24/60
670/682 [============================&gt;.] - ETA: 0s - loss: 0.5715 - accuracy: 0.7125
Epoch 24: val_loss improved from 0.56882 to 0.56875, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5711 - accuracy: 0.7133 - val_loss: 0.5688 - val_accuracy: 0.7158
Epoch 25/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.5710 - accuracy: 0.7134
Epoch 25: val_loss improved from 0.56875 to 0.56869, saving model to best_drop.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.5711 - accuracy: 0.7133 - val_loss: 0.5687 - val_accuracy: 0.7158
Epoch 26/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.5714 - accuracy: 0.7129
Epoch 26: val_loss improved from 0.56869 to 0.56859, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5710 - accuracy: 0.7132 - val_loss: 0.5686 - val_accuracy: 0.7158
Epoch 27/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.5706 - accuracy: 0.7136
Epoch 27: val_loss improved from 0.56859 to 0.56854, saving model to best_drop.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.5708 - accuracy: 0.7132 - val_loss: 0.5685 - val_accuracy: 0.7158
Epoch 28/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.5710 - accuracy: 0.7132
Epoch 28: val_loss improved from 0.56854 to 0.56845, saving model to best_drop.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.5710 - accuracy: 0.7133 - val_loss: 0.5685 - val_accuracy: 0.7158
Epoch 29/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.5691 - accuracy: 0.7143
Epoch 29: val_loss improved from 0.56845 to 0.56842, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5707 - accuracy: 0.7135 - val_loss: 0.5684 - val_accuracy: 0.7158
Epoch 30/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.5696 - accuracy: 0.7151
Epoch 30: val_loss improved from 0.56842 to 0.56832, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5706 - accuracy: 0.7133 - val_loss: 0.5683 - val_accuracy: 0.7158
Epoch 31/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.5710 - accuracy: 0.7125
Epoch 31: val_loss improved from 0.56832 to 0.56827, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5704 - accuracy: 0.7135 - val_loss: 0.5683 - val_accuracy: 0.7158
Epoch 32/60
654/682 [===========================&gt;..] - ETA: 0s - loss: 0.5683 - accuracy: 0.7150
Epoch 32: val_loss improved from 0.56827 to 0.56826, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5703 - accuracy: 0.7135 - val_loss: 0.5683 - val_accuracy: 0.7158
Epoch 33/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.5699 - accuracy: 0.7150
Epoch 33: val_loss improved from 0.56826 to 0.56820, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5704 - accuracy: 0.7136 - val_loss: 0.5682 - val_accuracy: 0.7158
Epoch 34/60
663/682 [============================&gt;.] - ETA: 0s - loss: 0.5717 - accuracy: 0.7125
Epoch 34: val_loss improved from 0.56820 to 0.56814, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5703 - accuracy: 0.7138 - val_loss: 0.5681 - val_accuracy: 0.7158
Epoch 35/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.5718 - accuracy: 0.7120
Epoch 35: val_loss did not improve from 0.56814
682/682 [==============================] - 2s 2ms/step - loss: 0.5702 - accuracy: 0.7135 - val_loss: 0.5683 - val_accuracy: 0.7164
Epoch 36/60
678/682 [============================&gt;.] - ETA: 0s - loss: 0.5704 - accuracy: 0.7139
Epoch 36: val_loss improved from 0.56814 to 0.56811, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5702 - accuracy: 0.7141 - val_loss: 0.5681 - val_accuracy: 0.7158
Epoch 37/60
653/682 [===========================&gt;..] - ETA: 0s - loss: 0.5704 - accuracy: 0.7139
Epoch 37: val_loss improved from 0.56811 to 0.56811, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5700 - accuracy: 0.7138 - val_loss: 0.5681 - val_accuracy: 0.7158
Epoch 38/60
674/682 [============================&gt;.] - ETA: 0s - loss: 0.5699 - accuracy: 0.7142
Epoch 38: val_loss improved from 0.56811 to 0.56806, saving model to best_drop.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.5700 - accuracy: 0.7139 - val_loss: 0.5681 - val_accuracy: 0.7158
Epoch 39/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.5697 - accuracy: 0.7141
Epoch 39: val_loss did not improve from 0.56806
682/682 [==============================] - 1s 2ms/step - loss: 0.5699 - accuracy: 0.7139 - val_loss: 0.5681 - val_accuracy: 0.7158
Epoch 40/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.5710 - accuracy: 0.7124
Epoch 40: val_loss improved from 0.56806 to 0.56803, saving model to best_drop.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.5699 - accuracy: 0.7135 - val_loss: 0.5680 - val_accuracy: 0.7170
Epoch 41/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.5697 - accuracy: 0.7141
Epoch 41: val_loss improved from 0.56803 to 0.56794, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5698 - accuracy: 0.7139 - val_loss: 0.5679 - val_accuracy: 0.7158
Epoch 42/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.5702 - accuracy: 0.7138
Epoch 42: val_loss did not improve from 0.56794
682/682 [==============================] - 2s 2ms/step - loss: 0.5697 - accuracy: 0.7141 - val_loss: 0.5680 - val_accuracy: 0.7158
Epoch 43/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.5672 - accuracy: 0.7167
Epoch 43: val_loss did not improve from 0.56794
682/682 [==============================] - 2s 2ms/step - loss: 0.5697 - accuracy: 0.7141 - val_loss: 0.5682 - val_accuracy: 0.7158
Epoch 44/60
650/682 [===========================&gt;..] - ETA: 0s - loss: 0.5704 - accuracy: 0.7131
Epoch 44: val_loss did not improve from 0.56794
682/682 [==============================] - 2s 2ms/step - loss: 0.5696 - accuracy: 0.7144 - val_loss: 0.5680 - val_accuracy: 0.7158
Epoch 45/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.5680 - accuracy: 0.7158
Epoch 45: val_loss improved from 0.56794 to 0.56791, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5696 - accuracy: 0.7144 - val_loss: 0.5679 - val_accuracy: 0.7164
Epoch 46/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.5681 - accuracy: 0.7154
Epoch 46: val_loss improved from 0.56791 to 0.56782, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5696 - accuracy: 0.7138 - val_loss: 0.5678 - val_accuracy: 0.7164
Epoch 47/60
682/682 [==============================] - ETA: 0s - loss: 0.5695 - accuracy: 0.7136
Epoch 47: val_loss improved from 0.56782 to 0.56781, saving model to best_drop.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.5695 - accuracy: 0.7136 - val_loss: 0.5678 - val_accuracy: 0.7158
Epoch 48/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.5692 - accuracy: 0.7153
Epoch 48: val_loss did not improve from 0.56781
682/682 [==============================] - 2s 2ms/step - loss: 0.5694 - accuracy: 0.7147 - val_loss: 0.5680 - val_accuracy: 0.7158
Epoch 49/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.5690 - accuracy: 0.7142
Epoch 49: val_loss did not improve from 0.56781
682/682 [==============================] - 1s 2ms/step - loss: 0.5694 - accuracy: 0.7141 - val_loss: 0.5681 - val_accuracy: 0.7158
Epoch 50/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.5694 - accuracy: 0.7152
Epoch 50: val_loss improved from 0.56781 to 0.56778, saving model to best_drop.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.5694 - accuracy: 0.7148 - val_loss: 0.5678 - val_accuracy: 0.7170
Epoch 51/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.5678 - accuracy: 0.7150
Epoch 51: val_loss did not improve from 0.56778
682/682 [==============================] - 1s 2ms/step - loss: 0.5693 - accuracy: 0.7145 - val_loss: 0.5679 - val_accuracy: 0.7164
Epoch 52/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.5681 - accuracy: 0.7160
Epoch 52: val_loss did not improve from 0.56778
682/682 [==============================] - 2s 2ms/step - loss: 0.5694 - accuracy: 0.7144 - val_loss: 0.5679 - val_accuracy: 0.7158
Epoch 53/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.5697 - accuracy: 0.7146
Epoch 53: val_loss improved from 0.56778 to 0.56772, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5694 - accuracy: 0.7148 - val_loss: 0.5677 - val_accuracy: 0.7158
Epoch 54/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.5668 - accuracy: 0.7173
Epoch 54: val_loss did not improve from 0.56772
682/682 [==============================] - 2s 2ms/step - loss: 0.5694 - accuracy: 0.7148 - val_loss: 0.5679 - val_accuracy: 0.7164
Epoch 55/60
662/682 [============================&gt;.] - ETA: 0s - loss: 0.5679 - accuracy: 0.7156
Epoch 55: val_loss improved from 0.56772 to 0.56769, saving model to best_drop.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.5693 - accuracy: 0.7151 - val_loss: 0.5677 - val_accuracy: 0.7158
Epoch 56/60
653/682 [===========================&gt;..] - ETA: 0s - loss: 0.5686 - accuracy: 0.7159
Epoch 56: val_loss did not improve from 0.56769
682/682 [==============================] - 2s 2ms/step - loss: 0.5693 - accuracy: 0.7144 - val_loss: 0.5677 - val_accuracy: 0.7158
Epoch 57/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.5698 - accuracy: 0.7148
Epoch 57: val_loss did not improve from 0.56769
682/682 [==============================] - 2s 2ms/step - loss: 0.5690 - accuracy: 0.7150 - val_loss: 0.5680 - val_accuracy: 0.7164
Epoch 58/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.5690 - accuracy: 0.7149
Epoch 58: val_loss improved from 0.56769 to 0.56767, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5692 - accuracy: 0.7148 - val_loss: 0.5677 - val_accuracy: 0.7158
Epoch 59/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.5695 - accuracy: 0.7136
Epoch 59: val_loss did not improve from 0.56767
682/682 [==============================] - 2s 2ms/step - loss: 0.5692 - accuracy: 0.7142 - val_loss: 0.5679 - val_accuracy: 0.7164
Epoch 60/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.5684 - accuracy: 0.7162
Epoch 60: val_loss did not improve from 0.56767
682/682 [==============================] - 2s 2ms/step - loss: 0.5693 - accuracy: 0.7148 - val_loss: 0.5678 - val_accuracy: 0.7152
54/54 [==============================] - 0s 2ms/step - loss: 0.5677 - accuracy: 0.7158
Drop Feature blue-collar
data frame x has shape: (8516, 7)
['education', 'pdays', 'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx', 'euribor3m']
Epoch 1/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.6868 - accuracy: 0.5790
Epoch 1: val_loss improved from inf to 0.64977, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6858 - accuracy: 0.5821 - val_loss: 0.6498 - val_accuracy: 0.7111
Epoch 2/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.6298 - accuracy: 0.7115
Epoch 2: val_loss improved from 0.64977 to 0.61118, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6298 - accuracy: 0.7116 - val_loss: 0.6112 - val_accuracy: 0.7129
Epoch 3/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.6020 - accuracy: 0.7131
Epoch 3: val_loss improved from 0.61118 to 0.59191, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.6025 - accuracy: 0.7114 - val_loss: 0.5919 - val_accuracy: 0.7123
Epoch 4/60
654/682 [===========================&gt;..] - ETA: 0s - loss: 0.5894 - accuracy: 0.7119
Epoch 4: val_loss improved from 0.59191 to 0.58280, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5890 - accuracy: 0.7122 - val_loss: 0.5828 - val_accuracy: 0.7152
Epoch 5/60
654/682 [===========================&gt;..] - ETA: 0s - loss: 0.5835 - accuracy: 0.7098
Epoch 5: val_loss improved from 0.58280 to 0.57799, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5824 - accuracy: 0.7117 - val_loss: 0.5780 - val_accuracy: 0.7152
Epoch 6/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.5789 - accuracy: 0.7122
Epoch 6: val_loss improved from 0.57799 to 0.57507, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5788 - accuracy: 0.7125 - val_loss: 0.5751 - val_accuracy: 0.7134
Epoch 7/60
679/682 [============================&gt;.] - ETA: 0s - loss: 0.5763 - accuracy: 0.7131
Epoch 7: val_loss improved from 0.57507 to 0.57359, saving model to best_drop.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.5768 - accuracy: 0.7126 - val_loss: 0.5736 - val_accuracy: 0.7152
Epoch 8/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.5755 - accuracy: 0.7123
Epoch 8: val_loss improved from 0.57359 to 0.57264, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5755 - accuracy: 0.7123 - val_loss: 0.5726 - val_accuracy: 0.7152
Epoch 9/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.5746 - accuracy: 0.7131
Epoch 9: val_loss improved from 0.57264 to 0.57205, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5747 - accuracy: 0.7128 - val_loss: 0.5720 - val_accuracy: 0.7152
Epoch 10/60
665/682 [============================&gt;.] - ETA: 0s - loss: 0.5738 - accuracy: 0.7125
Epoch 10: val_loss improved from 0.57205 to 0.57172, saving model to best_drop.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.5741 - accuracy: 0.7128 - val_loss: 0.5717 - val_accuracy: 0.7152
Epoch 11/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.5740 - accuracy: 0.7126
Epoch 11: val_loss improved from 0.57172 to 0.57141, saving model to best_drop.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.5736 - accuracy: 0.7129 - val_loss: 0.5714 - val_accuracy: 0.7152
Epoch 12/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.5738 - accuracy: 0.7126
Epoch 12: val_loss improved from 0.57141 to 0.57128, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5733 - accuracy: 0.7129 - val_loss: 0.5713 - val_accuracy: 0.7152
Epoch 13/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.5728 - accuracy: 0.7134
Epoch 13: val_loss improved from 0.57128 to 0.57098, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5729 - accuracy: 0.7129 - val_loss: 0.5710 - val_accuracy: 0.7152
Epoch 14/60
671/682 [============================&gt;.] - ETA: 0s - loss: 0.5721 - accuracy: 0.7134
Epoch 14: val_loss improved from 0.57098 to 0.57076, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5728 - accuracy: 0.7128 - val_loss: 0.5708 - val_accuracy: 0.7152
Epoch 15/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.5720 - accuracy: 0.7138
Epoch 15: val_loss improved from 0.57076 to 0.57061, saving model to best_drop.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.5725 - accuracy: 0.7129 - val_loss: 0.5706 - val_accuracy: 0.7152
Epoch 16/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.5721 - accuracy: 0.7131
Epoch 16: val_loss improved from 0.57061 to 0.57048, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5723 - accuracy: 0.7129 - val_loss: 0.5705 - val_accuracy: 0.7152
Epoch 17/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.5722 - accuracy: 0.7131
Epoch 17: val_loss improved from 0.57048 to 0.57036, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5721 - accuracy: 0.7129 - val_loss: 0.5704 - val_accuracy: 0.7152
Epoch 18/60
680/682 [============================&gt;.] - ETA: 0s - loss: 0.5721 - accuracy: 0.7126
Epoch 18: val_loss improved from 0.57036 to 0.57034, saving model to best_drop.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.5719 - accuracy: 0.7129 - val_loss: 0.5703 - val_accuracy: 0.7152
Epoch 19/60
661/682 [============================&gt;.] - ETA: 0s - loss: 0.5727 - accuracy: 0.7120
Epoch 19: val_loss improved from 0.57034 to 0.57017, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5716 - accuracy: 0.7129 - val_loss: 0.5702 - val_accuracy: 0.7152
Epoch 20/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.5710 - accuracy: 0.7135
Epoch 20: val_loss improved from 0.57017 to 0.57007, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5714 - accuracy: 0.7129 - val_loss: 0.5701 - val_accuracy: 0.7152
Epoch 21/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.5706 - accuracy: 0.7131
Epoch 21: val_loss did not improve from 0.57007
682/682 [==============================] - 2s 2ms/step - loss: 0.5713 - accuracy: 0.7129 - val_loss: 0.5701 - val_accuracy: 0.7152
Epoch 22/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.5712 - accuracy: 0.7120
Epoch 22: val_loss improved from 0.57007 to 0.56996, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5712 - accuracy: 0.7129 - val_loss: 0.5700 - val_accuracy: 0.7152
Epoch 23/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.5706 - accuracy: 0.7134
Epoch 23: val_loss improved from 0.56996 to 0.56987, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5710 - accuracy: 0.7129 - val_loss: 0.5699 - val_accuracy: 0.7152
Epoch 24/60
682/682 [==============================] - ETA: 0s - loss: 0.5707 - accuracy: 0.7128
Epoch 24: val_loss did not improve from 0.56987
682/682 [==============================] - 2s 2ms/step - loss: 0.5707 - accuracy: 0.7128 - val_loss: 0.5699 - val_accuracy: 0.7152
Epoch 25/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.5702 - accuracy: 0.7137
Epoch 25: val_loss improved from 0.56987 to 0.56967, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5709 - accuracy: 0.7129 - val_loss: 0.5697 - val_accuracy: 0.7152
Epoch 26/60
664/682 [============================&gt;.] - ETA: 0s - loss: 0.5699 - accuracy: 0.7136
Epoch 26: val_loss improved from 0.56967 to 0.56967, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5707 - accuracy: 0.7129 - val_loss: 0.5697 - val_accuracy: 0.7152
Epoch 27/60
676/682 [============================&gt;.] - ETA: 0s - loss: 0.5702 - accuracy: 0.7130
Epoch 27: val_loss did not improve from 0.56967
682/682 [==============================] - 2s 2ms/step - loss: 0.5704 - accuracy: 0.7129 - val_loss: 0.5700 - val_accuracy: 0.7152
Epoch 28/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.5705 - accuracy: 0.7129
Epoch 28: val_loss improved from 0.56967 to 0.56958, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5705 - accuracy: 0.7129 - val_loss: 0.5696 - val_accuracy: 0.7152
Epoch 29/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.5699 - accuracy: 0.7132
Epoch 29: val_loss improved from 0.56958 to 0.56944, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5704 - accuracy: 0.7129 - val_loss: 0.5694 - val_accuracy: 0.7152
Epoch 30/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.5699 - accuracy: 0.7128
Epoch 30: val_loss did not improve from 0.56944
682/682 [==============================] - 2s 2ms/step - loss: 0.5702 - accuracy: 0.7130 - val_loss: 0.5695 - val_accuracy: 0.7152
Epoch 31/60
653/682 [===========================&gt;..] - ETA: 0s - loss: 0.5709 - accuracy: 0.7121
Epoch 31: val_loss improved from 0.56944 to 0.56943, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5702 - accuracy: 0.7129 - val_loss: 0.5694 - val_accuracy: 0.7158
Epoch 32/60
682/682 [==============================] - ETA: 0s - loss: 0.5702 - accuracy: 0.7129
Epoch 32: val_loss improved from 0.56943 to 0.56934, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5702 - accuracy: 0.7129 - val_loss: 0.5693 - val_accuracy: 0.7158
Epoch 33/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.5709 - accuracy: 0.7120
Epoch 33: val_loss did not improve from 0.56934
682/682 [==============================] - 2s 2ms/step - loss: 0.5701 - accuracy: 0.7129 - val_loss: 0.5694 - val_accuracy: 0.7158
Epoch 34/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.5706 - accuracy: 0.7126
Epoch 34: val_loss did not improve from 0.56934
682/682 [==============================] - 2s 2ms/step - loss: 0.5700 - accuracy: 0.7132 - val_loss: 0.5694 - val_accuracy: 0.7158
Epoch 35/60
675/682 [============================&gt;.] - ETA: 0s - loss: 0.5696 - accuracy: 0.7130
Epoch 35: val_loss did not improve from 0.56934
682/682 [==============================] - 2s 2ms/step - loss: 0.5700 - accuracy: 0.7130 - val_loss: 0.5694 - val_accuracy: 0.7158
Epoch 36/60
655/682 [===========================&gt;..] - ETA: 0s - loss: 0.5699 - accuracy: 0.7133
Epoch 36: val_loss improved from 0.56934 to 0.56915, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5699 - accuracy: 0.7132 - val_loss: 0.5691 - val_accuracy: 0.7158
Epoch 37/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.5705 - accuracy: 0.7123
Epoch 37: val_loss did not improve from 0.56915
682/682 [==============================] - 1s 2ms/step - loss: 0.5696 - accuracy: 0.7130 - val_loss: 0.5698 - val_accuracy: 0.7158
Epoch 38/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.5698 - accuracy: 0.7132
Epoch 38: val_loss improved from 0.56915 to 0.56908, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5698 - accuracy: 0.7132 - val_loss: 0.5691 - val_accuracy: 0.7158
Epoch 39/60
666/682 [============================&gt;.] - ETA: 0s - loss: 0.5692 - accuracy: 0.7129
Epoch 39: val_loss did not improve from 0.56908
682/682 [==============================] - 1s 2ms/step - loss: 0.5697 - accuracy: 0.7130 - val_loss: 0.5692 - val_accuracy: 0.7158
Epoch 40/60
657/682 [===========================&gt;..] - ETA: 0s - loss: 0.5700 - accuracy: 0.7129
Epoch 40: val_loss improved from 0.56908 to 0.56907, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5697 - accuracy: 0.7132 - val_loss: 0.5691 - val_accuracy: 0.7158
Epoch 41/60
672/682 [============================&gt;.] - ETA: 0s - loss: 0.5706 - accuracy: 0.7131
Epoch 41: val_loss improved from 0.56907 to 0.56900, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5697 - accuracy: 0.7133 - val_loss: 0.5690 - val_accuracy: 0.7158
Epoch 42/60
656/682 [===========================&gt;..] - ETA: 0s - loss: 0.5703 - accuracy: 0.7133
Epoch 42: val_loss improved from 0.56900 to 0.56894, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5696 - accuracy: 0.7133 - val_loss: 0.5689 - val_accuracy: 0.7158
Epoch 43/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.5695 - accuracy: 0.7134
Epoch 43: val_loss improved from 0.56894 to 0.56893, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5696 - accuracy: 0.7133 - val_loss: 0.5689 - val_accuracy: 0.7158
Epoch 44/60
654/682 [===========================&gt;..] - ETA: 0s - loss: 0.5687 - accuracy: 0.7135
Epoch 44: val_loss improved from 0.56893 to 0.56892, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5695 - accuracy: 0.7130 - val_loss: 0.5689 - val_accuracy: 0.7158
Epoch 45/60
677/682 [============================&gt;.] - ETA: 0s - loss: 0.5694 - accuracy: 0.7129
Epoch 45: val_loss did not improve from 0.56892
682/682 [==============================] - 2s 2ms/step - loss: 0.5694 - accuracy: 0.7132 - val_loss: 0.5694 - val_accuracy: 0.7158
Epoch 46/60
673/682 [============================&gt;.] - ETA: 0s - loss: 0.5687 - accuracy: 0.7135
Epoch 46: val_loss improved from 0.56892 to 0.56887, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5695 - accuracy: 0.7132 - val_loss: 0.5689 - val_accuracy: 0.7158
Epoch 47/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.5683 - accuracy: 0.7140
Epoch 47: val_loss did not improve from 0.56887
682/682 [==============================] - 2s 2ms/step - loss: 0.5693 - accuracy: 0.7133 - val_loss: 0.5689 - val_accuracy: 0.7158
Epoch 48/60
681/682 [============================&gt;.] - ETA: 0s - loss: 0.5693 - accuracy: 0.7137
Epoch 48: val_loss did not improve from 0.56887
682/682 [==============================] - 2s 2ms/step - loss: 0.5694 - accuracy: 0.7135 - val_loss: 0.5689 - val_accuracy: 0.7158
Epoch 49/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.5691 - accuracy: 0.7137
Epoch 49: val_loss improved from 0.56887 to 0.56880, saving model to best_drop.hdf5
682/682 [==============================] - 2s 2ms/step - loss: 0.5694 - accuracy: 0.7133 - val_loss: 0.5688 - val_accuracy: 0.7158
Epoch 50/60
660/682 [============================&gt;.] - ETA: 0s - loss: 0.5687 - accuracy: 0.7138
Epoch 50: val_loss did not improve from 0.56880
682/682 [==============================] - 2s 2ms/step - loss: 0.5692 - accuracy: 0.7133 - val_loss: 0.5689 - val_accuracy: 0.7158
Epoch 51/60
667/682 [============================&gt;.] - ETA: 0s - loss: 0.5702 - accuracy: 0.7129
Epoch 51: val_loss improved from 0.56880 to 0.56879, saving model to best_drop.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.5694 - accuracy: 0.7133 - val_loss: 0.5688 - val_accuracy: 0.7158
Epoch 52/60
668/682 [============================&gt;.] - ETA: 0s - loss: 0.5703 - accuracy: 0.7118
Epoch 52: val_loss improved from 0.56879 to 0.56877, saving model to best_drop.hdf5
682/682 [==============================] - 1s 2ms/step - loss: 0.5692 - accuracy: 0.7133 - val_loss: 0.5688 - val_accuracy: 0.7158
Epoch 53/60
659/682 [===========================&gt;..] - ETA: 0s - loss: 0.5686 - accuracy: 0.7141
Epoch 53: val_loss did not improve from 0.56877
682/682 [==============================] - 1s 2ms/step - loss: 0.5693 - accuracy: 0.7133 - val_loss: 0.5688 - val_accuracy: 0.7158
Epoch 54/60
658/682 [===========================&gt;..] - ETA: 0s - loss: 0.5694 - accuracy: 0.7135
Epoch 54: val_loss did not improve from 0.56877
682/682 [==============================] - 2s 2ms/step - loss: 0.5692 - accuracy: 0.7135 - val_loss: 0.5688 - val_accuracy: 0.7158
Epoch 55/60
669/682 [============================&gt;.] - ETA: 0s - loss: 0.5696 - accuracy: 0.7126
Epoch 55: val_loss did not improve from 0.56877
682/682 [==============================] - 2s 2ms/step - loss: 0.5690 - accuracy: 0.7133 - val_loss: 0.5691 - val_accuracy: 0.7164
Epoch 56/60
 31/682 [&gt;.............................] - ETA: 1s - loss: 0.6039 - accuracy: 0.6645</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=7501bfba-da07-4d91-83f7-8567ea75dfd2">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [15]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">drop_result</span><span class="o">.</span><span class="n">head</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[15]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>&lt;bound method NDFrame.head of           Features_Removed  Validation_Accuracy
0              Dropped age            72.108042
1       Dropped unemployed            72.166765
2        Dropped housemaid            72.108042
3       Dropped management            72.108042
4       Dropped technician            72.108042
5             Dropped loan            71.931887
6     Dropped entrepreneur            72.049326
7    Dropped self-employed            72.108042
8         Dropped divorced            71.990603
9          Dropped housing            72.049326
10        Dropped services            72.284204
11         Dropped retired            71.579564
12         Dropped student            71.755725
13        Dropped campaign            71.462125
14         Dropped married            71.579564
15          Dropped admin.            71.462125
16          Dropped single            71.168524
17   Dropped cons.conf.idx            71.579564
18     Dropped blue-collar            71.579564
19       Dropped education            71.520847
20  Dropped cons.price.idx            71.520847
21        Dropped poutcome            71.520847
22           Dropped pdays            71.520847
23        Dropped previous            71.579564
24       Dropped euribor3m            71.579564
25    Dropped emp.var.rate            50.088078&gt;</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=6e61d0cd-d7de-463e-b5bd-f1b7705844c3">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="4.6-Graph-Results-of-Dropping-Certain-Features-from-the-Model">4.6 Graph Results of Dropping Certain Features from the Model<a class="anchor-link" href="#4.6-Graph-Results-of-Dropping-Certain-Features-from-the-Model">¶</a></h4>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=ae99a790-0be3-4489-b0b3-d2fbbd5d5b82">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [16]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Plotting the accuracy by feature bar graph</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">)),</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">drop_result</span><span class="p">[</span><span class="s1">'Features_Removed'</span><span class="p">],</span> <span class="n">drop_result</span><span class="p">[</span><span class="s1">'Validation_Accuracy'</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Feature'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Validation Accuracy (%)'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Validation Accuracy After Dropping Specific Features'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="s1">'vertical'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,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"/>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=ebd797ca-2d94-4d11-8e3e-32efce442c55">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="Conclusion">Conclusion<a class="anchor-link" href="#Conclusion">¶</a></h4><p>Looking at the results of dropping specific columns it does not appear that there were any particular columns that were adding severe noise or bias to the model because little changed in the overall accuracy of the model as the columns with low effect on the model's accuracy were removed from the dataset. Accuracy for the most part stayed about the same as the initial model built in Phase 3 that had about a 72% validation accuracy. I thought there would have been a bigger difference once columns like age, the euribor and previous contacts features were removed. This is because the age was not a good predictor for the output variable with only 44% accuracy and the euribor and previous features had higher accuracies with 71% and 59% respectively so I thought accuracy would have dropped more with their removal. In the end there was little change from iteration to iteration of the loop until the emp.var.rate feature was dropped which is when the model had no input features avaliable to train from so the model simply had to guess and ended up with a 50% accuracy rate because of basic probability. I suspect the minimal impact is because as seen earlier in Phase 1 none of these features had a strong correlation with the output variable so it does not appear that any of them have a strong impact on the prediction model. I believe that if there were features with a stronger correlation to the output variable then this model would potentially perform better overall and we would have seen a greater change in this phase. At the same time this was a good exercise because I'd questioned in Phase 1 if I should have removed the poutcome and pdays features from the model to prevent noise and looking at the results here that removing them would have not drastically changed my overall results as initially thought.</p>
</div>
</div>
</div>
</div>
</main>
</body>
</html>
